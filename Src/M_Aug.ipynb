{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ughSNYApZinJ"
      },
      "source": [
        "#**M_aug**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Table of Contents**\n",
        "\n",
        "1. [Overview](#Overview)\n",
        "2. [Setup & Imports](#Setup-&-Imports)\n",
        "3. [Data Loading and Preprocessing](#data-loading-and-preprocessing)\n",
        "4. [Filter for Java Samples](#)\n",
        "5. [Translating the Javacode samples](#Translating-the-Javacode-samples)\n",
        "6. [Tokenization](#Tokenization)\n",
        "7. [Training and Fine-Tuning](#Training-and-Fine-Tuning)\n",
        "8. [Evaluation](#Evaluation)"
      ],
      "metadata": {
        "id": "m1KLwAPaUc8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This notebook features the fine-tuning of CodeBERT and the translation of Java code samples to C++. The model is designed to classify code changes into two labels:\n",
        "\n",
        "0 – High quality, no further review required\n",
        "\n",
        "1 – Low quality, requires additional review\n",
        "\n"
      ],
      "metadata": {
        "id": "VY98hsoGUXmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install datasets\n",
        "!pip install -U transformers\n",
        "!pip install --upgrade transformers\n",
        "!pip install datasets --upgrade\n",
        "!pip install openai\n",
        "!pip install --upgrade openai\n",
        "!pip install azure-core"
      ],
      "metadata": {
        "id": "b_FomjTeAT_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIwiAlhIvUYD"
      },
      "source": [
        "## **Setup & Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ3XCxeNYRUw"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments , EarlyStoppingCallback\n",
        "from datasets import load_dataset , concatenate_datasets, DatasetDict , Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score , confusion_matrix , classification_report\n",
        "from sklearn.utils import resample\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import random\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "import os\n",
        "from openai import AzureOpenAI\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuN1zm57YXRD"
      },
      "source": [
        "## **Data Loading and Preprocessing**\n",
        "\n",
        "- Load the dataset from Hugging Face.\n",
        "- Filter the dataset to retain only Java samples.\n",
        "- Combine the original code (`oldf`) and the corresponding (`patch`) into a unified field, `code_change`, representing the complete code change.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwifu5ruYZzF"
      },
      "outputs": [],
      "source": [
        "# Load dataset from Hugging Face\n",
        "train_ds = load_dataset(\"fasterinnerlooper/codereviewer\", \"train_quality\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0sf9wGovjRv"
      },
      "outputs": [],
      "source": [
        "# Remove unnecessary columns not needed for training or evaluation\n",
        "train_ds = train_ds.remove_columns(['msg','id','idx','proj'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iljtYhc_vm81"
      },
      "source": [
        "**Filter for Java Samples**\n",
        "To build a language-specific dataset for Java, we apply two filtering strategies:\n",
        "\n",
        "- For `y = 0`: Use the existing `lang == 'java'` metadata.\n",
        "- For `y = 1`: Use a heuristic function (`is_java`) that detects Java -like syntax based on keyword patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX1mnZrLYfdu"
      },
      "outputs": [],
      "source": [
        "# Filter the datasets to keep only C++ samples\n",
        "train_ds_java = train_ds.filter(lambda example: example['lang'] == 'java')\n",
        "train_ds_null_lang = train_ds.filter(lambda ex: ex[\"lang\"] is None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME5Yr7-CY5Wt"
      },
      "outputs": [],
      "source": [
        "#special Java kewords\n",
        "java_keywords = [ \"System.out.println\",\n",
        "    \"implements\",\n",
        "    \"extends\",\n",
        "    \"package\",\n",
        "    \"import java.\",\n",
        "    \"@Override\",\n",
        "    \"throws\",\n",
        "    \"new Scanner(\",\n",
        "    \"public static void main(String[] args)\"\n",
        "    \"interface\",\n",
        "    \"enum\",\n",
        "    \"@FunctionalInterface\",\n",
        "    \"@SuppressWarnings\",\n",
        "    \"synchronized\",\n",
        "    \"final\",\n",
        "    \"transient\",\n",
        "    \"volatile\",\n",
        "    \"strictfp\",\n",
        "    \"String\",\n",
        "    \"Integer\",\n",
        "    \"Double\",\n",
        "    \"Boolean\",\n",
        "    \"ArrayList\",\n",
        "    \"HashMap\",\n",
        "    \"LinkedList\",\n",
        "    \"HashSet\",\n",
        "    \"TreeMap\",\n",
        "    \"java.util.\",\n",
        "    \"java.io.\",\n",
        "    \"java.lang.\",\n",
        "    \"java.time.\",\n",
        "    \"Scanner\",\n",
        "    \"BufferedReader\",\n",
        "    \"InputStreamReader\",\n",
        "    \"FileWriter\",\n",
        " ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sfBeJqqYtVX"
      },
      "outputs": [],
      "source": [
        "def is_java(code: str) -> bool:\n",
        "\n",
        "    \"\"\"\n",
        "    Analyze the source code to determine if it resembles java code.\n",
        "    This function evaluates the provided string by searching for a predefined set\n",
        "    of java keywords.\n",
        "    If at least two such keywords are present, the code is considered\n",
        "    to exhibit characteristics of java.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    code : str\n",
        "        The input source code as a single string.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if two or more java keywords are found; otherwise, False.\n",
        "    \"\"\"\n",
        "\n",
        "    count = sum(1 for kw in java_keywords if kw in code)\n",
        "    return count >= 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "java_labeled_samplesY1 = train_ds_null_lang.filter(\n",
        "    lambda ex: ex['y'] == 1 and is_java(ex['oldf'])\n",
        ")"
      ],
      "metadata": {
        "id": "cTzCplRD4Ti3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZpCxeDOwJ4F",
        "outputId": "6b9c2124-32fa-40d9-ba85-1f2e0e03ee10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y=1 20343\n",
            "y=0 18988\n",
            "y=1 2000\n",
            "y=0 2000\n",
            "Counter({0: 2000, 1: 2000})\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# === Java Training Dataset Preparation ===\n",
        "\n",
        "# Extract the 'train' splits from both labeled datasets:\n",
        "# - java_train_0: Java samples with label y=0 (high quality)\n",
        "# - java_train_1: Java samples with label y=1 (low quality)\n",
        "\n",
        "\n",
        "java_train_0 = train_ds_java['train']\n",
        "java_train_1 = java_labeled_samplesY1['train']\n",
        "print(\"y=1\",len(java_train_1))\n",
        "print(\"y=0\",len(java_train_0))\n",
        "\n",
        "\n",
        "# Select up to 600 samples from each label to ensure a balanced dataset.\n",
        "java_train_0_sampled = java_train_0.select(range(min(600, len(java_train_0))))\n",
        "java_train_1_sampled = java_train_1.select(range(min(600, len(java_train_1))))\n",
        "print(\"y=1\",len(java_train_1_sampled))\n",
        "print(\"y=0\",len(java_train_0_sampled))\n",
        "\n",
        "# Combine both subsets into a single dataset\n",
        "combined_java_train = concatenate_datasets([java_train_0_sampled, java_train_1_sampled])\n",
        "\n",
        "# Shuffle the combined dataset to randomize the order of examples.\n",
        "combined_java_train = combined_java_train.shuffle(seed=42)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "print(Counter(combined_java_train['y']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2G2ZtqTwcFe"
      },
      "outputs": [],
      "source": [
        "train_dataset_java = DatasetDict({\n",
        "    'train': combined_java_train\n",
        "})\n",
        "\n",
        "\n",
        "y1_examples = train_dataset_java['train'].filter(lambda ex: ex['y'] == 1)\n",
        "print(f\"Number of examples with y=1: {len(y1_examples)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjZGiesFwrNd"
      },
      "source": [
        "### **Extract Diff with Context**\n",
        "\n",
        "To capture code changes with surrounding context, we extract:\n",
        "\n",
        "- Modified lines from `patch` (additions and deletions)  \n",
        "- `context_lines` before and after each change  \n",
        "\n",
        "Lines are annotated as:\n",
        "\n",
        "- `[ADD]` for added lines  \n",
        "- `[DEL]` for deleted lines  \n",
        "\n",
        "Metadata (e.g., `+++`, `---`, `@@`) is removed to keep the output clean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn111V5Dwpg5"
      },
      "outputs": [],
      "source": [
        "def extract_diff_with_context(old_code: str, patch: str, context_lines: int ) -> str:\n",
        "\n",
        "    \"\"\"\n",
        "    Extracts code changes from a unified diff (`patch`) along with surrounding context lines.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    old_code : str\n",
        "        Original code before the changes (not used in processing, but retained for interface consistency).\n",
        "\n",
        "    patch : str\n",
        "        Unified diff string representing the code changes.\n",
        "\n",
        "    context_lines : int, optional (default=5)\n",
        "        Number of lines to include before and after each change for contextual understanding.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    str\n",
        "        A formatted string containing:\n",
        "        - Context lines around each change\n",
        "        - `[ADD]`-prefixed lines for additions\n",
        "        - `[DEL]`-prefixed lines for deletions\n",
        "        - Excludes non-informative diff metadata (e.g., '+++', '---', '@@')\n",
        "\n",
        "    Example output:\n",
        "        some unchanged line\n",
        "        [DEL] removed line\n",
        "        [ADD] added line\n",
        "        another unchanged line\n",
        "    \"\"\"\n",
        "\n",
        "    patch_lines = patch.splitlines()\n",
        "    context_indices = set()\n",
        "\n",
        "    for i, line in enumerate(patch_lines):\n",
        "        if line.startswith(('+', '-')) and not line.startswith(('+++', '---')):\n",
        "            start = max(0, i - context_lines)\n",
        "            end = min(len(patch_lines), i + context_lines + 1)\n",
        "            context_indices.update(range(start, end))\n",
        "\n",
        "    extracted_lines = []\n",
        "    for i in sorted(context_indices):\n",
        "        line = patch_lines[i]\n",
        "        if line.startswith(('@@', '+++', '---')):\n",
        "            continue\n",
        "        if line.startswith('+') and not line.startswith('+++'):\n",
        "            line = '[ADD] ' + line[1:].strip()\n",
        "        elif line.startswith('-') and not line.startswith('---'):\n",
        "            line = '[DEL] ' + line[1:].strip()\n",
        "        extracted_lines.append(line)\n",
        "\n",
        "    return \"\\n\".join(extracted_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQfN8tY_wxJz"
      },
      "outputs": [],
      "source": [
        "def apply_extract_diff(example, context_lines=3):\n",
        "    # Extract diff with context for each example row\n",
        "    return {\n",
        "        \"extracted_diff\": extract_diff_with_context(\n",
        "            example[\"oldf\"], example[\"patch\"], context_lines=context_lines\n",
        "        )\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVQgvMsZwyp5"
      },
      "outputs": [],
      "source": [
        "def print_diff_examples(dataset, num_examples, context_lines):\n",
        "\n",
        "    \"\"\"\n",
        "    Print a few examples from the dataset showing the original code, the patch,\n",
        "    and the extracted diff with context.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "        dataset: Each item should have 'oldf' and 'patch'.\n",
        "        num_examples (int): Number of examples to print.\n",
        "        context_lines (int): Number of context lines to include.\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        example = dataset[i]\n",
        "        old_code = example.get('oldf', '')\n",
        "        patch = example.get('patch', '')\n",
        "        extracted = extract_diff_with_context(old_code, patch, context_lines=context_lines)\n",
        "\n",
        "        print(f\"Example {i + 1}\")\n",
        "        print(extracted)\n",
        "        print(\"=\" * 60, \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srO8dJHdw00q",
        "outputId": "38efb762-8af6-406f-f019-2cfd6022681c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1\n",
            "           + \" for defining a delegate runner to be used.\");\n",
            "     }\n",
            " \n",
            "[DEL] if (pluginClassLoaders != null && !pluginClassLoaders.isEmpty()) {\n",
            "[DEL] builders.add(0, new IsolatedClassLoaderExtensionsManagerConfigurationBuilder(pluginClassLoaders));\n",
            "[ADD] if (artifactClassLoaderHolderAdapter != null\n",
            "[ADD] && !artifactClassLoaderHolderAdapter.getPluginsArtifactClassLoaders().isEmpty()) {\n",
            "[ADD] builders.add(0, new IsolatedClassLoaderExtensionsManagerConfigurationBuilder(artifactClassLoaderHolderAdapter\n",
            "[ADD] .getPluginsArtifactClassLoaders()));\n",
            "     }\n",
            "   }\n",
            "[DEL] \n",
            " }\n",
            "============================================================ \n",
            "\n",
            "Example 2\n",
            "  */\n",
            " public class SpillableMapBasedFileSystemView extends HoodieTableFileSystemView {\n",
            " \n",
            "[DEL] private static final Logger LOG = LogManager.getLogger(SpillableMapBasedFileSystemView.class);\n",
            "[ADD] private static final Logger LOG = LoggerFactory.getLogger(SpillableMapBasedFileSystemView.class);\n",
            " \n",
            "   private final long maxMemoryForFileGroupMap;\n",
            "   private final long maxMemoryForPendingCompaction;\n",
            "============================================================ \n",
            "\n",
            "Example 3\n",
            " import java.util.Map;\n",
            " import java.util.SortedSet;\n",
            " \n",
            "[ADD] import org.osgi.util.promise.Promise;\n",
            "[ADD] \n",
            " import aQute.bnd.osgi.Processor;\n",
            " import aQute.bnd.version.Version;\n",
            " \n",
            "============================================================ \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_diff_examples(train_dataset_java['train'], 3, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j05qfKmxHnu"
      },
      "outputs": [],
      "source": [
        "train_ds_java = train_dataset_java.map(\n",
        "    apply_extract_diff,\n",
        "    batched=False,\n",
        "    remove_columns=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiJJBB6Ixdvl"
      },
      "outputs": [],
      "source": [
        "def count_words_batch(examples):\n",
        "    return {'word_count': [len(text.split()) for text in examples['extracted_diff']]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RgYwNEWxfGw"
      },
      "outputs": [],
      "source": [
        "train_ds_java = train_ds_java.map(count_words_batch, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgQlSyN7xoWH"
      },
      "outputs": [],
      "source": [
        "def count_examples_within_word_limit(dataset_split, word_limit=512):\n",
        "    \"\"\"\n",
        "    Count how many examples in a dataset split have word counts within the given limit.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    dataset_split : Dataset\n",
        "        A Hugging Face Dataset split (e.g., train_ds_java['train']).\n",
        "        Assumes it has a 'word_count' field.\n",
        "\n",
        "    word_limit : int, optional (default=512)\n",
        "        Maximum number of words allowed to be counted as 'within limit'.\n",
        "\n",
        "    Prints:\n",
        "    -------\n",
        "    Number and percentage of examples with word counts <= word_limit.\n",
        "    \"\"\"\n",
        "    word_counts = dataset_split['word_count']\n",
        "    total = len(word_counts)\n",
        "    num_within_limit = sum(1 for wc in word_counts if wc <= word_limit)\n",
        "\n",
        "    print(f\"Examples with ≤ {word_limit} words: {num_within_limit} out of {total}\")\n",
        "    print(f\"Percentage: {num_within_limit / total * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4Y5uQj4xqRo",
        "outputId": "55099fa0-47b5-46fd-f74c-53e3bac6e9a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples with ≤ 512 words: 3924 out of 4000\n",
            "Percentage: 98.10%\n"
          ]
        }
      ],
      "source": [
        "count_examples_within_word_limit(train_ds_java['train'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LetHM9jfxwu3",
        "outputId": "fcb7a0f8-0045-420b-c9b1-d0edcdaa6d55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['           + \" for defining a delegate runner to be used.\");\\n     }\\n \\n[DEL] if (pluginClassLoaders != null && !pluginClassLoaders.isEmpty()) {\\n[DEL] builders.add(0, new IsolatedClassLoaderExtensionsManagerConfigurationBuilder(pluginClassLoaders));\\n[ADD] if (artifactClassLoaderHolderAdapter != null\\n[ADD] && !artifactClassLoaderHolderAdapter.getPluginsArtifactClassLoaders().isEmpty()) {\\n[ADD] builders.add(0, new IsolatedClassLoaderExtensionsManagerConfigurationBuilder(artifactClassLoaderHolderAdapter\\n[ADD] .getPluginsArtifactClassLoaders()));\\n     }\\n   }\\n[DEL] \\n }',\n",
              " '  */\\n public class SpillableMapBasedFileSystemView extends HoodieTableFileSystemView {\\n \\n[DEL] private static final Logger LOG = LogManager.getLogger(SpillableMapBasedFileSystemView.class);\\n[ADD] private static final Logger LOG = LoggerFactory.getLogger(SpillableMapBasedFileSystemView.class);\\n \\n   private final long maxMemoryForFileGroupMap;\\n   private final long maxMemoryForPendingCompaction;',\n",
              " ' import java.util.Map;\\n import java.util.SortedSet;\\n \\n[ADD] import org.osgi.util.promise.Promise;\\n[ADD] \\n import aQute.bnd.osgi.Processor;\\n import aQute.bnd.version.Version;\\n ',\n",
              " '             if (_networkName != null) {\\n                 netBuilder.append(\"<target dev=\\'\" + _networkName + \"\\'/>\\\\n\");\\n             }\\n[ADD] if (_mtu > 0 && _mtu <= 9000) {\\n[ADD] netBuilder.append(\"<mtu size=\\'\" + _mtu + \"\\'/>\\\\n\");\\n[ADD] }\\n             if (_macAddr != null) {\\n                 netBuilder.append(\"<mac address=\\'\" + _macAddr + \"\\'/>\\\\n\");\\n             }',\n",
              " ' \\n         /// <summary>\\n         ///  Removes an annually bolded date. If the date is not found in the\\n[DEL] ///  bolded date list, then no action is taken. If date occurs more than\\n[DEL] ///  once in the bolded date list, then only the first date is removed. When\\n[DEL] ///  comparing dates, only the day and month are used. Be sure to call\\n[DEL] ///  UpdateBoldedDates afterwards.\\n[ADD] ///  bolded date set, then no action is taken. If date occurs in the bolded\\n[ADD] ///  date set, then date is removed. When comparing dates, only the day\\n[ADD] ///  and month are used. Be sure to call UpdateBoldedDates() afterwards.\\n         /// </summary>\\n         public void RemoveAnnuallyBoldedDate(DateTime date)\\n         {\\n[DEL] int length = _annualArrayOfDates.Count;\\n[DEL] int i = 0;\\n[DEL] for (; i < length; ++i)\\n[ADD] foreach (DateTime item in _annualArrayOfDates)\\n             {\\n[DEL] if (CompareDayAndMonth(_annualArrayOfDates[i], date))\\n[ADD] if (CompareDayAndMonth(item, date))\\n                 {\\n[DEL] _annualArrayOfDates.RemoveAt(i);\\n[DEL] break;\\n[DEL] }\\n[DEL] }\\n[ADD] _annualArrayOfDates.Remove(item);\\n[ADD] _monthsOfYear[date.Month - 1] &= ~(0x00000001 << (date.Day - 1));\\n \\n[DEL] --length;\\n[DEL] for (int j = i; j < length; ++j)\\n[DEL] {\\n[DEL] if (CompareDayAndMonth(_annualArrayOfDates[j], date))\\n[DEL] {\\n                     return;\\n                 }\\n             }\\n[DEL] \\n[DEL] _monthsOfYear[date.Month - 1] &= ~(0x00000001 << (date.Day - 1));\\n         }\\n \\n         /// <summary>\\n[DEL] ///  Removes a bolded date. If the date is not found in the bolded date list,\\n[DEL] ///  then no action is taken. If date occurs more than once in the bolded\\n[DEL] ///  date list, then only the first date is removed.\\n[DEL] ///  Be sure to call UpdateBoldedDates() afterwards.\\n[ADD] ///  Removes a bolded date. If the date is not found in the bolded date set,\\n[ADD] ///  then no action is taken. If date occurs in the bolded date set, then\\n[ADD] ///  the date is removed. Be sure to call UpdateBoldedDates() afterwards.\\n         /// </summary>\\n         public void RemoveBoldedDate(DateTime date)\\n         {\\n[DEL] int length = _arrayOfDates.Count;\\n[DEL] for (int i = 0; i < length; ++i)\\n[ADD] foreach (DateTime item in _arrayOfDates)\\n             {\\n[DEL] if (DateTime.Compare(_arrayOfDates[i].Date, date.Date) == 0)\\n[ADD] if (item.Date == date.Date)\\n                 {\\n[DEL] _arrayOfDates.RemoveAt(i);\\n[ADD] _arrayOfDates.Remove(item);\\n[ADD] \\n                     return;\\n                 }\\n             }\\n         }\\n \\n         /// <summary>\\n[DEL] ///  Removes a monthly bolded date. If the date is not found in the\\n[DEL] ///  bolded date list, then no action is taken. If date occurs more than\\n[DEL] ///  once in the bolded date list, then only the first date is removed. When\\n[DEL] ///  comparing dates, only the day and month are used. Be sure to call\\n[DEL] ///  UpdateBoldedDates afterwards.\\n[ADD] ///  Removes a monthly bolded date. If the date is not found in the bolded\\n[ADD] ///  date set, then no action is taken. If date occurs in the bolded date set,\\n[ADD] ///  then the date is removed. When comparing dates, only the day are used.\\n[ADD] ///  Be sure to call UpdateBoldedDates() afterwards.\\n         /// </summary>\\n         public void RemoveMonthlyBoldedDate(DateTime date)\\n         {\\n[DEL] int length = _monthlyArrayOfDates.Count;\\n[DEL] int i = 0;\\n[DEL] for (; i < length; ++i)\\n[ADD] foreach (DateTime item in _monthlyArrayOfDates)\\n             {\\n[DEL] if (CompareDayAndMonth(_monthlyArrayOfDates[i], date))\\n[ADD] if (item.Day == date.Day)\\n                 {\\n[DEL] _monthlyArrayOfDates.RemoveAt(i);\\n[DEL] break;\\n[DEL] }\\n[DEL] }\\n[ADD] _monthlyArrayOfDates.Remove(item);\\n[ADD] _datesToBoldMonthly &= ~(0x00000001 << (date.Day - 1));\\n \\n[DEL] --length;\\n[DEL] for (int j = i; j < length; ++j)\\n[DEL] {\\n[DEL] if (CompareDayAndMonth(_monthlyArrayOfDates[j], date))\\n[DEL] {\\n                     return;\\n                 }\\n             }\\n[DEL] \\n[DEL] _datesToBoldMonthly &= ~(0x00000001 << (date.Day - 1));\\n         }\\n \\n         private void ResetAnnuallyBoldedDates() => _annualArrayOfDates.Clear();',\n",
              " '   private Optional<String> getField( Object[] row, String field ) {\\n     int messageFieldIndex = getInputRowMeta().indexOfValue( field );\\n     checkArgument( messageFieldIndex > -1, getString( PKG, \"MQTTProducer.Error.FieldNotFound\", field ) );\\n[DEL] return ofNullable( row[ messageFieldIndex ] ).map( f -> f.toString() );\\n[ADD] return ofNullable( row[ messageFieldIndex ] ).map( Object::toString );\\n   }\\n \\n   @Override public void stopRunning( StepMetaInterface stepMetaInterface, StepDataInterface stepDataInterface )',\n",
              " ' package com.ichi2.anki;\\n \\n import android.annotation.SuppressLint;\\n[ADD] import android.annotation.TargetApi;\\n import android.app.Activity;\\n import android.content.ActivityNotFoundException;\\n import android.content.BroadcastReceiver;',\n",
              " '             DeferredTypeHandler<InitializeWeakSetConstructor>::GetDefaultInstance());\\n         AddFunction(globalObject, PropertyIds::WeakSet, weakSetConstructor);\\n \\n[ADD] registryConstructor = nullptr;\\n[ADD] loaderConstructor = nullptr;\\n[ADD] moduleStatusConstructor = nullptr;\\n[ADD] \\n[ADD] if (scriptContext->GetConfig()->IsES6ModuleEnabled())\\n[ADD] {\\n[ADD] registryConstructor = CreateBuiltinConstructor(&JavascriptRegistry::EntryInfo::NewInstance,\\n[ADD] DeferredTypeHandler<InitializeRegistryConstructor>::GetDefaultInstance());\\n[ADD] \\n[ADD] loaderConstructor = CreateBuiltinConstructor(&JavascriptLoader::EntryInfo::NewInstance,\\n[ADD] DeferredTypeHandler<InitializeLoaderConstructor>::GetDefaultInstance());\\n[ADD] \\n[ADD] moduleStatusConstructor = CreateBuiltinConstructor(&JavascriptModuleStatus::EntryInfo::NewInstance,\\n[ADD] DeferredTypeHandler<InitializeModuleStatusConstructor>::GetDefaultInstance());\\n[ADD] \\n[ADD] // TODO: This should construct something else\\n[ADD] moduleConstructor = CreateBuiltinConstructor(&JavascriptModuleStatus::EntryInfo::NewInstance,\\n[ADD] DeferredTypeHandler<InitializeModuleConstructor>::GetDefaultInstance());\\n[ADD] }\\n[ADD] \\n         generatorFunctionConstructor = nullptr;\\n \\n         if (scriptContext->GetConfig()->IsES6GeneratorsEnabled())',\n",
              " '     this.extraMetadata = extraMetadata;\\n   }\\n \\n[DEL] protected Option<HoodieCleanerPlan> createCleanerPlan() {\\n[DEL] return execute();\\n[ADD] private int getCommitInfo() {\\n[ADD] Option<HoodieInstant> lastCleanInstant = table.getActiveTimeline().getCleanerTimeline().filterCompletedInstants().lastInstant();\\n[ADD] HoodieTimeline commitTimeline = table.getActiveTimeline().getCommitTimeline().filterCompletedInstants();\\n[ADD] \\n[ADD] String latestCleanTs;\\n[ADD] int commitsSinceLastCleaning = 0;\\n[ADD] if (lastCleanInstant.isPresent()) {\\n[ADD] latestCleanTs = lastCleanInstant.get().getTimestamp();\\n[ADD] commitsSinceLastCleaning = commitTimeline.findInstantsAfter(latestCleanTs).countInstants();\\n[ADD] } else {\\n[ADD] String firstCommitTs = commitTimeline.firstInstant().get().getTimestamp();\\n[ADD] commitsSinceLastCleaning = commitTimeline.findInstantsAfterOrEquals(firstCommitTs, Integer.MAX_VALUE).countInstants();\\n[ADD] }\\n[ADD] \\n[ADD] return commitsSinceLastCleaning;\\n[ADD] }\\n[ADD] \\n[ADD] private boolean needCleaning(CleaningTriggerStrategy strategy) {\\n[ADD] boolean cleaningNeeded;\\n[ADD] if (strategy == CleaningTriggerStrategy.NUM_COMMITS) {\\n[ADD] int numberOfCommits = getCommitInfo();\\n[ADD] int maxInlineCommitsForNextClean = config.getInlineCleaningMaxCommits();\\n[ADD] cleaningNeeded = numberOfCommits >= maxInlineCommitsForNextClean;\\n[ADD] } else {\\n[ADD] throw new HoodieException(\"Unsupported cleaning trigger strategy: \" + config.getInlineCleaningTriggerStrategy());\\n[ADD] }\\n[ADD] \\n[ADD] return cleaningNeeded;\\n   }\\n \\n   /**',\n",
              " ' import java.util.ArrayList;\\n import java.util.Collection;\\n import java.util.Iterator;\\n[ADD] import java.util.Map;\\n import java.util.UUID;\\n \\n[ADD] import org.springframework.beans.DirectFieldAccessor;\\n import org.springframework.integration.Message;\\n[ADD] import org.springframework.integration.MessageHeaders;\\n[ADD] import org.springframework.integration.support.MessageBuilder;\\n import org.springframework.jmx.export.annotation.ManagedAttribute;\\n import org.springframework.util.Assert;\\n ',\n",
              " '         mActionBarSpinner.setVisibility(View.VISIBLE);\\n \\n         // Setup Task Handler\\n[DEL] mTaskHandler = new AnkiStatsTaskHandler(col);\\n[ADD] mTaskHandler = AnkiStatsTaskHandler.getInstance(col);\\n \\n         // Dirty way to get text size from a TextView with current style, change if possible\\n         float size = new FixedTextView(this).getTextSize();',\n",
              " '  * @author Jesse\\n  */\\n public interface GroupRepositoryCustom {\\n[ADD] /**\\n[ADD] * Find the group with the given groupId (where groupId is a string).  The string will be converted\\n[ADD] * to an integer for making the query.\\n[ADD] *\\n[ADD] * @param groupId the groupid.\\n[ADD] * @return the group with the given groupid\\n[ADD] */\\n[ADD] @Nullable\\n[ADD] Group findOne(@Nonnull String groupId);\\n[ADD] \\n     /**\\n      * Find a group given one of the Reserved groups enumeration values.\\n      *',\n",
              " '       if (getStatistics().isEnabled()) {\\n         getStatistics().incReceivedEvents();\\n       }\\n[ADD] \\n[ADD] FlowCallStack flowCallStack = event.getFlowCallStack();\\n[ADD] if (flowCallStack instanceof DefaultFlowCallStack) {\\n[ADD] ((DefaultFlowCallStack) flowCallStack).push(new FlowStackElement(AbstractPipeline.this.getName(), null));\\n[ADD] }\\n       notificationFirer.dispatch(new PipelineMessageNotification(createInfo(event, null, AbstractPipeline.this),\\n                                                                  AbstractPipeline.this.getName(), PROCESS_START));\\n ',\n",
              " ' package org.ray.api.test;\\n \\n[DEL] import java.util.ArrayList;\\n[DEL] import java.util.List;\\n import org.junit.Assert;\\n import org.junit.Test;\\n import org.junit.runner.RunWith;\\n import org.ray.api.Ray;\\n import org.ray.api.RayActor;\\n import org.ray.api.RayObject;\\n[DEL] import org.ray.api.RayRemote;\\n[DEL] import org.ray.api.UniqueID;\\n[ADD] import org.ray.api.annotation.RayRemote;\\n[ADD] import org.ray.api.function.RayFunc2;\\n[ADD] import org.ray.api.id.UniqueId;\\n \\n @RunWith(MyRunner.class)\\n public class ActorTest {\\n \\n   @RayRemote\\n[DEL] public static Integer sayWorld(Integer n, RayActor<ActorTest.Adder> adder) {\\n[DEL] RayObject<Integer> result = Ray.call(ActorTest.Adder::add, adder, 1);\\n[DEL] return result.get() + n;\\n[DEL] }\\n[DEL] \\n[DEL] @Test\\n[DEL] public void test() {\\n[DEL] \\n[DEL] RayActor<ActorTest.Adder> adder = Ray.create(ActorTest.Adder.class);\\n[DEL] Ray.call(Adder::set, adder, 10);\\n[DEL] RayObject<Integer> result = Ray.call(Adder::add, adder, 1);\\n[DEL] Assert.assertEquals(11, (int) result.get());\\n[DEL] \\n[DEL] RayActor<Adder> secondAdder = Ray.create(Adder.class);\\n[DEL] RayObject<Integer> result2 = Ray.call(Adder::add, secondAdder, 1);\\n[DEL] Assert.assertEquals(1, (int) result2.get());\\n[DEL] \\n[DEL] RayObject<Integer> result3 = Ray.call(Adder::add2, 1);\\n[DEL] Assert.assertEquals(2, (int) result3.get());\\n[ADD] public static class Counter {\\n \\n[DEL] RayObject<Integer> result4 = Ray.call(ActorTest::sayWorld, 2, adder);\\n[DEL] Assert.assertEquals(14, (int) result4.get());\\n[ADD] private int value = 0;\\n \\n[DEL] RayActor<Adder2> adder2 = Ray.create(Adder2.class);\\n[DEL] Ray.call(Adder2::setAdder, adder2, adder);\\n[DEL] RayObject<Integer> result5 = Ray.call(Adder2::increase, adder2);\\n[DEL] Assert.assertEquals(1, (int) result5.get());\\n[DEL] \\n[DEL] List list = new ArrayList<>();\\n[DEL] list.add(adder);\\n[DEL] Ray.call(Adder2::setAdderList, adder2, list);\\n[DEL] \\n[DEL] RayObject<Integer> result7 = Ray.call(Adder2::testActorList, adder2);\\n[DEL] Assert.assertEquals(14, (int) result7.get());\\n[DEL] \\n[DEL] List tempList = new ArrayList<>();\\n[DEL] tempList.add(result);\\n[DEL] Ray.call(Adder::setObjectList, adder, tempList);\\n[DEL] RayObject<Integer> result8 = Ray.call(Adder::testObjectList, adder);\\n[DEL] Assert.assertEquals(11, (int) result8.get());\\n[ADD] public int incr(int delta) {\\n[ADD] value += delta;\\n[ADD] return value;\\n[ADD] }\\n   }\\n \\n   @RayRemote\\n[DEL] public static class Adder {\\n[DEL] \\n[DEL] private List<RayObject<Integer>> objectList;\\n[DEL] private Integer sum = 0;\\n[DEL] \\n[DEL] public static Integer add2(Integer n) {\\n[DEL] return n + 1;\\n[DEL] }\\n[DEL] \\n[DEL] public Integer set(Integer n) {\\n[DEL] sum = n;\\n[DEL] return sum;\\n[DEL] }\\n[DEL] \\n[DEL] public Integer increase() {\\n[DEL] return (++sum);\\n[DEL] }\\n[DEL] \\n[DEL] public Integer add(Integer n) {\\n[DEL] return (sum += n);\\n[DEL] }\\n[DEL] \\n[DEL] public Integer setObjectList(List<RayObject<Integer>> objectList) {\\n[DEL] this.objectList = objectList;\\n[DEL] return 1;\\n[DEL] }\\n[DEL] \\n[DEL] public Integer testObjectList() {\\n[DEL] return ((RayObject<Integer>) objectList.get(0)).get();\\n[DEL] }\\n[ADD] public static int testActorAsFirstParameter(RayActor<Counter> actor, int delta) {\\n[ADD] RayObject<Integer> res = Ray.call(Counter::incr, actor, delta);\\n[ADD] return res.get();\\n   }\\n \\n   @RayRemote\\n[DEL] public static class Adder2 {\\n[DEL] \\n[DEL] private RayActor<Adder> adder;\\n[DEL] \\n[DEL] private List<RayActor<Adder>> adderList;\\n[DEL] \\n[DEL] private UniqueID id;\\n[DEL] private Integer sum = 0;\\n[DEL] \\n[DEL] public static Integer add2(Adder a, Integer n) {\\n[DEL] return n + 1;\\n[DEL] }\\n[DEL] \\n[DEL] public Integer set(Integer n) {\\n[DEL] sum = n;\\n[DEL] return sum;\\n[DEL] }\\n[DEL] \\n[DEL] public Integer increase() {\\n[DEL] RayObject<Integer> result = Ray.call(Adder::increase, adder);\\n[DEL] Assert.assertEquals(13, (int) result.get());\\n[DEL] return (++sum);\\n[DEL] }\\n[DEL] \\n[DEL] public Integer testActorList() {\\n[DEL] RayActor<Adder> temp = adderList.get(0);\\n[DEL] RayObject<Integer> result = Ray.call(Adder::increase, temp);\\n[DEL] return result.get();\\n[DEL] }\\n[DEL] \\n[DEL] public Integer add(Integer n) {\\n[DEL] return (sum += n);\\n[DEL] }\\n[DEL] \\n[DEL] public RayActor<Adder> getAdder() {\\n[DEL] return adder;\\n[DEL] }\\n[DEL] \\n[DEL] public Integer setAdder(RayActor<Adder> adder) {\\n[DEL] this.adder = adder;\\n[DEL] return 0;\\n[DEL] }\\n[DEL] \\n[DEL] public UniqueID getId() {\\n[DEL] return id;\\n[DEL] }\\n[ADD] public static int testActorAsSecondParameter(int delta, RayActor<Counter> actor) {\\n[ADD] RayObject<Integer> res = Ray.call(Counter::incr, actor, delta);\\n[ADD] return res.get();\\n[ADD] }\\n \\n[DEL] public Integer setId(UniqueID id) {\\n[DEL] this.id = id;\\n[DEL] adder = new RayActor<>(id);\\n[DEL] return 0;\\n[DEL] }\\n[ADD] @Test\\n[ADD] public void testCreateAndCallActor() {\\n[ADD] // Test creating an actor\\n[ADD] RayActor<Counter> actor = Ray.createActor(Counter.class);\\n[ADD] Assert.assertNotEquals(actor.getId(), UniqueId.NIL);\\n[ADD] // Test calling an actor\\n[ADD] RayFunc2<Counter, Integer, Integer> f = Counter::incr;\\n[ADD] Assert.assertEquals(Integer.valueOf(1), Ray.call(f, actor, 1).get());\\n[ADD] Assert.assertEquals(Integer.valueOf(11), Ray.call(Counter::incr, actor, 10).get());\\n[ADD] }\\n \\n[DEL] public Integer setAdderList(List<RayActor<Adder>> adderList) {\\n[DEL] this.adderList = adderList;\\n[DEL] return 0;\\n[DEL] }\\n[ADD] @Test\\n[ADD] public void testPassActorAsParameter() {\\n[ADD] RayActor<Counter> actor = Ray.createActor(Counter.class);\\n[ADD] RayFunc2<RayActor, Integer, Integer> f = ActorTest::testActorAsFirstParameter;\\n[ADD] Assert.assertEquals(Integer.valueOf(1),\\n[ADD] Ray.call(ActorTest::testActorAsFirstParameter, actor, 1).get());\\n[ADD] Assert.assertEquals(Integer.valueOf(11),\\n[ADD] Ray.call(ActorTest::testActorAsSecondParameter, 10, actor).get());\\n   }\\n }',\n",
              " '         return Collections.unmodifiableList(applications);\\n     }\\n \\n[ADD] @Override\\n[ADD] public List<Domain> getDomains()\\n[ADD] {\\n[ADD] return Collections.unmodifiableList(domains);\\n[ADD] }\\n[ADD] \\n     /**\\n      * @return URL/lastModified of apps which previously failed to deploy\\n      */\\n[DEL] public Map<URL, Long> getZombieMap()\\n[ADD] public Map<URL, Long> getApplicationsZombieMap()\\n     {\\n[DEL] Map<URL, Long> result = new HashMap<URL, Long>();\\n[DEL] \\n[DEL] for (String app : zombieMap.keySet())\\n[DEL] {\\n[DEL] ZombieFile file = zombieMap.get(app);\\n[DEL] result.put(file.url, file.lastUpdated);\\n[DEL] }\\n[ADD] return applicationDeployer.getArtifactsZombieMap();\\n[ADD] }\\n \\n[DEL] return result;\\n[ADD] public Map<URL, Long> getDomainsZombieMap()\\n[ADD] {\\n[ADD] return domainDeployer.getArtifactsZombieMap();\\n     }\\n \\n     protected MuleDeployer getDeployer()\\n     {\\n[DEL] return deployer;\\n[ADD] return applicationDeployer.getDeployer();\\n     }\\n \\n     public void setAppFactory(ApplicationFactory appFactory)\\n     {\\n[DEL] this.appFactory = appFactory;\\n[ADD] this.applicationDeployer.setArtifactFactory(appFactory);\\n     }\\n \\n     public void setDeployer(MuleDeployer deployer)\\n     {\\n[DEL] this.deployer = deployer;\\n[ADD] this.applicationDeployer.setDeployer(deployer);\\n     }\\n \\n     public ApplicationFactory getAppFactory()\\n     {\\n[DEL] return appFactory;\\n[ADD] //Cast required to maintain backward compatibility.\\n[ADD] return (ApplicationFactory) applicationDeployer.getArtifactFactory();\\n     }\\n \\n     @Override\\n[DEL] public ReentrantLock getLock() {\\n[DEL] return lock;\\n[DEL] }\\n[DEL] \\n[DEL] protected void onApplicationInstalled(Application a)\\n[ADD] public ReentrantLock getLock()\\n     {\\n[DEL] trackApplication(a);\\n[ADD] return deploymentInProgressLock;\\n     }\\n \\n[DEL] private void trackApplication(Application application)\\n[ADD] protected void undeploy(Application app)\\n     {\\n[DEL] Application previousApplication = findApplication(application.getAppName());\\n[DEL] applications.remove(previousApplication);\\n[DEL] \\n[DEL] applications.add(application);\\n[ADD] applicationDeployer.undeploy(app);\\n     }\\n \\n[DEL] protected void undeploy(Application app)\\n[ADD] public void undeployDomain(String appName)\\n     {\\n[DEL] if (logger.isInfoEnabled())\\n[DEL] {\\n[DEL] logger.info(\"================== Request to Undeploy Application: \" + app.getAppName());\\n[DEL] }\\n[DEL] \\n[DEL] try\\n[DEL] {\\n[DEL] deploymentListener.onUndeploymentStart(app.getAppName());\\n[DEL] \\n[DEL] applications.remove(app);\\n[DEL] guardedUndeploy(app);\\n[DEL] \\n[DEL] deploymentListener.onUndeploymentSuccess(app.getAppName());\\n[DEL] }\\n[DEL] catch (RuntimeException e)\\n[DEL] {\\n[DEL] deploymentListener.onUndeploymentFailure(app.getAppName(), e);\\n[DEL] throw e;\\n[DEL] }\\n[ADD] domainDeployer.undeploy(appName);\\n     }\\n \\n     @Override\\n     public void undeploy(String appName)\\n     {\\n[DEL] Application app = (Application) CollectionUtils.find(applications, new BeanPropertyValueEqualsPredicate(\"appName\", appName));\\n[DEL] undeploy(app);\\n[ADD] applicationDeployer.undeploy(appName);\\n     }\\n \\n     @Override\\n     public void deploy(URL appArchiveUrl) throws IOException\\n     {\\n[DEL] Application application;\\n[DEL] \\n[DEL] try\\n[DEL] {\\n[DEL] try\\n[DEL] {\\n[DEL] application = guardedInstallFrom(appArchiveUrl);\\n[DEL] trackApplication(application);\\n[DEL] }\\n[DEL] catch (Throwable t)\\n[DEL] {\\n[DEL] File appArchive = new File(appArchiveUrl.toURI());\\n[DEL] String appName = StringUtils.removeEnd(appArchive.getName(), ZIP_FILE_SUFFIX);\\n[DEL] \\n[DEL] //// error text has been created by the deployer already\\n[DEL] final String msg = miniSplash(String.format(\"Failed to deploy app \\'%s\\', see below\", appName));\\n[DEL] logger.error(msg, t);\\n[DEL] \\n[DEL] addZombieFile(appName, appArchive);\\n[DEL] \\n[DEL] deploymentListener.onDeploymentFailure(appName, t);\\n[DEL] \\n[DEL] throw t;\\n[DEL] }\\n[DEL] \\n[DEL] deployApplication(application);\\n[DEL] }\\n[DEL] catch (Throwable t)\\n[DEL] {\\n[DEL] if (t instanceof DeploymentException)\\n[DEL] {\\n[DEL] // re-throw\\n[DEL] throw ((DeploymentException) t);\\n[DEL] }\\n[DEL] \\n[DEL] final String msg = \"Failed to deploy from URL: \" + appArchiveUrl;\\n[DEL] throw new DeploymentException(MessageFactory.createStaticMessage(msg), t);\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] private void guardedDeploy(Application application)\\n[DEL] {\\n[DEL] try\\n[DEL] {\\n[DEL] if (!lock.tryLock(0, TimeUnit.SECONDS))\\n[DEL] {\\n[DEL] return;\\n[DEL] }\\n[DEL] \\n[DEL] deployer.deploy(application);\\n[DEL] }\\n[DEL] catch (InterruptedException e)\\n[DEL] {\\n[DEL] Thread.currentThread().interrupt();\\n[DEL] }\\n[DEL] finally\\n[DEL] {\\n[DEL] if (lock.isHeldByCurrentThread())\\n[DEL] {\\n[DEL] lock.unlock();\\n[DEL] }\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] private void guardedUndeploy(Application app)\\n[DEL] {\\n[DEL] try\\n[DEL] {\\n[DEL] if (!lock.tryLock(0, TimeUnit.SECONDS))\\n[DEL] {\\n[DEL] return;\\n[DEL] }\\n[DEL] \\n[DEL] deployer.undeploy(app);\\n[DEL] }\\n[DEL] catch (InterruptedException e)\\n[DEL] {\\n[DEL] Thread.currentThread().interrupt();\\n[DEL] }\\n[DEL] finally\\n[DEL] {\\n[DEL] if (lock.isHeldByCurrentThread())\\n[DEL] {\\n[DEL] lock.unlock();\\n[DEL] }\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] private Application guardedInstallFrom(URL appArchiveUrl) throws IOException\\n[DEL] {\\n[DEL] try\\n[DEL] {\\n[DEL] if (!lock.tryLock(0, TimeUnit.SECONDS))\\n[DEL] {\\n[DEL] throw new IOException(ANOTHER_DEPLOYMENT_OPERATION_IS_IN_PROGRESS);\\n[DEL] }\\n[DEL] \\n[DEL] return deployer.installFrom(appArchiveUrl);\\n[DEL] }\\n[DEL] catch (InterruptedException e)\\n[DEL] {\\n[DEL] Thread.currentThread().interrupt();\\n[DEL] throw new IOException(INSTALL_OPERATION_HAS_BEEN_INTERRUPTED);\\n[DEL] }\\n[DEL] finally\\n[DEL] {\\n[DEL] if (lock.isHeldByCurrentThread())\\n[DEL] {\\n[DEL] lock.unlock();\\n[DEL] }\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] protected void addZombieApp(Application application)\\n[DEL] {\\n[DEL] final File appDir = new File(MuleContainerBootstrapUtils.getMuleAppsDir(), application.getAppName()) ;\\n[DEL] \\n[DEL] String resource = application.getDescriptor().getConfigResources()[0];\\n[DEL] File resourceFile = new File(appDir, resource);\\n[DEL] ZombieFile zombieFile = new ZombieFile();\\n[DEL] \\n[DEL] if (resourceFile.exists())\\n[DEL] {\\n[DEL] try\\n[DEL] {\\n[DEL] zombieFile.url = resourceFile.toURI().toURL();\\n[DEL] zombieFile.lastUpdated = resourceFile.lastModified();\\n[DEL] \\n[DEL] zombieMap.put(application.getAppName(), zombieFile);\\n[DEL] }\\n[DEL] catch (MalformedURLException e)\\n[DEL] {\\n[DEL] // Ignore resource\\n[DEL] }\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] protected void addZombieFile(String appName, File marker)\\n[DEL] {\\n[DEL] // no sync required as deploy operations are single-threaded\\n[DEL] if (marker == null)\\n[DEL] {\\n[DEL] return;\\n[DEL] }\\n[DEL] \\n[DEL] if (!marker.exists())\\n[DEL] {\\n[DEL] return;\\n[DEL] }\\n[DEL] \\n[DEL] try\\n[DEL] {\\n[DEL] long lastModified = marker.lastModified();\\n[DEL] \\n[DEL] ZombieFile zombieFile = new ZombieFile();\\n[DEL] zombieFile.url = marker.toURI().toURL();\\n[DEL] zombieFile.lastUpdated = lastModified;\\n[DEL] \\n[DEL] zombieMap.put(appName, zombieFile);\\n[DEL] }\\n[DEL] catch (MalformedURLException e)\\n[DEL] {\\n[DEL] logger.debug(String.format(\"Failed to mark an exploded app [%s] as a zombie\", marker.getName()), e);\\n[DEL] }\\n[ADD] applicationDeployer.deploy(appArchiveUrl);\\n     }\\n \\n     @Override',\n",
              " ' \\t\\t\\t\\t\\tLabels: labels,\\n \\t\\t\\t\\t},\\n \\t\\t\\t\\tSpec: api.PodSpec{\\n[ADD] InitContainers: []api.Container{\\n[ADD] {\\n[ADD] Name:            \"init\",\\n[ADD] Image:           jobShimImage,\\n[ADD] Command:         []string{\"cp\", \"/job-shim\", \"/pfs/job-shim\"},\\n[ADD] ImagePullPolicy: api.PullPolicy(jobImagePullPolicy),\\n[ADD] Env:             jobEnv,\\n[ADD] VolumeMounts:    volumeMounts,\\n[ADD] },\\n[ADD] },\\n \\t\\t\\t\\t\\tContainers: []api.Container{\\n \\t\\t\\t\\t\\t\\t{\\n[DEL] Name:    \"user\",\\n[DEL] Image:   image,\\n[DEL] Command: []string{\"/job-shim\", jobInfo.JobID},\\n[DEL] SecurityContext: &api.SecurityContext{\\n[DEL] Privileged: &trueVal, // god is this dumb\\n[DEL] },\\n[ADD] Name:            \"user\",\\n[ADD] Image:           userImage,\\n[ADD] Command:         []string{\"/pfs/job-shim\", jobInfo.JobID},\\n \\t\\t\\t\\t\\t\\t\\tImagePullPolicy: api.PullPolicy(jobImagePullPolicy),\\n \\t\\t\\t\\t\\t\\t\\tEnv:             jobEnv,\\n \\t\\t\\t\\t\\t\\t\\tVolumeMounts:    volumeMounts,',\n",
              " '    *\\n    * @param partitionPath Partition path\\n    */\\n[DEL] protected void createMarkerFile(String partitionPath) {\\n[DEL] Path markerPath = makeNewMarkerPath(partitionPath);\\n[DEL] try {\\n[DEL] LOG.info(\"Creating Marker Path=\" + markerPath);\\n[DEL] fs.create(markerPath, false).close();\\n[DEL] } catch (IOException e) {\\n[DEL] throw new HoodieException(\"Failed to create marker file \" + markerPath, e);\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] /**\\n[DEL] * THe marker path will be <base-path>/.hoodie/.temp/<instant_ts>/2019/04/25/filename.\\n[DEL] */\\n[DEL] private Path makeNewMarkerPath(String partitionPath) {\\n[DEL] Path markerRootPath = new Path(hoodieTable.getMetaClient().getMarkerFolderPath(instantTime));\\n[DEL] Path path = FSUtils.getPartitionPath(markerRootPath, partitionPath);\\n[DEL] try {\\n[DEL] fs.mkdirs(path); // create a new partition as needed.\\n[DEL] } catch (IOException e) {\\n[DEL] throw new HoodieIOException(\"Failed to make dir \" + path, e);\\n[DEL] }\\n[DEL] return new Path(path.toString(), FSUtils.makeMarkerFile(instantTime, writeToken, fileId));\\n[ADD] protected void createMarkerFile(String partitionPath, String dataFileName) {\\n[ADD] MarkerFiles markerFiles = new MarkerFiles(hoodieTable, instantTime);\\n[ADD] markerFiles.createMarkerFile(partitionPath, dataFileName, getIOType());\\n   }\\n \\n   public Schema getWriterSchema() {',\n",
              " \"   // Stores the key and corresponding value's latest metadata spilled to disk\\n   private final Map<T, DiskBasedMap.ValueMetadata> inMemoryMetadataOfSpilledData;\\n \\n[ADD] private transient Thread shutdownThread = null;\\n[ADD] \\n   public LazyFileIterable(String filePath, Map<T, DiskBasedMap.ValueMetadata> map) {\\n     this.filePath = filePath;\\n     this.inMemoryMetadataOfSpilledData = map;\",\n",
              " '         } else {\\n             service = (String) channel.getAttribute(ChangeTelnetHandler.SERVICE_KEY);\\n             if (service != null && service.length() > 0) {\\n[DEL] buf.append(\"Use default service \" + service + \".\\\\r\\\\n\");\\n[ADD] buf.append(\"Use default service \").append(service).append(\".\\\\r\\\\n\");\\n             }\\n         }\\n[DEL] if (service == null || service.length() == 0) {\\n[DEL] for (Exporter<?> exporter : DubboProtocol.getDubboProtocol().getExporters()) {\\n[DEL] if (buf.length() > 0) {\\n[DEL] buf.append(\"\\\\r\\\\n\");\\n[DEL] }\\n[DEL] buf.append(exporter.getInvoker().getInterface().getName());\\n[DEL] if (detail) {\\n[DEL] buf.append(\" -> \");\\n[DEL] buf.append(exporter.getInvoker().getUrl());\\n[DEL] }\\n[DEL] }\\n[ADD] \\n[ADD] if (StringUtils.isEmpty(service)) {\\n[ADD] printAllServices(buf, detail);\\n         } else {\\n[DEL] Invoker<?> invoker = null;\\n[DEL] for (Exporter<?> exporter : DubboProtocol.getDubboProtocol().getExporters()) {\\n[DEL] if (service.equals(exporter.getInvoker().getInterface().getSimpleName())\\n[DEL] || service.equals(exporter.getInvoker().getInterface().getName())\\n[DEL] || service.equals(exporter.getInvoker().getUrl().getPath())) {\\n[DEL] invoker = exporter.getInvoker();\\n[DEL] break;\\n[ADD] printSpecifiedService(service, buf, detail);\\n[ADD] \\n[ADD] if (buf.length() == 0) {\\n[ADD] buf.append(\"No such service: \").append(service);\\n[ADD] }\\n[ADD] }\\n[ADD] return buf.toString();\\n[ADD] }\\n[ADD] \\n[ADD] private void printAllServices(StringBuilder buf, boolean detail) {\\n[ADD] printAllProvidedServices(buf, detail);\\n[ADD] printAllReferredServices(buf, detail);\\n[ADD] }\\n[ADD] \\n[ADD] private void printAllProvidedServices(StringBuilder buf, boolean detail) {\\n[ADD] if (!ApplicationModel.allProviderModels().isEmpty()) {\\n[ADD] buf.append(\"PROVIDER:\\\\r\\\\n\");\\n[ADD] }\\n[ADD] \\n[ADD] for (ProviderModel provider : ApplicationModel.allProviderModels()) {\\n[ADD] buf.append(provider.getServiceName());\\n[ADD] if (detail) {\\n[ADD] buf.append(\" -> \");\\n[ADD] buf.append(\" published: \");\\n[ADD] buf.append(isRegistered(provider.getServiceName()) ? \"Y\" : \"N\");\\n[ADD] }\\n[ADD] buf.append(\"\\\\r\\\\n\");\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private void printAllReferredServices(StringBuilder buf, boolean detail) {\\n[ADD] if (!ApplicationModel.allConsumerModels().isEmpty()) {\\n[ADD] buf.append(\"CONSUMER:\\\\r\\\\n\");\\n[ADD] }\\n[ADD] \\n[ADD] for (ConsumerModel consumer : ApplicationModel.allConsumerModels()) {\\n[ADD] buf.append(consumer.getServiceName());\\n[ADD] if (detail) {\\n[ADD] buf.append(\" -> \");\\n[ADD] buf.append(\" addresses: \");\\n[ADD] buf.append(getConsumerAddressNum(consumer.getServiceName()));\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private void printSpecifiedService(String service, StringBuilder buf, boolean detail) {\\n[ADD] printSpecifiedProvidedService(service, buf, detail);\\n[ADD] printSpecifiedReferredService(service, buf, detail);\\n[ADD] }\\n[ADD] \\n[ADD] private void printSpecifiedProvidedService(String service, StringBuilder buf, boolean detail) {\\n[ADD] for (ProviderModel provider : ApplicationModel.allProviderModels()) {\\n[ADD] if (service.equalsIgnoreCase(provider.getServiceName())\\n[ADD] || service.equalsIgnoreCase(provider.getServiceInterfaceClass().getName())\\n[ADD] || service.equalsIgnoreCase(provider.getServiceInterfaceClass().getSimpleName())) {\\n[ADD] buf.append(provider.getServiceName()).append(\" (as provider):\\\\r\\\\n\");\\n[ADD] for (ProviderMethodModel method : provider.getAllMethods()) {\\n[ADD] printMethod(method.getMethod(), buf, detail);\\n                 }\\n             }\\n[DEL] if (invoker != null) {\\n[DEL] Method[] methods = invoker.getInterface().getMethods();\\n[DEL] for (Method method : methods) {\\n[DEL] if (buf.length() > 0) {\\n[DEL] buf.append(\"\\\\r\\\\n\");\\n[DEL] }\\n[DEL] if (detail) {\\n[DEL] buf.append(ReflectUtils.getName(method));\\n[DEL] } else {\\n[DEL] buf.append(method.getName());\\n[DEL] }\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private void printSpecifiedReferredService(String service, StringBuilder buf, boolean detail) {\\n[ADD] for (ConsumerModel consumer : ApplicationModel.allConsumerModels()) {\\n[ADD] if (service.equalsIgnoreCase(consumer.getServiceName())\\n[ADD] || service.equalsIgnoreCase(consumer.getServiceInterfaceClass().getName())\\n[ADD] || service.equalsIgnoreCase(consumer.getServiceInterfaceClass().getSimpleName())) {\\n[ADD] buf.append(consumer.getServiceName()).append(\" (as consumer):\\\\r\\\\n\");\\n[ADD] for (ConsumerMethodModel method : consumer.getAllMethods()) {\\n[ADD] printMethod(method.getMethod(), buf, detail);\\n                 }\\n[DEL] } else {\\n[DEL] buf.append(\"No such service \" + service);\\n             }\\n         }\\n[DEL] return buf.toString();\\n     }\\n \\n[ADD] private void printMethod(Method method, StringBuilder buf, boolean detail) {\\n[ADD] if (detail) {\\n[ADD] buf.append(\\'\\\\t\\').append(ReflectUtils.getName(method));\\n[ADD] } else {\\n[ADD] buf.append(\\'\\\\t\\').append(method.getName());\\n[ADD] }\\n[ADD] buf.append(\"\\\\r\\\\n\");\\n[ADD] }\\n }',\n",
              " ' \\n         // 2 Read the total size of the block\\n         blocksize = inputStream.readInt();\\n[ADD] this.logBlockSize = blocksize;\\n       } else {\\n         // 1 Read the total size of the block\\n         blocksize = (int) inputStream.readLong();',\n",
              " '     this.resumeQueue = initializeDagQueue(this.numThreads);\\n     this.scheduledExecutorPool = Executors.newScheduledThreadPool(numThreads);\\n     this.pollingInterval = ConfigUtils.getInt(config, JOB_STATUS_POLLING_INTERVAL_KEY, DEFAULT_JOB_STATUS_POLLING_INTERVAL);\\n[ADD] this.retentionInterval = ConfigUtils.getInt(config, FAILED_DAG_POLLING_INTERVAL, DEFAULT_FAILED_DAG_POLLING_INTERVAL);\\n     this.instrumentationEnabled = instrumentationEnabled;\\n     if (instrumentationEnabled) {\\n       MetricContext metricContext = Instrumented.getMetricContext(ConfigUtils.configToState(ConfigFactory.empty()), getClass());',\n",
              " ' \\t}\\n \\t\\n \\tprotected String getTemporaryFileSuffix() {\\n[ADD] Assert.notNull(temporaryFileSuffix, \"\\'temporaryFileSuffix\\' must not be null\");\\n \\t\\treturn this.temporaryFileSuffix;\\n \\t}\\n ',\n",
              " ' import javax.annotation.CheckForNull;\\n \\n import java.util.ArrayList;\\n[DEL] import java.util.HashMap;\\n[ADD] import java.util.Arrays;\\n import java.util.Iterator;\\n import java.util.List;\\n import java.util.Map;',\n",
              " '     public LinkedHashMap<String, String> extractTitles(@Nonnull String id) throws Exception {\\n         AbstractMetadata metadata = findOne(id);\\n \\n[DEL] if (metadata == null)\\n[ADD] // If metadata not found or it is not a type metadata then return null\\n[ADD] if (metadata == null || metadata.getDataInfo().getType() != MetadataType.METADATA)\\n             return null;\\n \\n         Element md = Xml.loadString(metadata.getData(), false);',\n",
              " ' \\t\\t\\t\\tchar charAtO1 = s1.charAtInternal(o1++, s1Value);\\n \\t\\t\\t\\tchar charAtO2 = s2.charAtInternal(o2++, s2Value);\\n \\n[ADD] /*[IF Java16]*/\\n[ADD] if (Character.isHighSurrogate(charAtO1) && Character.isHighSurrogate(charAtO2) && (o1 < end)) {\\n[ADD] int codepointAtO1 = Character.toCodePoint(charAtO1, s1.charAtInternal(o1++, s1Value));\\n[ADD] int codepointAtO2 = Character.toCodePoint(charAtO2, s2.charAtInternal(o2++, s2Value));\\n[ADD] if ((codepointAtO1 != codepointAtO2) && (compareValue(codepointAtO1) != compareValue(codepointAtO2))) {\\n[ADD] return false;\\n[ADD] }\\n[ADD] }\\n[ADD] /*[ENDIF] Java16 */\\n[ADD] \\n \\t\\t\\t\\tif (charAtO1 != charAtO2 &&\\n \\t\\t\\t\\t\\t\\ttoUpperCase(charAtO1) != toUpperCase(charAtO2) &&\\n \\t\\t\\t\\t\\t\\ttoLowerCase(charAtO1) != toLowerCase(charAtO2)) {',\n",
              " ' package org.sonar.java.checks;\\n \\n import com.google.common.collect.ImmutableList;\\n[ADD] \\n import org.sonar.check.Rule;\\n import org.sonar.java.model.ModifiersUtils;\\n import org.sonar.plugins.java.api.IssuableSubscriptionVisitor;\\n[ADD] import org.sonar.plugins.java.api.semantic.Type;\\n[ADD] import org.sonar.plugins.java.api.tree.BaseTreeVisitor;\\n import org.sonar.plugins.java.api.tree.ClassTree;\\n import org.sonar.plugins.java.api.tree.MethodTree;\\n import org.sonar.plugins.java.api.tree.Modifier;\\n[ADD] import org.sonar.plugins.java.api.tree.NewClassTree;\\n import org.sonar.plugins.java.api.tree.Tree;\\n import org.sonar.plugins.java.api.tree.Tree.Kind;\\n[ADD] import org.sonar.plugins.java.api.tree.TypeTree;\\n \\n import java.util.List;\\n ',\n",
              " '     @Override\\n     public void reserve(NicProfile nic, Network network, VirtualMachineProfile vm, DeployDestination dest, ReservationContext context)\\n             throws InsufficientVirtualNetworkCapacityException, InsufficientAddressCapacityException {\\n[DEL] nic.setBroadcastUri(network.getBroadcastUri());\\n[DEL] nic.setIsolationUri(network.getBroadcastUri());\\n[DEL] \\n[DEL] s_logger.debug(\"Handling reserve() call back to with Create a new VM or add an interface to existing VM in network \" + network.getName());\\n[DEL] DataCenter dc = _dcDao.findById(network.getDataCenterId());\\n[DEL] Account networksAccount = _accountDao.findById(network.getAccountId());\\n[DEL] DomainVO networksDomain = _domainDao.findById(network.getDomainId());\\n[DEL] //Get the Account details and find the type\\n[DEL] long networkOwnedBy = network.getAccountId();\\n[DEL] AccountVO neworkAccountDetails = _accountDao.findById(networkOwnedBy);\\n[DEL] if (neworkAccountDetails.getType() == Account.ACCOUNT_TYPE_PROJECT) {\\n[DEL] throw new InsufficientVirtualNetworkCapacityException(\"CS project support is \" + \"not yet implemented in NuageVsp\", DataCenter.class, dc.getId());\\n[ADD] boolean lockedNetwork = lockNetworkForUserVm(network, vm);\\n[ADD] if (lockedNetwork && s_logger.isDebugEnabled()) {\\n[ADD] s_logger.debug(\"Locked network \" + network.getId() + \" for creation of user VM \" + vm.getInstanceName());\\n         }\\n \\n[DEL] //NicProfile does not contain the NIC UUID. We need this information to set it in the VMInterface and VPort\\n[DEL] //that we create in VSP\\n[DEL] NicVO nicFrmDB = _nicDao.findById(nic.getId());\\n[DEL] long networkOfferingId = _ntwkOfferingDao.findById(network.getNetworkOfferingId()).getId();\\n[DEL] boolean isL3Network = isL3Network(networkOfferingId);\\n[DEL] Long vpcId = network.getVpcId();\\n[DEL] String vpcUuid = null;\\n[DEL] if (vpcId != null) {\\n[DEL] Vpc vpcObj = _vpcDao.findById(vpcId);\\n[DEL] vpcUuid = vpcObj.getUuid();\\n[DEL] }\\n[DEL] HostVO nuageVspHost = getNuageVspHost(network.getPhysicalNetworkId());\\n[DEL] ReserveVmInterfaceVspCommand cmd = new ReserveVmInterfaceVspCommand(nicFrmDB.getUuid(), nic.getMacAddress(), network.getUuid(), isL3Network, vpcUuid,\\n[DEL] networksDomain.getUuid(), networksAccount.getUuid(), vm.getType().equals(VirtualMachine.Type.DomainRouter), network.getBroadcastUri().getPath().substring(1),\\n[DEL] vm.getInstanceName(), vm.getUuid(), networksDomain.getUuid(), networksAccount.getUuid());\\n[DEL] ReserveVmInterfaceVspAnswer answer = (ReserveVmInterfaceVspAnswer)_agentMgr.easySend(nuageVspHost.getId(), cmd);\\n[DEL] \\n[DEL] if (answer == null || !answer.getResult()) {\\n[DEL] s_logger.error(\"ReserveVmInterfaceNuageVspCommand failed\");\\n[DEL] if ((null != answer) && (null != answer.getDetails())) {\\n[DEL] s_logger.error(answer.getDetails());\\n[ADD] try {\\n[ADD] if (s_logger.isDebugEnabled()) {\\n[ADD] s_logger.debug(\"Handling reserve() call back to with Create a new VM or add an interface to existing VM in network \" + network.getName());\\n[ADD] }\\n[ADD] nic.setBroadcastUri(network.getBroadcastUri());\\n[ADD] nic.setIsolationUri(network.getBroadcastUri());\\n[ADD] DataCenter dc = _dcDao.findById(network.getDataCenterId());\\n[ADD] Account networksAccount = _accountDao.findById(network.getAccountId());\\n[ADD] DomainVO networksDomain = _domainDao.findById(network.getDomainId());\\n[ADD] //Get the Account details and find the type\\n[ADD] long networkOwnedBy = network.getAccountId();\\n[ADD] AccountVO neworkAccountDetails = _accountDao.findById(networkOwnedBy);\\n[ADD] if (neworkAccountDetails.getType() == Account.ACCOUNT_TYPE_PROJECT) {\\n[ADD] throw new InsufficientVirtualNetworkCapacityException(\"CS project support is not yet implemented in NuageVsp\", DataCenter.class, dc.getId());\\n[ADD] }\\n[ADD] \\n[ADD] //NicProfile does not contain the NIC UUID. We need this information to set it in the VMInterface and VPort\\n[ADD] //that we create in VSP\\n[ADD] NicVO nicFrmDB = _nicDao.findById(nic.getId());\\n[ADD] NetworkOffering networkOffering = _ntwkOfferingDao.findById(network.getNetworkOfferingId());\\n[ADD] boolean isDomainRouter = vm.getType().equals(VirtualMachine.Type.DomainRouter);\\n[ADD] URI broadcastUri = network.getBroadcastUri();\\n[ADD] if (Strings.isNullOrEmpty(broadcastUri.getPath()) || !broadcastUri.getPath().startsWith(\"/\")) {\\n[ADD] throw new IllegalStateException(\"The broadcast URI path \" + network.getBroadcastUri() + \" is empty or in an incorrect format.\");\\n[ADD] }\\n[ADD] String domainRouterIp = network.getBroadcastUri().getPath().substring(1);\\n[ADD] boolean isL3Network = isL3Network(network);\\n[ADD] boolean isSharedNetwork = networkOffering.getGuestType() == GuestType.Shared;\\n[ADD] Long vpcId = network.getVpcId();\\n[ADD] String vpcUuid = null;\\n[ADD] if (vpcId != null) {\\n[ADD] Vpc vpcObj = _vpcDao.findById(vpcId);\\n[ADD] vpcUuid = vpcObj.getUuid();\\n[ADD] }\\n[ADD] HostVO nuageVspHost = getNuageVspHost(network.getPhysicalNetworkId());\\n[ADD] IPAddressVO staticNatIp = _ipAddressDao.findByVmIdAndNetworkId(network.getId(), vm.getId());\\n[ADD] \\n[ADD] ReserveVmInterfaceVspCommand.Builder cmdBuilder = new ReserveVmInterfaceVspCommand.Builder()\\n[ADD] .nicUuid(nicFrmDB.getUuid())\\n[ADD] .nicMacAddress(nic.getMacAddress())\\n[ADD] .networkUuid(network.getUuid())\\n[ADD] .isL3Network(isL3Network)\\n[ADD] .isSharedNetwork(isSharedNetwork)\\n[ADD] .vpcUuid(vpcUuid)\\n[ADD] .networkDomainUuid(networksDomain.getUuid())\\n[ADD] .networksAccountUuid(networksAccount.getUuid())\\n[ADD] .isDomainRouter(isDomainRouter)\\n[ADD] .domainRouterIp(domainRouterIp)\\n[ADD] .vmInstanceName(vm.getInstanceName())\\n[ADD] .vmUuid(vm.getUuid())\\n[ADD] .vmUserName(networksDomain.getUuid())\\n[ADD] .vmUserDomainName(networksAccount.getUuid())\\n[ADD] .useStaticIp(true)\\n[ADD] .staticIp(nic.getIPv4Address());\\n[ADD] if (staticNatIp != null) {\\n[ADD] VlanVO staticNatVlan = _vlanDao.findById(staticNatIp.getVlanId());\\n[ADD] cmdBuilder = cmdBuilder.staticNatIpUuid(staticNatIp.getUuid())\\n[ADD] .staticNatIpAddress(staticNatIp.getAddress().addr())\\n[ADD] .isStaticNatIpAllocated(staticNatIp.getState().equals(IpAddress.State.Allocated))\\n[ADD] .isOneToOneNat(staticNatIp.isOneToOneNat())\\n[ADD] .staticNatVlanUuid(staticNatVlan.getUuid())\\n[ADD] .staticNatVlanGateway(staticNatVlan.getVlanGateway())\\n[ADD] .staticNatVlanNetmask(staticNatVlan.getVlanNetmask());\\n[ADD] }\\n[ADD] \\n[ADD] Answer answer = _agentMgr.easySend(nuageVspHost.getId(), cmdBuilder.build());\\n[ADD] if (answer == null || !answer.getResult()) {\\n[ADD] s_logger.error(\"ReserveVmInterfaceNuageVspCommand failed for NIC \" + nic.getId() + \" attached to VM \" + vm.getId() + \" in network \" + network.getId());\\n[ADD] if ((null != answer) && (null != answer.getDetails())) {\\n[ADD] s_logger.error(answer.getDetails());\\n[ADD] }\\n[ADD] throw new InsufficientVirtualNetworkCapacityException(\"Failed to reserve VM in Nuage VSP.\", Network.class, network.getId());\\n[ADD] }\\n[ADD] \\n[ADD] if (isDomainRouter) {\\n[ADD] nic.setIPv4Address(domainRouterIp);\\n[ADD] }\\n[ADD] \\n[ADD] } finally {\\n[ADD] if (network != null && lockedNetwork) {\\n[ADD] _networkDao.releaseFromLockTable(network.getId());\\n[ADD] if (s_logger.isDebugEnabled()) {\\n[ADD] s_logger.debug(\"Unlocked network \" + network.getId() + \" for creation of user VM \" + vm.getInstanceName());\\n[ADD] }\\n             }\\n[DEL] throw new InsufficientVirtualNetworkCapacityException(\"Failed to reserve VM in Nuage VSP.\", Network.class, network.getId());\\n         }\\n[DEL] List<Map<String, String>> vmInterfacesDetails = answer.getInterfaceDetails();\\n[DEL] setIPGatewayMaskInfo(network, nic, vmInterfacesDetails);\\n     }\\n \\n     @Override\\n     protected boolean canHandle(NetworkOffering offering, final NetworkType networkType, final PhysicalNetwork physicalNetwork) {\\n[DEL] if (networkType == NetworkType.Advanced && isMyTrafficType(offering.getTrafficType()) && offering.getGuestType() == Network.GuestType.Isolated\\n[ADD] if (networkType == NetworkType.Advanced && isMyTrafficType(offering.getTrafficType()) && (offering.getGuestType() == Network.GuestType.Isolated || offering.getGuestType() == Network.GuestType.Shared)\\n                 && isMyIsolationMethod(physicalNetwork)) {\\n             return true;\\n         } else {\\n[DEL] s_logger.trace(\"We only take care of Guest networks of type   \" + GuestType.Isolated + \" in zone of type \" + NetworkType.Advanced);\\n[DEL] return false;\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public boolean release(NicProfile nic, VirtualMachineProfile vm, String reservationId) {\\n[DEL] long networkId = nic.getNetworkId();\\n[DEL] Network network = _networkDao.findById(networkId);\\n[DEL] s_logger.debug(\"Handling release() call back, which is called when a VM is stopped or destroyed, to delete the VM with state \" + vm.getVirtualMachine().getState()\\n[DEL] + \" from netork \" + network.getName());\\n[DEL] if (vm.getVirtualMachine().getState().equals(VirtualMachine.State.Stopping)) {\\n[DEL] try {\\n[DEL] HostVO nuageVspHost = getNuageVspHost(network.getPhysicalNetworkId());\\n[DEL] ReleaseVmVspCommand cmd = new ReleaseVmVspCommand(network.getUuid(), vm.getUuid(), vm.getInstanceName());\\n[DEL] ReleaseVmVspAnswer answer = (ReleaseVmVspAnswer)_agentMgr.easySend(nuageVspHost.getId(), cmd);\\n[DEL] if (answer == null || !answer.getResult()) {\\n[DEL] s_logger.error(\"ReleaseVmNuageVspCommand for VM \" + vm.getUuid() + \" failed\");\\n[DEL] if ((null != answer) && (null != answer.getDetails())) {\\n[DEL] s_logger.error(answer.getDetails());\\n[DEL] }\\n[DEL] }\\n[DEL] } catch (InsufficientVirtualNetworkCapacityException e) {\\n[DEL] s_logger.debug(\"Handling release() call back. Failed to delete CS VM \" + vm.getInstanceName() + \" in VSP. \" + e.getMessage());\\n[ADD] if (s_logger.isTraceEnabled()) {\\n[ADD] s_logger.trace(\"We only take care of Guest networks of type   \" + GuestType.Isolated + \" in zone of type \" + NetworkType.Advanced);\\n             }\\n[DEL] } else {\\n[DEL] s_logger.debug(\"Handling release() call back. VM \" + vm.getInstanceName() + \" is in \" + vm.getVirtualMachine().getState() + \" state. So, the CS VM is not deleted.\"\\n[DEL] + \" This could be a case where VM interface is deleted. deallocate() call back should be called later\");\\n[ADD] return false;\\n         }\\n[DEL] \\n[DEL] return super.release(nic, vm, reservationId);\\n     }\\n \\n     @Override\\n     @DB\\n     public void deallocate(Network network, NicProfile nic, VirtualMachineProfile vm) {\\n[ADD] boolean lockedNetwork = lockNetworkForUserVm(network, vm);\\n[ADD] if (lockedNetwork && s_logger.isDebugEnabled()) {\\n[ADD] s_logger.debug(\"Locked network \" + network.getId() + \" for deallocation of user VM \" + vm.getInstanceName());\\n[ADD] }\\n \\n         try {\\n[DEL] s_logger.debug(\"Handling deallocate() call back, which is called when a VM is destroyed or interface is removed, \" + \"to delete VM Interface with IP \"\\n[DEL] + nic.getIPv4Address() + \" from a VM \" + vm.getInstanceName() + \" with state \" + vm.getVirtualMachine().getState());\\n[ADD] if (s_logger.isDebugEnabled()) {\\n[ADD] s_logger.debug(\"Handling deallocate() call back, which is called when a VM is destroyed or interface is removed, \" + \"to delete VM Interface with IP \"\\n[ADD] + nic.getIPv4Address() + \" from a VM \" + vm.getInstanceName() + \" with state \" + vm.getVirtualMachine().getState());\\n[ADD] }\\n             DomainVO networksDomain = _domainDao.findById(network.getDomainId());\\n             NicVO nicFrmDd = _nicDao.findById(nic.getId());\\n[DEL] long networkOfferingId = _ntwkOfferingDao.findById(network.getNetworkOfferingId()).getId();\\n[ADD] NetworkOffering networkOffering = _ntwkOfferingDao.findById(network.getNetworkOfferingId());\\n[ADD] boolean isL3Network = isL3Network(network);\\n[ADD] boolean isSharedNetwork = networkOffering.getGuestType() == GuestType.Shared;\\n[ADD] boolean isExpunging = vm.getVirtualMachine().getState() == VirtualMachine.State.Expunging;\\n             Long vpcId = network.getVpcId();\\n             String vpcUuid = null;\\n             if (vpcId != null) {',\n",
              " '             }\\n         }\\n \\n[ADD] final Long storagePolicyId = cmd.getStoragePolicy();\\n[ADD] if (storagePolicyId != null) {\\n[ADD] if (vsphereStoragePolicyDao.findById(storagePolicyId) == null) {\\n[ADD] throw new InvalidParameterValueException(\"Please specify a valid vSphere storage policy id\");\\n[ADD] }\\n[ADD] }\\n[ADD] \\n         return createServiceOffering(userId, cmd.isSystem(), vmType, cmd.getServiceOfferingName(), cpuNumber, memory, cpuSpeed, cmd.getDisplayText(),\\n                 cmd.getProvisioningType(), localStorageRequired, offerHA, limitCpuUse, volatileVm, cmd.getTags(), cmd.getDomainIds(), cmd.getZoneIds(), cmd.getHostTag(),\\n                 cmd.getNetworkRate(), cmd.getDeploymentPlanner(), details, isCustomizedIops, cmd.getMinIops(), cmd.getMaxIops(),',\n",
              " ' \\n         super.finalizeNetworkRulesForNetwork(cmds, domainRouterVO, provider, guestNetworkId);\\n \\n[ADD] final VpcVO vpc = _vpcDao.findById(domainRouterVO.getVpcId());\\n         if (domainRouterVO.getVpcId() != null) {\\n[DEL] \\n             if (domainRouterVO.getState() == State.Starting || domainRouterVO.getState() == State.Running) {\\n                 if (_networkModel.isProviderSupportServiceInNetwork(guestNetworkId, Service.NetworkACL, Provider.VPCVirtualRouter)) {\\n                     final List<NetworkACLItemVO> networkACLs = _networkACLMgr.listNetworkACLItems(guestNetworkId);\\n                     if (networkACLs != null && !networkACLs.isEmpty()) {\\n[DEL] s_logger.debug(\"Found \" + networkACLs.size() + \" network ACLs to apply as a part of VPC VR \" + domainRouterVO + \" start for guest network id=\" + guestNetworkId);\\n[ADD] s_logger.debug(\\n[ADD] \"Found \" + networkACLs.size() + \" network ACLs to apply as a part of VPC VR \" + domainRouterVO + \" start for guest network id=\" + guestNetworkId);\\n                         _commandSetupHelper.createNetworkACLsCommands(networkACLs, domainRouterVO, cmds, guestNetworkId, false);\\n                     }\\n                 }\\n[ADD] if (_vpcOffServiceDao.areServicesSupportedByNetworkOffering(vpc.getVpcOfferingId(), Service.VPCDynamicRouting)\\n[ADD] && _networkModel.areServicesSupportedInNetwork(guestNetworkId, Service.VPCDynamicRouting)) {\\n[ADD] try {\\n[ADD] _commandSetupHelper.createQuaggaConfigCommand(domainRouterVO, vpc.getId(), cmds);\\n[ADD] } catch (BadCIDRException ex) {\\n[ADD] s_logger.debug(ex);\\n[ADD] throw new CloudRuntimeException(\"The cidr for dynamic routing is bad \" + ex);\\n[ADD] }\\n[ADD] }\\n             }\\n         }\\n     }',\n",
              " ' \\n     public static final Logger s_logger = Logger.getLogger(KVMGuru.class);\\n \\n[ADD] public static final String DPDK_VHOST_USER_MODE = \"DPDK-VHOSTUSER\";\\n[ADD] public static final String DPDK_NUMA = ApiConstants.EXTRA_CONFIG + \"-dpdk-numa\";\\n[ADD] public static final String DPDK_HUGE_PAGES = ApiConstants.EXTRA_CONFIG + \"-dpdk-hugepages\";\\n[ADD] \\n[ADD] public enum DPDKvHostUserMode {\\n[ADD] CLIENT(\"client\"), SERVER(\"server\");\\n[ADD] \\n[ADD] private String str;\\n[ADD] \\n[ADD] DPDKvHostUserMode(String str) {\\n[ADD] this.str = str;\\n[ADD] }\\n[ADD] \\n[ADD] public static DPDKvHostUserMode fromValue(String val) {\\n[ADD] if (val.equalsIgnoreCase(\"client\")) {\\n[ADD] return CLIENT;\\n[ADD] } else if (val.equalsIgnoreCase(\"server\")) {\\n[ADD] return SERVER;\\n[ADD] } else {\\n[ADD] throw new IllegalArgumentException(\"Invalid DPDK vHost User mode:\" + val);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public String toString() {\\n[ADD] return str;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n     @Override\\n     public HypervisorType getHypervisorType() {\\n         return HypervisorType.KVM;',\n",
              " '   public static final String COMPACTION_PRIORITIZER_ALIAS = COMPACTION_PRIORITIZATION_PREFIX + \"prioritizerAlias\";\\n   public static final String COMPACTION_ESTIMATOR = COMPACTION_PRIORITIZATION_PREFIX + \"estimator\";\\n \\n[ADD] /***\\n[ADD] * Configuration properties related to Re-compaction\\n[ADD] */\\n[ADD] public static String COMPACTION_DIRECTORY_FORMAT = \"compaction_%s\";\\n[ADD] public static String RECOMPACTION_WRITE_TO_NEW_FOLDER = \"recompaction.write.to.new.folder\";\\n[ADD] \\n[ADD] \\n   /**\\n    * Configuration related to ConfigStore based copy/retention\\n    */',\n",
              " '             }\\n         });\\n         when(event.getMessage()).thenReturn(message);\\n[ADD] \\n[ADD] IdempotentRedeliveryPolicyTestCase.serializer = SerializationTestUtils.getJavaSerializerWithMockContext();\\n[ADD] \\n         irp.setMaxRedeliveryCount(MAX_REDELIVERY_COUNT);\\n         irp.setUseSecureHash(true);\\n         irp.setFlowConstruct(mock(FlowConstruct.class));',\n",
              " '       @Advice.Argument(0) final ServletRequest request,\\n       @Advice.Argument(1) final ServletResponse response,\\n       @Advice.Thrown final Throwable throwable,\\n[DEL] @Advice.Local(\"otelSpan\") Span span,\\n[DEL] @Advice.Local(\"otelScope\") Scope scope) {\\n[ADD] @Advice.Local(\"otelSpan\") final Span span,\\n[ADD] @Advice.Local(\"otelScope\") final Scope scope) {\\n     if (scope == null) {\\n       return;\\n     }\\n     scope.close();\\n \\n     if (request instanceof HttpServletRequest && response instanceof HttpServletResponse) {\\n[DEL] TRACER.setPrincipal((HttpServletRequest) request);\\n[ADD] TRACER.setPrincipal(span, (HttpServletRequest) request);\\n \\n       if (span == null) {\\n         return;\\n       }\\n \\n[DEL] Integer responseStatus =\\n[ADD] final Integer responseStatus =\\n           InstrumentationContext.get(ServletResponse.class, Integer.class).get(response);\\n \\n       if (throwable == null) {',\n",
              " '     Object o1 = (List<String>) foo(); // Noncompliant [[sc=18;ec=30]] {{Remove this unnecessary cast to \"List\".}}\\n     Object o2 = (List<? extends String>) foo(); // Noncompliant {{Remove this unnecessary cast to \"List\".}}\\n     Object o3 = (List<? super String>) foo(); // Noncompliant {{Remove this unnecessary cast to \"List\".}}\\n[DEL] String s1 = (String) obj; //Compliant\\n[ADD] String s1 = (String) obj; // Compliant\\n     String s2 = (String) s1; // Noncompliant {{Remove this unnecessary cast to \"String\".}}\\n     A a = (A) new B(); // Noncompliant {{Remove this unnecessary cast to \"A\".}}\\n     A[][] as = (A[][]) new B[1][1]; // Noncompliant {{Remove this unnecessary cast to \"A[][]\".}}\\n     B b;\\n     fun(b);\\n[DEL] fun((A) b);\\n[ADD] fun((A) b); // Compliant - exception to distinguish the method to call\\n     List<B> bees = new java.util.ArrayList<B>();\\n     java.util.List<A> aaas = (java.util.List) bees;\\n[DEL] C c = new C((A)null);\\n[ADD] C c = new C((A)null); // Compliant - exception to distinguish the constructor to call\\n     foo((List<List<A>>) (List<?>) foo2()); // compliant\\n     obj = (Unknown<String>) unknown;\\n     String[] stringList = (String[]) list.toArray(new String[0]); // Compliant',\n",
              " '       this.worker = worker;\\n     }\\n \\n[DEL] /*\\n[DEL] * Report the lull to the StreamingDataflowWorker that is stuck in addition to logging the\\n[DEL] * lull.\\n[DEL] */\\n[DEL] @Override\\n[DEL] public void reportLull(Thread trackedThread, long millis) {\\n[DEL] super.reportLull(trackedThread, millis);\\n[DEL] // Also report the failure to the list of pending failures to report on the worker thread\\n[DEL] // so that the failure gets communicated to the StreamingDataflowWorker.\\n[DEL] String errorMessage = getLullMessage(trackedThread, Duration.millis(millis));\\n[DEL] worker.addFailure(errorMessage);\\n[DEL] }\\n[DEL] \\n     /**\\n      * Take sample is only called by the ExecutionStateSampler thread. It is the only place that\\n      * increments totalMillisInState, however the reporting thread periodically calls extractUpdate',\n",
              " ' import javax.xml.transform.Source;\\n import javax.xml.transform.Transformer;\\n import javax.xml.transform.TransformerConfigurationException;\\n[ADD] import javax.xml.transform.TransformerException;\\n import javax.xml.transform.TransformerFactory;\\n import javax.xml.transform.dom.DOMResult;\\n import javax.xml.transform.dom.DOMSource;\\n import javax.xml.transform.sax.SAXSource;\\n[ADD] import javax.xml.transform.stream.StreamResult;\\n import javax.xml.transform.stream.StreamSource;\\n \\n[ADD] import net.sf.saxon.jaxp.SaxonTransformerFactory;\\n import org.apache.commons.io.output.ByteArrayOutputStream;\\n[DEL] import org.dom4j.io.DOMReader;\\n import org.dom4j.io.DOMWriter;\\n import org.dom4j.io.DocumentSource;\\n import org.w3c.dom.Document;\\n[ADD] import org.w3c.dom.Node;\\n import org.xml.sax.InputSource;\\n \\n /**',\n",
              " ' \\n         // return the list of attachment names\\n         FunctionalTestComponent fc = (FunctionalTestComponent) component;\\n[DEL] fc.setReturnData(message.getOutboundAttachmentNames().toString());\\n[ADD] fc.setReturnData(message.getOutboundAttachmentNames());\\n     }\\n \\n     @Test',\n",
              " ' \\n \\t/**\\n \\t * Returns the total available physical memory on the system in bytes.\\n[DEL] *\\n[ADD] *\\n \\t * @since 1.8\\n \\t * @return the total available physical memory on the system in bytes.\\n \\t */',\n",
              " '     HoodieTimer restoreTimer = new HoodieTimer();\\n     restoreTimer.startTimer();\\n \\n[DEL] // Get all the commits on the timeline after the provided commit time\\n[DEL] List<HoodieInstant> instantsToRollback = table.getActiveTimeline().getWriteTimeline()\\n[DEL] .getReverseOrderedInstants()\\n[DEL] .filter(instant -> HoodieActiveTimeline.GREATER_THAN.test(instant.getTimestamp(), restoreInstantTime))\\n[DEL] .collect(Collectors.toList());\\n[ADD] Option<HoodieInstant> restoreInstant = table.getRestoreTimeline()\\n[ADD] .filterInflightsAndRequested()\\n[ADD] .filter(instant -> instant.getTimestamp().equals(instantTime))\\n[ADD] .firstInstant();\\n[ADD] if (!restoreInstant.isPresent()) {\\n[ADD] throw new HoodieRollbackException(\"No pending restore instants found to execute restore\");\\n[ADD] }\\n[ADD] try {\\n[ADD] List<HoodieInstant> instantsToRollback = getInstantsToRollback(restoreInstant.get());\\n[ADD] ValidationUtils.checkArgument(restoreInstant.get().getState().equals(HoodieInstant.State.REQUESTED)\\n[ADD] || restoreInstant.get().getState().equals(HoodieInstant.State.INFLIGHT));\\n[ADD] Map<String, List<HoodieRollbackMetadata>> instantToMetadata = new HashMap<>();\\n[ADD] if (restoreInstant.get().isRequested()) {\\n[ADD] table.getActiveTimeline().transitionRestoreRequestedToInflight(restoreInstant.get());\\n[ADD] }\\n \\n[DEL] Map<String, List<HoodieRollbackMetadata>> instantToMetadata = new HashMap<>();\\n[DEL] table.getActiveTimeline().createNewInstant(new HoodieInstant(true, HoodieTimeline.RESTORE_ACTION, instantTime));\\n[DEL] instantsToRollback.forEach(instant -> {\\n[DEL] instantToMetadata.put(instant.getTimestamp(), Collections.singletonList(rollbackInstant(instant)));\\n[DEL] LOG.info(\"Deleted instant \" + instant);\\n[DEL] });\\n[ADD] instantsToRollback.forEach(instant -> {\\n[ADD] instantToMetadata.put(instant.getTimestamp(), Collections.singletonList(rollbackInstant(instant)));\\n[ADD] LOG.info(\"Deleted instant \" + instant);\\n[ADD] });\\n \\n[DEL] try {\\n       return finishRestore(instantToMetadata,\\n           instantsToRollback,\\n           restoreTimer.endTimer()\\n       );\\n     } catch (IOException io) {\\n[DEL] throw new HoodieRollbackException(\"unable to rollback instants \" + instantsToRollback, io);\\n[ADD] throw new HoodieRestoreException(\"unable to Restore instant \" + restoreInstant.get(), io);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private List<HoodieInstant> getInstantsToRollback(HoodieInstant restoreInstant) throws IOException {\\n[ADD] List<HoodieInstant> instantsToRollback = new ArrayList<>();\\n[ADD] HoodieRestorePlan restorePlan = RestoreUtils.getRestorePlan(table.getMetaClient(), restoreInstant);\\n[ADD] for (HoodieInstantInfo instantInfo : restorePlan.getInstantsToRollback()) {\\n[ADD] // If restore crashed mid-way, there are chances that some commits are already rolled back,\\n[ADD] // but some are not. so, we can ignore those commits which are fully rolledback in previous attempt if any.\\n[ADD] Option<HoodieInstant> rollbackInstantOpt = table.getActiveTimeline().getWriteTimeline()\\n[ADD] .filter(instant -> instant.getTimestamp().equals(instantInfo.getCommitTime()) && instant.getAction().equals(instantInfo.getAction())).firstInstant();\\n[ADD] if (rollbackInstantOpt.isPresent()) {\\n[ADD] instantsToRollback.add(rollbackInstantOpt.get());\\n[ADD] } else {\\n[ADD] LOG.warn(\"Ignoring already rolledback instant \" + instantInfo.toString());\\n[ADD] }\\n     }\\n[ADD] return instantsToRollback;\\n   }\\n \\n   protected abstract HoodieRollbackMetadata rollbackInstant(HoodieInstant rollbackInstant);',\n",
              " '         return listBuilder.build();\\n     }\\n \\n[DEL] public String getTotalTimerName() {\\n[DEL] return totalTimerName;\\n[ADD] public String getConditionHitsCounterName() {\\n[ADD] return conditionHitsCounterName;\\n[ADD] }\\n[ADD] \\n[ADD] public String getConditionMissesCounterName() {\\n[ADD] return conditionMissesCounterName;\\n[ADD] }\\n[ADD] \\n[ADD] public String getConditionTimerName() {\\n[ADD] return conditionTimerName;\\n[ADD] }\\n[ADD] \\n[ADD] public String getExecutionTimerName() {\\n[ADD] return executionTimerName;\\n     }\\n \\n     public String getConverterTimerName() {\\n         return converterTimerName;\\n     }\\n \\n[ADD] public String getCompleteTimerName() {\\n[ADD] return completeTimerName;\\n[ADD] }\\n[ADD] \\n     public long getExceptionCount() {\\n         return exceptions.get();\\n     }',\n",
              " '     return contextMimeType.orElse(mediaType).withCharset(contextEncoding.orElse(existingEncoding));\\n   }\\n \\n[ADD] private Runnable incrementOpenedStream(ConfigurationInstance config) {\\n[ADD] return () -> tryToMutateConfigurationStats(config, (MutableConfigurationStats::addOpenedStream));\\n[ADD] }\\n[ADD] \\n[ADD] private Runnable decrementOpenedStream(ConfigurationInstance config) {\\n[ADD] return () -> tryToMutateConfigurationStats(config, (MutableConfigurationStats::discountOpenedStream));\\n[ADD] }\\n[ADD] \\n   protected class ConnectedInputStreamWrapper extends ProxyInputStream {\\n \\n     private final ConnectionHandler<?> connectionHandler;\\n[ADD] private Runnable onClose;\\n \\n[DEL] private ConnectedInputStreamWrapper(InputStream delegate, ConnectionHandler<?> connectionHandler) {\\n[ADD] private ConnectedInputStreamWrapper(InputStream delegate, ConnectionHandler<?> connectionHandler, Runnable onCreate,\\n[ADD] Runnable onClose) {\\n       super(delegate);\\n       this.connectionHandler = connectionHandler;\\n[ADD] this.onClose = onClose;\\n[ADD] onCreate.run();\\n     }\\n \\n     /**',\n",
              " '          @Override\\n          public void execute()\\n          {\\n[DEL] saveThenExecute(null, renderCommand);\\n[ADD] saveThenExecute(null, true, renderCommand);\\n          }\\n       };\\n ',\n",
              " ' \\n package org.apache.hudi.avro;\\n \\n[ADD] import org.apache.avro.specific.SpecificRecordBase;\\n import org.apache.hudi.common.model.HoodieRecord;\\n import org.apache.hudi.common.util.StringUtils;\\n import org.apache.hudi.common.util.collection.Pair;',\n",
              " '   public void should_use_aar_library() {\\n     SonarScanner scanner = aarProjectSonarScanner();\\n     scanner.setProperty(\"sonar.java.libraries\", aarPath);\\n[ADD] TestUtils.provisionProject(ORCHESTRATOR, PROJECT_KEY_AAR,PROJECT_KEY_DIT,\"java\",\"using-aar-dep\");\\n     ORCHESTRATOR.executeBuild(scanner);\\n     assertThat(getNumberOfViolations(PROJECT_KEY_AAR)).isEqualTo(1);\\n   }',\n",
              " '     for (Map.Entry<String, String> entry : map.entrySet()) {\\n       args[i++] = \"--\" + entry.getKey() + \"=\" + entry.getValue();\\n     }\\n[DEL] PipelineOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().create();\\n[ADD] \\n[ADD] PipelineOptions options = null;\\n[ADD] try {\\n[ADD] options = PipelineOptionsFactory.fromArgs(args).withValidation().create();\\n[ADD] } catch (IllegalArgumentException e) {\\n[ADD] Matcher matcher =\\n[ADD] Pattern.compile(\"Class (.+) missing a property named \\\\\\\\\\'(.+)\\\\\\\\\\'\\\\\\\\.(.*)\")\\n[ADD] .matcher(e.getMessage());\\n[ADD] if (matcher.matches()) {\\n[ADD] throw new IllegalArgumentException(\\n[ADD] String.format(\\n[ADD] \"You may have SET an unregistered option \\'%s\\', which could have failed current \"\\n[ADD] + \"query. You could run \\'RESET %s\\' in Shell to reset the option.\",\\n[ADD] matcher.group(2), matcher.group(2)));\\n[ADD] } else {\\n[ADD] throw e;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n     options.as(ApplicationNameOptions.class).setAppName(\"BeamSql\");\\n     return options;\\n   }',\n",
              " '                 {\\n                     try\\n                     {\\n[DEL] beanDefinitionType = Class.forName(cdm.getBeanDefinition().getBeanClassName());\\n[ADD] String beanClassName = cdm.getBeanDefinition().getBeanClassName();\\n[ADD] if (beanClassName != null)\\n[ADD] {\\n[ADD] beanDefinitionType = forName(beanClassName);\\n[ADD] }\\n[ADD] else\\n[ADD] {\\n[ADD] //Happens in case of spring:property\\n[ADD] beanDefinitionType = Object.class;\\n[ADD] }\\n                     }\\n                     catch (ClassNotFoundException e)\\n                     {\\n[DEL] throw new RuntimeException(e);\\n[ADD] beanDefinitionType = Object.class;\\n                     }\\n                 }\\n             }',\n",
              " '   @Test\\n   public void testNotSendiningCorrelationIDWithTemporaryQueue() throws Exception {\\n     InternalMessage response =\\n[DEL] flowRunner(\"JMSNoCorrelationIDTemporaryQueue\").nonBlocking().withPayload(TEST_MESSAGE).run().getMessage();\\n[ADD] flowRunner(\"JMSNoCorrelationIDTemporaryQueue\").withPayload(TEST_MESSAGE).run().getMessage();\\n     verify(\"JMSNoCorrelationIDTemporaryQueue\");\\n     verify(\"JMSNoCorrelationIDTarget\");\\n     assertEchoResponse(response);',\n",
              " ' \\t\\tMountPath: \"/pach-bin\",\\n \\t})\\n \\n[ADD] return &jobOptions{\\n[ADD] labels:             labels,\\n[ADD] parallelism:        parallelism,\\n[ADD] userImage:          userImage,\\n[ADD] jobShimImage:       jobShimImage,\\n[ADD] jobImagePullPolicy: jobImagePullPolicy,\\n[ADD] jobEnv:             jobEnv,\\n[ADD] volumes:            volumes,\\n[ADD] volumeMounts:       volumeMounts,\\n[ADD] }, nil\\n[ADD] }\\n[ADD] \\n[ADD] func service(kubeClient *kube.Client, jobInfo *persist.JobInfo, jobShimImage string, jobImagePullPolicy string, internalPort int32, externalPort int32) (*api.ReplicationController, *api.Service, error) {\\n[ADD] \\n[ADD] options, err := getJobOptions(kubeClient, jobInfo, jobShimImage, jobImagePullPolicy)\\n[ADD] if err != nil {\\n[ADD] return nil, nil, err\\n[ADD] }\\n[ADD] rc := &api.ReplicationController{\\n[ADD] TypeMeta: unversioned.TypeMeta{\\n[ADD] Kind:       \"ReplicationController\",\\n[ADD] APIVersion: \"v1\",\\n[ADD] },\\n[ADD] ObjectMeta: api.ObjectMeta{\\n[ADD] Name:   jobInfo.JobID,\\n[ADD] Labels: options.labels,\\n[ADD] },\\n[ADD] Spec: api.ReplicationControllerSpec{\\n[ADD] Selector: options.labels,\\n[ADD] Replicas: options.parallelism,\\n[ADD] Template: &api.PodTemplateSpec{\\n[ADD] ObjectMeta: api.ObjectMeta{\\n[ADD] Name:   jobInfo.JobID,\\n[ADD] Labels: options.labels,\\n[ADD] },\\n[ADD] Spec: api.PodSpec{\\n[ADD] InitContainers: []api.Container{\\n[ADD] {\\n[ADD] Name:            \"init\",\\n[ADD] Image:           options.jobShimImage,\\n[ADD] Command:         []string{\"/pach/job-shim.sh\"},\\n[ADD] ImagePullPolicy: api.PullPolicy(options.jobImagePullPolicy),\\n[ADD] Env:             options.jobEnv,\\n[ADD] VolumeMounts:    options.volumeMounts,\\n[ADD] },\\n[ADD] },\\n[ADD] Containers: []api.Container{\\n[ADD] {\\n[ADD] Name:    \"user\",\\n[ADD] Image:   options.userImage,\\n[ADD] Command: []string{\"/pach-bin/guest.sh\", jobInfo.JobID},\\n[ADD] SecurityContext: &api.SecurityContext{\\n[ADD] Privileged: &trueVal, // god is this dumb\\n[ADD] },\\n[ADD] ImagePullPolicy: api.PullPolicy(options.jobImagePullPolicy),\\n[ADD] Env:             options.jobEnv,\\n[ADD] VolumeMounts:    options.volumeMounts,\\n[ADD] },\\n[ADD] },\\n[ADD] RestartPolicy: \"Always\",\\n[ADD] Volumes:       options.volumes,\\n[ADD] },\\n[ADD] },\\n[ADD] },\\n[ADD] }\\n[ADD] service := &api.Service{\\n[ADD] TypeMeta: unversioned.TypeMeta{\\n[ADD] Kind:       \"Service\",\\n[ADD] APIVersion: \"v1\",\\n[ADD] },\\n[ADD] ObjectMeta: api.ObjectMeta{\\n[ADD] Name:   fmt.Sprintf(\"pach-%v\", jobInfo.JobID),\\n[ADD] Labels: options.labels,\\n[ADD] },\\n[ADD] Spec: api.ServiceSpec{\\n[ADD] Type:     api.ServiceTypeNodePort,\\n[ADD] Selector: options.labels,\\n[ADD] Ports: []api.ServicePort{\\n[ADD] {\\n[ADD] Port:     internalPort,\\n[ADD] Name:     \"user-service-port\",\\n[ADD] NodePort: externalPort,\\n[ADD] },\\n[ADD] },\\n[ADD] },\\n[ADD] }\\n[ADD] return rc, service, nil\\n[ADD] }\\n[ADD] \\n[ADD] // Convert a persist.JobInfo into a Kubernetes batch.Job spec\\n[ADD] func job(kubeClient *kube.Client, jobInfo *persist.JobInfo, jobShimImage string, jobImagePullPolicy string) (*batch.Job, error) {\\n[ADD] options, err := getJobOptions(kubeClient, jobInfo, jobShimImage, jobImagePullPolicy)\\n[ADD] if err != nil {\\n[ADD] return nil, err\\n[ADD] }\\n \\treturn &batch.Job{\\n \\t\\tTypeMeta: unversioned.TypeMeta{\\n \\t\\t\\tKind:       \"Job\",',\n",
              " '  */\\n package aQute.lib.filter;\\n \\n[ADD] import java.lang.invoke.MethodHandle;\\n[ADD] import java.lang.invoke.MethodHandles;\\n[ADD] import java.lang.invoke.MethodType;\\n import java.lang.reflect.Array;\\n import java.lang.reflect.Constructor;\\n import java.math.BigDecimal;\\n import java.math.BigInteger;\\n[ADD] import java.util.ArrayList;\\n import java.util.Collection;\\n import java.util.Dictionary;\\n[DEL] import java.util.Hashtable;\\n[ADD] import java.util.List;\\n import java.util.Map;\\n \\n public class Filter {\\n[DEL] final char\\t\\t\\tWILDCARD\\t= 65535;\\n[ADD] \\n[ADD] // eliminate reflection on bnd/osgi Version during resolver operations\\n[ADD] static Class<?>\\t\\tBND_VERSION;\\n[ADD] static MethodHandle\\tBND_parseVersion_MH;\\n[ADD] static Class<?>\\t\\tOSGI_VERSION;\\n[ADD] static MethodHandle\\tOSGI_valueOf_MH;\\n[ADD] static {\\n[ADD] MethodHandles.Lookup publicLookup = MethodHandles.publicLookup();\\n[ADD] try {\\n[ADD] BND_VERSION = Class.forName(\"aQute.bnd.version.Version\");\\n[ADD] MethodType mt = MethodType.methodType(BND_VERSION, String.class);\\n[ADD] BND_parseVersion_MH = publicLookup.findStatic(BND_VERSION, \"parseVersion\", mt);\\n[ADD] } catch (ClassNotFoundException | IllegalAccessException | NoSuchMethodException e) {\\n[ADD] BND_VERSION = null;\\n[ADD] BND_parseVersion_MH = null;\\n[ADD] }\\n[ADD] try {\\n[ADD] OSGI_VERSION = Class.forName(\"org.osgi.framework.Version\");\\n[ADD] MethodType mt = MethodType.methodType(OSGI_VERSION, String.class);\\n[ADD] OSGI_valueOf_MH = publicLookup.findStatic(OSGI_VERSION, \"valueOf\", mt);\\n[ADD] } catch (ClassNotFoundException | IllegalAccessException | NoSuchMethodException e) {\\n[ADD] OSGI_VERSION = null;\\n[ADD] OSGI_valueOf_MH = null;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] static final String\\t\\t\\t\\tGARBAGE\\t\\t= \"Trailing garbage\";\\n[ADD] static final String\\t\\t\\t\\tMALFORMED\\t= \"Malformed query\";\\n[ADD] static final String\\t\\t\\t\\tEMPTY\\t\\t= \"Empty list\";\\n[ADD] static final String\\t\\t\\t\\tSUBEXPR\\t\\t= \"No subexpression\";\\n[ADD] static final String\\t\\t\\t\\tOPERATOR\\t= \"Undefined operator\";\\n[ADD] static final String\\t\\t\\t\\tTRUNCATED\\t= \"Truncated expression\";\\n[ADD] static final String\\t\\t\\t\\tEQUALITY\\t= \"Only equality supported\";\\n[ADD] \\n[ADD] final static char\\t\\t\\t\\tWILDCARD\\t= 65535;\\n \\n \\tfinal static int\\tEQ\\t\\t\\t= 0;\\n \\tfinal static int\\tLE\\t\\t\\t= 1;',\n",
              " '           new Path(config.getBasePath()), FSUtils.getPartitionPath(config.getBasePath(), partitionPath));\\n       partitionMetadata.trySave(getPartitionId());\\n       createMarkerFile(partitionPath, FSUtils.makeDataFileName(this.instantTime, this.writeToken, this.fileId, hoodieTable.getBaseFileExtension()));\\n[DEL] this.fileWriter = HoodieFileWriterFactory.getFileWriter(instantTime, path, hoodieTable, config, writerSchemaWithMetafields, this.taskContextSupplier);\\n[ADD] this.fileWriter = HoodieFileWriterFactory.getFileWriter(instantTime, path, hoodieTable, config,\\n[ADD] writeSchemaWithMetaFields, this.taskContextSupplier);\\n     } catch (IOException e) {\\n       throw new HoodieInsertException(\"Failed to initialize HoodieStorageWriter for path \" + path, e);\\n     }',\n",
              " ' #ifdef INTL_ICU\\n     typedef const char * (*GetAvailableLocaleFunc)(int);\\n     typedef int (*CountAvailableLocaleFunc)(void);\\n[DEL] static bool findLocale(const char *langtag, CountAvailableLocaleFunc countAvailable, GetAvailableLocaleFunc getAvailable)\\n[ADD] static bool findLocale(JavascriptString *langtag, CountAvailableLocaleFunc countAvailable, GetAvailableLocaleFunc getAvailable)\\n     {\\n[DEL] UErrorCode status = U_ZERO_ERROR;\\n         char localeID[ULOC_FULLNAME_CAPACITY] = { 0 };\\n[DEL] int32_t length = 0;\\n[DEL] uloc_forLanguageTag(langtag, localeID, ULOC_FULLNAME_CAPACITY, &length, &status);\\n[DEL] ICU_ASSERT(status, length > 0);\\n[ADD] BCP47_TO_ICU(langtag->GetSz(), langtag->GetLength(), localeID, ULOC_FULLNAME_CAPACITY);\\n[ADD] \\n[ADD] // TODO(jahorto): can we binary search this instead?\\n         for (int i = 0; i < countAvailable(); i++)\\n         {\\n             if (strcmp(localeID, getAvailable(i)) == 0)',\n",
              " ' \\n                 }\\n \\n[ADD] dataMan.forceIndexChanges();\\n                 result.totalMetadata++;\\n             } catch (Throwable t) {\\n                 errors.add(new HarvestError(this.context, t));\\n                 log.error(\"Unable to process record from csw (\" + this.params.getName() + \")\");\\n                 log.error(\"   Record failed: \" +  ri.uuid + \". Error is: \" + t.getMessage());\\n[ADD] log.error(t);\\n             } finally {\\n                 result.originalMetadata++;\\n             }\\n         }\\n[DEL] dataMan.forceIndexChanges();\\n[ADD] }\\n \\n[DEL] log.debug(\"End of alignment for : \" + params.getName());\\n[ADD] /**\\n[ADD] * Remove records no longer on the remote CSW server\\n[ADD] *\\n[ADD] * @param records\\n[ADD] * @throws Exception\\n[ADD] */\\n[ADD] @Transactional(value=TxType.REQUIRES_NEW)\\n[ADD] public HarvestResult cleanupRemovedRecords(Set<String> records) throws Exception {\\n[ADD] \\n[ADD] if (cancelMonitor.get()) {\\n[ADD] return result;\\n[ADD] }\\n \\n[ADD] for (String uuid : localUuids.getUUIDs()) {\\n[ADD] if (!records.contains(uuid)) {\\n[ADD] String id = localUuids.getID(uuid);\\n[ADD] log.debug(\"  - Removing old metadata with local id:\" + id);\\n[ADD] metadataManager.deleteMetadata(context, id);\\n[ADD] result.locallyRemoved++;\\n[ADD] }\\n[ADD] }\\n[ADD] dataMan.forceIndexChanges();\\n[ADD] \\n         return result;\\n[DEL] }\\n[ADD] }\\n \\n \\n     private void addMetadata(RecordInfo ri, Integer ownerId, Integer groupId, String uuid) throws Exception {',\n",
              " '     }\\n \\n     private Optional<IndexedRecord> getIndexedRecord(HoodieRecord<T> hoodieRecord) {\\n[ADD] Optional recordMetadata = hoodieRecord.getData().getMetadata();\\n         try {\\n             Optional<IndexedRecord> avroRecord = hoodieRecord.getData().getInsertValue(schema);\\n ',\n",
              " '     SonarScanner scanner = ditProjectSonarScanner();\\n     scanner.setProperty(\"sonar.java.binaries\", \"target/classes\");\\n     scanner.setProperty(\"sonar.java.libraries\", guavaJarPath);\\n[ADD] TestUtils.provisionProject(ORCHESTRATOR, PROJECT_KEY_DIT,PROJECT_KEY_DIT,\"java\",\"dit-check\");\\n     ORCHESTRATOR.executeBuild(scanner);\\n     assertThat(getNumberOfViolations(PROJECT_KEY_DIT)).isEqualTo(2);\\n   }',\n",
              " ' @Rule(\\n   key = \"S1145\",\\n   name = \"\\\\\"if\\\\\" statement conditions should not unconditionally evaluate to \\\\\"true\\\\\" or to \\\\\"false\\\\\"\",\\n[DEL] tags = {\"bug\", \"cwe\", \"misra\", \"security\"},\\n[DEL] priority = Priority.MAJOR)\\n[DEL] @ActivatedByDefault\\n[ADD] priority = Priority.MAJOR,\\n[ADD] status = \"DEPRECATED\")\\n @SqaleSubCharacteristic(RulesDefinition.SubCharacteristics.READABILITY)\\n @SqaleConstantRemediation(\"2min\")\\n public class IfConditionAlwaysTrueOrFalseCheck extends SubscriptionBaseVisitor {',\n",
              " '       task.shutdown();\\n     }\\n \\n[DEL] for (Task task: this.tasks) {\\n[ADD] for (Task task : this.tasks) {\\n       task.awaitShutdown(1000);\\n     }\\n[ADD] \\n[ADD] for (Task task : this.tasks) {\\n[ADD] if (task.cancel()) {\\n[ADD] log.info(\"Task {} cancelled.\", task.getTaskId());\\n[ADD] } else {\\n[ADD] log.info(\"Task {} could not be cancelled.\", task.getTaskId());\\n[ADD] }\\n[ADD] }\\n   }\\n \\n   private void persistTaskStateStore()',\n",
              " '      * Create a new {@link InProcessBundle} for the specified {@link PCollection} without a key.\\n      */\\n     public static <T> InProcessBundle<T> unkeyed(PCollection<T> pcollection) {\\n[DEL] return new InProcessBundle<T>(pcollection, false, null);\\n[ADD] return new InProcessBundle<T>(pcollection, null);\\n     }\\n \\n     /**\\n      * Create a new {@link InProcessBundle} for the specified {@link PCollection} with the specified\\n      * key.\\n[DEL] *\\n[DEL] * <p>See {@link CommittedBundle#getKey()} and {@link CommittedBundle#isKeyed()} for more\\n[DEL] * information.\\n      */\\n     public static <T> InProcessBundle<T> keyed(PCollection<T> pcollection, Object key) {\\n[DEL] return new InProcessBundle<T>(pcollection, true, key);\\n[ADD] return new InProcessBundle<T>(pcollection, key);\\n     }\\n \\n[DEL] private InProcessBundle(PCollection<T> pcollection, boolean keyed, Object key) {\\n[ADD] private InProcessBundle(PCollection<T> pcollection, Object key) {\\n       this.pcollection = pcollection;\\n[DEL] this.keyed = keyed;\\n       this.key = key;\\n       this.elements = ImmutableList.builder();\\n     }',\n",
              " ' import org.mule.runtime.api.meta.model.parameter.ParameterGroupModel;\\n import org.mule.runtime.api.meta.model.parameter.ParameterModel;\\n import org.mule.runtime.api.meta.model.parameter.ParameterizedModel;\\n[ADD] import org.mule.runtime.api.metadata.DataType;\\n[ADD] import org.mule.runtime.api.metadata.TypedValue;\\n import org.mule.runtime.app.declaration.api.ParameterElementDeclaration;\\n import org.mule.runtime.app.declaration.api.ParameterGroupElementDeclaration;\\n import org.mule.runtime.app.declaration.api.ParameterizedElementDeclaration;',\n",
              " '       verify(contextSelector).getContext(LOGGER_NAME, regionClassLoader, true);\\n     });\\n   }\\n[ADD] \\n[ADD] @Test\\n[ADD] public void when_RecursiveLoggerContextInstantiationException_expect_fallback_dispatch_using_system_classloader() {\\n[ADD] // Expected Loggers\\n[ADD] Logger currentClassLoaderLogger = mock(Logger.class);\\n[ADD] Logger regionClassLoaderLogger = mock(Logger.class);\\n[ADD] when(loggerContext.getLogger(any(), any())).thenReturn(currentClassLoaderLogger);\\n[ADD] when(regionClassLoaderLoggerContext.getLogger(any(), any())).thenReturn(regionClassLoaderLogger);\\n[ADD] when(artifactAwareContextSelector.getContextWithResolvedContextClassLoader(currentClassLoader))\\n[ADD] .thenAnswer(invocation -> loggerContext);\\n[ADD] // Triggers of the expected Loggers\\n[ADD] when(artifactAwareContextSelector.getContextWithResolvedContextClassLoader(regionClassLoader))\\n[ADD] .thenThrow(RecursiveLoggerContextInstantiationException.class)\\n[ADD] .thenAnswer(invocation -> regionClassLoaderLoggerContext);\\n[ADD] // Class under test\\n[ADD] DispatchingLogger dispatchingLogger = new DispatchingLogger(originalLogger, currentClassLoader.hashCode(),\\n[ADD] loggerContext, artifactAwareContextSelector, messageFactory) {\\n[ADD] \\n[ADD] @Override\\n[ADD] public String getName() {\\n[ADD] return LOGGER_NAME;\\n[ADD] }\\n[ADD] };\\n[ADD] // Test and assertions\\n[ADD] withContextClassLoader(regionClassLoader, () -> {\\n[ADD] dispatchingLogger.info(\"Fallback Test Message\");\\n[ADD] dispatchingLogger.info(\"Test Message\");\\n[ADD] });\\n[ADD] verify(currentClassLoaderLogger, times(1)).info(\"Fallback Test Message\");\\n[ADD] verify(regionClassLoaderLogger, times(1)).info(\"Test Message\");\\n[ADD] }\\n[ADD] \\n }',\n",
              " '     line_dash=[4, 4]\\n )\\n \\n[ADD] \\n[ADD] class MeasureTool(Drag, Inspection):\\n[ADD] names = List(String, help=\"\"\"\\n[ADD] A list of names to query for. If set, only renderers that have a matching\\n[ADD] value for their ``name`` attribute will be used.\\n[ADD] \"\"\")\\n[ADD] \\n[ADD] renderers = Either(Auto, List(Instance(Renderer)), default=\"auto\", help=\"\"\"\\n[ADD] An explicit list of renderers to hit test against. If unset,\\n[ADD] defaults to all renderers on a plot.\\n[ADD] \"\"\")\\n[ADD] \\n[ADD] \\n[ADD] overlay = Instance(BoxAnnotation, default=DEFAULT_POLY_OVERLAY, help=\"\"\"\\n[ADD] A shaded annotation drawn to indicate the selection region.\\n[ADD] \"\"\")\\n[ADD] \\n[ADD] origin = Enum(\"corner\", \"center\", default=\"corner\", help=\"\"\"\\n[ADD] Indicates whether the rectangular selection area should originate from a corner\\n[ADD] (top-left or bottom-right depending on direction) or the center of the box.\\n[ADD] \"\"\")\\n[ADD] \\n[ADD] attachment = Enum(TooltipAttachment, help=\"\"\"\\n[ADD] Whether the tooltip should be displayed to the left or right of the cursor\\n[ADD] position or above or below it, or if it should be automatically placed\\n[ADD] in the horizontal or vertical dimension.\\n[ADD] \"\"\")\\n[ADD] \\n[ADD] show_arrow = Bool(default=True, help=\"\"\"\\n[ADD] Whether tooltip\\'s arrow should be shown.\\n[ADD] \"\"\")\\n[ADD] \\n[ADD] tooltips = Either(String, List(Tuple(String, String)),\\n[ADD] default=[\\n[ADD] (\"delta_x\", \"$dis[0]\"),\\n[ADD] (\"delta_y\", \"$dis[1]\"),\\n[ADD] ], help=\"\"\"\\n[ADD] The (name, field) pairs describing what the hover tool should\\n[ADD] display when there is a hit.\"\"\"\\n[ADD] )\\n[ADD] \\n class LassoSelectTool(Drag, SelectTool):\\n     \\'\\'\\' *toolbar icon*: |lasso_select_icon|\\n ',\n",
              " '     return span;\\n   }\\n \\n[ADD] /**\\n[ADD] * Creates new scoped context with the given span.\\n[ADD] *\\n[ADD] * <p>Attaches new context to the request to avoid creating duplicate server spans.\\n[ADD] */\\n[ADD] public Scope startScope(Span span, STORAGE storage) {\\n[ADD] // TODO we could do this in one go, but TracingContextUtils.CONTEXT_SPAN_KEY is private\\n[ADD] Context serverSpanContext = Context.current().withValue(CONTEXT_SERVER_SPAN_KEY, span);\\n[ADD] Context newContext = withSpan(span, serverSpanContext);\\n[ADD] attachServerContext(newContext, storage);\\n[ADD] return withScopedContext(newContext);\\n[ADD] }\\n[ADD] \\n[ADD] // TODO should end methods remove SPAN attribute from request as well?\\n[ADD] public void end(Span span, int responseStatus) {\\n[ADD] setStatus(span, responseStatus);\\n[ADD] span.end();\\n[ADD] }\\n[ADD] \\n[ADD] /** Ends given span exceptionally with default response status code 500. */\\n[ADD] public void endExceptionally(Span span, Throwable throwable) {\\n[ADD] endExceptionally(span, throwable, 500);\\n[ADD] }\\n[ADD] \\n[ADD] public void endExceptionally(Span span, Throwable throwable, int responseStatus) {\\n[ADD] if (responseStatus == 200) {\\n[ADD] // TODO I think this is wrong.\\n[ADD] // We must report that response status that was actually sent to end user\\n[ADD] // We may change span status, but not http_status attribute\\n[ADD] responseStatus = 500;\\n[ADD] }\\n[ADD] onError(span, unwrapThrowable(throwable));\\n[ADD] end(span, responseStatus);\\n[ADD] }\\n[ADD] \\n[ADD] public Span getCurrentSpan() {\\n[ADD] return tracer.getCurrentSpan();\\n[ADD] }\\n[ADD] \\n[ADD] public Span getServerSpan(STORAGE storage) {\\n[ADD] Context attachedContext = getServerContext(storage);\\n[ADD] return attachedContext == null ? null : CONTEXT_SERVER_SPAN_KEY.get(attachedContext);\\n[ADD] }\\n[ADD] /**\\n[ADD] * Returns context stored to the given request-response-loop storage by {@link\\n[ADD] * #attachServerContext(Context, STORAGE)}.\\n[ADD] *\\n[ADD] * <p>May be null.\\n[ADD] */\\n[ADD] public abstract Context getServerContext(STORAGE storage);\\n[ADD] \\n   protected void onConnection(Span span, CONNECTION connection) {\\n     SemanticAttributes.NET_PEER_IP.set(span, peerHostIP(connection));\\n     final Integer port = peerPort(connection);',\n",
              " ' import org.mule.module.http.internal.listener.async.ResponseStatusCallback;\\n \\n import java.io.ByteArrayInputStream;\\n[ADD] import java.util.HashMap;\\n import java.util.Map;\\n \\n import org.slf4j.Logger;',\n",
              " ' import javax.ws.rs.core.MediaType;\\n import java.io.IOException;\\n import java.util.Collections;\\n[DEL] import java.util.List;\\n import java.util.Map;\\n import java.util.Set;\\n import java.util.stream.Collectors;',\n",
              " ' \\n /**\\n  * Use the registryLock when reading/writing/iterating over the contents of the registry hashmap.\\n[ADD] * @deprecated as of 3.7.0. Use {@link SimpleRegistry instead}.\\n  */\\n[DEL] //@ThreadSafe\\n[ADD] @Deprecated\\n public class TransientRegistry extends AbstractRegistry\\n {\\n ',\n",
              " '         this.dagManagerThreads = new DagManagerThread[numThreads];\\n         for (int i = 0; i < numThreads; i++) {\\n           DagManagerThread dagManagerThread = new DagManagerThread(jobStatusRetriever, dagStateStore, failedDagStateStore,\\n[DEL] queue[i], cancelQueue[i], resumeQueue[i], instrumentationEnabled, defaultQuota, perUserQuota, failedDagIds,\\n[ADD] runQueue[i], cancelQueue[i], resumeQueue[i], instrumentationEnabled, defaultQuota, perUserQuota, failedDagIds,\\n               allSuccessfulMeter, allFailedMeter);\\n           this.dagManagerThreads[i] = dagManagerThread;\\n           this.scheduledExecutorPool.scheduleAtFixedRate(dagManagerThread, 0, this.pollingInterval, TimeUnit.SECONDS);',\n",
              " '       asList(\"ordered\", \"insert\", \"count\", \"find\", \"create\");\\n \\n   private JsonWriterSettings createJsonWriterSettings(int maxNormalizedQueryLength) {\\n[DEL] JsonWriterSettings settings = new JsonWriterSettings(false);\\n[ADD] JsonWriterSettings settings = null;\\n     try {\\n       // The static JsonWriterSettings.builder() method was introduced in the 3.5 release\\n       Optional<Method> buildMethod =',\n",
              " ' import static java.util.Optional.of;\\n import static org.mockito.Answers.RETURNS_DEEP_STUBS;\\n import static org.mockito.ArgumentMatchers.any;\\n[ADD] import static org.mockito.ArgumentMatchers.anyString;\\n import static org.mockito.Mockito.mock;\\n[ADD] import static org.mockito.Mockito.times;\\n[ADD] import static org.mockito.Mockito.verify;\\n import static org.mockito.Mockito.when;\\n import static org.mule.metadata.api.model.MetadataFormat.JAVA;\\n import static org.mule.runtime.api.meta.model.parameter.ParameterGroupModel.DEFAULT_GROUP_NAME;',\n",
              " ' \\t}\\n \\n \\tpublic CompositeFileListFilter<F> addFilter(FileListFilter<F> filter) {\\n[DEL] return this.addFilters(Collections.singletonList(filter));\\n[ADD] return addFilters(Collections.singletonList(filter));\\n \\t}\\n \\n \\t/**',\n",
              " '             throw new InvalidParameterValueException(\"Vm \" + userVm + \" should be stopped to do SSH Key reset\");\\n         }\\n \\n[DEL] SSHKeyPairVO s = _sshKeyPairDao.findByName(owner.getAccountId(), owner.getDomainId(), cmd.getName());\\n[DEL] if (s == null) {\\n[DEL] throw new InvalidParameterValueException(\"A key pair with name \\'\" + cmd.getName() + \"\\' does not exist for account \" + owner.getAccountName()\\n[DEL] + \" in specified domain id\");\\n[ADD] String keypairnames = \"\";\\n[ADD] \\n[ADD] List<SSHKeyPairVO> s_list = null;\\n[ADD] \\n[ADD] if (cmd.getNames() != null) {\\n[ADD] s_list = _sshKeyPairDao.findByNames(owner.getAccountId(), owner.getDomainId(), cmd.getNames());\\n[ADD] keypairnames = String.join(\", \", cmd.getNames());\\n[ADD] }\\n[ADD] \\n[ADD] if (s_list == null) {\\n[ADD] throw new InvalidParameterValueException(\"Any key pair with the given names does not exist for account \" + owner.getAccountName()\\n[ADD] + \" in specified domain id\");\\n         }\\n \\n         _accountMgr.checkAccess(caller, null, true, userVm);\\n \\n[DEL] String sshPublicKey = s.getPublicKey();\\n[ADD] String sshPublicKey = \"\";\\n \\n[DEL] boolean result = resetVMSSHKeyInternal(vmId, sshPublicKey);\\n[ADD] if (s_list != null) {\\n[ADD] for (SSHKeyPairVO s_each : s_list) {\\n[ADD] String publicKey = s_each.getPublicKey();\\n[ADD] sshPublicKey = sshPublicKey + publicKey;\\n[ADD] sshPublicKey = sshPublicKey + \"\\\\n\";\\n[ADD] s_logger.info(\"the public key for keypair name \" + s_each.getName() + \" is \" + publicKey);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] boolean result = resetVMSSHKeyInternal(vmId, sshPublicKey, keypairnames);\\n \\n[ADD] UserVmVO vm = _vmDao.findById(vmId);\\n[ADD] _vmDao.loadDetails(vm);\\n         if (!result) {\\n             throw new CloudRuntimeException(\"Failed to reset SSH Key for the virtual machine \");\\n         }\\n \\n[DEL] removeEncryptedPasswordFromUserVmVoDetails(userVm);\\n[ADD] removeEncryptedPasswordFromUserVmVoDetails(vm);\\n \\n[DEL] return userVm;\\n[ADD] return vm;\\n     }\\n \\n     protected void removeEncryptedPasswordFromUserVmVoDetails(UserVmVO userVmVo) {',\n",
              " '     public void setup() throws IllegalArgumentException,\\n             IllegalAccessException, NoSuchFieldException, SecurityException {\\n         highAvailabilityManager = new HighAvailabilityManagerImpl();\\n[DEL] for (Field injectField : HighAvailabilityManagerImpl.class\\n[DEL] .getDeclaredFields()) {\\n[ADD] for (Field injectField : HighAvailabilityManagerImpl.class.getDeclaredFields()) {\\n             if (injectField.isAnnotationPresent(Inject.class)) {\\n                 injectField.setAccessible(true);\\n[DEL] injectField.set(highAvailabilityManager, this.getClass()\\n[DEL] .getDeclaredField(injectField.getName()).get(this));\\n[ADD] injectField.set(highAvailabilityManager, this.getClass().getDeclaredField(injectField.getName()).get(this));\\n[ADD] } else if (injectField.getName().equals(\"_workers\")) {\\n[ADD] injectField.setAccessible(true);\\n[ADD] for (Class<?> clz : HighAvailabilityManagerImpl.class.getDeclaredClasses()) {\\n[ADD] if (clz.getName().equals(\"com.cloud.ha.HighAvailabilityManagerImpl$WorkerThread\")) {\\n[ADD] Object obj = Array.newInstance(clz, 0);\\n[ADD] injectField.set(highAvailabilityManager, obj);\\n[ADD] }\\n[ADD] }\\n             }\\n         }\\n     }',\n",
              " ' public class HoodieWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieClient {\\n \\n   private static Logger logger = LogManager.getLogger(HoodieWriteClient.class);\\n[ADD] private static final String UPDATE_STR = \"update\";\\n[ADD] private static final String LOOKUP_STR = \"lookup\";\\n   private final boolean rollbackInFlight;\\n   private final transient HoodieMetrics metrics;\\n   private final transient HoodieIndex<T> index;',\n",
              " ' @Alpha\\n public class HivePartition extends HiveRegistrationUnit {\\n \\n[ADD] private final List<String> values;\\n[ADD] \\n   private HivePartition(Builder builder) {\\n     super(builder);\\n     this.values = ImmutableList.<String> copyOf(builder.values);\\n   }\\n \\n[DEL] private List<String> values;\\n[DEL] \\n   public static class Builder extends HiveRegistrationUnit.Builder<Builder> {\\n \\n     private List<String> values = Lists.newArrayList();',\n",
              " ' \\t\\tthis.classLoader = classLoader;\\n \\t}\\n \\n[ADD] @Override\\n[ADD] public void setApplicationEventPublisher(ApplicationEventPublisher applicationEventPublisher) {\\n[ADD] this.applicationEventPublisher = applicationEventPublisher;\\n[ADD] }\\n[ADD] \\n \\t/*\\n \\t * Lifecycle implementation\\n \\t */',\n",
              " '     @Override\\n     public MetadataReport getMetadataReport(URL url) {\\n         url = url.setPath(MetadataReport.class.getName())\\n[DEL] .addParameter(Constants.INTERFACE_KEY, MetadataReport.class.getName())\\n                 .removeParameters(Constants.EXPORT_KEY, Constants.REFER_KEY);\\n         String key = url.toServiceString();\\n         // Lock the registry access process to ensure a single instance of the registry',\n",
              " '     ResourceManager resourceManager;\\n     @Inject\\n     private AnnotationDao annotationDao;\\n[ADD] @Inject\\n[ADD] Ipv6Service _ipv6Service;\\n[ADD] @Inject\\n[ADD] DataCenterIpv6AddressDao _ipv6AddressDao;\\n \\n     List<NetworkGuru> networkGurus;\\n ',\n",
              " ' \\n package org.mule.runtime.deployment.model.api;\\n \\n[ADD] import org.mule.runtime.api.metadata.MetadataService;\\n import org.mule.runtime.core.api.MuleContext;\\n import org.mule.runtime.core.api.connectivity.ConnectivityTestingService;\\n import org.mule.runtime.module.artifact.Artifact;',\n",
              " ' \\n     private final AtomicBoolean awaited = new AtomicBoolean(false);\\n \\n[ADD] private volatile BootstrapTakeoverState bootstrapTakeoverState = BootstrapTakeoverState.AUTO;\\n[ADD] \\n     private final Lock lock = new ReentrantLock();\\n \\n     private final Condition condition = lock.newCondition();',\n",
              " '     return typedSelf();\\n   }\\n \\n[DEL] public HttpClientBuilder getHttpClientBuilder() {\\n[DEL] return this.httpClientBuilder;\\n[DEL] }\\n[DEL] \\n   public B withHttpClientConnectionManager(HttpClientConnectionManager connManager) {\\n     this.httpConnManager = connManager;\\n     return typedSelf();\\n   }\\n \\n[DEL] HttpClientConnectionManager getHttpConnManager() {\\n[DEL] return this.httpConnManager;\\n[ADD] public B withLogger(Logger logger) {\\n[ADD] this.logger = Optional.fromNullable(logger);\\n[ADD] return typedSelf();\\n   }\\n \\n[ADD] void validate() {\\n[ADD] Preconditions.checkNotNull(getState(), \"State is required for \" + this.getClass().getSimpleName());\\n[ADD] Preconditions.checkNotNull(getHttpClientBuilder(), \"HttpClientBuilder is required for \" + this.getClass().getSimpleName());\\n[ADD] Preconditions.checkNotNull(getHttpConnManager(), \"HttpConnManager is required for \" + this.getClass().getSimpleName());\\n[ADD] }\\n }',\n",
              " ' import static org.mule.runtime.internal.dsl.DslConstants.RECONNECT_FOREVER_ELEMENT_IDENTIFIER;\\n import static org.mule.runtime.internal.dsl.DslConstants.REDELIVERY_POLICY_ELEMENT_IDENTIFIER;\\n import static org.mule.runtime.internal.dsl.DslConstants.TLS_CONTEXT_ELEMENT_IDENTIFIER;\\n[ADD] import static org.mule.runtime.internal.dsl.DslConstants.TRANSFORM_IDENTIFIER;\\n import static org.mule.runtime.internal.dsl.DslConstants.VALUE_ATTRIBUTE_NAME;\\n[DEL] \\n import org.mule.metadata.api.model.ArrayType;\\n import org.mule.metadata.api.model.MetadataType;\\n import org.mule.metadata.api.model.ObjectType;',\n",
              " '     case Js::OpCode::Coerce_Str:\\n         AssertMsg(instr->GetDst()->GetValueType().IsString(),\\n             \"Creator of this instruction should have set the type\");\\n[ADD] \\n[ADD] // Due to fall through and the fact that Ld_A only takes one source,\\n[ADD] // free the other source here.\\n[ADD] if (instr->GetSrc2())\\n[ADD] {\\n[ADD] instr->FreeSrc2();\\n[ADD] }\\n[ADD] \\n         // fall-through\\n     case Js::OpCode::Coerce_StrOrRegex:\\n         // We don\\'t set the ValueType of src1 for Coerce_StrOrRegex, hence skip the ASSERT',\n",
              " '         {\\n             maxExecutionTime = effectiveTotal;\\n         }\\n[DEL] averageExecutionTime = Math.round(totalExecTime / executedEvent);\\n[ADD] averageExecutionTime = executedEvent > 0 ? Math.round(totalExecTime / executedEvent) : 0;\\n     }\\n \\n     /**',\n",
              " '       argsDeclaration = argsDeclaration.substring(0, argsDeclaration.length() - 2);\\n       // Print the first line (method signature).\\n       newLine(1, String.format(\\n[DEL] \"public static%s %s %s(%s) {\",\\n[ADD] \"%s%s %s %s(%s) {\", modifiers,\\n           genericTypes.isEmpty() ? \"\" : \" \" + genericTypes, returnType, callFunc, argsDeclaration\\n       ));\\n ',\n",
              " '     return key.replaceAll(CHILD_ELEMENT_KEY_PREFIX, \"\").replaceAll(CHILD_ELEMENT_KEY_SUFFIX, \"\");\\n   }\\n \\n[DEL] private void checkParameterGroupExclusivenessForModel(EnrichableModel model, Set<String> resolverKeys)\\n[ADD] protected void checkParameterGroupExclusivenessForModel(EnrichableModel model, Set<String> resolverKeys)\\n       throws ConfigurationException {\\n     Optional<List<ParameterGroup>> exclusiveGroups =\\n         model.getModelProperty(ParameterGroupModelProperty.class).map(mp -> mp.getExclusiveGroups());',\n",
              " '     @DB\\n     @ActionEvent(eventType = EventTypes.EVENT_TAGS_DELETE, eventDescription = \"deleting resource tags\")\\n     public boolean deleteTags(List<String> resourceIds, ResourceObjectType resourceType, Map<String, String> tags) {\\n[DEL] Account caller = CallContext.current().getCallingAccount();\\n[DEL] \\n[DEL] SearchBuilder<ResourceTagVO> sb = _resourceTagDao.createSearchBuilder();\\n[DEL] sb.and().op(\"resourceId\", sb.entity().getResourceId(), SearchCriteria.Op.IN);\\n[DEL] sb.or(\"resourceUuid\", sb.entity().getResourceUuid(), SearchCriteria.Op.IN);\\n[DEL] sb.cp();\\n[DEL] sb.and(\"resourceType\", sb.entity().getResourceType(), SearchCriteria.Op.EQ);\\n[DEL] \\n[DEL] SearchCriteria<ResourceTagVO> sc = sb.create();\\n[DEL] sc.setParameters(\"resourceId\", resourceIds.toArray());\\n[DEL] sc.setParameters(\"resourceUuid\", resourceIds.toArray());\\n[DEL] sc.setParameters(\"resourceType\", resourceType);\\n \\n[DEL] List<? extends ResourceTag> resourceTags = _resourceTagDao.search(sc, null);\\n[DEL] ;\\n[ADD] Account caller = CallContext.current().getCallingAccount();\\n[ADD] List<? extends ResourceTag> resourceTags = getResourceTags(resourceIds, resourceType);\\n         final List<ResourceTag> tagsToRemove = new ArrayList<ResourceTag>();\\n \\n         // Finalize which tags should be removed',\n",
              " '         }\\n \\n \\n[ADD] @SuppressWarnings(\"deprecation\") // startActivityForResult\\n         @Override\\n         protected void initSubscreen() {\\n             addPreferencesFromResource(R.xml.preferences_appearance);',\n",
              " '     shutdownListeners.clear();\\n   }\\n \\n[DEL] private <T> T createCustomInstance(String classLocation) {\\n[ADD] private <T> T createInstance(String classLocation) {\\n     try {\\n       Class clazz = createClass(classLocation);\\n       return (T) clazz.newInstance();',\n",
              " \"       }\\n       Set<String> jobNames = new HashSet<>();\\n       jobNames.add(Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\\n[DEL] join(flowGroup, flowName, expectedJobNames.get(i), sourceNodes.get(i), destinationNodes.get(i)));\\n[ADD] join(flowGroup, flowName, expectedJobNames.get(i), (Joiner.on(':').join(sourceNodes.get(i), destinationNodes.get(i), edgeNames.get(i)))));\\n       if (i < 8) {\\n         jobNames.add(Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\\n[DEL] join(flowGroup, flowName, expectedJobNames.get(i + 1), sourceNodes.get(i + 1), destinationNodes.get(i + 1)));\\n[ADD] join(flowGroup, flowName, expectedJobNames.get(i + 1), (Joiner.on(':').join(sourceNodes.get(i + 1), destinationNodes.get(i + 1), edgeNames.get(i + 1)))));\\n       }\\n \\n       for (DagNode<JobExecutionPlan> dagNode : currentHopNodes) {\",\n",
              " '         int numPending = tracker.addNewAsPending(res);\\n         if (numPending > 0) {\\n           LOG.info(\\n[DEL] \"{} - polling returned {} results, of which {} were new. The output is {}.\",\\n[ADD] \"{} - current round of polling took {} ms and returned {} results, \"\\n[ADD] + \"of which {} were new. The output is {}.\",\\n               c.element(),\\n[ADD] new Duration(now, Instant.now()).getMillis(),\\n               res.getOutputs().size(),\\n               numPending,\\n               BoundedWindow.TIMESTAMP_MAX_VALUE.equals(res.getWatermark())\\n[DEL] ? \"complete\"\\n[DEL] : \"incomplete\");\\n[ADD] ? \"final\"\\n[ADD] : \"not yet final\");\\n         }\\n       }\\n[DEL] while (tracker.hasPending()) {\\n[ADD] int numEmittedInThisRound = 0;\\n[ADD] int numTotalPending = tracker.getNumPending();\\n[ADD] int numPreviouslyEmitted = tracker.currentRestriction().completed.size();\\n[ADD] int numTotalKnown = numPreviouslyEmitted + numTotalPending;\\n[ADD] while (true) {\\n         c.updateWatermark(tracker.getWatermark());\\n[DEL] \\n[DEL] TimestampedValue<OutputT> nextPending = tracker.tryClaimNextPending();\\n[DEL] if (nextPending == null) {\\n[DEL] return stop();\\n[ADD] Map.Entry<HashCode, TimestampedValue<OutputT>> entry = tracker.getNextPending();\\n[ADD] if (entry == null || !tracker.tryClaim(entry.getKey())) {\\n[ADD] break;\\n         }\\n[ADD] TimestampedValue<OutputT> nextPending = entry.getValue();\\n         c.outputWithTimestamp(\\n             KV.of(c.element(), nextPending.getValue()), nextPending.getTimestamp());\\n[ADD] ++numEmittedInThisRound;\\n       }\\n[ADD] LOG.info(\\n[ADD] \"{} - emitted {} new results (of {} total known: {} emitted so far, {} more to emit).\",\\n[ADD] c.element(),\\n[ADD] numEmittedInThisRound,\\n[ADD] numTotalKnown,\\n[ADD] numEmittedInThisRound + numPreviouslyEmitted,\\n[ADD] numTotalPending - numEmittedInThisRound);\\n       Instant watermark = tracker.getWatermark();\\n       if (watermark != null) {\\n         // Null means the poll result did not provide a watermark and there were no new elements,',\n",
              " '     }\\n \\n     StatementExpressionListTreeImpl result = new StatementExpressionListTreeImpl(statements.build());\\n[DEL] result.prependChildren(Lists.<AstNode>newArrayList());\\n \\n     return result;\\n   }',\n",
              " '                     if (cause instanceof InterruptedException || cause instanceof InterruptedIOException)\\n                     {\\n                         logger.error(\"Process was interrupted (InterruptedException), ceasing process\");\\n[ADD] lastRetryInterrupted = true;\\n                         break;\\n                     }\\n                     else',\n",
              " ' \\n import org.springframework.messaging.MessagingException;\\n import org.springframework.scheduling.SchedulingAwareRunnable;\\n[DEL] import org.springframework.scheduling.TaskScheduler;\\n[DEL] import org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler;\\n import org.springframework.util.Assert;\\n \\n /**',\n",
              " ' \\n \\t@Test\\n \\tpublic void testNetSingleUseWithInbound() throws Exception {\\n[DEL] final AtomicReference<ServerSocket> serverSocket = new AtomicReference<ServerSocket>();\\n[ADD] final AtomicReference<ServerSocket> serverSocket = new AtomicReference<>();\\n \\t\\tfinal CountDownLatch latch = new CountDownLatch(1);\\n \\t\\tfinal Semaphore semaphore = new Semaphore(0);\\n \\t\\tfinal AtomicBoolean done = new AtomicBoolean();',\n",
              " ' \\n     CopyableDatasetMetadata metadata = CopyableDatasetMetadata.deserialize(\\n         datasetWorkUnitStates.iterator().next().getProp(CopySource.SERIALIZED_COPYABLE_DATASET));\\n[DEL] Path datasetWriterOutputPath = new Path(new Path(writerOutputDir, datasetAndPartition.identifier()),\\n[DEL] PathUtils.withoutLeadingSeparator(metadata.getDatasetTargetRoot()));\\n[ADD] Path datasetWriterOutputPath = new Path(this.writerOutputDir, datasetAndPartition.identifier());\\n \\n     log.info(String\\n         .format(\"Publishing fileSet from %s to %s\", datasetWriterOutputPath, metadata.getDatasetTargetRoot()));\\n \\n[DEL] HadoopUtils.renameRecursively(fs, datasetWriterOutputPath, metadata.getDatasetTargetRoot());\\n[ADD] HadoopUtils.renameRecursively(fs, datasetWriterOutputPath, findPathRoot(metadata.getDatasetTargetRoot()));\\n \\n     fs.delete(datasetWriterOutputPath, true);\\n ',\n",
              " '  */\\n package ${package};\\n \\n[DEL] import ${package}.common.ExampleBigQueryTableOptions;\\n[DEL] import ${package}.common.ExampleOptions;\\n[DEL] import ${package}.common.ExampleUtils;\\n import com.google.api.services.bigquery.model.TableFieldSchema;\\n import com.google.api.services.bigquery.model.TableReference;\\n import com.google.api.services.bigquery.model.TableRow;',\n",
              " '    */\\n   private DataWriter<Object> buildWriter()\\n       throws IOException {\\n[ADD] String writerId = this.taskId;\\n[ADD] \\n[ADD] // Add the task starting time if configured.\\n[ADD] // This is used to reduce file name collisions which can happen due to the scheduling of a task on multiple workers.\\n[ADD] // One case where this occurs is when a worker gets disconnected from a Helix cluster.\\n[ADD] if (this.taskState.getPropAsBoolean(ConfigurationKeys.WRITER_ADD_TASK_TIMESTAMP, false)) {\\n[ADD] writerId = this.taskId + \"_\" + this.taskState.getProp(ConfigurationKeys.TASK_START_TIME_MILLIS_KEY, \"0\");\\n[ADD] }\\n[ADD] \\n     DataWriterBuilder<Object, Object> builder = this.taskContext.getDataWriterBuilder(this.branches, this.index)\\n         .writeTo(Destination.of(this.taskContext.getDestinationType(this.branches, this.index), this.taskState))\\n[DEL] .writeInFormat(this.taskContext.getWriterOutputFormat(this.branches, this.index)).withWriterId(this.taskId)\\n[ADD] .writeInFormat(this.taskContext.getWriterOutputFormat(this.branches, this.index)).withWriterId(writerId)\\n         .withSchema(this.convertedSchema.orNull()).withBranches(this.branches).forBranch(this.index);\\n     if (this.taskAttemptId.isPresent()) {\\n       builder.withAttemptId(this.taskAttemptId.get());',\n",
              " ' import static org.slf4j.LoggerFactory.getLogger;\\n import static reactor.core.publisher.Flux.error;\\n import static reactor.core.publisher.Flux.from;\\n[DEL] import static reactor.core.publisher.Flux.just;\\n[DEL] \\n import org.mule.runtime.api.component.AbstractComponent;\\n import org.mule.runtime.api.component.Component;\\n import org.mule.runtime.api.component.location.ComponentLocation;',\n",
              " '         break;\\n       default:\\n         throw new KettleException( \"It\\'s not possible to rename Class [\" + element.getClass().getName()\\n[DEL] + \"] to the repository\" );\\n[ADD] + \"] to the repository\" );\\n[ADD] }\\n[ADD] \\n[ADD] readWriteLock.writeLock().lock();\\n[ADD] try {\\n[ADD] pur.moveFile( file.getId(), buf.toString(), null );\\n[ADD] } finally {\\n[ADD] readWriteLock.writeLock().unlock();\\n     }\\n[DEL] pur.moveFile( file.getId(), buf.toString(), null );\\n   }\\n \\n   /**',\n",
              " '     assertEquals(SCHEMA, new AvroRecordSchema().schemaFor(TypeDescriptor.of(TestAvro.class)));\\n   }\\n \\n[ADD] @Test\\n[ADD] public void testOptionsRecordSchema() {\\n[ADD] assertEquals(\\n[ADD] OPTIONS_SCHEMA, new AvroRecordSchema().schemaFor(TypeDescriptor.of(TestOptionsAvro.class)));\\n[ADD] }\\n[ADD] \\n   @Test\\n   public void testPojoSchema() {\\n     assertEquals(POJO_SCHEMA, new AvroRecordSchema().schemaFor(TypeDescriptor.of(AvroPojo.class)));',\n",
              " '     }\\n \\n     Map<String, AddSpecResponse> responseMap = this.flowCatalog.put(flowSpec, triggerListener);\\n[DEL] HttpStatus httpStatus = HttpStatus.S_201_CREATED;\\n[ADD] HttpStatus httpStatus;\\n \\n     if (flowConfig.hasExplain() && flowConfig.isExplain()) {\\n       //This is an Explain request. So no resource is actually created.\\n       //Enrich original FlowConfig entity by adding the compiledFlow to the properties map.\\n       StringMap props = flowConfig.getProperties();\\n[DEL] AddSpecResponse<String> addSpecResponse = responseMap.getOrDefault(GOBBLIN_SERVICE_JOB_SCHEDULER_LISTENER_CLASS, null);\\n[ADD] AddSpecResponse<String> addSpecResponse = responseMap.getOrDefault(ServiceConfigKeys.GOBBLIN_SERVICE_JOB_SCHEDULER_LISTENER_CLASS, null);\\n       props.put(\"gobblin.flow.compiled\",\\n           addSpecResponse != null && addSpecResponse.getValue() != null ? StringEscapeUtils.escapeJson(addSpecResponse.getValue()) : \"\");\\n       flowConfig.setProperties(props);\\n[ADD] }\\n[ADD] \\n[ADD] if (flowConfig.hasExplain() && flowConfig.isExplain()) {\\n       //Return response with 200 status code, since no resource is actually created.\\n       httpStatus = HttpStatus.S_200_OK;\\n[ADD] } else if (Boolean.parseBoolean(responseMap.getOrDefault(ServiceConfigKeys.COMPILATION_SUCCESSFUL, new AddSpecResponse<>(\"false\")).getValue().toString())) {\\n[ADD] httpStatus = HttpStatus.S_201_CREATED;\\n[ADD] } else {\\n[ADD] httpStatus = HttpStatus.S_400_BAD_REQUEST;\\n     }\\n[ADD] \\n     return new CreateKVResponse(new ComplexResourceKey<>(flowConfig.getId(), flowStatusId), flowConfig, httpStatus);\\n   }\\n ',\n",
              " ' import org.mule.util.concurrent.Latch;\\n \\n import java.io.IOException;\\n[DEL] import java.net.SocketException;\\n import java.util.concurrent.CountDownLatch;\\n import java.util.concurrent.TimeUnit;\\n import java.util.concurrent.atomic.AtomicInteger;\\n \\n import org.apache.commons.io.IOUtils;\\n import org.apache.http.HttpResponse;\\n[DEL] import org.apache.http.NoHttpResponseException;\\n import org.apache.http.client.fluent.Executor;\\n import org.apache.http.client.fluent.Request;\\n import org.apache.http.client.fluent.Response;',\n",
              " '         Option<List<HoodieRecord>> records = HoodieTableMetadataUtil.convertInstantToMetaRecords(datasetMetaClient, instant, metadata.getSyncedInstantTime());\\n         if (records.isPresent()) {\\n           commit(records.get(), MetadataPartitionType.FILES.partitionPath(), instant.getTimestamp());\\n[ADD] // re-init the table metadata, for any future writes.\\n[ADD] initTableMetadata();\\n         }\\n       }\\n[DEL] // re-init the table metadata, for any future writes.\\n[DEL] initTableMetadata();\\n     } catch (IOException ioe) {\\n       throw new HoodieIOException(\"Unable to sync instants from data to metadata table.\", ioe);\\n     }',\n",
              " '                         }\\n                     }\\n                     if (delete) {\\n[DEL] jedis.publish(key, UNREGISTER);\\n[ADD] redisClient.publish(key, UNREGISTER);\\n                     }\\n                 }\\n             }',\n",
              " '       VirtualField<CouchbaseRequest, Span> virtualField =\\n           VirtualField.find(CouchbaseRequest.class, Span.class);\\n \\n[ADD] // TODO add support for peer service name\\n       Span span = virtualField.get(request);\\n       if (span != null) {\\n[DEL] NetPeerAttributes.INSTANCE.setNetPeer(span, remoteHostname, null);\\n[ADD] if (remoteHostname != null) {\\n[ADD] span.setAttribute(SemanticAttributes.NET_PEER_NAME, remoteHostname);\\n[ADD] }\\n \\n         if (remoteSocket != null) {\\n           int splitIndex = remoteSocket.lastIndexOf(\":\");',\n",
              " '         assertEquals(\"Latest commit should be 001\",readClient.latestCommit(), newCommitTime);\\n         assertEquals(\"Must contain 200 records\", readClient.readCommit(newCommitTime).count(), records.size());\\n         // Should have 100 records in table (check using Index), all in locations marked at commit\\n[DEL] List<HoodieRecord> taggedRecords = index.tagLocation(jsc.parallelize(records, 1), new HoodieTableMetaClient(fs, basePath)).collect();\\n[ADD] HoodieTableMetaClient metaClient = new HoodieTableMetaClient(fs, basePath);\\n[ADD] HoodieTable table = HoodieTable.getHoodieTable(metaClient, getConfig());\\n[ADD] \\n[ADD] List<HoodieRecord> taggedRecords = index.tagLocation(jsc.parallelize(records, 1), table).collect();\\n         checkTaggedRecords(taggedRecords, \"001\");\\n \\n         /**',\n",
              " '     }\\n     // run helper injector on all instrumenters\\n     if (assertPass) {\\n[DEL] for (Object instrumenter : loadAllInstrumenters(agentClassLoader)) {\\n[DEL] if (!(instrumenter instanceof Instrumenter.Default\\n[DEL] || instrumenter instanceof InstrumentationModule)) {\\n[DEL] // only default Instrumenters and modules use muzzle. Skip custom instrumenters.\\n[DEL] continue;\\n[DEL] }\\n[ADD] for (InstrumentationModule instrumentationModule :\\n[ADD] ServiceLoader.load(InstrumentationModule.class, agentClassLoader)) {\\n         try {\\n           // verify helper injector works\\n[DEL] Method getHelperClassNames = instrumenter.getClass().getMethod(\"helperClassNames\");\\n[DEL] getHelperClassNames.setAccessible(true);\\n[DEL] String[] helperClassNames = (String[]) getHelperClassNames.invoke(instrumenter);\\n[ADD] String[] helperClassNames = instrumentationModule.helperClassNames();\\n           if (helperClassNames.length > 0) {\\n             new HelperInjector(\\n                     MuzzleGradlePluginUtil.class.getSimpleName(),',\n",
              " '                 ldapSettings.getSearchBase(),\\n                 ldapSettings.getSearchPattern(),\\n                 ldapSettings.getDisplayNameAttribute(),\\n[DEL] ldapSettings.getDefaultGroup());\\n[ADD] ldapSettings.getDefaultGroup(),\\n[ADD] ldapSettings.getGroupMapping(),\\n[ADD] ldapSettings.getGroupSearchBase(),\\n[ADD] ldapSettings.getGroupIdAttribute(),\\n[ADD] ldapSettings.getAdditionalDefaultGroups());\\n     }\\n \\n     @POST\\n     @Timed\\n[ADD] @RequiresPermissions(RestPermissions.LDAP_EDIT)\\n     @ApiOperation(\"Test LDAP Configuration\")\\n     @Path(\"/test\")\\n     @Consumes(MediaType.APPLICATION_JSON)',\n",
              " ' import com.ibm.j9ddr.vm29.pointer.helper.J9RASHelper;\\n import com.ibm.j9ddr.vm29.types.I32;\\n import com.ibm.j9ddr.vm29.types.UDATA;\\n[ADD] import java.lang.reflect.InvocationTargetException;\\n \\n class MMObjectAccessBarrier_V1 extends MMObjectAccessBarrier\\n {',\n",
              " '         final LoggerContext ctx;\\n         try\\n         {\\n[DEL] final int key = computeKey(classLoader);\\n[DEL] ctx = activeContexts.get(key, new Callable<LoggerContext>()\\n[ADD] final Integer key = computeKey(classLoader);\\n[ADD] // If possible, avoid using guava cache since the callable puts unwanted pressure on the garbage collector.\\n[ADD] if (builtContexts.containsKey(key))\\n[ADD] {\\n[ADD] ctx = builtContexts.get(key);\\n[ADD] }\\n[ADD] else\\n             {\\n[DEL] @Override\\n[DEL] public LoggerContext call() throws Exception\\n[ADD] synchronized (this)\\n                 {\\n[DEL] return artifactAwareContextSelector.buildContext(classLoader);\\n[ADD] if (builtContexts.containsKey(key))\\n[ADD] {\\n[ADD] ctx = builtContexts.get(key);\\n[ADD] }\\n[ADD] else\\n[ADD] {\\n[ADD] ctx = doGetLoggerContext(classLoader, key);\\n[ADD] }\\n                 }\\n[DEL] });\\n[ADD] }\\n         }\\n         catch (ExecutionException e)\\n         {',\n",
              " '     } catch (TimeoutException te) {\\n       LOGGER.error(\"Timeout in stopping the service manager\", te);\\n     } finally {\\n[ADD] disconnectHelixManager();\\n[ADD] \\n       // Stop metric reporting\\n       this.jmxReporter.stop();\\n[DEL] \\n[DEL] if (this.helixManager.isConnected()) {\\n[DEL] this.helixManager.disconnect();\\n[DEL] }\\n     }\\n   }\\n \\n[ADD] /**\\n[ADD] * Get additional metadata required for the {@link gobblin.metrics.event.EventSubmitter}.\\n[ADD] */\\n[ADD] private Map<String, String> getEventSubmitterMetadata(String applicationName, String applicationId) {\\n[ADD] return new ImmutableMap.Builder<String, String>().put(GobblinYarnEventNames.YARN_APPLICATION_NAME, applicationName)\\n[ADD] .put(GobblinYarnEventNames.YARN_APPLICATION_ID, applicationId).build();\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Build the {@link HelixManager} for the Application Master.\\n[ADD] */\\n[ADD] private HelixManager buildHelixManager(Config config, ContainerId containerId, String zkConnectionString)\\n[ADD] throws UnknownHostException {\\n[ADD] String helixInstanceName = YarnHelixUtils.getHelixInstanceName(YarnHelixUtils.getHostname(), containerId);\\n[ADD] return HelixManagerFactory.getZKHelixManager(\\n[ADD] config.getString(GobblinYarnConfigurationKeys.HELIX_CLUSTER_NAME_KEY), helixInstanceName,\\n[ADD] InstanceType.CONTROLLER, zkConnectionString);\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Build the {@link FileSystem} for the Application Master.\\n[ADD] */\\n[ADD] private FileSystem buildFileSystem(Config config)\\n[ADD] throws IOException {\\n[ADD] return config.hasPath(ConfigurationKeys.FS_URI_KEY) ? FileSystem\\n[ADD] .get(URI.create(config.getString(ConfigurationKeys.FS_URI_KEY)), new Configuration())\\n[ADD] : FileSystem.get(new Configuration());\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Build the {@link YarnService} for the Application Master.\\n[ADD] */\\n[ADD] private YarnService buildYarnService(Config config, String applicationName, String applicationId,\\n[ADD] YarnConfiguration yarnConfiguration, FileSystem fs)\\n[ADD] throws Exception {\\n[ADD] return new YarnService(config, applicationName, applicationId, yarnConfiguration, fs, this.eventBus);\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Build the {@link GobblinHelixJobScheduler} for the Application Master.\\n[ADD] */\\n[ADD] private GobblinHelixJobScheduler buildGobblinHelixJobScheduler(Config config, Path appWorkDir,\\n[ADD] Map<String, String> eventMetadata)\\n[ADD] throws Exception {\\n[ADD] Properties properties = YarnHelixUtils.configToProperties(config);\\n[ADD] return new GobblinHelixJobScheduler(properties, this.helixManager, this.eventBus, appWorkDir, eventMetadata);\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Build the {@link JobConfigurationManager} for the Application Master.\\n[ADD] */\\n[ADD] private JobConfigurationManager buildJobConfigurationManager(Config config) {\\n[ADD] Optional<String> jobConfPackagePath =\\n[ADD] config.hasPath(GobblinYarnConfigurationKeys.JOB_CONF_PATH_KEY) ? Optional\\n[ADD] .of(config.getString(GobblinYarnConfigurationKeys.JOB_CONF_PATH_KEY)) : Optional.<String>absent();\\n[ADD] return new JobConfigurationManager(this.eventBus, jobConfPackagePath);\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Build the {@link YarnContainerSecurityManager} for the Application Master.\\n[ADD] */\\n[ADD] private YarnContainerSecurityManager buildYarnContainerSecurityManager(Config config, FileSystem fs) {\\n[ADD] return new YarnContainerSecurityManager(config, fs, this.eventBus);\\n[ADD] }\\n[ADD] \\n   @SuppressWarnings(\"unused\")\\n   @Subscribe\\n   public void handleApplicationMasterShutdownRequest(ApplicationMasterShutdownRequest shutdownRequest) {\\n     stop();\\n   }\\n \\n[ADD] @VisibleForTesting\\n[ADD] EventBus getEventBus() {\\n[ADD] return this.eventBus;\\n[ADD] }\\n[ADD] \\n[ADD] @VisibleForTesting\\n[ADD] void connectHelixManager() {\\n[ADD] try {\\n[ADD] this.helixManager.connect();\\n[ADD] this.helixManager.addLiveInstanceChangeListener(new GobblinLiveInstanceChangeListener());\\n[ADD] this.helixManager.getMessagingService().registerMessageHandlerFactory(\\n[ADD] Message.MessageType.SHUTDOWN.toString(), new ControllerShutdownMessageHandlerFactory());\\n[ADD] this.helixManager.getMessagingService().registerMessageHandlerFactory(\\n[ADD] Message.MessageType.USER_DEFINE_MSG.toString(), new ControllerUserDefinedMessageHandlerFactory()\\n[ADD] );\\n[ADD] } catch (Exception e) {\\n[ADD] LOGGER.error(\"HelixManager failed to connect\", e);\\n[ADD] throw Throwables.propagate(e);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] @VisibleForTesting\\n[ADD] void disconnectHelixManager() {\\n[ADD] if (this.helixManager.isConnected()) {\\n[ADD] this.helixManager.disconnect();\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] @VisibleForTesting\\n[ADD] boolean isHelixManagerConnected() {\\n[ADD] return this.helixManager.isConnected();\\n[ADD] }\\n[ADD] \\n   private void registerJvmMetrics() {\\n     registerMetricSetWithPrefix(\"jvm.gc\", new GarbageCollectorMetricSet());\\n     registerMetricSetWithPrefix(\"jvm.memory\", new MemoryUsageGaugeSet());',\n",
              " ' \\n     @Override\\n     public void visitBinaryExpression(BinaryExpressionTree tree) {\\n[DEL] checkDeferredConstraint(tree);\\n[ADD] checkDeferredConstraint();\\n     }\\n \\n     @Override\\n     public void visitAssignmentExpression(AssignmentExpressionTree tree) {\\n[DEL] checkDeferredConstraint(tree);\\n[ADD] checkDeferredConstraint();\\n     }\\n \\n     @Override\\n     public void visitUnaryExpression(UnaryExpressionTree tree) {\\n[DEL] checkDeferredConstraint(tree);\\n[ADD] checkDeferredConstraint();\\n     }\\n \\n     @Override\\n     public void visitTypeCast(TypeCastTree tree) {\\n[DEL] checkDeferredConstraint(tree);\\n[ADD] checkDeferredConstraint();\\n     }\\n \\n[DEL] private void checkDeferredConstraint(Tree tree) {\\n[ADD] private void checkDeferredConstraint() {\\n       SymbolicValue sv = programState.peekValue();\\n       if (sv instanceof DeferredStatusHolderSV) {\\n[DEL] addZeroConstraint(sv, tree, ((DeferredStatusHolderSV) sv).deferredStatus);\\n[ADD] addZeroConstraint(sv, ((DeferredStatusHolderSV) sv).deferredStatus);\\n       }\\n     }\\n \\n[DEL] private void addZeroConstraint(SymbolicValue sv, Tree tree, Status status) {\\n[DEL] programState = programState.addConstraint(sv, new ZeroConstraint(tree, status));\\n[ADD] private void addZeroConstraint(SymbolicValue sv, Status status) {\\n[ADD] programState = programState.addConstraint(sv, new ZeroConstraint(status));\\n     }\\n   }\\n }',\n",
              " ' /**\\n  * OrderComparator\\n  */\\n[DEL] public class WrapperComparator implements Comparator<Object> {\\n[ADD] public class WrapperComparator implements Comparator<Class> {\\n \\n[DEL] public static final Comparator<Object> COMPARATOR = new WrapperComparator();\\n[ADD] public static final Comparator<Class> COMPARATOR = new WrapperComparator();\\n \\n     @Override\\n[DEL] public int compare(Object o1, Object o2) {\\n[ADD] public int compare(Class o1, Class o2) {\\n         if (o1 == null && o2 == null) {\\n             return 0;\\n         }',\n",
              " ' \\t\\t\\t\\t\\t\\treturn 0;\\n \\t\\t\\t\\t\\t}\\n \\t\\t\\t\\t}\\n[ADD] \\n[ADD] public void rollbackFromFileToListEnd(List<F> filteredFiles, F file) {\\n[ADD] if (AbstractInboundFileSynchronizer.this.filter instanceof ReversibleFileListFilter) {\\n[ADD] ((ReversibleFileListFilter<F>) AbstractInboundFileSynchronizer.this.filter)\\n[ADD] .rollback(file, filteredFiles);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n \\t\\t\\t});\\n \\t\\t\\tif (this.logger.isDebugEnabled()) {\\n \\t\\t\\t\\tthis.logger.debug(transferred + \" files transferred\");',\n",
              " '   private final transient FileSystem fs;\\n   private final transient JavaSparkContext jsc;\\n   private final HoodieWriteConfig config;\\n[ADD] private final boolean rollbackInFlight;\\n   private final transient HoodieMetrics metrics;\\n   private final transient HoodieIndex<T> index;\\n   private transient Timer.Context writeContext = null;\\n[ADD] private transient Timer.Context compactionTimer;\\n \\n   /**\\n    * @param jsc',\n",
              " \"    * will not register the same type twice even if requested to\\n    *\\n    * @param metadataType a {@link MetadataType} describing a pojo type\\n[DEL] * @param baseType a {@link MetadataType} describing a pojo's base type\\n[DEL] * @param description the type's description\\n[ADD] * @param baseType     a {@link MetadataType} describing a pojo's base type\\n[ADD] * @param description  the type's description\\n    * @return the reference name of the complexType\\n    */\\n   private String registerPojoType(MetadataType metadataType, MetadataType baseType, String description) {\",\n",
              " '         TemplateProfile profile = super.prepareDelete(cmd);\\n         VMTemplateVO template = profile.getTemplate();\\n         List<Long> zoneIdList = profile.getZoneIdList();\\n[DEL] \\n[DEL] if (template.getTemplateType() == TemplateType.SYSTEM) {\\n[ADD] // TODO: Add a check to see if this template is used by console proxy, router or storagevm\\n[ADD] if (CallContext.current().getCallingAccount().getRoleId() != RoleType.Admin.getId() && template.getTemplateType() == TemplateType.SYSTEM){\\n             throw new InvalidParameterValueException(\"The DomR template cannot be deleted.\");\\n         }\\n \\n[ADD] \\n[ADD] \\n         if (zoneIdList != null && (storeMgr.getImageStoreWithFreeCapacity(zoneIdList.get(0)) == null)) {\\n             throw new InvalidParameterValueException(\"Failed to find a secondary storage in the specified zone.\");\\n         }',\n",
              " ' public class SourcePollingChannelAdapter extends AbstractPollingEndpoint\\n \\t\\timplements TrackableComponent {\\n \\n[ADD] private final MessagingTemplate messagingTemplate = new MessagingTemplate();\\n[ADD] \\n[ADD] private final Collection<Advice> appliedAdvices = new HashSet<>();\\n[ADD] \\n \\tprivate volatile MessageSource<?> source;\\n \\n \\tprivate volatile MessageChannel outputChannel;',\n",
              " ' \\tif err != nil || state == nil {\\n \\t\\treturn nil, errors.Wrapf(err, \"at root: %s\", root.Hex())\\n \\t}\\n[DEL] wrapper, err := state.ValidatorWrapper(addr)\\n[ADD] wrapper, err := state.ValidatorWrapper(addr, false)\\n \\tif err != nil {\\n \\t\\treturn nil, errors.Wrapf(err, \"at root: %s\", root.Hex())\\n \\t}',\n",
              " \"                 out.dims = [6, 1]\\n \\n     Args:\\n[DEL] x (Variable): Input variable which could be a Tensor or LoDTensor.\\n[DEL] y (Variable|None): If provided, output's LoD would be derived\\n[DEL] from :attr:`y`.\\n[ADD] x (Variable): Input variable which could be a Tensor or LoDTensor.\\n[ADD] The data type should be int32, int64, float32 or float64.\\n[ADD] y (Variable|None): If provided, output's LoD would be derived from :attr:`y`.\\n[ADD] If y's lod level>0, the data type should be int32, int64, float32 or float64.\\n[ADD] If y's lod level=0, the data type should be int32 or int64.\\n         target_lod (list|tuple|None): One level LoD which should be considered\\n                                       as target LoD when :attr:`y` not provided.\\n \",\n",
              " '     Pipeline pipeline = Pipeline.create(options);\\n \\n     /**\\n[DEL] * Concept #1: the Dataflow SDK lets us run the same pipeline with either a bounded or\\n[ADD] * Concept #1: the Beam SDK lets us run the same pipeline with either a bounded or\\n      * unbounded input source.\\n      */\\n     PCollection<String> input = pipeline',\n",
              " ' \\t */\\n \\tstatic AttachHandler mainHandler = new AttachHandler();\\n \\tstatic volatile Thread currentAttachThread = mainHandler; /* Join on this when shutting down */\\n[DEL] private Vector<Attachment> attachments = new Vector<Attachment>();\\n[ADD] private Vector<Attachment> attachments = new Vector<>();\\n \\tprivate static String vmId = \"\"; /* ID of the currently running VM *///$NON-NLS-1$\\n \\t/**\\n \\t * Human-friendly name for VM',\n",
              " ' \\n import java.util.ArrayList;\\n import java.util.List;\\n[ADD] import java.util.Map;\\n[ADD] \\n import org.apache.avro.AvroRuntimeException;\\n import org.apache.avro.Schema;\\n import org.apache.avro.SchemaBuilder;',\n",
              " ' \\n   @Override\\n   public T copy(T t) {\\n[DEL] try {\\n[DEL] return CoderUtils.clone(coder, t);\\n[DEL] } catch (CoderException e) {\\n[DEL] throw new RuntimeException(\"Could not clone.\", e);\\n[ADD] if (fasterCopy) {\\n[ADD] return t;\\n[ADD] } else {\\n[ADD] try {\\n[ADD] return CoderUtils.clone(coder, t);\\n[ADD] } catch (CoderException e) {\\n[ADD] throw new RuntimeException(\"Could not clone.\", e);\\n[ADD] }\\n     }\\n   }\\n ',\n",
              " ' import org.apache.dubbo.qos.command.CommandContext;\\n import org.apache.dubbo.qos.command.annotation.Cmd;\\n import org.apache.dubbo.qos.textui.TTable;\\n[DEL] import org.apache.dubbo.registry.support.ConsumerInvokerWrapper;\\n[DEL] import org.apache.dubbo.registry.support.ProviderConsumerRegTable;\\n[DEL] import org.apache.dubbo.registry.support.ProviderInvokerWrapper;\\n import org.apache.dubbo.rpc.model.ApplicationModel;\\n import org.apache.dubbo.rpc.model.ConsumerModel;\\n import org.apache.dubbo.rpc.model.ProviderModel;\\n \\n import java.util.Collection;\\n[DEL] import java.util.Set;\\n[ADD] \\n[ADD] import static org.apache.dubbo.registry.support.ProviderConsumerRegTable.getConsumerAddressNum;\\n[ADD] import static org.apache.dubbo.registry.support.ProviderConsumerRegTable.isRegistered;\\n \\n @Cmd(name = \"ls\", summary = \"ls service\", example = {\\n         \"ls\"',\n",
              " '     }\\n \\n \\n[ADD] \\n[ADD] private static class UndoSuspendCard extends Undoable {\\n[ADD] private final Card suspendedCard;\\n[ADD] \\n[ADD] \\n[ADD] public UndoSuspendCard(Card suspendedCard) {\\n[ADD] super(Collection.DismissType.SUSPEND_CARD);\\n[ADD] this.suspendedCard = suspendedCard;\\n[ADD] }\\n[ADD] \\n[ADD] \\n[ADD] public long undo(Collection col) {\\n[ADD] Timber.i(\"UNDO: Suspend Card %d\", suspendedCard.getId());\\n[ADD] suspendedCard.flush(false);\\n[ADD] return suspendedCard.getId();\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] \\n[ADD] private static class UndoDeleteNote extends Undoable {\\n[ADD] private final Note note;\\n[ADD] private final ArrayList<Card> allCs;\\n[ADD] private final long cid;\\n[ADD] \\n[ADD] \\n[ADD] public UndoDeleteNote(Note note, ArrayList<Card> allCs, long cid) {\\n[ADD] super(Collection.DismissType.DELETE_NOTE);\\n[ADD] this.note = note;\\n[ADD] this.allCs = allCs;\\n[ADD] this.cid = cid;\\n[ADD] }\\n[ADD] \\n[ADD] \\n[ADD] public long undo(Collection col) {\\n[ADD] Timber.i(\"Undo: Delete note\");\\n[ADD] ArrayList<Long> ids = new ArrayList<>();\\n[ADD] note.flush(note.getMod(), false);\\n[ADD] ids.add(note.getId());\\n[ADD] for (Card c : allCs) {\\n[ADD] c.flush(false);\\n[ADD] ids.add(c.getId());\\n[ADD] }\\n[ADD] col.getDb().execute(\"DELETE FROM graves WHERE oid IN \" + Utils.ids2str(Utils.collection2Array(ids)));\\n[ADD] return cid;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] \\n     private TaskData doInBackgroundDismissNote(TaskData param) {\\n         Collection col = getCol();\\n         AbstractSched sched = col.getSched();',\n",
              " '                 Timber.e(e, \"Could not dismiss mProgressDialog. The Activity must have been destroyed while the AsyncTask was running\");\\n                 AnkiDroidApp.sendExceptionReport(e, \"DeckPicker.onPostExecute\", \"Could not dismiss mProgressDialog\");\\n             }\\n[DEL] syncMessage = data.message;\\n[ADD] String syncMessage = data.message;\\n             if (!data.success) {\\n                 Object[] result = (Object[]) data.result;\\n                 if (result[0] instanceof String) {\\n                     String resultType = (String) result[0];\\n[DEL] if (\"badAuth\".equals(resultType)) {\\n[DEL] // delete old auth information\\n[DEL] SharedPreferences preferences = AnkiDroidApp.getSharedPrefs(getBaseContext());\\n[DEL] Editor editor = preferences.edit();\\n[DEL] editor.putString(\"username\", \"\");\\n[DEL] editor.putString(\"hkey\", \"\");\\n[DEL] editor.apply();\\n[DEL] // then show not logged in dialog\\n[DEL] showSyncErrorDialog(SyncErrorDialog.DIALOG_USER_NOT_LOGGED_IN_SYNC);\\n[DEL] } else if (\"noChanges\".equals(resultType)) {\\n[DEL] SyncStatus.markSyncCompleted();\\n[DEL] // show no changes message, use false flag so we don\\'t show \"sync error\" as the Dialog title\\n[DEL] showSyncLogMessage(R.string.sync_no_changes_message, \"\");\\n[DEL] } else if (\"clockOff\".equals(resultType)) {\\n[DEL] long diff = (Long) result[1];\\n[DEL] if (diff >= 86100) {\\n[DEL] // The difference if more than a day minus 5 minutes acceptable by ankiweb error\\n[DEL] dialogMessage = res.getString(R.string.sync_log_clocks_unsynchronized, diff,\\n[DEL] res.getString(R.string.sync_log_clocks_unsynchronized_date));\\n[DEL] } else if (Math.abs((diff % 3600.0) - 1800.0) >= 1500.0) {\\n[DEL] // The difference would be within limit if we adjusted the time by few hours\\n[DEL] // It doesn\\'t work for all timezones, but it covers most and it\\'s a guess anyway\\n[DEL] dialogMessage = res.getString(R.string.sync_log_clocks_unsynchronized, diff,\\n[DEL] res.getString(R.string.sync_log_clocks_unsynchronized_tz));\\n[DEL] } else {\\n[DEL] dialogMessage = res.getString(R.string.sync_log_clocks_unsynchronized, diff, \"\");\\n[DEL] }\\n[DEL] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[DEL] } else if (\"fullSync\".equals(resultType)) {\\n[DEL] if (getCol().isEmpty()) {\\n[DEL] // don\\'t prompt user to resolve sync conflict if local collection empty\\n[DEL] sync(FULL_DOWNLOAD);\\n[DEL] // TODO: Also do reverse check to see if AnkiWeb collection is empty if Anki Desktop\\n[DEL] // implements it\\n[DEL] } else {\\n[DEL] // If can\\'t be resolved then automatically then show conflict resolution dialog\\n[DEL] showSyncErrorDialog(SyncErrorDialog.DIALOG_SYNC_CONFLICT_RESOLUTION);\\n[DEL] }\\n[DEL] } else if (\"basicCheckFailed\".equals(resultType)) {\\n[DEL] dialogMessage = res.getString(R.string.sync_basic_check_failed, res.getString(R.string.check_db));\\n[DEL] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[DEL] } else if (\"dbError\".equals(resultType)) {\\n[DEL] showSyncErrorDialog(SyncErrorDialog.DIALOG_SYNC_CORRUPT_COLLECTION, syncMessage);\\n[DEL] } else if (\"overwriteError\".equals(resultType)) {\\n[DEL] dialogMessage = res.getString(R.string.sync_overwrite_error);\\n[DEL] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[DEL] } else if (\"remoteDbError\".equals(resultType)) {\\n[DEL] dialogMessage = res.getString(R.string.sync_remote_db_error);\\n[DEL] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[DEL] } else if (\"sdAccessError\".equals(resultType)) {\\n[DEL] dialogMessage = res.getString(R.string.sync_write_access_error);\\n[DEL] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[DEL] } else if (\"finishError\".equals(resultType)) {\\n[DEL] dialogMessage = res.getString(R.string.sync_log_finish_error);\\n[DEL] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[DEL] } else if (\"connectionError\".equals(resultType)) {\\n[DEL] dialogMessage = res.getString(R.string.sync_connection_error);\\n[DEL] if (result.length >= 1 && result[1] instanceof Exception) {\\n[DEL] dialogMessage += \"\\\\n\\\\n\" + ((Exception)result[1]).getLocalizedMessage();\\n[DEL] }\\n[DEL] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[DEL] } else if (\"IOException\".equals(resultType)) {\\n[DEL] handleDbError();\\n[DEL] } else if (\"genericError\".equals(resultType)) {\\n[DEL] dialogMessage = res.getString(R.string.sync_generic_error);\\n[DEL] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[DEL] } else if (\"OutOfMemoryError\".equals(resultType)) {\\n[DEL] dialogMessage = res.getString(R.string.error_insufficient_memory);\\n[DEL] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[DEL] } else if (\"sanityCheckError\".equals(resultType)) {\\n[DEL] dialogMessage = res.getString(R.string.sync_sanity_failed);\\n[DEL] showSyncErrorDialog(SyncErrorDialog.DIALOG_SYNC_SANITY_ERROR,\\n[DEL] joinSyncMessages(dialogMessage, syncMessage));\\n[DEL] } else if (\"serverAbort\".equals(resultType)) {\\n[DEL] // syncMsg has already been set above, no need to fetch it here.\\n[DEL] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[DEL] } else if (\"mediaSyncServerError\".equals(resultType)) {\\n[DEL] dialogMessage = res.getString(R.string.sync_media_error_check);\\n[DEL] showSyncErrorDialog(SyncErrorDialog.DIALOG_MEDIA_SYNC_ERROR,\\n[DEL] joinSyncMessages(dialogMessage, syncMessage));\\n[DEL] } else if (\"customSyncServerUrl\".equals(resultType)) {\\n[DEL] String url = result.length > 1 && result[1] instanceof CustomSyncServerUrlException\\n[DEL] ? ((CustomSyncServerUrlException)result[1]).getUrl() : \"unknown\";\\n[DEL] dialogMessage = res.getString(R.string.sync_error_invalid_sync_server, url);\\n[DEL] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[DEL] } else {\\n[DEL] if (result.length > 1 && result[1] instanceof Integer) {\\n[DEL] int code = (Integer) result[1];\\n[DEL] dialogMessage = rewriteError(code);\\n[DEL] if (dialogMessage == null) {\\n[DEL] dialogMessage = res.getString(R.string.sync_log_error_specific,\\n[DEL] Integer.toString(code), result[2]);\\n[ADD] switch (resultType) {\\n[ADD] case \"badAuth\":\\n[ADD] // delete old auth information\\n[ADD] SharedPreferences preferences = AnkiDroidApp.getSharedPrefs(getBaseContext());\\n[ADD] Editor editor = preferences.edit();\\n[ADD] editor.putString(\"username\", \"\");\\n[ADD] editor.putString(\"hkey\", \"\");\\n[ADD] editor.apply();\\n[ADD] // then show not logged in dialog\\n[ADD] showSyncErrorDialog(SyncErrorDialog.DIALOG_USER_NOT_LOGGED_IN_SYNC);\\n[ADD] break;\\n[ADD] case \"noChanges\":\\n[ADD] SyncStatus.markSyncCompleted();\\n[ADD] // show no changes message, use false flag so we don\\'t show \"sync error\" as the Dialog title\\n[ADD] showSyncLogMessage(R.string.sync_no_changes_message, \"\");\\n[ADD] break;\\n[ADD] case \"clockOff\":\\n[ADD] long diff = (Long) result[1];\\n[ADD] if (diff >= 86100) {\\n[ADD] // The difference if more than a day minus 5 minutes acceptable by ankiweb error\\n[ADD] dialogMessage = res.getString(R.string.sync_log_clocks_unsynchronized, diff,\\n[ADD] res.getString(R.string.sync_log_clocks_unsynchronized_date));\\n[ADD] } else if (Math.abs((diff % 3600.0) - 1800.0) >= 1500.0) {\\n[ADD] // The difference would be within limit if we adjusted the time by few hours\\n[ADD] // It doesn\\'t work for all timezones, but it covers most and it\\'s a guess anyway\\n[ADD] dialogMessage = res.getString(R.string.sync_log_clocks_unsynchronized, diff,\\n[ADD] res.getString(R.string.sync_log_clocks_unsynchronized_tz));\\n[ADD] } else {\\n[ADD] dialogMessage = res.getString(R.string.sync_log_clocks_unsynchronized, diff, \"\");\\n                             }\\n[DEL] } else if (result[0] instanceof String) {\\n[DEL] dialogMessage = res.getString(R.string.sync_log_error_specific, Integer.toString(-1), result[0]);\\n[DEL] } else {\\n[ADD] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[ADD] break;\\n[ADD] case \"fullSync\":\\n[ADD] if (getCol().isEmpty()) {\\n[ADD] // don\\'t prompt user to resolve sync conflict if local collection empty\\n[ADD] sync(FULL_DOWNLOAD);\\n[ADD] // TODO: Also do reverse check to see if AnkiWeb collection is empty if Anki Desktop\\n[ADD] // implements it\\n[ADD] } else {\\n[ADD] // If can\\'t be resolved then automatically then show conflict resolution dialog\\n[ADD] showSyncErrorDialog(SyncErrorDialog.DIALOG_SYNC_CONFLICT_RESOLUTION);\\n[ADD] }\\n[ADD] break;\\n[ADD] case \"basicCheckFailed\":\\n[ADD] dialogMessage = res.getString(R.string.sync_basic_check_failed, res.getString(R.string.check_db));\\n[ADD] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[ADD] break;\\n[ADD] case \"dbError\":\\n[ADD] showSyncErrorDialog(SyncErrorDialog.DIALOG_SYNC_CORRUPT_COLLECTION, syncMessage);\\n[ADD] break;\\n[ADD] case \"overwriteError\":\\n[ADD] dialogMessage = res.getString(R.string.sync_overwrite_error);\\n[ADD] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[ADD] break;\\n[ADD] case \"remoteDbError\":\\n[ADD] dialogMessage = res.getString(R.string.sync_remote_db_error);\\n[ADD] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[ADD] break;\\n[ADD] case \"sdAccessError\":\\n[ADD] dialogMessage = res.getString(R.string.sync_write_access_error);\\n[ADD] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[ADD] break;\\n[ADD] case \"finishError\":\\n[ADD] dialogMessage = res.getString(R.string.sync_log_finish_error);\\n[ADD] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[ADD] break;\\n[ADD] case \"connectionError\":\\n[ADD] dialogMessage = res.getString(R.string.sync_connection_error);\\n[ADD] if (result.length >= 1 && result[1] instanceof Exception) {\\n[ADD] dialogMessage += \"\\\\n\\\\n\" + ((Exception) result[1]).getLocalizedMessage();\\n[ADD] }\\n[ADD] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[ADD] break;\\n[ADD] case \"IOException\":\\n[ADD] handleDbError();\\n[ADD] break;\\n[ADD] case \"genericError\":\\n                             dialogMessage = res.getString(R.string.sync_generic_error);\\n[DEL] }\\n[DEL] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[ADD] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[ADD] break;\\n[ADD] case \"OutOfMemoryError\":\\n[ADD] dialogMessage = res.getString(R.string.error_insufficient_memory);\\n[ADD] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[ADD] break;\\n[ADD] case \"sanityCheckError\":\\n[ADD] dialogMessage = res.getString(R.string.sync_sanity_failed);\\n[ADD] showSyncErrorDialog(SyncErrorDialog.DIALOG_SYNC_SANITY_ERROR,\\n[ADD] joinSyncMessages(dialogMessage, syncMessage));\\n[ADD] break;\\n[ADD] case \"serverAbort\":\\n[ADD] // syncMsg has already been set above, no need to fetch it here.\\n[ADD] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[ADD] break;\\n[ADD] case \"mediaSyncServerError\":\\n[ADD] dialogMessage = res.getString(R.string.sync_media_error_check);\\n[ADD] showSyncErrorDialog(SyncErrorDialog.DIALOG_MEDIA_SYNC_ERROR,\\n[ADD] joinSyncMessages(dialogMessage, syncMessage));\\n[ADD] break;\\n[ADD] case \"customSyncServerUrl\":\\n[ADD] String url = result.length > 1 && result[1] instanceof CustomSyncServerUrlException\\n[ADD] ? ((CustomSyncServerUrlException) result[1]).getUrl() : \"unknown\";\\n[ADD] dialogMessage = res.getString(R.string.sync_error_invalid_sync_server, url);\\n[ADD] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[ADD] break;\\n[ADD] default:\\n[ADD] if (result.length > 1 && result[1] instanceof Integer) {\\n[ADD] int code = (Integer) result[1];\\n[ADD] dialogMessage = rewriteError(code);\\n[ADD] if (dialogMessage == null) {\\n[ADD] dialogMessage = res.getString(R.string.sync_log_error_specific,\\n[ADD] Integer.toString(code), result[2]);\\n[ADD] }\\n[ADD] } else if (result[0] instanceof String) {\\n[ADD] dialogMessage = res.getString(R.string.sync_log_error_specific, Integer.toString(-1), result[0]);\\n[ADD] } else {\\n[ADD] dialogMessage = res.getString(R.string.sync_generic_error);\\n[ADD] }\\n[ADD] showSyncErrorMessage(joinSyncMessages(dialogMessage, syncMessage));\\n[ADD] break;\\n                     }\\n                 } else {\\n                     dialogMessage = res.getString(R.string.sync_generic_error);',\n",
              " '   }\\n \\n   private static void addResourceToConf(Configuration conf) {\\n[DEL] conf.setStrings(getHdfsUriHadoopPropertyName(), HDFS_URI_DEFAULT);\\n     addHadoopConfigPropertiesToConf(conf);\\n[ADD] String hdfsUriKey = getHdfsUriHadoopPropertyName();\\n     if (CompactionRunner.properties.containsKey(HDFS_URI)) {\\n[DEL] conf.setStrings(getHdfsUriHadoopPropertyName(), CompactionRunner.properties.getProperty(HDFS_URI));\\n[ADD] conf.set(hdfsUriKey, CompactionRunner.properties.getProperty(HDFS_URI));\\n     }\\n[ADD] if (Strings.isNullOrEmpty(conf.get(hdfsUriKey))) {\\n[ADD] conf.set(hdfsUriKey, HDFS_URI_DEFAULT);\\n[ADD] }\\n[ADD] CompactionRunner.properties.setProperty(HDFS_URI, conf.get(hdfsUriKey));\\n   }\\n \\n   private static void addHadoopConfigPropertiesToConf(Configuration conf) {',\n",
              " ' \\t\\t} catch (Throwable e) {\\n \\t\\t\\tthrow new IbmAttachOperationFailedException(\"startLocalManagementAgent error starting agent:\" + e.getClass() + \" \" + e.getMessage());\\t\\t //$NON-NLS-1$ //$NON-NLS-2$\\n \\t\\t}\\n[DEL] /*\\n[DEL] * sun.management.Agent.startLocalManagementAgent() in Java 8 sets\\n[DEL] * c.s.m.j.localConnectorAddress in System.properties.\\n[DEL] * jdk.internal.agent.Agent.startLocalManagementAgent() in Java 9 sets\\n[DEL] * c.s.m.j.localConnectorAddress in a different Properties object.\\n[DEL] */\\n[ADD] \\n \\t\\tProperties systemProperties = com.ibm.oti.vm.VM.getVMLangAccess().internalGetProperties();\\n \\t\\tString addr;\\n[DEL] /*[IF Sidecar19-SE-B165]*/\\n \\t\\tsynchronized (systemProperties) {\\n \\t\\t\\taddr = systemProperties.getProperty(LOCAL_CONNECTOR_ADDRESS);\\n \\t\\t\\tif (Objects.isNull(addr)) {',\n",
              " '  * <p>\\n  * The consume operation expects an XML body and a set of headers and attachments if required.\\n  * <p>\\n[DEL] * For the cases where no input parameters are required the {@link SoapRequestGenerator} will generate a body to perform the\\n[DEL] * operation if a {@code null} value is passed.\\n  *\\n  * @since 4.0\\n  */\\n public class ConsumeOperation {\\n \\n[DEL] private final SoapRequestGenerator requestGenerator = new SoapRequestGenerator();\\n[DEL] private final SoapResponseGenerator responseGenerator = new SoapResponseGenerator();\\n[DEL] \\n   /**\\n    * Consumes an operation from a SOAP Web Service.\\n    *',\n",
              " '             _currentEditor = control;\\n         }\\n \\n[DEL] private int CountPropsFromOutline(GridEntryCollection entries)\\n[ADD] private int CountPropertiesFromOutline(GridEntryCollection entries)\\n         {\\n[DEL] Debug.WriteLineIf(CompModSwitches.DebugGridView.TraceVerbose, \"PropertyGridView:CountPropsFromOutLine\");\\n[ADD] Debug.WriteLineIf(CompModSwitches.DebugGridView.TraceVerbose, \"PropertyGridView:CountPropertiesFromOutline\");\\n             if (entries is null)\\n             {\\n                 return 0;\\n             }\\n \\n[DEL] int cProps = entries.Count;\\n[ADD] int propertyCount = entries.Count;\\n             for (int i = 0; i < entries.Count; i++)\\n             {\\n[DEL] if (((GridEntry)entries[i]).InternalExpanded)\\n[ADD] if (entries[i].InternalExpanded)\\n                 {\\n[DEL] cProps += CountPropsFromOutline(((GridEntry)entries[i]).Children);\\n[ADD] propertyCount += CountPropertiesFromOutline(entries[i].Children);\\n                 }\\n             }\\n \\n[DEL] return cProps;\\n[ADD] return propertyCount;\\n         }\\n \\n         /// <summary>',\n",
              " '   @Test\\n   public void typeIsInstanceOfGivenClassFromAttribute() throws ClassNotFoundException {\\n     ObjectTypeVisitor visitor = new ObjectTypeVisitor(baseComponentModelBuilder()\\n[DEL] .withRawParameter(\"type\", \"org.mule.runtime.core.internal.processor.ReferenceProcessor\").build());\\n[ADD] .withRawParameter(\"type\", LOGGER_PROCESSOR_FQCN).build());\\n     TypeDefinition typeDefinition = fromConfigurationAttribute(\"type\");\\n     typeDefinition.visit(visitor);\\n[DEL] assertTrue(ReferenceProcessor.class.isAssignableFrom(visitor.getType()));\\n[ADD] assertTrue(LoggerMessageProcessor.class.isAssignableFrom(visitor.getType()));\\n   }\\n \\n   @Test\\n   public void typeIsInstanceOfCheckedClassFromAttribute() throws ClassNotFoundException {\\n     ObjectTypeVisitor visitor = new ObjectTypeVisitor(baseComponentModelBuilder()\\n[DEL] .withRawParameter(\"type\", \"org.mule.runtime.core.internal.processor.ReferenceProcessor\").build());\\n[ADD] .withRawParameter(\"type\", LOGGER_PROCESSOR_FQCN).build());\\n     TypeDefinition typeDefinition = fromConfigurationAttribute(\"type\")\\n[DEL] .checkingThatIsClassOrInheritsFrom(ReferenceProcessor.class);\\n[ADD] .checkingThatIsClassOrInheritsFrom(LoggerMessageProcessor.class);\\n     typeDefinition.visit(visitor);\\n[DEL] assertTrue(ReferenceProcessor.class.isAssignableFrom(visitor.getType()));\\n[ADD] assertTrue(LoggerMessageProcessor.class.isAssignableFrom(visitor.getType()));\\n   }\\n \\n   @Test\\n   public void typeIsInstanceOfClassInheritedFromCheckedClassFromAttribute() throws ClassNotFoundException {\\n     ObjectTypeVisitor visitor = new ObjectTypeVisitor(baseComponentModelBuilder()\\n[DEL] .withRawParameter(\"type\", \"org.mule.runtime.core.internal.processor.ReferenceProcessor\").build());\\n[ADD] .withRawParameter(\"type\", LOGGER_PROCESSOR_FQCN).build());\\n     // Check that ReferenceProcessor inherits from AbstractProcessor\\n     TypeDefinition typeDefinition = fromConfigurationAttribute(\"type\")\\n[DEL] .checkingThatIsClassOrInheritsFrom(AbstractProcessor.class);\\n[ADD] .checkingThatIsClassOrInheritsFrom(AbstractComponent.class);\\n     typeDefinition.visit(visitor);\\n[DEL] assertTrue(AbstractProcessor.class.isAssignableFrom(visitor.getType()));\\n[ADD] assertTrue(AbstractComponent.class.isAssignableFrom(visitor.getType()));\\n   }\\n \\n   @Test',\n",
              " '             return names;\\n         }\\n \\n[DEL] \\n[ADD] @SuppressWarnings(\"deprecation\")\\n         @Override\\n         public void onActivityResult(int requestCode, int resultCode, Intent data) {\\n             super.onActivityResult(requestCode, resultCode, data);',\n",
              " ' \\n     private GroupCorrelation groupCorrelation;\\n \\n[ADD] @Override\\n[ADD] public EventContext getInternalContext() {\\n[ADD] return this.getContext();\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public Map<String, ?> getInternalParameters() {\\n[ADD] return unmodifiableMap(internalParameters);\\n[ADD] }\\n[ADD] \\n     @Override\\n     public Optional<GroupCorrelation> getGroupCorrelation() {\\n       return ofNullable(groupCorrelation);',\n",
              " '             *byteLength = typedArrayBase->GetByteLength();\\n         }\\n     }\\n[ADD] \\n[ADD] #if ENABLE_TTD\\n[ADD] Js::ScriptContext* scriptContext = JsrtContext::GetCurrent()->GetScriptContext();\\n[ADD] if(PERFORM_JSRT_TTD_RECORD_ACTION_CHECK(scriptContext))\\n[ADD] {\\n[ADD] if(arrayBuffer != nullptr)\\n[ADD] {\\n[ADD] BEGIN_JS_RUNTIME_CALLROOT_EX(scriptContext, false)\\n[ADD] {\\n[ADD] PERFORM_JSRT_TTD_RECORD_ACTION_WRESULT(scriptContext, scriptContext->GetThreadContext()->TTDLog->RecordJsRTGetTypedArrayInfo(scriptContext, typedArray, &__ttd_resultPtr));\\n[ADD] PERFORM_JSRT_TTD_RECORD_ACTION_PROCESS_RESULT(arrayBuffer);\\n[ADD] }\\n[ADD] END_JS_RUNTIME_CALL(scriptContext);\\n[ADD] }\\n[ADD] }\\n[ADD] #endif\\n[ADD] \\n     END_JSRT_NO_EXCEPTION\\n }\\n ',\n",
              " ' \\n import java.io.IOException;\\n import java.net.URL;\\n[ADD] import java.util.Map;\\n[ADD] import java.util.concurrent.ExecutorService;\\n[ADD] import java.util.concurrent.Executors;\\n \\n import org.apache.curator.test.TestingServer;\\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;\\n import org.apache.helix.HelixException;\\n[ADD] import org.apache.helix.HelixManager;\\n[ADD] import org.apache.helix.HelixManagerFactory;\\n[ADD] import org.apache.helix.InstanceType;\\n import org.apache.helix.PropertyPathBuilder;\\n import org.apache.helix.manager.zk.ZkClient;\\n import org.slf4j.Logger;',\n",
              " ' \\n   @Test\\n   @Issue(\"MULE-18041\")\\n[ADD] @Issue(\"MULE-18562\")\\n   @Story(ERROR_HANDLING)\\n[ADD] @Ignore\\n   public void supressMessagingException() {\\n     when(event.getError()).thenReturn(Optional.empty());\\n     when(operationModel.getErrorModels()).thenReturn(singleton(newError(CONNECTIVITY_ERROR_IDENTIFIER, ERROR_NAMESPACE).build()));',\n",
              " '           : new Path(publisherOutputDir, status.getPath().getName());\\n \\n       LOG.info(String.format(\"Moving %s to %s\", status.getPath(), finalOutputPath));\\n[DEL] parallelRunner.renamePath(status.getPath(), finalOutputPath, Optional.<String> absent());\\n[ADD] parallelRunner.movePath(status.getPath(), this.publisherFileSystemByBranches.get(branchId),\\n[ADD] finalOutputPath, Optional.<String> absent());\\n     }\\n   }\\n ',\n",
              " ' \\t\\t\\telse {\\n \\t\\t\\t\\tInteger priority1 = message1.getHeaders().getPriority();\\n \\t\\t\\t\\tInteger priority2 = message2.getHeaders().getPriority();\\n[DEL] \\n[ADD] \\n \\t\\t\\t\\tpriority1 = priority1 != null ? priority1 : 0;\\n \\t\\t\\t\\tpriority2 = priority2 != null ? priority2 : 0;\\n \\t\\t\\t\\tcompareResult = priority2.compareTo(priority1);\\n \\t\\t\\t}\\n[DEL] \\n[ADD] \\n \\t\\t\\tif (compareResult == 0){\\n \\t\\t\\t\\tLong sequence1 = ((MessageWrapper) message1).getSequence();\\n \\t\\t\\t\\tLong sequence2 = ((MessageWrapper) message2).getSequence();',\n",
              " ' \\n \\tprivate volatile MessageChannel defaultErrorChannel;\\n \\n[ADD] private volatile String defaultErrorChannelName;\\n[ADD] \\n \\tprivate volatile long sendTimeout = 1000;\\n \\n ',\n",
              " ' \\n   private Map<String, Element> getAllOperations(ProcessingEnvironment processingEnv, Element element) {\\n     Map<String, Element> elements = new LinkedHashMap<>();\\n[DEL] Configurations configurations = processor.getAnnotationFromType(processingEnv, (TypeElement) element, Configurations.class);\\n[DEL] if (configurations != null) {\\n[DEL] List<TypeElement> configs = processor.getAnnotationClassesValue(element, Configurations.class, configurations.value());\\n[DEL] configs.forEach(c -> elements.putAll(getOperationMethodElements(processingEnv, c)));\\n[DEL] }\\n[ADD] \\n[ADD] processor.getArrayClassAnnotationValue(element, Configurations.class, \"value\", processingEnv)\\n[ADD] .forEach(c -> elements.putAll(getOperationMethodElements(processingEnv, c)));\\n[ADD] \\n     elements.putAll(getOperationMethodElements(processingEnv, element));\\n     return elements;\\n   }\\n \\n   private Map<String, Element> getOperationMethodElements(ProcessingEnvironment processingEnv, Element withOperationsElement) {\\n     ImmutableMap.Builder<String, Element> methods = ImmutableMap.builder();\\n[DEL] Operations operationsAnnotation =\\n[DEL] processor.getAnnotationFromType(processingEnv, (TypeElement) withOperationsElement, Operations.class);\\n[DEL] if (operationsAnnotation != null) {\\n[DEL] final Class<?>[] operationsClasses = operationsAnnotation.value();\\n[DEL] for (Class<?> operationClass : operationsClasses) {\\n[DEL] while (operationClass != null && !operationClass.equals(Object.class)) {\\n[DEL] TypeElement operationElement = processingEnv.getElementUtils().getTypeElement(operationClass.getName());\\n[DEL] if (operationElement != null) {\\n[DEL] for (Method operation : getApiMethods(operationClass)) {\\n[DEL] operationElement.getEnclosedElements().stream()\\n[DEL] .filter(e -> e.getSimpleName().toString().equals(operation.getName())).findFirst()\\n[DEL] .ifPresent(operationMethodElement -> methods.put(operation.getName(), operationMethodElement));\\n[DEL] }\\n[DEL] operationClass = operationClass.getSuperclass();\\n[DEL] }\\n[ADD] \\n[ADD] List<TypeElement> operationsClasses =\\n[ADD] processor.getArrayClassAnnotationValue(withOperationsElement, Operations.class, \"value\", processingEnv);\\n[ADD] \\n[ADD] for (TypeElement operationElement : operationsClasses) {\\n[ADD] while (operationElement != null\\n[ADD] && !processingEnv.getTypeUtils().isSameType(operationElement.asType(), objectType.asType())) {\\n[ADD] for (ExecutableElement operation : getApiMethods(operationElement, processingEnv)) {\\n[ADD] operationElement.getEnclosedElements().stream()\\n[ADD] .filter(e -> e.getSimpleName().toString().equals(operation.getSimpleName().toString())).findFirst()\\n[ADD] .ifPresent(operationMethodElement -> methods.put(operation.getSimpleName().toString(), operationMethodElement));\\n[ADD] }\\n[ADD] Element superClass = processingEnv.getTypeUtils().asElement(operationElement.getSuperclass());\\n[ADD] if (superClass instanceof TypeElement) {\\n[ADD] operationElement = (TypeElement) superClass;\\n[ADD] } else {\\n[ADD] operationElement = null;\\n         }\\n       }\\n     }\\n     return methods.build();\\n   }\\n[DEL] //TODO MULE-14311 - REVERT THIS TEMPORALLY\\n[DEL] //    return new ConfigurationASTElement((TypeElement) withOperationsElement, processingEnv).getOperationContainers().stream()\\n[DEL] //        .map(OperationContainerElement::getOperations)\\n[DEL] //        .flatMap(Collection::stream)\\n[DEL] //        .collect(toMap(NamedObject::getName, op -> (op).getExecutableElement()));\\n[DEL] //  }\\n }',\n",
              " '   protected MetadataService metadataService;\\n   protected ClassTypeLoader typeLoader = ExtensionsTypeLoaderFactory.getDefault().createTypeLoader();\\n   protected BaseTypeBuilder typeBuilder = BaseTypeBuilder.create(JAVA);\\n[ADD] protected MetadataComponentDescriptorProvider provider;\\n \\n   @Parameterized.Parameter\\n[DEL] public ResolutionType resolutionType = EXPLICIT_RESOLUTION;\\n[DEL] \\n[DEL] @Parameterized.Parameter(1)\\n[DEL] public MetadataComponentDescriptorProvider provider = explicitMetadataResolver;\\n[ADD] public ResolutionType resolutionType;\\n \\n   @Override\\n   protected Class<?>[] getAnnotatedExtensionClasses() {',\n",
              " ' \\tpublic Message<Object> receive() {\\n \\t\\tAssert.isTrue(this.initialized, \"This class is not yet initialized. Invoke its afterPropertiesSet() method\");\\n \\t\\tMessage<Object> message = null;\\n[DEL] Query query = new BasicQuery(this.queryExpression.getValue(this.evaluationContext, String.class));\\n[ADD] Object value = this.queryExpression.getValue(this.evaluationContext, Object.class);\\n[ADD] Assert.notNull(value, \"\\'queryExpression\\' must not evaluate to null\");\\n[ADD] Query query;\\n[ADD] if (value instanceof String) {\\n[ADD] query = new BasicQuery((String) value);\\n[ADD] }\\n[ADD] else if (value instanceof Query) {\\n[ADD] query = ((Query) value);\\n[ADD] }\\n[ADD] else {\\n[ADD] throw new AssertionError(\"\\'queryExpression\\' must evaluate to String or org.springframework.data.mongodb.core.query.Query\");\\n[ADD] }\\n[ADD] \\n \\t\\tAssert.notNull(query, \"\\'queryExpression\\' must not evaluate to null\");\\n \\t\\tString collectionName = this.collectionNameExpression.getValue(this.evaluationContext, String.class);\\n \\t\\tAssert.notNull(collectionName, \"\\'collectionNameExpression\\' must not evaluate to null\");',\n",
              " '     private final DBEventDefinitionService eventDefinitionService;\\n     private final DBNotificationGracePeriodService notificationGracePeriodService;\\n     private final Map<String, EventNotification.Factory> eventNotificationFactories;\\n[DEL] private final long retryTimeInMinutes;\\n[DEL] \\n[ADD] private final EventsConfiguration eventsConfiguration;\\n \\n     @Inject\\n     public EventNotificationExecutionJob(@Assisted JobDefinitionDto jobDefinition,\\n                                          DBNotificationService dbNotificationService,\\n                                          DBEventDefinitionService eventDefinitionService,\\n                                          DBNotificationGracePeriodService notificationGracePeriodService,\\n[DEL] Map<String, EventNotification.Factory> eventNotificationFactories) {\\n[ADD] Map<String, EventNotification.Factory> eventNotificationFactories,\\n[ADD] EventsConfigurationProvider configurationProvider) {\\n         this.jobConfig = (Config) jobDefinition.config();\\n         this.notificationService = dbNotificationService;\\n         this.eventDefinitionService = eventDefinitionService;\\n         this.notificationGracePeriodService = notificationGracePeriodService;\\n         this.eventNotificationFactories = eventNotificationFactories;\\n[DEL] this.retryTimeInMinutes = 1L; // TODO: Make retry time configurable in the plugin settings\\n[ADD] this.eventsConfiguration = configurationProvider.get();\\n     }\\n \\n     @Override',\n",
              " '         for (UserVmJoinVO vm : userVms) {\\n             ramtotal += Long.valueOf(vm.getRamSize());\\n         }\\n[DEL] return ramtotal;\\n[ADD] \\n[ADD] ServiceOffering defaultRouterOffering = null;\\n[ADD] String globalRouterOffering = VirtualNetworkApplianceManager.VirtualRouterServiceOffering.value();\\n[ADD] if (globalRouterOffering != null) {\\n[ADD] defaultRouterOffering = _serviceOfferingDao.findByUuid(globalRouterOffering);\\n[ADD] }\\n[ADD] if (defaultRouterOffering == null) {\\n[ADD] defaultRouterOffering =  _serviceOfferingDao.findByName(ServiceOffering.routerDefaultOffUniqueName);\\n[ADD] }\\n[ADD] GenericSearchBuilder<ServiceOfferingVO, SumCount> memorySearch = _serviceOfferingDao.createSearchBuilder(SumCount.class);\\n[ADD] memorySearch.select(\"sum\", Func.SUM, memorySearch.entity().getRamSize());\\n[ADD] memorySearch.select(\"count\", Func.COUNT, (Object[])null);\\n[ADD] SearchBuilder<VMInstanceVO> join1 = _vmDao.createSearchBuilder();\\n[ADD] join1.and(\"accountId\", join1.entity().getAccountId(), Op.EQ);\\n[ADD] join1.and(\"type\", join1.entity().getType(), Op.IN);\\n[ADD] join1.and(\"state\", join1.entity().getState(), SearchCriteria.Op.NIN);\\n[ADD] join1.and(\"displayVm\", join1.entity().isDisplayVm(), Op.EQ);\\n[ADD] memorySearch.join(\"offerings\", join1, memorySearch.entity().getId(), join1.entity().getServiceOfferingId(), JoinBuilder.JoinType.INNER);\\n[ADD] memorySearch.done();\\n[ADD] \\n[ADD] SearchCriteria<SumCount> sc = memorySearch.create();\\n[ADD] sc.setJoinParameters(\"offerings\", \"accountId\", accountId);\\n[ADD] sc.setJoinParameters(\"offerings\", \"type\", VirtualMachine.Type.DomainRouter); // domain routers\\n[ADD] sc.setJoinParameters(\"offerings\", \"displayVm\", 1);\\n[ADD] \\n[ADD] if (VirtualMachineManager.ResoureCountRunningVMsonly.value()) {\\n[ADD] sc.setJoinParameters(\"offerings\", \"state\", new Object[] {State.Destroyed, State.Error, State.Expunging, State.Stopped});\\n[ADD] } else {\\n[ADD] sc.setJoinParameters(\"offerings\", \"state\", new Object[] {State.Destroyed, State.Error, State.Expunging});\\n[ADD] }\\n[ADD] sc.setJoinParameters(\"offerings\", \"displayVm\", 1);\\n[ADD] List<SumCount> memory = _serviceOfferingDao.customSearch(sc, null);\\n[ADD] if (memory != null) {\\n[ADD] // Calculate VR memory count depending on global setting\\n[ADD] if (VirtualMachineManager.ResourceCountRouters.valueIn(domain.getId())) {\\n[ADD] ramtotal += memory.get(0).sum;\\n[ADD] if (VirtualMachineManager.ResourceCountRoutersType.valueIn(domain.getId())\\n[ADD] .equalsIgnoreCase(VirtualMachineManager.COUNT_DELTA_VR_RESOURCES)) {\\n[ADD] ramtotal = ramtotal - memory.get(0).count * defaultRouterOffering.getRamSize();\\n[ADD] }\\n[ADD] }\\n[ADD] return ramtotal;\\n[ADD] } else {\\n[ADD] return ramtotal;\\n[ADD] }\\n     }\\n \\n     public long calculateSecondaryStorageForAccount(long accountId) {',\n",
              " ' \\t */\\n \\tpublic JavaPreprocessor(OutputStream metadataOut, File inputFile, OutputStream out, File outputFile) {\\n \\t\\tsuper();\\n[ADD] \\n[ADD] String osname = System.getProperty(\"os.name\");\\n[ADD] if (\"z/OS\".equalsIgnoreCase(osname)) {\\n[ADD] charset = Charset.forName(\"IBM-1047\");\\n[ADD] } else {\\n[ADD] charset = Charset.forName(\"US-ASCII\");\\n[ADD] }\\n[ADD] \\n \\t\\tthis.inStack = new Stack<>();\\n \\t\\tthis.inFile = inputFile;\\n[DEL] this.out = new OutputStreamWriter(out, ASCII);\\n[ADD] this.out = new OutputStreamWriter(out, charset);\\n \\n \\t\\tif (metadataOut != null) {\\n[DEL] this.metadataOut = new OutputStreamWriter(metadataOut, ASCII);\\n[ADD] this.metadataOut = new OutputStreamWriter(metadataOut, charset);\\n \\t\\t\\ttry {\\n \\t\\t\\t\\tthis.metadataOut.write(inputFile.getAbsolutePath());\\n \\t\\t\\t\\tthis.metadataOut.write(newLine);',\n",
              " '    {\\n       if (!isCheckable())\\n          return;\\n[ADD] \\n       checked_ = checked;\\n[DEL] \\n[DEL] // sync desktop menus\\n       if (Desktop.isDesktop())\\n          DesktopMenuCallback.setCommandChecked(id_, checked_);\\n    }',\n",
              " '   }\\n \\n   /**\\n[DEL] * Prompt user to close all open Jobs & Transformations.  Save setting to warn user the next time.\\n[ADD] * Prompt user to close all open Jobs & Transformations if they have execute permissions.\\n[ADD] * If they don\\'t have execute permission then warn user if they really want to disconnect\\n[ADD] * from repository.  If yes, close all tabs.\\n[ADD] *\\n[ADD] * @return If user agrees with closing of tabs then return true so we can disconnect from the repo.\\n    */\\n[DEL] public void closeAllJobsAndTransformations() {\\n[DEL] // Query user to see if they want to close trans/jobs\\n[DEL] if ( props.showCloseAllFilesWarning() ) {\\n[DEL] MessageDialogWithToggle md = new MessageDialogWithToggle( getShell(), BaseMessages.getString( PKG, \"Spoon.Dialog.PromptToCloseAll.Title\" ), null,\\n[ADD] public boolean closeAllJobsAndTransformations() {\\n[ADD] // Check to see if there are any open jobs/trans\\n[ADD] if ( getActiveMeta() == null ) {\\n[ADD] return true;\\n[ADD] }\\n[ADD] \\n[ADD] // If user does not have Execute permissions, then display a warning.  If they agree with\\n[ADD] // close warning then close jobs/trans\\n[ADD] // If user has Execute permissions then give them opportunity to change their minds.\\n[ADD] boolean operationsNotAllowed = RepositorySecurityUI\\n[ADD] .verifyOperations( shell, rep, false, RepositoryOperation.EXECUTE_TRANSFORMATION, RepositoryOperation.EXECUTE_JOB );\\n[ADD] \\n[ADD] MessageDialog md;\\n[ADD] final int answer;\\n[ADD] if ( operationsNotAllowed ) {\\n[ADD] // User does not have Execute permissions.  Warn them that we are going to close files\\n[ADD] // If cancel is clicked, then don\\'t disconnect\\n[ADD] md = new MessageDialog( getShell(), BaseMessages.getString( PKG, \"Spoon.Dialog.WarnToCloseAll.Title\" ), null,\\n[ADD] BaseMessages.getString( PKG, \"Spoon.Dialog.WarnToCloseAll.Message\" ), MessageDialog.WARNING,\\n[ADD] new String[] { BaseMessages.getString( PKG, \"Spoon.Message.Warning.Yes\" ), BaseMessages\\n[ADD] .getString( PKG, \"Spoon.Message.Warning.No\" ) }, 0 );\\n[ADD] \\n[ADD] answer = md.open();\\n[ADD] } else if ( props.showCloseAllFilesWarning() ) {\\n[ADD] md = new MessageDialogWithToggle( getShell(),\\n[ADD] BaseMessages.getString( PKG, \"Spoon.Dialog.PromptToCloseAll.Title\" ), null,\\n           BaseMessages.getString( PKG, \"Spoon.Dialog.PromptToCloseAll.Message\" ), MessageDialog.QUESTION,\\n[DEL] new String[] { BaseMessages.getString( PKG, \"Spoon.Message.Warning.Yes\" ),\\n[DEL] BaseMessages.getString( PKG, \"Spoon.Message.Warning.No\" ),\\n[DEL] BaseMessages.getString( PKG, \"Spoon.Message.Warning.Cancel\" ) },\\n[DEL] 1,\\n[ADD] new String[] { BaseMessages.getString( PKG, \"Spoon.Message.Warning.Yes\" ), BaseMessages\\n[ADD] .getString( PKG, \"Spoon.Message.Warning.No\" ), BaseMessages\\n[ADD] .getString( PKG, \"Spoon.Message.Warning.Cancel\" ) }, 0,\\n           BaseMessages.getString( PKG, \"Spoon.Dialog.PromptToCloseAll.DontAskAgain.Label\" ),\\n           !props.showCloseAllFilesWarning() );\\n \\n       MessageDialogWithToggle.setDefaultImage( GUIResource.getInstance().getImageSpoon() );\\n \\n[DEL] final int answer = md.open();\\n[DEL] if ( ( answer & 0xFF ) == 0 ) {\\n[DEL] // User specified that they want to close all\\n[DEL] Spoon.getInstance().closeAllFiles();\\n[DEL] }\\n[ADD] answer = md.open();\\n \\n[DEL] props.showSetCloseAllFilesWarning( !md.getToggleState() );\\n[ADD] // Save the property\\n[ADD] props.showSetCloseAllFilesWarning( !( (MessageDialogWithToggle) md ).getToggleState() );\\n       props.saveProps();\\n     } else {\\n       // User did not want warning before closing files - close them now.\\n       Spoon.getInstance().closeAllFiles();\\n[ADD] return true;\\n[ADD] }\\n[ADD] \\n[ADD] // If user acknowledged closing of all tabs, then close them.\\n[ADD] if ( ( answer & 0xFF )  == 0 ) {\\n[ADD] // Yes - User specified that they want to close all\\n[ADD] Spoon.getInstance().closeAllFiles();\\n[ADD] } else if ( answer == 257 /* No button */ ) {\\n[ADD] // No - don\\'t close tabs\\n[ADD] return true;\\n[ADD] } else {\\n[ADD] // Cancel\\n[ADD] return false;\\n     }\\n[ADD] \\n[ADD] // User agreed to close the tabs.\\n[ADD] return true;\\n   }\\n \\n   public void closeSpoonBrowser() {',\n",
              " '       readLock.lock();\\n       String partition = formatPartitionKey(partitionStr);\\n       ensurePartitionLoadedCorrectly(partition);\\n[DEL] return fetchAllStoredFileGroups(partition).map(fileGroup -> {\\n[DEL] Option<FileSlice> fileSlice = fileGroup.getLatestFileSliceBeforeOrOn(maxInstantTime);\\n[DEL] // if the file-group is under construction, pick the latest before compaction instant time.\\n[DEL] if (fileSlice.isPresent()) {\\n[DEL] fileSlice = Option.of(fetchMergedFileSlice(fileGroup, fileSlice.get()));\\n[DEL] }\\n[DEL] return fileSlice;\\n[DEL] }).filter(Option::isPresent).map(Option::get).map(this::addBootstrapBaseFileIfPresent);\\n[ADD] return fetchAllStoredFileGroups(partition)\\n[ADD] .filter(fg -> !isFileGroupReplacedBeforeOrOn(fg.getFileGroupId(), maxInstantTime))\\n[ADD] .map(fileGroup -> {\\n[ADD] Option<FileSlice> fileSlice = fileGroup.getLatestFileSliceBeforeOrOn(maxInstantTime);\\n[ADD] // if the file-group is under construction, pick the latest before compaction instant time.\\n[ADD] if (fileSlice.isPresent()) {\\n[ADD] fileSlice = Option.of(fetchMergedFileSlice(fileGroup, fileSlice.get()));\\n[ADD] }\\n[ADD] return fileSlice;\\n[ADD] }).filter(Option::isPresent).map(Option::get).map(this::addBootstrapBaseFileIfPresent);\\n     } finally {\\n       readLock.unlock();\\n     }',\n",
              " '                      else\\n                      {\\n                         externalEditCheckInterval_.reset();\\n[ADD] isWaitingForUserResponseToExternalEdit_ = true;\\n                         globalDisplay_.showYesNoMessage(\\n                               GlobalDisplay.MSG_WARNING,\\n                               \"File Changed\",',\n",
              " '   boolean getUsesProvidedSparkContext();\\n   void setUsesProvidedSparkContext(boolean value);\\n \\n[ADD] /**\\n[ADD] * List of local files to make available to workers.\\n[ADD] *\\n[ADD] * <p>Jars are placed on the worker\\'s classpath.\\n[ADD] *\\n[ADD] * <p>The default value is the list of jars from the main program\\'s classpath.\\n[ADD] */\\n[ADD] @Description(\"Jar-Files to send to all workers and put on the classpath. \"\\n[ADD] + \"The default value is all files from the classpath.\")\\n[ADD] @Default.InstanceFactory(EmptyPathList.class)\\n[ADD] List<String> getFilesToStage();\\n[ADD] void setFilesToStage(List<String> value);\\n[ADD] \\n[ADD] /** Returns an empty path list, to avoid handling null. */\\n[ADD] class EmptyPathList implements DefaultValueFactory<List<String>> {\\n[ADD] @Override\\n[ADD] public List<String> create(PipelineOptions options) {\\n[ADD] return new ArrayList<>();\\n[ADD] }\\n[ADD] }\\n }',\n",
              " ' import android.os.Build;\\n \\n import com.ichi2.anki.reviewer.ReviewerCustomFonts;\\n[ADD] import com.ichi2.libanki.Card;\\n import com.ichi2.themes.Themes;\\n \\n import java.util.regex.Matcher;',\n",
              " ' import java.util.Optional;\\n import java.util.function.Consumer;\\n import java.util.function.Function;\\n[DEL] import java.util.function.Predicate;\\n import java.util.function.Supplier;\\n \\n /**\\n[DEL] * Copied from java.util.Optional and made Serializable along with methods to convert to/from standard Option\\n[ADD] * Provides functionality same as java.util.Optional but is also made Serializable. Additional APIs are provided to\\n[ADD] * convert to/from java.util.Optional\\n  */\\n public final class Option<T> implements Serializable {\\n \\n   private static final long serialVersionUID = 0L;\\n \\n[DEL] /**\\n[DEL] * Common instance for {@code empty()}.\\n[DEL] */\\n[DEL] private static final Option<?> EMPTY = new Option<>();\\n[ADD] private static final Option<?> NULL_VAL = new Option<>();\\n[ADD] \\n[ADD] private final T val;\\n \\n   /**\\n[DEL] * If non-null, the value; if null, indicates no value is present\\n[ADD] * Convert to java Optional\\n    */\\n[DEL] private final T value;\\n[ADD] public Optional<T> toJavaOptional() {\\n[ADD] return Optional.ofNullable(val);\\n[ADD] }\\n \\n   /**\\n[DEL] * Constructs an empty instance.\\n[DEL] *\\n[DEL] * @implNote Generally only one empty instance, {@link Option#EMPTY}, should exist per VM.\\n[ADD] * Convert from java.util.Optional\\n[ADD] *\\n[ADD] * @param v java.util.Optional object\\n[ADD] * @param <T> type of the value stored in java.util.Optional object\\n[ADD] * @return Option\\n    */\\n[ADD] public static <T> Option<T> fromJavaOptional(Optional<T> v) {\\n[ADD] return Option.ofNullable(v.orElse(null));\\n[ADD] }\\n[ADD] \\n   private Option() {\\n[DEL] this.value = null;\\n[ADD] this.val = null;\\n   }\\n \\n[DEL] /**\\n[DEL] * Returns an empty {@code Option} instance. No value is present for this Option.\\n[DEL] *\\n[DEL] * @param <T> Type of the non-existent value\\n[DEL] * @return an empty {@code Option}\\n[DEL] * @apiNote Though it may be tempting to do so, avoid testing if an object is empty by comparing with {@code ==}\\n[DEL] *          against instances returned by {@code Option.empty()}. There is no guarantee that it is a singleton.\\n[DEL] *          Instead, use {@link #isPresent()}.\\n[DEL] */\\n[DEL] public static <T> Option<T> empty() {\\n[DEL] @SuppressWarnings(\"unchecked\")\\n[DEL] Option<T> t = (Option<T>) EMPTY;\\n[DEL] return t;\\n[ADD] private Option(T val) {\\n[ADD] if (null == val) {\\n[ADD] throw new NullPointerException(\"Expected a non-null value. Got null\");\\n[ADD] }\\n[ADD] this.val = val;\\n   }\\n \\n[DEL] /**\\n[DEL] * Constructs an instance with the value present.\\n[DEL] *\\n[DEL] * @param value the non-null value to be present\\n[DEL] * @throws NullPointerException if value is null\\n[DEL] */\\n[DEL] private Option(T value) {\\n[DEL] this.value = Objects.requireNonNull(value);\\n[ADD] public static <T> Option<T> empty() {\\n[ADD] return (Option<T>) NULL_VAL;\\n   }\\n \\n[DEL] /**\\n[DEL] * Returns an {@code Option} with the specified present non-null value.\\n[DEL] *\\n[DEL] * @param <T> the class of the value\\n[DEL] * @param value the value to be present, which must be non-null\\n[DEL] * @return an {@code Option} with the value present\\n[DEL] * @throws NullPointerException if value is null\\n[DEL] */\\n   public static <T> Option<T> of(T value) {\\n     return new Option<>(value);\\n   }\\n \\n[DEL] /**\\n[DEL] * Returns an {@code Option} describing the specified value, if non-null, otherwise returns an empty {@code Option}.\\n[DEL] *\\n[DEL] * @param <T> the class of the value\\n[DEL] * @param value the possibly-null value to describe\\n[DEL] * @return an {@code Option} with a present value if the specified value is non-null, otherwise an empty {@code\\n[DEL] * Option}\\n[DEL] */\\n   public static <T> Option<T> ofNullable(T value) {\\n[DEL] return value == null ? empty() : of(value);\\n[DEL] }\\n[DEL] \\n[DEL] /**\\n[DEL] * If a value is present in this {@code Option}, returns the value, otherwise throws {@code NoSuchElementException}.\\n[DEL] *\\n[DEL] * @return the non-null value held by this {@code Option}\\n[DEL] * @throws NoSuchElementException if there is no value present\\n[DEL] * @see Option#isPresent()\\n[DEL] */\\n[DEL] public T get() {\\n[DEL] if (value == null) {\\n[DEL] throw new NoSuchElementException(\"No value present\");\\n[DEL] }\\n[DEL] return value;\\n[ADD] return null == value ? empty() : of(value);\\n   }\\n \\n[DEL] /**\\n[DEL] * Return {@code true} if there is a value present, otherwise {@code false}.\\n[DEL] *\\n[DEL] * @return {@code true} if there is a value present, otherwise {@code false}\\n[DEL] */\\n   public boolean isPresent() {\\n[DEL] return value != null;\\n[ADD] return null != val;\\n   }\\n \\n[DEL] /**\\n[DEL] * If a value is present, invoke the specified consumer with the value, otherwise do nothing.\\n[DEL] *\\n[DEL] * @param consumer block to be executed if a value is present\\n[DEL] * @throws NullPointerException if value is present and {@code consumer} is null\\n[DEL] */\\n[DEL] public void ifPresent(Consumer<? super T> consumer) {\\n[DEL] if (value != null) {\\n[DEL] consumer.accept(value);\\n[ADD] public T get() {\\n[ADD] if (null == val) {\\n[ADD] throw new NoSuchElementException(\"No value present in Option\");\\n     }\\n[ADD] return val;\\n   }\\n \\n[DEL] /**\\n[DEL] * If a value is present, and the value matches the given predicate, return an {@code Option} describing the value,\\n[DEL] * otherwise return an empty {@code Option}.\\n[DEL] *\\n[DEL] * @param predicate a predicate to apply to the value, if present\\n[DEL] * @return an {@code Option} describing the value of this {@code Option} if a value is present and the value matches\\n[DEL] *         the given predicate, otherwise an empty {@code Option}\\n[DEL] * @throws NullPointerException if the predicate is null\\n[DEL] */\\n[DEL] public Option<T> filter(Predicate<? super T> predicate) {\\n[DEL] Objects.requireNonNull(predicate);\\n[DEL] if (!isPresent()) {\\n[DEL] return this;\\n[DEL] } else {\\n[DEL] return predicate.test(value) ? this : empty();\\n[ADD] public void ifPresent(Consumer<? super T> consumer) {\\n[ADD] if (val != null) {\\n[ADD] // process the value\\n[ADD] consumer.accept(val);\\n     }\\n   }\\n \\n[DEL] /**\\n[DEL] * If a value is present, apply the provided mapping function to it, and if the result is non-null, return an {@code\\n[DEL] * Option} describing the result. Otherwise return an empty {@code Option}.\\n[DEL] *\\n[DEL] * @param <U> The type of the result of the mapping function\\n[DEL] * @param mapper a mapping function to apply to the value, if present\\n[DEL] * @return an {@code Option} describing the result of applying a mapping function to the value of this {@code Option},\\n[DEL] *         if a value is present, otherwise an empty {@code Option}\\n[DEL] * @throws NullPointerException if the mapping function is null\\n[DEL] * @apiNote This method supports post-processing on optional values, without the need to explicitly check for a return\\n[DEL] *          status. For example, the following code traverses a stream of file names, selects one that has not yet\\n[DEL] *          been processed, and then opens that file, returning an {@code Option<FileInputStream>}:\\n[DEL] *\\n[DEL] *          <pre>\\n[DEL] * {@code\\n[DEL] *     Option<FileInputStream> fis =\\n[DEL] *         names.stream().filter(name -> !isProcessedYet(name))\\n[DEL] *                       .findFirst()\\n[DEL] *                       .map(name -> new FileInputStream(name));\\n[DEL] * }\\n[DEL] *          </pre>\\n[DEL] *\\n[DEL] *          Here, {@code findFirst} returns an {@code Option<String>}, and then {@code map} returns an {@code\\n[DEL] * Option<FileInputStream>} for the desired file if one exists.\\n[DEL] */\\n   public <U> Option<U> map(Function<? super T, ? extends U> mapper) {\\n[DEL] Objects.requireNonNull(mapper);\\n     if (!isPresent()) {\\n       return empty();\\n     } else {\\n[DEL] return Option.ofNullable(mapper.apply(value));\\n[ADD] return Option.ofNullable(mapper.apply(val));\\n     }\\n   }\\n \\n[DEL] /**\\n[DEL] * If a value is present, apply the provided {@code Option}-bearing mapping function to it, return that result,\\n[DEL] * otherwise return an empty {@code Option}. This method is similar to {@link #map(Function)}, but the provided mapper\\n[DEL] * is one whose result is already an {@code Option}, and if invoked, {@code flatMap} does not wrap it with an\\n[DEL] * additional {@code Option}.\\n[DEL] *\\n[DEL] * @param <U> The type parameter to the {@code Option} returned by\\n[DEL] * @param mapper a mapping function to apply to the value, if present the mapping function\\n[DEL] * @return the result of applying an {@code Option}-bearing mapping function to the value of this {@code Option}, if a\\n[DEL] *         value is present, otherwise an empty {@code Option}\\n[DEL] * @throws NullPointerException if the mapping function is null or returns a null result\\n[DEL] */\\n[DEL] public <U> Option<U> flatMap(Function<? super T, Option<U>> mapper) {\\n[DEL] Objects.requireNonNull(mapper);\\n[DEL] if (!isPresent()) {\\n[DEL] return empty();\\n[DEL] } else {\\n[DEL] return Objects.requireNonNull(mapper.apply(value));\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] /**\\n[DEL] * Return the value if present, otherwise return {@code other}.\\n[DEL] *\\n[DEL] * @param other the value to be returned if there is no value present, may be null\\n[DEL] * @return the value, if present, otherwise {@code other}\\n[DEL] */\\n   public T orElse(T other) {\\n[DEL] return value != null ? value : other;\\n[ADD] return val != null ? val : other;\\n   }\\n \\n[DEL] /**\\n[DEL] * Return the value if present, otherwise invoke {@code other} and return the result of that invocation.\\n[DEL] *\\n[DEL] * @param other a {@code Supplier} whose result is returned if no value is present\\n[DEL] * @return the value if present otherwise the result of {@code other.get()}\\n[DEL] * @throws NullPointerException if value is not present and {@code other} is null\\n[DEL] */\\n   public T orElseGet(Supplier<? extends T> other) {\\n[DEL] return value != null ? value : other.get();\\n[ADD] return val != null ? val : other.get();\\n   }\\n \\n[DEL] /**\\n[DEL] * Return the contained value, if present, otherwise throw an exception to be created by the provided supplier.\\n[DEL] *\\n[DEL] * @param <X> Type of the exception to be thrown\\n[DEL] * @param exceptionSupplier The supplier which will return the exception to be thrown\\n[DEL] * @return the present value\\n[DEL] * @throws X if there is no value present\\n[DEL] * @throws NullPointerException if no value is present and {@code exceptionSupplier} is null\\n[DEL] * @apiNote A method reference to the exception constructor with an empty argument list can be used as the supplier.\\n[DEL] *          For example, {@code IllegalStateException::new}\\n[DEL] */\\n   public <X extends Throwable> T orElseThrow(Supplier<? extends X> exceptionSupplier) throws X {\\n[DEL] if (value != null) {\\n[DEL] return value;\\n[ADD] if (val != null) {\\n[ADD] return val;\\n     } else {\\n       throw exceptionSupplier.get();\\n     }\\n   }\\n \\n[DEL] /**\\n[DEL] * Indicates whether some other object is \"equal to\" this Option. The other object is considered equal if:\\n[DEL] * <ul>\\n[DEL] * <li>it is also an {@code Option} and;\\n[DEL] * <li>both instances have no value present or;\\n[DEL] * <li>the present values are \"equal to\" each other via {@code equals()}.\\n[DEL] * </ul>\\n[DEL] *\\n[DEL] * @param obj an object to be tested for equality\\n[DEL] * @return {code true} if the other object is \"equal to\" this object otherwise {@code false}\\n[DEL] */\\n   @Override\\n[DEL] public boolean equals(Object obj) {\\n[DEL] if (this == obj) {\\n[ADD] public boolean equals(Object o) {\\n[ADD] if (this == o) {\\n       return true;\\n     }\\n[DEL] \\n[DEL] if (!(obj instanceof Option)) {\\n[ADD] if (o == null || getClass() != o.getClass()) {\\n       return false;\\n     }\\n[DEL] \\n[DEL] Option<?> other = (Option<?>) obj;\\n[DEL] return Objects.equals(value, other.value);\\n[ADD] Option<?> option = (Option<?>) o;\\n[ADD] return Objects.equals(val, option.val);\\n   }\\n \\n[DEL] /**\\n[DEL] * Returns the hash code value of the present value, if any, or 0 (zero) if no value is present.\\n[DEL] *\\n[DEL] * @return hash code value of the present value or 0 if no value is present\\n[DEL] */\\n   @Override\\n   public int hashCode() {\\n[DEL] return Objects.hashCode(value);\\n[ADD] return Objects.hash(val);\\n   }\\n \\n[DEL] /**\\n[DEL] * Returns a non-empty string representation of this Option suitable for debugging. The exact presentation format is\\n[DEL] * unspecified and may vary between implementations and versions.\\n[DEL] *\\n[DEL] * @return the string representation of this instance\\n[DEL] * @implSpec If a value is present the result must include its string representation in the result. Empty and present\\n[DEL] *           Optionals must be unambiguously differentiable.\\n[DEL] */\\n   @Override\\n   public String toString() {\\n[DEL] return value != null ? String.format(\"Option[%s]\", value) : \"Option.empty\";\\n[DEL] }\\n[DEL] \\n[DEL] /**\\n[DEL] * Convert to java Optional\\n[DEL] */\\n[DEL] public Optional<T> toJavaOptional() {\\n[DEL] return Optional.ofNullable(value);\\n[DEL] }\\n[DEL] \\n[DEL] /**\\n[DEL] * Convert from java.util.Optional\\n[DEL] */\\n[DEL] public static <T> Option<T> fromJavaOptional(Optional<T> v) {\\n[DEL] return Option.ofNullable(v.orElse(null));\\n[ADD] return \"Option{\" + \"val=\" + val + \\'}\\';\\n   }\\n }',\n",
              " '       return null;\\n     }\\n   }\\n[ADD] \\n[ADD] /**\\n[ADD] * @return whether the {@link ComponentModel} defines a streaming operation that uses a connection.\\n[ADD] *\\n[ADD] * @since 4.2.3 - 4.3.0\\n[ADD] */\\n[ADD] public static Boolean isConnectedStreamingOperation(ComponentModel componentModel) {\\n[ADD] if (componentModel instanceof ConnectableComponentModel) {\\n[ADD] ConnectableComponentModel connectableComponentModel = (ConnectableComponentModel) componentModel;\\n[ADD] return (connectableComponentModel.requiresConnection()\\n[ADD] && (connectableComponentModel.supportsStreaming()\\n[ADD] || connectableComponentModel.getModelProperty(PagedOperationModelProperty.class).isPresent()));\\n[ADD] }\\n[ADD] return false;\\n[ADD] }\\n }',\n",
              " '     }\\n   }\\n \\n[DEL] @SuppressWarnings(\"unchecked\")\\n   @VisibleForTesting\\n   static Set<RuleKey> getSecurityRuleKeys(boolean sonarSecurityBefore78) {\\n[ADD] String ruleKeysMethod = sonarSecurityBefore78 ? \"getRuleKeys\" : \"getSecurityRuleKeys\";\\n[ADD] return getExternalRuleKeys(SECURITY_RULES_CLASS_NAME, ruleKeysMethod, \"security\", sonarSecurityBefore78);\\n[ADD] }\\n[ADD] \\n[ADD] @VisibleForTesting\\n[ADD] static Set<RuleKey> getDataflowBugDetectionRuleKeys() {\\n[ADD] return getExternalRuleKeys(DBD_RULES_CLASS_NAME, DBD_RULE_KEYS_METHOD_NAME, \"dataflow bug detection\", false);\\n[ADD] }\\n[ADD] \\n[ADD] @SuppressWarnings(\"unchecked\")\\n[ADD] @VisibleForTesting\\n[ADD] static Set<RuleKey> getExternalRuleKeys(String className, String ruleKeysMethod, String rulesCategory, boolean sonarSecurityBefore78) {\\n     try {\\n[DEL] Class<?> javaRulesClass = Class.forName(\"com.sonar.plugins.security.api.JavaRules\");\\n[DEL] String ruleKeysMethod = sonarSecurityBefore78 ? \"getRuleKeys\" : \"getSecurityRuleKeys\";\\n[ADD] Class<?> javaRulesClass = Class.forName(className);\\n       Method getRuleKeysMethod = javaRulesClass.getMethod(ruleKeysMethod);\\n       Set<String> ruleKeys = (Set<String>) getRuleKeysMethod.invoke(null);\\n[ADD] Method getRepositoryKeyMethod = javaRulesClass.getMethod(GET_REPOSITORY_KEY);\\n       String repositoryKey;\\n       if (sonarSecurityBefore78) {\\n         repositoryKey = CheckList.REPOSITORY_KEY;\\n       } else {\\n[DEL] Method getRepositoryKeyMethod = javaRulesClass.getMethod(\"getRepositoryKey\");\\n         repositoryKey = (String) getRepositoryKeyMethod.invoke(null);\\n       }\\n       return ruleKeys.stream().map(k -> RuleKey.of(repositoryKey, k)).collect(Collectors.toSet());\\n[DEL] \\n     } catch (ClassNotFoundException e) {\\n[DEL] LOG.debug(\"com.sonar.plugins.security.api.JavaRules is not found, no security rules added to Sonar way java profile: \" + e.getMessage());\\n[ADD] LOG.debug(String.format(\"%s is not found, no %s rules added to Sonar way java profile: %s\", className, rulesCategory, e.getMessage()));\\n     } catch (NoSuchMethodException e) {\\n[DEL] LOG.debug(\"Method is not found, no security rules added to Sonar way java profile: \" + e.getMessage());\\n[ADD] LOG.debug(String.format(\"Method is not found, no %s rules added to Sonar way java profile: %s\", rulesCategory, e.getMessage()));\\n     } catch (IllegalAccessException e) {\\n[DEL] LOG.debug(\"[IllegalAccessException] no security rules added to Sonar way java profile: \" + e.getMessage());\\n[ADD] LOG.debug(String.format(\"[IllegalAccessException] no %s rules added to Sonar way java profile: %s\", rulesCategory, e.getMessage()));\\n     } catch (InvocationTargetException e) {\\n[DEL] LOG.debug(\"[InvocationTargetException] no security rules added to Sonar way java profile: \" + e.getMessage());\\n[ADD] LOG.debug(String.format(\"[InvocationTargetException] no %s rules added to Sonar way java profile: %s\", rulesCategory, e.getMessage()));\\n     }\\n[DEL] \\n     return new HashSet<>();\\n   }\\n \\n   private static boolean isSonarSecurityBefore78() {\\n     try {\\n[DEL] Class<?> javaRulesClass = Class.forName(\"com.sonar.plugins.security.api.JavaRules\");\\n[DEL] javaRulesClass.getMethod(\"getRepositoryKey\");\\n[ADD] Class<?> javaRulesClass = Class.forName(SECURITY_RULES_CLASS_NAME);\\n[ADD] javaRulesClass.getMethod(GET_REPOSITORY_KEY);\\n       return false;\\n \\n     } catch (NoSuchMethodException | ClassNotFoundException e) {',\n",
              " ' \\n         #endregion\\n \\n[ADD] #region backup/timer\\n[ADD] \\n[ADD] /// <summary>\\n[ADD] /// Backup all the files\\n[ADD] /// </summary>\\n[ADD] protected void SaveBackupFiles(object state)\\n[ADD] {\\n[ADD] foreach (var workspace in Workspaces)\\n[ADD] {\\n[ADD] if (!workspace.HasUnsavedChanges)\\n[ADD] continue;\\n[ADD] \\n[ADD] string fileName;\\n[ADD] if (string.IsNullOrEmpty(workspace.FileName))\\n[ADD] {\\n[ADD] fileName = \"recent_new_file_\" + workspace.Guid;\\n[ADD] var ext = workspace is HomeWorkspaceModel ? \".DYN\" : \".DYF\";\\n[ADD] fileName += ext;\\n[ADD] }\\n[ADD] else\\n[ADD] {\\n[ADD] fileName = Path.GetFileName(workspace.FileName);\\n[ADD] }\\n[ADD] \\n[ADD] var savePath = Path.Combine(pathManager.BackupDirectory, fileName);\\n[ADD] workspace.SaveAs(savePath, null);\\n[ADD] Logger.Log(\"Backup file is saved: \" + savePath);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] /// <summary>\\n[ADD] /// Start the timer to backup files periodically\\n[ADD] /// </summary>\\n[ADD] public void StartBackupFilesTimer()\\n[ADD] {\\n[ADD] if (backupFilesTimer == null)\\n[ADD] {\\n[ADD] backupFilesTimer = new Timer(SaveBackupFiles);\\n[ADD] }\\n[ADD] \\n[ADD] backupFilesTimer.Change(PreferenceSettings.BackupInterval, PreferenceSettings.BackupInterval);\\n[ADD] Logger.Log(String.Format(\"Backup files timer is started with an interval of {0} milliseconds\", PreferenceSettings.BackupInterval));\\n[ADD] }\\n[ADD] \\n[ADD] #endregion\\n[ADD] \\n         #region internal methods\\n \\n         internal void PostUIActivation(object parameter)',\n",
              " '   public static final String METAFOLDER_NAME = \".hoodie\";\\n   public static final String TEMPFOLDER_NAME = METAFOLDER_NAME + File.separator + \".temp\";\\n   public static final String AUXILIARYFOLDER_NAME = METAFOLDER_NAME + File.separator + \".aux\";\\n[ADD] public static final String BOOTSTRAP_INDEX_ROOT_FOLDER_PATH = AUXILIARYFOLDER_NAME + File.separator + \".bootstrap\";\\n[ADD] public static final String BOOTSTRAP_INDEX_BY_PARTITION_FOLDER_PATH = BOOTSTRAP_INDEX_ROOT_FOLDER_PATH\\n[ADD] + File.separator + \".partitions\";\\n[ADD] public static final String BOOTSTRAP_INDEX_BY_FILE_ID_FOLDER_PATH = BOOTSTRAP_INDEX_ROOT_FOLDER_PATH + File.separator\\n[ADD] + \".fileids\";\\n[ADD] \\n   public static final String MARKER_EXTN = \".marker\";\\n \\n   private String basePath;',\n",
              " '     long totalInsertRecordsWritten = 0;\\n     for (List<HoodieWriteStat> stats : partitionToWriteStats.values()) {\\n       for (HoodieWriteStat stat : stats) {\\n[DEL] if (stat.getPrevCommit() != null && stat.getPrevCommit().equalsIgnoreCase(\"null\")) {\\n[ADD] // determine insert rows in every file\\n[ADD] if (stat.getPrevCommit() != null) {\\n           totalInsertRecordsWritten += stat.getNumInserts();\\n         }\\n       }',\n",
              " '     }\\n \\n \\n[ADD] private void openHelpUrl(Uri helpUrl) {\\n[ADD] openUrl(helpUrl);\\n[ADD] }\\n[ADD] \\n[ADD] \\n     /**\\n      * Scroll the deck list so that it is centered on the current deck.\\n      *',\n",
              " ' \\t\\t\\tthis.messageRowMapper = new MessageRowMapper(this.deserializer, this.lobHandler);\\n \\t\\t}\\n \\n[DEL] if (this.jdbcTemplate.getFetchSize() != 1 && LOGGER.isWarnEnabled()) {\\n[DEL] LOGGER.warn(\"The jdbcTemplate\\'s fetch size is not 1. This may cause FIFO issues with Oracle databases.\");\\n[ADD] if (this.jdbcTemplate.getFetchSize() != 1) {\\n[ADD] LOGGER.warn(() -> \"The jdbcTemplate\\'s fetch size is not 1. This may cause FIFO issues with Oracle databases.\");\\n \\t\\t}\\n \\n \\t\\tif (this.preparedStatementSetter == null) {',\n",
              " '    * Valid descriptions.\\n    * @param value Valid descriptions.\\n    */\\n[DEL] public void doSomething(int value) {           // Compliant\\n[ADD] public void doSomething1(int value) {           // Compliant\\n  }\\n \\n  /**\\n   * Valid descriptions.\\n   * @return foo Valid descriptions.\\n    */\\n[DEL] public int doSomething(int value) {            // Noncompliant {{Document the parameter(s): value}}\\n[ADD] public int doSomething2(int value) {            // Noncompliant {{Document the parameter(s): value}}\\n    return value;\\n  }\\n \\n  /** Valid descriptions.\\n   *  */\\n[DEL] public int doSomething() {                     // Noncompliant {{Document this method return value.}}\\n[ADD] public int doSomething3() {                     // Noncompliant {{Document this method return value.}}\\n    return value;\\n  }\\n }',\n",
              " '     ClassTree enumTree = (ClassTree) tree;\\n     for (Tree member : enumTree.members()) {\\n       if (member.is(Tree.Kind.VARIABLE)) {\\n[DEL] ModifiersTree modifiers = ((VariableTree) member).modifiers();\\n[ADD] VariableTree variableTree = (VariableTree) member;\\n[ADD] ModifiersTree modifiers = variableTree.modifiers();\\n         ModifierKeywordTree publicModifier = ModifiersUtils.getModifier(modifiers, Modifier.PUBLIC);\\n[DEL] if (publicModifier != null && !ModifiersUtils.hasModifier(modifiers, Modifier.STATIC)\\n[DEL] // FIXME SONARJAVA-1604 final mutable field should raise issues\\n[DEL] && !ModifiersUtils.hasModifier(modifiers, Modifier.FINAL)) {\\n[ADD] if (publicModifier != null && (isNotStaticOrFinal(variableTree,variableTree.modifiers())|| isMutableFinalMember(variableTree))) {\\n           reportIssue(publicModifier, \"Lower the visibility of this field.\");\\n         }\\n       } else if (member.is(Tree.Kind.METHOD)) {',\n",
              " '    * themselves on some other condition.\\n    */\\n   protected boolean defaultEnabled() {\\n[DEL] // TODO (trask) caching this value statically requires changing (or removing) the tests that\\n[DEL] //  rely on updating the value\\n[DEL] return Config.get().getBooleanProperty(\"otel.instrumentation.common.default-enabled\", true);\\n[ADD] return DEFAULT_ENABLED;\\n[ADD] }\\n[ADD] \\n[ADD] // visible for testing\\n[ADD] static boolean initDefaultEnabled(Config config) {\\n[ADD] return config.getBooleanProperty(\"otel.instrumentation.common.default-enabled\", true);\\n[ADD] }\\n[ADD] \\n[ADD] // visible for testing\\n[ADD] static boolean initEnabled(\\n[ADD] Config config, List<String> instrumentationNames, boolean defaultEnabled) {\\n[ADD] return config.isInstrumentationEnabled(instrumentationNames, defaultEnabled);\\n   }\\n }',\n",
              " '   @Override\\n   public void close() throws IOException {\\n     this.applicationLauncher.close();\\n[ADD] \\n[ADD] if (this.jobCatalog instanceof Service) {\\n[ADD] ((Service) this.jobCatalog).stopAsync().awaitTerminated();\\n[ADD] }\\n   }\\n \\n   /**',\n",
              " '    * @param tableName the table name\\n    * @throws IOException\\n    */\\n[DEL] public abstract void dropTableIfExists(String dbName, String tableName) throws IOException;\\n[ADD] public abstract void dropTableIfExists(String dbName, String tableName)\\n[ADD] throws IOException;\\n \\n   /**\\n    * Drop a partition if exists.',\n",
              " ' \\tpublic function reset_saved_auth_state() {\\n \\t\\t$this->xmlrpc_verification = null;\\n \\t}\\n[ADD] \\n[ADD] /**\\n[ADD] * Handles the login action for Authorizing the JSON API.\\n[ADD] */\\n[ADD] public function login_form_json_api_authorization() {\\n[ADD] $this->verify_json_api_authorization_request();\\n[ADD] \\n[ADD] add_action( \\'wp_login\\', array( $this, \\'store_json_api_authorization_token\\' ), 10, 2 );\\n[ADD] \\n[ADD] add_action( \\'login_message\\', array( $this, \\'login_message_json_api_authorization\\' ) );\\n[ADD] add_action( \\'login_form\\', array( $this, \\'preserve_action_in_login_form_for_json_api_authorization\\' ) );\\n[ADD] add_filter( \\'site_url\\', array( $this, \\'post_login_form_to_signed_url\\' ), 10, 3 );\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Make sure the login form is POSTed to the signed URL so we can reverify the request.\\n[ADD] *\\n[ADD] * @param string      $url     The complete site URL including scheme and path.\\n[ADD] * @param string      $path    Path relative to the site URL. Blank string if no path is specified.\\n[ADD] * @param string|null $scheme  Scheme to give the site URL context. Accepts \\'http\\', \\'https\\', \\'login\\'.\\n[ADD] * @return string Signed login form action URL.\\n[ADD] */\\n[ADD] public function post_login_form_to_signed_url( $url, $path, $scheme ) {\\n[ADD] if ( \\'wp-login.php\\' !== $path || ( \\'login_post\\' !== $scheme && \\'login\\' !== $scheme ) ) {\\n[ADD] return $url;\\n[ADD] }\\n[ADD] \\n[ADD] $parsed_url = wp_parse_url( $url );\\n[ADD] $url        = strtok( $url, \\'?\\' );\\n[ADD] $url        = \"$url?{$_SERVER[\\'QUERY_STRING\\']}\";\\n[ADD] if ( ! empty( $parsed_url[\\'query\\'] ) ) {\\n[ADD] $url .= \"&{$parsed_url[\\'query\\']}\";\\n[ADD] }\\n[ADD] \\n[ADD] return $url;\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Make sure the POSTed request is handled by the same action.\\n[ADD] */\\n[ADD] public function preserve_action_in_login_form_for_json_api_authorization() {\\n[ADD] echo \"<input type=\\'hidden\\' name=\\'action\\' value=\\'jetpack_json_api_authorization\\' />\\\\n\";\\n[ADD] echo \"<input type=\\'hidden\\' name=\\'jetpack_json_api_original_query\\' value=\\'\" . esc_url( set_url_scheme( $_SERVER[\\'HTTP_HOST\\'] . $_SERVER[\\'REQUEST_URI\\'] ) ) . \"\\' />\\\\n\";\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * If someone logs in to approve API access, store the Access Code in usermeta.\\n[ADD] *\\n[ADD] * @param string   $user_login Username.\\n[ADD] * @param \\\\WP_User $user       \\\\WP_User object of the logged-in user.\\n[ADD] */\\n[ADD] public function store_json_api_authorization_token( $user_login, $user ) {\\n[ADD] add_filter( \\'login_redirect\\', array( $this, \\'add_token_to_login_redirect_json_api_authorization\\' ), 10, 3 );\\n[ADD] add_filter( \\'allowed_redirect_hosts\\', array( $this, \\'allow_wpcom_public_api_domain\\' ) );\\n[ADD] $token = wp_generate_password( 32, false );\\n[ADD] update_user_meta( $user->ID, \\'jetpack_json_api_\\' . $this->json_api_authorization_request[\\'client_id\\'], $token );\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Add public-api.wordpress.com to the safe redirect whitelist - only added when someone allows API access.\\n[ADD] *\\n[ADD] * @param array $domains An array of allowed hosts.\\n[ADD] * @return array Array of allowed hosts, with the WP.com API domain included.\\n[ADD] */\\n[ADD] public function allow_wpcom_public_api_domain( $domains ) {\\n[ADD] $domains[] = \\'public-api.wordpress.com\\';\\n[ADD] return $domains;\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Whether the redirect URL is encoded.\\n[ADD] *\\n[ADD] * @param string $redirect_url Redirect URL.\\n[ADD] * @return bool True if the URL is encoded, false otherwise.\\n[ADD] */\\n[ADD] public function is_redirect_encoded( $redirect_url ) {\\n[ADD] return preg_match( \\'/https?%3A%2F%2F/i\\', $redirect_url ) > 0;\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Add all wordpress.com environments to the safe redirect whitelist.\\n[ADD] *\\n[ADD] * @param array $domains An array of allowed hosts.\\n[ADD] * @return array Array of allowed hosts, with the WP.com API domain included.\\n[ADD] */\\n[ADD] public function allow_wpcom_environments( $domains ) {\\n[ADD] $domains[] = \\'wordpress.com\\';\\n[ADD] $domains[] = \\'wpcalypso.wordpress.com\\';\\n[ADD] $domains[] = \\'horizon.wordpress.com\\';\\n[ADD] $domains[] = \\'calypso.localhost\\';\\n[ADD] return $domains;\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Add the Access Code details to the public-api.wordpress.com redirect.\\n[ADD] *\\n[ADD] * @todo Refactor to check properly if $user is a \\\\WP_Error.\\n[ADD] *\\n[ADD] * @param string             $redirect_to          The redirect destination URL.\\n[ADD] * @param string             $original_redirect_to The requested redirect destination URL passed as a parameter.\\n[ADD] * @param \\\\WP_User|\\\\WP_Error $user                 \\\\WP_User object if login was successful, \\\\WP_Error object otherwise.\\n[ADD] * @return string Login redirect URL, with token parameters added to it.\\n[ADD] */\\n[ADD] public function add_token_to_login_redirect_json_api_authorization( $redirect_to, $original_redirect_to, $user ) {\\n[ADD] return add_query_arg(\\n[ADD] urlencode_deep(\\n[ADD] array(\\n[ADD] \\'jetpack-code\\'    => get_user_meta( $user->ID, \\'jetpack_json_api_\\' . $this->json_api_authorization_request[\\'client_id\\'], true ),\\n[ADD] \\'jetpack-user-id\\' => (int) $user->ID,\\n[ADD] \\'jetpack-state\\'   => $this->json_api_authorization_request[\\'state\\'],\\n[ADD] )\\n[ADD] ),\\n[ADD] $redirect_to\\n[ADD] );\\n[ADD] }\\n[ADD] \\n[ADD] \\n[ADD] /**\\n[ADD] * Verifies the request by checking the signature.\\n[ADD] *\\n[ADD] * @todo Refactor to implement proper nonce verification.\\n[ADD] *\\n[ADD] * @since 4.6.0 Method was updated to use `$_REQUEST` instead of `$_GET` and `$_POST`. Method also updated to allow\\n[ADD] * passing in an `$environment` argument that overrides `$_REQUEST`. This was useful for integrating with SSO.\\n[ADD] * @since 7.7.0 This method has been moved from `\\\\Jetpack` to the connection package.\\n[ADD] *\\n[ADD] * @param null|array $environment Authorization data.\\n[ADD] */\\n[ADD] public function verify_json_api_authorization_request( $environment = null ) {\\n[ADD] $environment = is_null( $environment )\\n[ADD] ? $_REQUEST // phpcs:ignore WordPress.Security.NonceVerification.Recommended\\n[ADD] : $environment;\\n[ADD] \\n[ADD] // phpcs:ignore VariableAnalysis.CodeAnalysis.VariableAnalysis.UnusedVariable\\n[ADD] list( $env_token, $env_version, $env_user_id ) = explode( \\':\\', $environment[\\'token\\'] );\\n[ADD] $token = $this->get_access_token( $env_user_id, $env_token );\\n[ADD] if ( ! $token || empty( $token->secret ) ) {\\n[ADD] wp_die( esc_html__( \\'You must connect your Jetpack plugin to WordPress.com to use this feature.\\', \\'jetpack\\' ) );\\n[ADD] }\\n[ADD] \\n[ADD] $die_error = __( \\'Someone may be trying to trick you into giving them access to your site.  Or it could be you just encountered a bug :).  Either way, please close this window.\\', \\'jetpack\\' );\\n[ADD] \\n[ADD] // Host has encoded the request URL, probably as a result of a bad http => https redirect.\\n[ADD] if ( $this->is_redirect_encoded( $_GET[\\'redirect_to\\'] ) ) { // phpcs:ignore WordPress.Security.NonceVerification.Recommended\\n[ADD] /**\\n[ADD] * Jetpack authorization request Error.\\n[ADD] *\\n[ADD] * @since 7.5.0\\n[ADD] */\\n[ADD] do_action( \\'jetpack_verify_api_authorization_request_error_double_encode\\' );\\n[ADD] $die_error = sprintf(\\n[ADD] /* translators: %s is a URL */\\n[ADD] __( \\'Your site is incorrectly double-encoding redirects from http to https. This is preventing Jetpack from authenticating your connection. Please visit our <a href=\"%s\">support page</a> for details about how to resolve this.\\', \\'jetpack\\' ),\\n[ADD] \\'https://jetpack.com/support/double-encoding/\\'\\n[ADD] );\\n[ADD] }\\n[ADD] \\n[ADD] $jetpack_signature = new \\\\Jetpack_Signature( $token->secret, (int) \\\\Jetpack_Options::get_option( \\'time_diff\\' ) );\\n[ADD] \\n[ADD] if ( isset( $environment[\\'jetpack_json_api_original_query\\'] ) ) {\\n[ADD] $signature = $jetpack_signature->sign_request(\\n[ADD] $environment[\\'token\\'],\\n[ADD] $environment[\\'timestamp\\'],\\n[ADD] $environment[\\'nonce\\'],\\n[ADD] \\'\\',\\n[ADD] \\'GET\\',\\n[ADD] $environment[\\'jetpack_json_api_original_query\\'],\\n[ADD] null,\\n[ADD] true\\n[ADD] );\\n[ADD] } else {\\n[ADD] $signature = $jetpack_signature->sign_current_request(\\n[ADD] array(\\n[ADD] \\'body\\'   => null,\\n[ADD] \\'method\\' => \\'GET\\',\\n[ADD] )\\n[ADD] );\\n[ADD] }\\n[ADD] \\n[ADD] if ( ! $signature ) {\\n[ADD] wp_die( esc_html( $die_error ) );\\n[ADD] } elseif ( is_wp_error( $signature ) ) {\\n[ADD] wp_die( esc_html( $die_error ) );\\n[ADD] } elseif ( ! hash_equals( $signature, $environment[\\'signature\\'] ) ) {\\n[ADD] if ( is_ssl() ) {\\n[ADD] // If we signed an HTTP request on the Jetpack Servers, but got redirected to HTTPS by the local blog, check the HTTP signature as well.\\n[ADD] $signature = $jetpack_signature->sign_current_request(\\n[ADD] array(\\n[ADD] \\'scheme\\' => \\'http\\',\\n[ADD] \\'body\\'   => null,\\n[ADD] \\'method\\' => \\'GET\\',\\n[ADD] )\\n[ADD] );\\n[ADD] if ( ! $signature || is_wp_error( $signature ) || ! hash_equals( $signature, $environment[\\'signature\\'] ) ) {\\n[ADD] wp_die( esc_html( $die_error ) );\\n[ADD] }\\n[ADD] } else {\\n[ADD] wp_die( esc_html( $die_error ) );\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] $timestamp = (int) $environment[\\'timestamp\\'];\\n[ADD] $nonce     = stripslashes( (string) $environment[\\'nonce\\'] );\\n[ADD] \\n[ADD] if ( ! $this->connection->add_nonce( $timestamp, $nonce ) ) {\\n[ADD] // De-nonce the nonce, at least for 5 minutes.\\n[ADD] // We have to reuse this nonce at least once (used the first time when the initial request is made, used a second time when the login form is POSTed).\\n[ADD] $old_nonce_time = get_option( \"jetpack_nonce_{$timestamp}_{$nonce}\" );\\n[ADD] if ( $old_nonce_time < time() - 300 ) {\\n[ADD] wp_die( esc_html__( \\'The authorization process expired.  Please go back and try again.\\', \\'jetpack\\' ) );\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] // phpcs:ignore WordPress.PHP.DiscouragedPHPFunctions.obfuscation_base64_decode\\n[ADD] $data         = json_decode( base64_decode( stripslashes( $environment[\\'data\\'] ) ) );\\n[ADD] $data_filters = array(\\n[ADD] \\'state\\'        => \\'opaque\\',\\n[ADD] \\'client_id\\'    => \\'int\\',\\n[ADD] \\'client_title\\' => \\'string\\',\\n[ADD] \\'client_image\\' => \\'url\\',\\n[ADD] );\\n[ADD] \\n[ADD] foreach ( $data_filters as $key => $sanitation ) {\\n[ADD] if ( ! isset( $data->$key ) ) {\\n[ADD] wp_die( esc_html( $die_error ) );\\n[ADD] }\\n[ADD] \\n[ADD] switch ( $sanitation ) {\\n[ADD] case \\'int\\':\\n[ADD] $this->json_api_authorization_request[ $key ] = (int) $data->$key;\\n[ADD] break;\\n[ADD] case \\'opaque\\':\\n[ADD] $this->json_api_authorization_request[ $key ] = (string) $data->$key;\\n[ADD] break;\\n[ADD] case \\'string\\':\\n[ADD] $this->json_api_authorization_request[ $key ] = wp_kses( (string) $data->$key, array() );\\n[ADD] break;\\n[ADD] case \\'url\\':\\n[ADD] $this->json_api_authorization_request[ $key ] = esc_url_raw( (string) $data->$key );\\n[ADD] break;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] if ( empty( $this->json_api_authorization_request[\\'client_id\\'] ) ) {\\n[ADD] wp_die( esc_html( $die_error ) );\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Retrieve an updated login message for JSON API authorization.\\n[ADD] *\\n[ADD] * @return string Updated Login message.\\n[ADD] */\\n[ADD] public function login_message_json_api_authorization() {\\n[ADD] return \\'<p class=\"message\">\\' . sprintf(\\n[ADD] /* translators: %s is the name of the client you wish to authorize */\\n[ADD] esc_html__( \\'%s wants to access your site&#8217;s data.  Log in to authorize that access.\\', \\'jetpack\\' ),\\n[ADD] \\'<strong>\\' . esc_html( $this->json_api_authorization_request[\\'client_title\\'] ) . \\'</strong>\\'\\n[ADD] ) . \\'<img src=\"\\' . esc_url( $this->json_api_authorization_request[\\'client_image\\'] ) . \\'\" /></p>\\';\\n[ADD] }\\n }',\n",
              " '      * timers that fired to produce this bundle.\\n      */\\n     Instant getSynchronizedProcessingOutputWatermark();\\n[ADD] \\n[ADD] /**\\n[ADD] * Return a new {@link CommittedBundle} that is like this one, except calls to\\n[ADD] * {@link #getElements()} will return the provided elements. This bundle is unchanged.\\n[ADD] *\\n[ADD] * <p>\\n[ADD] * The value of {@link #getSynchronizedProcessingOutputWatermark()} of the returned\\n[ADD] * {@link CommittedBundle} is equal to the value returned from this one. This is used to ensure\\n[ADD] * a {@link PTransform} that could not complete processing on input elements properly holds the\\n[ADD] * synchronized processing time to the appropriate value.\\n[ADD] */\\n[ADD] CommittedBundle<T> withElements(Iterable<WindowedValue<T>> elements);\\n   }\\n \\n   /**',\n",
              " '     }\\n     return current;\\n   }\\n[ADD] \\n[ADD] private static class MethodVisitor extends BaseTreeVisitor {\\n[ADD] static Map<MethodTree, Boolean> cachedResults = new HashMap<>();\\n[ADD] boolean returnsAnArgumentMatcher = false;\\n[ADD] \\n[ADD] @Override\\n[ADD] public void visitMethod(MethodTree tree) {\\n[ADD] if (cachedResults.containsKey(tree)) {\\n[ADD] returnsAnArgumentMatcher = cachedResults.get(tree);\\n[ADD] return;\\n[ADD] }\\n[ADD] cachedResults.put(tree, Boolean.FALSE);\\n[ADD] List<ExpressionTree> terminalMethodInvocations = tree.block().body().stream()\\n[ADD] .filter(statement -> statement.is(Tree.Kind.RETURN_STATEMENT))\\n[ADD] .map(ReturnStatementTree.class::cast)\\n[ADD] .map(ReturnStatementTree::expression)\\n[ADD] .filter(expression -> expression.is(Tree.Kind.METHOD_INVOCATION))\\n[ADD] .collect(Collectors.toList());\\n[ADD] if (terminalMethodInvocations.isEmpty()) {\\n[ADD] return;\\n[ADD] }\\n[ADD] returnsAnArgumentMatcher = terminalMethodInvocations.stream()\\n[ADD] .allMatch(MockitoArgumentMatchersUsedOnAllParametersCheck::isArgumentMatcherLike);\\n[ADD] cachedResults.put(tree, returnsAnArgumentMatcher);\\n[ADD] }\\n[ADD] }\\n }',\n",
              " '         }\\n     }\\n \\n[ADD] @Override\\n[ADD] public Volume cloneDataVolume(CloneVMCmd cmd, long snapshotId, Volume volume) throws StorageUnavailableException {\\n[ADD] long vmId = cmd.getEntityId();\\n[ADD] return createVolumeFromSnapshot((VolumeVO) volume, snapshotId, vmId);\\n[ADD] }\\n[ADD] \\n     protected VolumeVO createVolumeFromSnapshot(VolumeVO volume, long snapshotId, Long vmId) throws StorageUnavailableException {\\n         VolumeInfo createdVolume = null;\\n         SnapshotVO snapshot = _snapshotDao.findById(snapshotId);',\n",
              " '    */\\n   public static GenericRecord convertRecordSchema(GenericRecord record, Schema newSchema) throws IOException {\\n     if (checkReaderWriterCompatibility(newSchema, record.getSchema()).getType() != COMPATIBLE) {\\n[DEL] LOG.warn(\"Record schema not compatible with writer schema. Converting record schema to writer schema may fail.\");\\n[ADD] LOG.debug(\"Record schema not compatible with writer schema. Converting record schema to writer schema may fail.\");\\n     }\\n \\n     Closer closer = Closer.create();',\n",
              " '         }\\n     }\\n \\n[ADD] protected class FetchRouterHealthChecksResultTask extends ManagedContextRunnable {\\n[ADD] public FetchRouterHealthChecksResultTask() {\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] protected void runInContext() {\\n[ADD] try {\\n[ADD] final List<DomainRouterVO> routers = _routerDao.listByStateAndManagementServer(VirtualMachine.State.Running, mgmtSrvrId);\\n[ADD] s_logger.info(\"Found \" + routers.size() + \" running routers. Fetching, analysing and updating DB for the health checks.\");\\n[ADD] if (!RouterHealthChecksEnabled.value()) {\\n[ADD] s_logger.debug(\"Skipping fetching of router health check results as router.health.checks.enabled is disabled\");\\n[ADD] return;\\n[ADD] }\\n[ADD] \\n[ADD] for (final DomainRouterVO router : routers) {\\n[ADD] GetRouterMonitorResultsAnswer answer = fetchAndUpdateRouterHealthChecks(router, false);\\n[ADD] List<String> failingChecks = getFailingChecks(router, answer);\\n[ADD] handleFailingChecks(router, failingChecks);\\n[ADD] }\\n[ADD] } catch (final Exception ex) {\\n[ADD] s_logger.error(\"Fail to complete the FetchRouterHealthChecksResultTask! \", ex);\\n[ADD] ex.printStackTrace();\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private List<String> getFailingChecks(DomainRouterVO router, GetRouterMonitorResultsAnswer answer) {\\n[ADD] if (answer == null) {\\n[ADD] s_logger.warn(\"Unable to fetch monitor results for router \" + router);\\n[ADD] updateRouterConnectivityHealthCheck(router.getId(), false, \"Communication failed\");\\n[ADD] return Arrays.asList(CONNECTIVITY_TEST);\\n[ADD] } else if (!answer.getResult()) {\\n[ADD] s_logger.warn(\"Failed to fetch monitor results from router \" + router + \" with details: \" + answer.getDetails());\\n[ADD] updateRouterConnectivityHealthCheck(router.getId(), false, \"Failed to fetch results with details: \" + answer.getDetails());\\n[ADD] return Arrays.asList(CONNECTIVITY_TEST);\\n[ADD] } else {\\n[ADD] updateRouterConnectivityHealthCheck(router.getId(), true, \"Successfully fetched data\");\\n[ADD] updateDbHealthChecksFromRouterResponse(router.getId(), answer.getMonitoringResults());\\n[ADD] return answer.getFailingChecks();\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private void handleFailingChecks(DomainRouterVO router, List<String> failingChecks) {\\n[ADD] if (failingChecks == null || failingChecks.size() == 0) {\\n[ADD] return;\\n[ADD] }\\n[ADD] \\n[ADD] String alertMessage = \"Health checks failed: \" + failingChecks.size() + \" failing checks on router \" + router.getUuid();\\n[ADD] _alertMgr.sendAlert(AlertType.ALERT_TYPE_DOMAIN_ROUTER, router.getDataCenterId(), router.getPodIdToDeployIn(),\\n[ADD] alertMessage, alertMessage);\\n[ADD] s_logger.warn(alertMessage + \". Checking failed health checks to see if router needs reboot\");\\n[ADD] \\n[ADD] String checkFailsToRecreateVr = RouterHealthChecksFailuresToRecreateVr.valueIn(router.getDataCenterId());\\n[ADD] StringBuilder failingChecksEvent = new StringBuilder(\"Router \" + router.getUuid() + \" has failing checks:\");\\n[ADD] boolean recreateRouter = false;\\n[ADD] for (String failedCheck : failingChecks) {\\n[ADD] failingChecksEvent.append(\" \").append(failedCheck);\\n[ADD] if (StringUtils.isNotBlank(checkFailsToRecreateVr) && checkFailsToRecreateVr.contains(failedCheck)) {\\n[ADD] recreateRouter = true;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] ActionEventUtils.onActionEvent(User.UID_SYSTEM, Account.ACCOUNT_ID_SYSTEM,\\n[ADD] Domain.ROOT_DOMAIN, EventTypes.EVENT_ROUTER_HEALTH_CHECKS, failingChecksEvent.toString());\\n[ADD] \\n[ADD] if (recreateRouter) {\\n[ADD] s_logger.warn(\"Health Check Alert: Found failing checks in \" +\\n[ADD] RouterHealthChecksFailuresToRecreateVrCK + \", attempting recreating router.\");\\n[ADD] recreateRouter(router.getId());\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private DomainRouterJoinVO getAnyRouterJoinWithVpc(long routerId) {\\n[ADD] List<DomainRouterJoinVO> routerJoinVOs = domainRouterJoinDao.searchByIds(routerId);\\n[ADD] for (DomainRouterJoinVO router : routerJoinVOs) {\\n[ADD] if (router.getRemoved() == null && router.getVpcId() != 0) {\\n[ADD] return router;\\n[ADD] }\\n[ADD] }\\n[ADD] return null;\\n[ADD] }\\n[ADD] \\n[ADD] private boolean restartVpcInDomainRouter(DomainRouterJoinVO router, User user) {\\n[ADD] try {\\n[ADD] s_logger.debug(\"Attempting restart VPC \" + router.getVpcName() + \" for router recreation \" + router.getUuid());\\n[ADD] ActionEventUtils.onActionEvent(User.UID_SYSTEM, Account.ACCOUNT_ID_SYSTEM,\\n[ADD] Domain.ROOT_DOMAIN, EventTypes.EVENT_ROUTER_HEALTH_CHECKS,\\n[ADD] \"Recreating router \" + router.getUuid() + \" by restarting VPC \" + router.getVpcUuid());\\n[ADD] return vpcService.restartVpc(router.getVpcId(), true, false, user);\\n[ADD] } catch (Exception e) {\\n[ADD] s_logger.error(\"Failed to restart VPC for router recreation \" +\\n[ADD] router.getVpcName() + \" ,router \" + router.getUuid(), e);\\n[ADD] return false;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private DomainRouterJoinVO getAnyRouterJoinWithGuestTraffic(long routerId) {\\n[ADD] List<DomainRouterJoinVO> routerJoinVOs = domainRouterJoinDao.searchByIds(routerId);\\n[ADD] for (DomainRouterJoinVO router : routerJoinVOs) {\\n[ADD] if (router.getRemoved() == null && router.getTrafficType() == TrafficType.Guest) {\\n[ADD] return router;\\n[ADD] }\\n[ADD] }\\n[ADD] return null;\\n[ADD] }\\n[ADD] \\n[ADD] private boolean restartGuestNetworkInDomainRouter(DomainRouterJoinVO router, User user) {\\n[ADD] try {\\n[ADD] s_logger.info(\"Attempting restart network \" + router.getNetworkName() + \" for router recreation \" + router.getUuid());\\n[ADD] ActionEventUtils.onActionEvent(User.UID_SYSTEM, Account.ACCOUNT_ID_SYSTEM,\\n[ADD] Domain.ROOT_DOMAIN, EventTypes.EVENT_ROUTER_HEALTH_CHECKS,\\n[ADD] \"Recreating router \" + router.getUuid() + \" by restarting network \" + router.getNetworkUuid());\\n[ADD] return networkService.restartNetwork(router.getNetworkId(), true, false, user);\\n[ADD] } catch (Exception e) {\\n[ADD] s_logger.error(\"Failed to restart network \" + router.getNetworkName() +\\n[ADD] \" for router recreation \" + router.getNetworkName(), e);\\n[ADD] return false;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Attempts recreation of router by restarting with cleanup a VPC if any or a guest network associated in case no VPC.\\n[ADD] * @param routerId - the id of the router to be recreated.\\n[ADD] * @return true if successfully restart is attempted else false.\\n[ADD] */\\n[ADD] private boolean recreateRouter(long routerId) {\\n[ADD] User systemUser = _userDao.getUser(User.UID_SYSTEM);\\n[ADD] \\n[ADD] // Find any VPC containing router join VO, restart it and return\\n[ADD] DomainRouterJoinVO routerJoinToRestart = getAnyRouterJoinWithVpc(routerId);\\n[ADD] if (routerJoinToRestart != null) {\\n[ADD] return restartVpcInDomainRouter(routerJoinToRestart, systemUser);\\n[ADD] }\\n[ADD] \\n[ADD] // If no VPC containing router join VO was found we look for a guest network traffic containing join VO and restart that.\\n[ADD] routerJoinToRestart = getAnyRouterJoinWithGuestTraffic(routerId);\\n[ADD] if (routerJoinToRestart != null) {\\n[ADD] return restartGuestNetworkInDomainRouter(routerJoinToRestart, systemUser);\\n[ADD] }\\n[ADD] \\n[ADD] s_logger.warn(\"Unable to find a valid guest network or VPC to restart for recreating router id \" + routerId);\\n[ADD] return false;\\n[ADD] }\\n[ADD] \\n[ADD] private Map<String, Map<String, RouterHealthCheckResultVO>> getHealthChecksFromDb(long routerId) {\\n[ADD] List<RouterHealthCheckResultVO> healthChecksList = routerHealthCheckResultDao.getHealthCheckResults(routerId);\\n[ADD] Map<String, Map<String, RouterHealthCheckResultVO>> healthCheckResults = new HashMap<>();\\n[ADD] if (healthChecksList.isEmpty()) {\\n[ADD] return healthCheckResults;\\n[ADD] }\\n[ADD] \\n[ADD] for (RouterHealthCheckResultVO healthCheck : healthChecksList) {\\n[ADD] if (!healthCheckResults.containsKey(healthCheck.getCheckType())) {\\n[ADD] healthCheckResults.put(healthCheck.getCheckType(), new HashMap<>());\\n[ADD] }\\n[ADD] healthCheckResults.get(healthCheck.getCheckType()).put(healthCheck.getCheckName(), healthCheck);\\n[ADD] }\\n[ADD] \\n[ADD] return healthCheckResults;\\n[ADD] }\\n[ADD] \\n[ADD] private RouterHealthCheckResultVO updateRouterConnectivityHealthCheck(final long routerId, boolean connected, String message) {\\n[ADD] boolean newEntry = false;\\n[ADD] RouterHealthCheckResultVO connectivityVO = routerHealthCheckResultDao.getRouterHealthCheckResult(routerId, CONNECTIVITY_TEST, \"basic\");\\n[ADD] if (connectivityVO == null) {\\n[ADD] connectivityVO = new RouterHealthCheckResultVO(routerId, CONNECTIVITY_TEST, \"basic\");\\n[ADD] newEntry = true;\\n[ADD] }\\n[ADD] \\n[ADD] connectivityVO.setCheckResult(connected);\\n[ADD] connectivityVO.setLastUpdateTime(new Date());\\n[ADD] if (StringUtils.isNotEmpty(message)) {\\n[ADD] connectivityVO.setCheckDetails(message.getBytes(com.cloud.utils.StringUtils.getPreferredCharset()));\\n[ADD] }\\n[ADD] \\n[ADD] if (newEntry) {\\n[ADD] routerHealthCheckResultDao.persist(connectivityVO);\\n[ADD] } else {\\n[ADD] routerHealthCheckResultDao.update(connectivityVO.getId(), connectivityVO);\\n[ADD] }\\n[ADD] \\n[ADD] return routerHealthCheckResultDao.getRouterHealthCheckResult(routerId, CONNECTIVITY_TEST, \"basic\");\\n[ADD] }\\n[ADD] \\n[ADD] private RouterHealthCheckResultVO parseHealthCheckVOFromJson(final long routerId,\\n[ADD] final String checkName, final String checkType, final Map<String, String> checkData,\\n[ADD] final Map<String, Map<String, RouterHealthCheckResultVO>> checksInDb) {\\n[ADD] boolean success = Boolean.parseBoolean(checkData.get(\"success\"));\\n[ADD] Date lastUpdate = new Date(Long.parseLong(checkData.get(\"lastUpdate\")));\\n[ADD] double lastRunDuration = Double.parseDouble(checkData.get(\"lastRunDuration\"));\\n[ADD] String message = checkData.get(\"message\");\\n[ADD] final RouterHealthCheckResultVO hcVo;\\n[ADD] boolean newEntry = false;\\n[ADD] if (checksInDb.containsKey(checkType) && checksInDb.get(checkType).containsKey(checkName)) {\\n[ADD] hcVo = checksInDb.get(checkType).get(checkName);\\n[ADD] } else {\\n[ADD] hcVo = new RouterHealthCheckResultVO(routerId, checkName, checkType);\\n[ADD] newEntry = true;\\n[ADD] }\\n[ADD] \\n[ADD] hcVo.setCheckResult(success);\\n[ADD] hcVo.setLastUpdateTime(lastUpdate);\\n[ADD] if (StringUtils.isNotEmpty(message)) {\\n[ADD] hcVo.setCheckDetails(message.getBytes(com.cloud.utils.StringUtils.getPreferredCharset()));\\n[ADD] }\\n[ADD] \\n[ADD] if (newEntry) {\\n[ADD] routerHealthCheckResultDao.persist(hcVo);\\n[ADD] } else {\\n[ADD] routerHealthCheckResultDao.update(hcVo.getId(), hcVo);\\n[ADD] }\\n[ADD] s_logger.info(\"Found health check \" + hcVo + \" which took running duration (ms) \" + lastRunDuration);\\n[ADD] return hcVo;\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] *\\n[ADD] * @param checksJson JSON expected is\\n[ADD] *                   {\\n[ADD] *                      checkType1: {\\n[ADD] *                          checkName1: {\\n[ADD] *                              success: true/false,\\n[ADD] *                              lastUpdate: date string,\\n[ADD] *                              lastRunDuration: ms spent on test,\\n[ADD] *                              message: detailed message from check execution\\n[ADD] *                          },\\n[ADD] *                          checkType2: .....\\n[ADD] *                      },\\n[ADD] *                      checkType2: ......\\n[ADD] *                   }\\n[ADD] * @return converts the above JSON into list of RouterHealthCheckResult.\\n[ADD] */\\n[ADD] private List<RouterHealthCheckResult> parseHealthCheckResults(\\n[ADD] final Map<String, Map<String, Map<String, String>>> checksJson, final long routerId) {\\n[ADD] final Map<String, Map<String, RouterHealthCheckResultVO>> checksInDb = getHealthChecksFromDb(routerId);\\n[ADD] List<RouterHealthCheckResult> healthChecks = new ArrayList<>();\\n[ADD] final String lastRunKey = \"lastRun\";\\n[ADD] for (String checkType : checksJson.keySet()) {\\n[ADD] if (checksJson.get(checkType).containsKey(lastRunKey)) { // Log last run of this check type run info\\n[ADD] Map<String, String> lastRun = checksJson.get(checkType).get(lastRunKey);\\n[ADD] s_logger.info(\"Found check types executed on VR \" + checkType + \", start: \" + lastRun.get(\"start\") +\\n[ADD] \", end: \" + lastRun.get(\"end\") + \", duration: \" + lastRun.get(\"duration\"));\\n[ADD] }\\n[ADD] \\n[ADD] for (String checkName : checksJson.get(checkType).keySet()) {\\n[ADD] if (lastRunKey.equals(checkName)) {\\n[ADD] continue;\\n[ADD] }\\n[ADD] \\n[ADD] try {\\n[ADD] final RouterHealthCheckResultVO hcVo = parseHealthCheckVOFromJson(\\n[ADD] routerId, checkName, checkType, checksJson.get(checkType).get(checkName), checksInDb);\\n[ADD] healthChecks.add(hcVo);\\n[ADD] } catch (Exception ex) {\\n[ADD] s_logger.error(\"Skipping health check: Exception while parsing check result data for router id \" + routerId +\\n[ADD] \", check type: \" + checkType + \", check name: \" + checkName + \":\" + ex.getLocalizedMessage(), ex);\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] return healthChecks;\\n[ADD] }\\n[ADD] \\n[ADD] private List<RouterHealthCheckResult> updateDbHealthChecksFromRouterResponse(final long routerId, final String monitoringResult) {\\n[ADD] if (StringUtils.isBlank(monitoringResult)) {\\n[ADD] s_logger.warn(\"Attempted parsing empty monitoring results string for router \" + routerId);\\n[ADD] return Collections.emptyList();\\n[ADD] }\\n[ADD] \\n[ADD] try {\\n[ADD] s_logger.debug(\"Parsing and updating DB health check data for router: \" + routerId + \" with data: \" + monitoringResult) ;\\n[ADD] final Type t = new TypeToken<Map<String, Map<String, Map<String, String>>>>() {}.getType();\\n[ADD] final Map<String, Map<String, Map<String, String>>> checks = GsonHelper.getGson().fromJson(monitoringResult, t);\\n[ADD] return parseHealthCheckResults(checks, routerId);\\n[ADD] } catch (JsonSyntaxException ex) {\\n[ADD] s_logger.error(\"Unable to parse the result of health checks due to \" + ex.getLocalizedMessage(), ex);\\n[ADD] }\\n[ADD] \\n[ADD] return Collections.emptyList();\\n[ADD] }\\n[ADD] \\n[ADD] private GetRouterMonitorResultsAnswer fetchAndUpdateRouterHealthChecks(DomainRouterVO router, boolean performFreshChecks) {\\n[ADD] if (!RouterHealthChecksEnabled.value()) {\\n[ADD] return null;\\n[ADD] }\\n[ADD] \\n[ADD] String controlIP = getRouterControlIP(router);\\n[ADD] if (StringUtils.isNotBlank(controlIP) && !controlIP.equals(\"0.0.0.0\")) {\\n[ADD] final GetRouterMonitorResultsCommand command = new GetRouterMonitorResultsCommand(performFreshChecks);\\n[ADD] command.setAccessDetail(NetworkElementCommand.ROUTER_IP, controlIP);\\n[ADD] command.setAccessDetail(NetworkElementCommand.ROUTER_NAME, router.getInstanceName());\\n[ADD] try {\\n[ADD] final Answer answer = _agentMgr.easySend(router.getHostId(), command);\\n[ADD] \\n[ADD] if (answer == null) {\\n[ADD] s_logger.warn(\"Unable to fetch monitoring results data from router \" + router.getHostName());\\n[ADD] return null;\\n[ADD] }\\n[ADD] if (answer instanceof GetRouterMonitorResultsAnswer) {\\n[ADD] return (GetRouterMonitorResultsAnswer) answer;\\n[ADD] } else {\\n[ADD] s_logger.warn(\"Unable to fetch health checks results to router \" + router.getHostName() + \" Received answer \" + answer.getDetails());\\n[ADD] return new GetRouterMonitorResultsAnswer(command, false, null, answer.getDetails());\\n[ADD] }\\n[ADD] } catch (final Exception e) {\\n[ADD] s_logger.warn(\"Error while collecting alerts from router: \" + router.getInstanceName(), e);\\n[ADD] return null;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] return null;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public boolean performRouterHealthChecks(long routerId) {\\n[ADD] DomainRouterVO router = _routerDao.findById(routerId);\\n[ADD] \\n[ADD] if (router == null) {\\n[ADD] throw new CloudRuntimeException(\"Unable to find router with id \" + routerId);\\n[ADD] }\\n[ADD] \\n[ADD] if (!RouterHealthChecksEnabled.value()) {\\n[ADD] throw new CloudRuntimeException(\"Router health checks are not enabled for router: \" + router);\\n[ADD] }\\n[ADD] \\n[ADD] s_logger.info(\"Running health check results for router \" + router.getUuid());\\n[ADD] \\n[ADD] final GetRouterMonitorResultsAnswer answer;\\n[ADD] boolean success = true;\\n[ADD] // Step 1: Update health check data on router and perform and retrieve health checks on router\\n[ADD] if (!updateRouterHealthChecksConfig(router)) {\\n[ADD] s_logger.warn(\"Unable to update health check config for fresh run successfully for router: \" + router + \", so trying to fetch last result.\");\\n[ADD] success = false;\\n[ADD] answer = fetchAndUpdateRouterHealthChecks(router, false);\\n[ADD] } else {\\n[ADD] s_logger.info(\"Successfully updated health check config for fresh run successfully for router: \" + router);\\n[ADD] answer = fetchAndUpdateRouterHealthChecks(router, true);\\n[ADD] }\\n[ADD] \\n[ADD] // Step 2: Update health checks values in database. We do this irrespective of new health check config.\\n[ADD] if (answer == null || !answer.getResult()) {\\n[ADD] success = false;\\n[ADD] updateRouterConnectivityHealthCheck(routerId, false,\\n[ADD] answer == null ? \"Communication failed \" : \"Failed to fetch results with details: \" + answer.getDetails());\\n[ADD] } else {\\n[ADD] updateRouterConnectivityHealthCheck(routerId, true, \"Successfully fetched data\");\\n[ADD] updateDbHealthChecksFromRouterResponse(routerId, answer.getMonitoringResults());\\n[ADD] }\\n[ADD] \\n[ADD] return success;\\n[ADD] }\\n[ADD] \\n[ADD] protected class UpdateRouterHealthChecksConfigTask extends ManagedContextRunnable {\\n[ADD] public UpdateRouterHealthChecksConfigTask() {\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] protected void runInContext() {\\n[ADD] try {\\n[ADD] final List<DomainRouterVO> routers = _routerDao.listByStateAndManagementServer(VirtualMachine.State.Running, mgmtSrvrId);\\n[ADD] s_logger.debug(\"Found \" + routers.size() + \" running routers. \");\\n[ADD] \\n[ADD] for (final DomainRouterVO router : routers) {\\n[ADD] updateRouterHealthChecksConfig(router);\\n[ADD] }\\n[ADD] } catch (final Exception ex) {\\n[ADD] s_logger.error(\"Fail to complete the UpdateRouterHealthChecksConfigTask! \", ex);\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private SetMonitorServiceCommand createMonitorServiceCommand(DomainRouterVO router, List<MonitorServiceTO> services,\\n[ADD] boolean reconfigure, boolean deleteFromProcessedCache) {\\n[ADD] final SetMonitorServiceCommand command = new SetMonitorServiceCommand(services);\\n[ADD] command.setAccessDetail(NetworkElementCommand.ROUTER_IP, getRouterControlIP(router));\\n[ADD] command.setAccessDetail(NetworkElementCommand.ROUTER_NAME, router.getInstanceName());\\n[ADD] command.setAccessDetail(SetMonitorServiceCommand.ROUTER_HEALTH_CHECKS_ENABLED, RouterHealthChecksEnabled.value().toString());\\n[ADD] command.setAccessDetail(SetMonitorServiceCommand.ROUTER_HEALTH_CHECKS_BASIC_INTERVAL, RouterHealthChecksBasicInterval.value().toString());\\n[ADD] command.setAccessDetail(SetMonitorServiceCommand.ROUTER_HEALTH_CHECKS_ADVANCED_INTERVAL, RouterHealthChecksAdvancedInterval.value().toString());\\n[ADD] command.setAccessDetail(SetMonitorServiceCommand.ROUTER_HEALTH_CHECKS_EXCLUDED, RouterHealthChecksToExclude.valueIn(router.getDataCenterId()));\\n[ADD] command.setHealthChecksConfig(getRouterHealthChecksConfig(router));\\n[ADD] command.setReconfigureAfterUpdate(reconfigure);\\n[ADD] command.setDeleteFromProcessedCache(deleteFromProcessedCache); // As part of updating\\n[ADD] return command;\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Updates router health check config to the virtual router that it uses for health checks.\\n[ADD] * @param router - the router ID that data needs to be sent to.\\n[ADD] * @return success of whether data was sent or not\\n[ADD] */\\n[ADD] private boolean updateRouterHealthChecksConfig(DomainRouterVO router) {\\n[ADD] if (!RouterHealthChecksEnabled.value()) {\\n[ADD] return false;\\n[ADD] }\\n[ADD] \\n[ADD] SetMonitorServiceCommand command = createMonitorServiceCommand(router, null,true, true);\\n[ADD] String controlIP = getRouterControlIP(router);\\n[ADD] if (StringUtils.isBlank(controlIP) || controlIP.equals(\"0.0.0.0\")) {\\n[ADD] s_logger.debug(\"Skipping update data on router \" + router.getUuid() + \" because controlIp is not correct.\");\\n[ADD] return false;\\n[ADD] }\\n[ADD] \\n[ADD] s_logger.info(\"Updating data for router health checks for router \" + router.getUuid());\\n[ADD] Answer origAnswer = null;\\n[ADD] try {\\n[ADD] origAnswer = _agentMgr.easySend(router.getHostId(), command);\\n[ADD] } catch (final Exception e) {\\n[ADD] s_logger.error(\"Error while sending update data for health check to router: \" + router.getInstanceName(), e);\\n[ADD] return false;\\n[ADD] }\\n[ADD] \\n[ADD] if (origAnswer == null) {\\n[ADD] s_logger.error(\"Unable to update health checks data to router \" + router.getHostName());\\n[ADD] return false;\\n[ADD] }\\n[ADD] \\n[ADD] GroupAnswer answer = null;\\n[ADD] if (origAnswer instanceof GroupAnswer) {\\n[ADD] answer = (GroupAnswer) origAnswer;\\n[ADD] } else {\\n[ADD] s_logger.error(\"Unable to update health checks data to router \" + router.getHostName() + \" Received answer \" + origAnswer.getDetails());\\n[ADD] return false;\\n[ADD] }\\n[ADD] \\n[ADD] if (!answer.getResult()) {\\n[ADD] s_logger.error(\"Unable to update health checks data to router \" + router.getHostName() + \", details : \" + answer.getDetails());\\n[ADD] }\\n[ADD] \\n[ADD] return answer.getResult();\\n[ADD] }\\n[ADD] \\n[ADD] private String getSystemThresholdsHealthChecksData(final DomainRouterVO router) {\\n[ADD] return new StringBuilder()\\n[ADD] .append(\"minDiskNeeded=\" + RouterHealthChecksFreeDiskSpaceThreshold.valueIn(router.getDataCenterId()))\\n[ADD] .append(\",maxCpuUsage=\" + RouterHealthChecksMaxCpuUsageThreshold.valueIn(router.getDataCenterId()))\\n[ADD] .append(\",maxMemoryUsage=\" + RouterHealthChecksMaxMemoryUsageThreshold.valueIn(router.getDataCenterId()) + \";\")\\n[ADD] .toString();\\n[ADD] }\\n[ADD] \\n[ADD] private String getRouterVersionHealthChecksData(final DomainRouterVO router) {\\n[ADD] if (router.getTemplateVersion() != null && router.getScriptsVersion() != null) {\\n[ADD] StringBuilder routerVersion = new StringBuilder()\\n[ADD] .append(\"templateVersion=\" + router.getTemplateVersion())\\n[ADD] .append(\",scriptsVersion=\" + router.getScriptsVersion());\\n[ADD] return routerVersion.toString();\\n[ADD] }\\n[ADD] return null;\\n[ADD] }\\n[ADD] \\n[ADD] private void updateWithPortForwardingRules(final DomainRouterJoinVO routerJoinVO, final UserVmJoinVO vm, final StringBuilder portData) {\\n[ADD] SearchBuilder<PortForwardingRuleVO> sbpf = portForwardingDao.createSearchBuilder();\\n[ADD] sbpf.and(\"networkId\", sbpf.entity().getNetworkId(), SearchCriteria.Op.EQ);\\n[ADD] sbpf.and(\"instanceId\", sbpf.entity().getVirtualMachineId(), SearchCriteria.Op.EQ);\\n[ADD] SearchCriteria<PortForwardingRuleVO> scpf = sbpf.create();\\n[ADD] scpf.setParameters(\"networkId\", routerJoinVO.getNetworkId());\\n[ADD] scpf.setParameters(\"instanceId\", vm.getId());\\n[ADD] List<PortForwardingRuleVO> portForwardingRules = portForwardingDao.search(scpf, null);\\n[ADD] for (PortForwardingRuleVO portForwardingRule : portForwardingRules) {\\n[ADD] portData.append(\"sourceIp=\").append(_ipAddressDao.findById(portForwardingRule.getSourceIpAddressId()).getAddress().toString())\\n[ADD] .append(\",sourcePortStart=\").append(portForwardingRule.getSourcePortStart())\\n[ADD] .append(\",sourcePortEnd=\").append(portForwardingRule.getSourcePortEnd())\\n[ADD] .append(\",destIp=\").append(portForwardingRule.getDestinationIpAddress())\\n[ADD] .append(\",destPortStart=\").append(portForwardingRule.getDestinationPortStart())\\n[ADD] .append(\",destPortEnd=\").append(portForwardingRule.getDestinationPortEnd()).append(\";\");\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private String getStickinessPolicies(long loadBalancingRuleId) {\\n[ADD] List<LBStickinessPolicyVO> stickinessPolicyVOs = lbStickinessPolicyDao.listByLoadBalancerId(loadBalancingRuleId, false);\\n[ADD] if (stickinessPolicyVOs != null && stickinessPolicyVOs.size() > 0) {\\n[ADD] StringBuilder stickiness = new StringBuilder();\\n[ADD] for (LBStickinessPolicyVO stickinessVO : stickinessPolicyVOs) {\\n[ADD] stickiness.append(stickinessVO.getMethodName()).append(\" \");\\n[ADD] }\\n[ADD] return stickiness.toString().trim();\\n[ADD] }\\n[ADD] return \"None\";\\n[ADD] }\\n[ADD] \\n[ADD] private void updateWithLbRules(final DomainRouterJoinVO routerJoinVO, final StringBuilder loadBalancingData) {\\n[ADD] List<? extends FirewallRuleVO> loadBalancerVOs = this.getLBRules(routerJoinVO);\\n[ADD] for (FirewallRuleVO firewallRuleVO : loadBalancerVOs) {\\n[ADD] List<LoadBalancerVMMapVO> vmMapVOs = _loadBalancerVMMapDao.listByLoadBalancerId(firewallRuleVO.getId(), false);\\n[ADD] if (vmMapVOs.size() > 0) {\\n[ADD] \\n[ADD] final NetworkOffering offering = _networkOfferingDao.findById(_networkDao.findById(routerJoinVO.getNetworkId()).getNetworkOfferingId());\\n[ADD] if (offering.getConcurrentConnections() == null) {\\n[ADD] loadBalancingData.append(\"maxconn=\").append(_configDao.getValue(Config.NetworkLBHaproxyMaxConn.key()));\\n[ADD] } else {\\n[ADD] loadBalancingData.append(\"maxconn=\").append(offering.getConcurrentConnections().toString());\\n[ADD] }\\n[ADD] \\n[ADD] loadBalancingData.append(\",sourcePortStart=\").append(firewallRuleVO.getSourcePortStart())\\n[ADD] .append(\",sourcePortEnd=\").append(firewallRuleVO.getSourcePortEnd());\\n[ADD] if (firewallRuleVO instanceof LoadBalancerVO) {\\n[ADD] LoadBalancerVO loadBalancerVO = (LoadBalancerVO) firewallRuleVO;\\n[ADD] loadBalancingData.append(\",sourceIp=\").append(_ipAddressDao.findById(loadBalancerVO.getSourceIpAddressId()).getAddress().toString())\\n[ADD] .append(\",destPortStart=\").append(loadBalancerVO.getDefaultPortStart())\\n[ADD] .append(\",destPortEnd=\").append(loadBalancerVO.getDefaultPortEnd())\\n[ADD] .append(\",algorithm=\").append(loadBalancerVO.getAlgorithm())\\n[ADD] .append(\",protocol=\").append(loadBalancerVO.getLbProtocol());\\n[ADD] } else if (firewallRuleVO instanceof ApplicationLoadBalancerRuleVO) {\\n[ADD] ApplicationLoadBalancerRuleVO appLoadBalancerVO = (ApplicationLoadBalancerRuleVO) firewallRuleVO;\\n[ADD] loadBalancingData.append(\",sourceIp=\").append(appLoadBalancerVO.getSourceIp())\\n[ADD] .append(\",destPortStart=\").append(appLoadBalancerVO.getDefaultPortStart())\\n[ADD] .append(\",destPortEnd=\").append(appLoadBalancerVO.getDefaultPortEnd())\\n[ADD] .append(\",algorithm=\").append(appLoadBalancerVO.getAlgorithm())\\n[ADD] .append(\",protocol=\").append(appLoadBalancerVO.getLbProtocol());\\n[ADD] }\\n[ADD] loadBalancingData.append(\",stickiness=\").append(getStickinessPolicies(firewallRuleVO.getId()));\\n[ADD] loadBalancingData.append(\",keepAliveEnabled=\").append(offering.isKeepAliveEnabled()).append(\",vmIps=\");\\n[ADD] for (LoadBalancerVMMapVO vmMapVO : vmMapVOs) {\\n[ADD] loadBalancingData.append(vmMapVO.getInstanceIp()).append(\" \");\\n[ADD] }\\n[ADD] loadBalancingData.setCharAt(loadBalancingData.length() - 1, \\';\\');\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private Map<String, String> getRouterHealthChecksConfig(final DomainRouterVO router) {\\n[ADD] Map<String, String> data = new HashMap<>();\\n[ADD] List<DomainRouterJoinVO> routerJoinVOs = domainRouterJoinDao.searchByIds(router.getId());\\n[ADD] StringBuilder vmsData = new StringBuilder();\\n[ADD] StringBuilder portData = new StringBuilder();\\n[ADD] StringBuilder loadBalancingData = new StringBuilder();\\n[ADD] StringBuilder gateways = new StringBuilder();\\n[ADD] gateways.append(\"gatewaysIps=\");\\n[ADD] for (DomainRouterJoinVO routerJoinVO : routerJoinVOs) {\\n[ADD] if (StringUtils.isNotBlank(routerJoinVO.getGateway())) {\\n[ADD] gateways.append(routerJoinVO.getGateway() + \" \");\\n[ADD] }\\n[ADD] SearchBuilder<UserVmJoinVO> sbvm = userVmJoinDao.createSearchBuilder();\\n[ADD] sbvm.and(\"networkId\", sbvm.entity().getNetworkId(), SearchCriteria.Op.EQ);\\n[ADD] SearchCriteria<UserVmJoinVO> scvm = sbvm.create();\\n[ADD] scvm.setParameters(\"networkId\", routerJoinVO.getNetworkId());\\n[ADD] List<UserVmJoinVO> vms = userVmJoinDao.search(scvm, null);\\n[ADD] for (UserVmJoinVO vm : vms) {\\n[ADD] if (vm.getState() != VirtualMachine.State.Running) {\\n[ADD] continue;\\n[ADD] }\\n[ADD] \\n[ADD] vmsData.append(\"vmName=\").append(vm.getName())\\n[ADD] .append(\",macAddress=\").append(vm.getMacAddress())\\n[ADD] .append(\",ip=\").append(vm.getIpAddress()).append(\";\");\\n[ADD] updateWithPortForwardingRules(routerJoinVO, vm, portData);\\n[ADD] }\\n[ADD] updateWithLbRules(routerJoinVO, loadBalancingData);\\n[ADD] }\\n[ADD] \\n[ADD] String routerVersion = getRouterVersionHealthChecksData(router);\\n[ADD] data.put(\"virtualMachines\", vmsData.toString());\\n[ADD] data.put(\"gateways\", gateways.toString());\\n[ADD] data.put(\"portForwarding\", portData.toString());\\n[ADD] data.put(\"haproxyData\", loadBalancingData.toString());\\n[ADD] data.put(\"systemThresholds\", getSystemThresholdsHealthChecksData(router));\\n[ADD] if (routerVersion != null) {\\n[ADD] data.put(\"routerVersion\", routerVersion);\\n[ADD] }\\n[ADD] return data;\\n[ADD] }\\n[ADD] \\n[ADD] private List<? extends FirewallRuleVO> getLBRules(final DomainRouterJoinVO router) {\\n[ADD] if (router.getRole() == Role.VIRTUAL_ROUTER) {\\n[ADD] SearchBuilder<LoadBalancerVO> sblb = _loadBalancerDao.createSearchBuilder();\\n[ADD] sblb.and(\"networkId\", sblb.entity().getNetworkId(), SearchCriteria.Op.EQ);\\n[ADD] sblb.and(\"sourceIpAddressId\", sblb.entity().getSourceIpAddressId(), SearchCriteria.Op.NNULL);\\n[ADD] SearchCriteria<LoadBalancerVO> sclb = sblb.create();\\n[ADD] sclb.setParameters(\"networkId\", router.getNetworkId());\\n[ADD] return _loadBalancerDao.search(sclb, null);\\n[ADD] } else if (router.getRole() == Role.INTERNAL_LB_VM) {\\n[ADD] SearchBuilder<ApplicationLoadBalancerRuleVO> sbalb = applicationLoadBalancerRuleDao.createSearchBuilder();\\n[ADD] sbalb.and(\"networkId\", sbalb.entity().getNetworkId(), SearchCriteria.Op.EQ);\\n[ADD] sbalb.and(\"sourceIpAddress\", sbalb.entity().getSourceIp(), SearchCriteria.Op.NNULL);\\n[ADD] SearchCriteria<ApplicationLoadBalancerRuleVO> sclb = sbalb.create();\\n[ADD] sclb.setParameters(\"networkId\", router.getNetworkId());\\n[ADD] return applicationLoadBalancerRuleDao.search(sclb, null);\\n[ADD] }\\n[ADD] return Collections.emptyList();\\n[ADD] }\\n[ADD] \\n     protected class CheckRouterAlertsTask extends ManagedContextRunnable {\\n         public CheckRouterAlertsTask() {\\n         }',\n",
              " '       kill.put(block, blockKill);\\n       gen.put(block, blockGen);\\n     }\\n[DEL] liveVariables.analyzeCFG(in, kill, gen);\\n[ADD] liveVariables.analyzeCFG(liveVariables.in, kill, gen);\\n     // out of exit block are empty by definition.\\n     if (!liveVariables.out.get(liveVariables.cfg.reversedBlocks().get(0)).isEmpty()) {\\n       throw new IllegalStateException(\"Out of exit block should be empty\");',\n",
              " '   @RunWith(JUnit4.class)\\n   public static class CombineWithContextTests extends SharedTestBase {\\n     @Test\\n[DEL] @Category(ValidatesRunner.class)\\n[ADD] @Category({ValidatesRunner.class, UsesSideInputs.class})\\n     @SuppressWarnings({\"rawtypes\", \"unchecked\"})\\n     public void testSimpleCombineWithContext() {\\n       runTestSimpleCombineWithContext(',\n",
              " '     }\\n   }\\n \\n[ADD] private void unparseExtractFunctions(\\n[ADD] SqlWriter writer, SqlCall call, int leftPrec, int rightPrec) {\\n[ADD] String funName = call.getOperator().getName();\\n[ADD] int operandCount = call.operandCount();\\n[ADD] SqlNode tz = null;\\n[ADD] \\n[ADD] final SqlWriter.Frame frame = writer.startFunCall(\"EXTRACT\");\\n[ADD] if (!funName.equals(\"$extract\") && (operandCount == 1 || operandCount == 2)) {\\n[ADD] // EXTRACT(DATE/TIME/DATETIME FROM timestamp_expression [AT TIME ZONE tz])\\n[ADD] // operand0: timestamp_expression\\n[ADD] // operand1: tz (optional)\\n[ADD] writer.literal(EXTRACT_FUNCTIONS.get(funName));\\n[ADD] if (operandCount == 2) {\\n[ADD] tz = call.operand(1);\\n[ADD] }\\n[ADD] } else if (funName.equals(\"$extract\") && (operandCount == 2 || operandCount == 3)) {\\n[ADD] // EXTRACT(date_part FROM timestamp_expression [AT TIME ZONE tz])\\n[ADD] // operand0: timestamp_expression\\n[ADD] // operand1: date_part\\n[ADD] // operand2: tz (optional)\\n[ADD] call.operand(1).unparse(writer, leftPrec, rightPrec);\\n[ADD] if (operandCount == 3) {\\n[ADD] tz = call.operand(2);\\n[ADD] }\\n[ADD] } else {\\n[ADD] throw new IllegalArgumentException(\\n[ADD] String.format(\"Unable to unparse %s with %d operands.\", funName, operandCount));\\n[ADD] }\\n[ADD] writer.literal(\"FROM\");\\n[ADD] call.operand(0).unparse(writer, leftPrec, rightPrec);\\n[ADD] if (tz != null) {\\n[ADD] writer.literal(\"AT TIME ZONE\");\\n[ADD] tz.unparse(writer, leftPrec, rightPrec);\\n[ADD] }\\n[ADD] writer.endFunCall(frame);\\n[ADD] }\\n[ADD] \\n   private TimeUnit validate(TimeUnit timeUnit) {\\n     switch (timeUnit) {\\n       case MICROSECOND:',\n",
              " '   private final DeployableArtifactClassLoaderFactory<ApplicationDescriptor> applicationClassLoaderFactory;\\n   private final ArtifactPluginRepository artifactPluginRepository;\\n   private final ArtifactClassLoaderFactory<ArtifactPluginDescriptor> artifactPluginClassLoaderFactory;\\n[ADD] private final ArtifactDescriptorFactory<ArtifactPluginDescriptor> artifactDescriptorFactory;\\n[ADD] private final DependenciesProvider dependenciesProvider;\\n \\n   /**\\n    * Creates an {@code ApplicationClassLoaderBuilderFactory} to create {@code ApplicationClassLoaderBuilder} instances.',\n",
              " '   public void try_catch() {\\n     boolean a = false, b = false, c = false, d = false;\\n     try {\\n[ADD] foo();\\n       b = true;\\n[ADD] foo();\\n       c = true;\\n[DEL] } catch (Exception e) {\\n[ADD] foo();\\n[ADD] } catch (IllegalArgumentException e) {\\n       if (a) { // Noncompliant {{Change this condition so that it does not always evaluate to \"false\"}}\\n       }\\n       if (b) {',\n",
              " \"         }\\n     }\\n \\n[ADD] /**\\n[ADD] * Returns true if at least one of the entries on the map 'volumeDataStoreMap' has both source and destination storage pools of Network Filesystem (NFS).\\n[ADD] */\\n[ADD] protected boolean isSourceAndDestinationPoolTypeOfNfs(Map<VolumeInfo, DataStore> volumeDataStoreMap) {\\n[ADD] for (Map.Entry<VolumeInfo, DataStore> entry : volumeDataStoreMap.entrySet()) {\\n[ADD] VolumeInfo srcVolumeInfo = entry.getKey();\\n[ADD] DataStore destDataStore = entry.getValue();\\n[ADD] \\n[ADD] StoragePoolVO destStoragePool = _storagePoolDao.findById(destDataStore.getId());\\n[ADD] StoragePoolVO sourceStoragePool = _storagePoolDao.findById(srcVolumeInfo.getPoolId());\\n[ADD] if (sourceStoragePool.getPoolType() == StoragePoolType.NetworkFilesystem && destStoragePool.getPoolType() == StoragePoolType.NetworkFilesystem) {\\n[ADD] return true;\\n[ADD] }\\n[ADD] }\\n[ADD] return false;\\n[ADD] }\\n[ADD] \\n     /**\\n      * Returns true. This method was implemented considering the classes that extend this {@link StorageSystemDataMotionStrategy} and cannot migrate volumes from certain types of source storage pools and/or to a different kind of destiny storage pool.\\n      */\",\n",
              " ' \\n   @Override\\n   public List<ConnectionProviderModelParser> getConnectionProviderModelParsers() {\\n[DEL] return JavaExtensionModelParserUtils.getConnectionProviderModelParsers(extensionElement,\\n[ADD] return JavaExtensionModelParserUtils.getConnectionProviderModelParsers(this, extensionElement,\\n                                                                            extensionElement.getConnectionProviders());\\n   }\\n ',\n",
              " ' \\n \\tprivate volatile boolean payloadAsBytes = false;\\n \\n[ADD] private volatile BeanFactory beanFactory;\\n[ADD] \\n[ADD] private volatile MessageBuilderFactory messageBuilderFactory = new DefaultMessageBuilderFactory();\\n[ADD] \\n[ADD] \\n \\tpublic DefaultPahoMessageConverter() {\\n \\t\\tthis (0, false);\\n \\t}',\n",
              " '     QueueSession s = mgr.getQueueSession();\\n     Queue q = s.getQueue(\"warmRecoverQueue\");\\n \\n[DEL] int toPopulate = 500;\\n[ADD] int toPopulate = 50;\\n \\n     // Populate queue\\n     Random rnd = new Random();',\n",
              " ' \\tpublic static LongRunningIntegrationTest longTests = new LongRunningIntegrationTest();\\n \\n \\t@Rule\\n[DEL] public Log4j2LevelAdjuster adjuster = Log4j2LevelAdjuster.trace();\\n[DEL] \\n[ADD] public Log4j2LevelAdjuster adjuster = Log4j2LevelAdjuster.trace()\\n[ADD] .categories(\"org.springframework.integration.ip\");\\n \\n \\t@Test\\n \\tpublic void testGoodNetSingle() throws Exception {',\n",
              " \" import static java.lang.String.format;\\n import static java.util.stream.Collectors.toCollection;\\n import static java.util.stream.Collectors.toList;\\n[DEL] import static java.util.stream.Collectors.toSet;\\n[DEL] import static org.mule.runtime.extension.api.values.ValueResolvingException.INVALID_VALUE_RESOLVER_NAME;\\n[DEL] import static org.mule.runtime.extension.api.values.ValueResolvingException.UNKNOWN;\\n import static org.mule.runtime.module.extension.internal.runtime.connectivity.oauth.ExtensionsOAuthUtils.withRefreshToken;\\n import static org.mule.runtime.module.extension.internal.value.ValueProviderUtils.cloneAndEnrichValue;\\n[ADD] import static org.mule.sdk.api.values.ValueResolvingException.INVALID_VALUE_RESOLVER_NAME;\\n[ADD] import static org.mule.sdk.api.values.ValueResolvingException.UNKNOWN;\\n \\n import org.mule.runtime.api.connection.ConnectionProvider;\\n import org.mule.runtime.api.meta.model.EnrichableModel;\\n import org.mule.runtime.api.meta.model.parameter.ParameterGroupModel;\\n import org.mule.runtime.api.meta.model.parameter.ParameterModel;\\n import org.mule.runtime.api.meta.model.parameter.ParameterizedModel;\\n[DEL] import org.mule.runtime.api.value.Value;\\n import org.mule.runtime.core.api.MuleContext;\\n[DEL] import org.mule.runtime.extension.api.values.ValueBuilder;\\n[DEL] import org.mule.runtime.extension.api.values.ValueProvider;\\n[DEL] import org.mule.runtime.extension.api.values.ValueResolvingException;\\n import org.mule.runtime.module.extension.internal.loader.java.property.ValueProviderFactoryModelProperty;\\n import org.mule.runtime.module.extension.internal.runtime.resolver.ParameterValueResolver;\\n import org.mule.runtime.module.extension.internal.util.ReflectionCache;\\n[ADD] import org.mule.sdk.api.values.Value;\\n[ADD] import org.mule.sdk.api.values.ValueBuilder;\\n[ADD] import org.mule.sdk.api.values.ValueProvider;\\n[ADD] import org.mule.sdk.api.values.ValueResolvingException;\\n \\n import java.util.LinkedHashSet;\\n import java.util.List;\\n import java.util.Set;\\n import java.util.function.Supplier;\\n[DEL] import java.util.stream.Collectors;\\n \\n /**\\n  * Resolves a parameter's {@link Value values} by coordinating the several moving parts that are affected by the {@link Value}\",\n",
              " '    */\\n   public static class PerWindowFiles extends FilenamePolicy {\\n \\n[DEL] private final String prefix;\\n[ADD] private final ResourceId baseFilename;\\n \\n[DEL] public PerWindowFiles(String prefix) {\\n[DEL] this.prefix = prefix;\\n[ADD] public PerWindowFiles(ResourceId baseFilename) {\\n[ADD] this.baseFilename = baseFilename;\\n     }\\n \\n     public String filenamePrefixForWindow(IntervalWindow window) {\\n[ADD] String prefix = baseFilename.isDirectory()\\n[ADD] ? \"\" : firstNonNull(baseFilename.getFilename(), \"\");\\n       return String.format(\"%s-%s-%s\",\\n           prefix, FORMATTER.print(window.start()), FORMATTER.print(window.end()));\\n     }\\n \\n     @Override\\n     public ResourceId windowedFilename(\\n[DEL] ResourceId outputDirectory, WindowedContext context, String extension) {\\n[ADD] WindowedContext context, FileMetadataProvider fileMetadataProvider) {\\n       IntervalWindow window = (IntervalWindow) context.getWindow();\\n[DEL] String filename = String.format(\\n[DEL] \"%s-%s-of-%s%s\",\\n[DEL] filenamePrefixForWindow(window), context.getShardNumber(), context.getNumShards(),\\n[DEL] extension);\\n[DEL] return outputDirectory.resolve(filename, StandardResolveOptions.RESOLVE_FILE);\\n[ADD] String filename =\\n[ADD] String.format(\\n[ADD] \"%s-%s-of-%s%s\",\\n[ADD] filenamePrefixForWindow(window),\\n[ADD] context.getShardNumber(),\\n[ADD] context.getNumShards(),\\n[ADD] fileMetadataProvider.getSuggestedFilenameSuffix());\\n[ADD] return baseFilename\\n[ADD] .getCurrentDirectory()\\n[ADD] .resolve(filename, StandardResolveOptions.RESOLVE_FILE);\\n     }\\n \\n     @Override\\n     public ResourceId unwindowedFilename(\\n[DEL] ResourceId outputDirectory, Context context, String extension) {\\n[ADD] Context context, FileMetadataProvider fileMetadataProvider) {\\n       throw new UnsupportedOperationException(\"Unsupported.\");\\n     }\\n   }',\n",
              " '    * Get the completed (commit + compaction) view of the file system for this table\\n    */\\n   public TableFileSystemView getCompletedFileSystemView() {\\n[DEL] return new HoodieTableFileSystemView(metaClient, getCommitsTimeline());\\n[ADD] return new HoodieTableFileSystemView(metaClient, metaClient.getCommitsTimeline());\\n   }\\n \\n   /**\\n    * Get only the completed (no-inflights) commit timeline\\n    */\\n   public HoodieTimeline getCompletedCommitTimeline() {\\n[DEL] return getCommitsTimeline().filterCompletedInstants();\\n[ADD] return metaClient.getCommitsTimeline().filterCompletedInstants();\\n   }\\n \\n   /**\\n    * Get only the inflights (no-completed) commit timeline\\n    */\\n   public HoodieTimeline getInflightCommitTimeline() {\\n[DEL] return getCommitsTimeline().filterInflights();\\n[ADD] return metaClient.getCommitsTimeline().filterInflights();\\n   }\\n \\n   /**',\n",
              " \"   /** Class of the original {@link DoFn} from which this signature was produced. */\\n   public abstract Class<? extends DoFn<?, ?>> fnClass();\\n \\n[ADD] /** Whether this {@link DoFn} does a bounded amount of work per element. */\\n[ADD] public abstract PCollection.IsBounded isBounded();\\n[ADD] \\n   /** Details about this {@link DoFn}'s {@link DoFn.ProcessElement} method. */\\n   public abstract ProcessElementMethod processElement();\\n \",\n",
              " ' \\n             if (useReplyToDestination && replyTo != null)\\n             {\\n[ADD] final int timeout = endpoint.getResponseTimeout();\\n[ADD] \\n                 consumer = createReplyToConsumer(msg, event, session, replyTo, topic);\\n \\n                 if (topic)',\n",
              " '         new AMRMClient.ContainerRequest(capability, preferredNodes, null, priority));\\n   }\\n \\n[DEL] private ContainerLaunchContext newContainerLaunchContext(Container container, String helixInstanceName)\\n[ADD] protected ContainerLaunchContext newContainerLaunchContext(Container container, String helixInstanceName)\\n       throws IOException {\\n     Path appWorkDir = GobblinClusterUtils.getAppWorkDirPath(this.fs, this.applicationName, this.applicationId);\\n     Path containerWorkDir = new Path(appWorkDir, GobblinYarnConfigurationKeys.CONTAINER_WORK_DIR_NAME);',\n",
              " '     return fileGroupIdBootstrapBaseFileMap;\\n   }\\n \\n[ADD] protected Map<HoodieFileGroupId, HoodieInstant> createFileIdToReplaceInstantMap(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups) {\\n[ADD] Map<HoodieFileGroupId, HoodieInstant> replacedFileGroupsMap = new ConcurrentHashMap<>(replacedFileGroups);\\n[ADD] return replacedFileGroupsMap;\\n[ADD] }\\n[ADD] \\n   /**\\n    * Create a file system view, as of the given timeline, with the provided file statuses.\\n    */',\n",
              " '     }\\n   };\\n \\n[ADD] private static final PathFilter PROPERTIES_PATH_FILTER = new PathFilter() {\\n[ADD] @Override\\n[ADD] public boolean accept(Path path) {\\n[ADD] String fileExtension = path.getName().substring(path.getName().lastIndexOf(\\'.\\') + 1);\\n[ADD] return fileExtension.equalsIgnoreCase(JOB_PROPS_FILE_EXTENSION);\\n[ADD] }\\n[ADD] };\\n[ADD] \\n[ADD] private static final PathFilter NON_PROPERTIES_PATH_FILTER = new PathFilter() {\\n[ADD] @Override\\n[ADD] public boolean accept(Path path) {\\n[ADD] return !PROPERTIES_PATH_FILTER.accept(path);\\n[ADD] }\\n[ADD] };\\n[ADD] \\n   /**\\n[DEL] * Load job configurations from job configuration files stored under the\\n[DEL] * root job configuration file directory.\\n[DEL] *\\n[ADD] * Load job configuration from job configuration files stored in general file system,\\n[ADD] * located by Path\\n    * @param properties Gobblin framework configuration properties\\n    * @return a list of job configurations in the form of {@link java.util.Properties}\\n    */\\n[DEL] public static List<Properties> loadJobConfigs(Properties properties)\\n[DEL] throws ConfigurationException {\\n[DEL] Preconditions.checkArgument(properties.containsKey(ConfigurationKeys.JOB_CONFIG_FILE_DIR_KEY),\\n[DEL] \"Missing configuration property: \" + ConfigurationKeys.JOB_CONFIG_FILE_DIR_KEY);\\n[DEL] \\n[ADD] public static List<Properties> loadGenericJobConfigs(Properties properties)\\n[ADD] throws ConfigurationException, IOException {\\n     List<Properties> jobConfigs = Lists.newArrayList();\\n[DEL] loadJobConfigsRecursive(jobConfigs, properties, getJobConfigurationFileExtensions(properties),\\n[DEL] new File(properties.getProperty(ConfigurationKeys.JOB_CONFIG_FILE_DIR_KEY)));\\n[ADD] loadGenericJobConfigsRecursive(jobConfigs, properties, getJobConfigurationFileExtensions(properties),\\n[ADD] new Path(properties.getProperty(ConfigurationKeys.JOB_CONFIG_FILE_GENERAL_PATH_KEY)));\\n     return jobConfigs;\\n   }\\n ',\n",
              " '       public void setReplaceMode(boolean value);\\n       HasClickHandlers getReplaceAllButton();\\n       String getReplaceText();\\n[DEL] boolean useGitIgnore();\\n \\n       HasClickHandlers getStopReplaceButton();\\n       void setStopReplaceButtonVisible(boolean visible);',\n",
              " '         // we can save space by removing the log of deletions\\n         mDb.execute(\"delete from graves\");\\n         mUsn += 1;\\n[DEL] mModels.beforeUpload();\\n[ADD] getModels().beforeUpload();\\n         mTags.beforeUpload();\\n         mDecks.beforeUpload();\\n         modSchemaNoCheck();',\n",
              " '     public static void onEnter(\\n         @Advice.Argument(0) String method,\\n         @Advice.Argument(1) String endpoint,\\n[DEL] @Advice.Local(\"otelSpan\") Span span,\\n[ADD] @Advice.Local(\"otelContext\") Context context,\\n         @Advice.Local(\"otelScope\") Scope scope,\\n         @Advice.Argument(value = 5, readOnly = false) ResponseListener responseListener) {\\n \\n[DEL] span = tracer().startSpan(null, method + \" \" + endpoint);\\n[DEL] scope = tracer().startScope(span);\\n[ADD] context = tracer().startSpan(currentContext(), null, method + \" \" + endpoint);\\n[ADD] scope = context.makeCurrent();\\n \\n[DEL] responseListener = new RestResponseListener(responseListener, span);\\n[ADD] responseListener = new RestResponseListener(responseListener, context);\\n     }\\n \\n     @Advice.OnMethodExit(onThrowable = Throwable.class, suppress = Throwable.class)\\n     public static void stopSpan(\\n         @Advice.Thrown Throwable throwable,\\n[DEL] @Advice.Local(\"otelSpan\") Span span,\\n[ADD] @Advice.Local(\"otelContext\") Context context,\\n         @Advice.Local(\"otelScope\") Scope scope) {\\n       scope.close();\\n       if (throwable != null) {\\n[DEL] tracer().endExceptionally(span, throwable);\\n[ADD] tracer().endExceptionally(context, throwable);\\n       }\\n     }\\n   }',\n",
              " '                     }\\n                 } else {\\n                     try {\\n[DEL] Element remoteFragment = resolveXLink(hrefUri, idSearch, srvContext);\\n[ADD] Element remoteFragment = resolveXLink(hrefUri, idSearch);\\n \\n                         // Not resolved in cache or using href\\n                         if (remoteFragment == null)\\n                             return hrefUri;\\n \\n[DEL] searchXLink(remoteFragment, action, srvContext);\\n[ADD] searchXLink(remoteFragment, action);\\n \\n                         if (show.equalsIgnoreCase(XLink.SHOW_REPLACE)) {\\n                             // replace this element with the fragment',\n",
              " ' import java.io.IOException;\\n import java.io.RandomAccessFile;\\n import java.util.ArrayList;\\n[ADD] import java.util.Arrays;\\n[ADD] import java.util.HashMap;\\n import java.util.HashSet;\\n import java.util.Iterator;\\n import java.util.List;\\n import java.util.Set;\\n[ADD] import java.util.stream.Collectors;\\n \\n /**\\n  * Utility methods to aid testing inside the HoodieClient module.',\n",
              " ' \\n   }\\n \\n[DEL] private void parseConnectionProvider(Builder definitionBuilder) {\\n[ADD] private void parseConnectionProvider(ComponentBuildingDefinition.Builder definitionBuilder) {\\n     if (!getConnectedComponents(extensionModel, configurationModel).isEmpty()) {\\n       definitionBuilder.withSetterParameterDefinition(\"requiresConnection\", fromFixedValue(true).build());\\n       definitionBuilder.withSetterParameterDefinition(\"connectionProviderResolver\",',\n",
              " '   /**\\n    * Build Hoodie write client.\\n    *\\n[DEL] * @param jsc Java Spark Context\\n[DEL] * @param basePath Base Path\\n[DEL] * @param schemaStr Schema\\n[ADD] * @param jsc         Java Spark Context\\n[ADD] * @param basePath    Base Path\\n[ADD] * @param schemaStr   Schema\\n    * @param parallelism Parallelism\\n    */\\n   public static HoodieWriteClient createHoodieClient(JavaSparkContext jsc, String basePath, String schemaStr,\\n[DEL] int parallelism, Option<String> compactionStrategyClass, TypedProperties properties) {\\n[ADD] int parallelism, Option<String> compactionStrategyClass, TypedProperties properties) {\\n     HoodieCompactionConfig compactionConfig = compactionStrategyClass\\n         .map(strategy -> HoodieCompactionConfig.newBuilder().withInlineCompaction(false)\\n             .withCompactionStrategy(ReflectionUtils.loadClass(strategy)).build())',\n",
              " '                         sourceInput.processRawMessage(rawMessage);\\n                         channel.basicAck(deliveryTag, false);\\n                     } catch (Exception e) {\\n[DEL] LOG.error(\"Error while trying to process AMQP message, requeuing message\", e);\\n[ADD] LOG.error(\"Error while trying to process AMQP message\", e);\\n                         if (channel.isOpen()) {\\n[DEL] channel.basicNack(deliveryTag, false, true);\\n[ADD] channel.basicNack(deliveryTag, false, requeueInvalid);\\n[ADD] \\n[ADD] if (LOG.isDebugEnabled()) {\\n[ADD] if (requeueInvalid) {\\n[ADD] LOG.debug(\"Re-queue message with delivery tag {}\", deliveryTag);\\n[ADD] } else {\\n[ADD] LOG.debug(\"Message with delivery tag {} not re-queued\", deliveryTag);\\n[ADD] }\\n[ADD] }\\n                         }\\n                     }\\n                 }',\n",
              " '          }});\\n    }\\n \\n[ADD] private void autoSave(Command onCompleted, Command onSilentFailure)\\n[ADD] {\\n[ADD] saveThenExecute(null, false, CommandUtil.join(postSaveCommand(), onCompleted), onSilentFailure);\\n[ADD] }\\n[ADD] \\n    public void save(Command onCompleted)\\n    {\\n[DEL] saveThenExecute(null, CommandUtil.join(postSaveCommand(),\\n[DEL] onCompleted));\\n[ADD] saveThenExecute(null, true, CommandUtil.join(postSaveCommand(), onCompleted));\\n    }\\n \\n    public void saveWithPrompt(final Command command, final Command onCancelled)',\n",
              " '   }\\n \\n   private Path constructSourcePath(DateTime date) {\\n[DEL] return new Path(this.sourceDir, this.sourcePartitionPrefix + Path.SEPARATOR\\n[DEL] + this.partitionPatternFormatter.print(date) + Path.SEPARATOR + this.sourcePartitionSuffix);\\n[ADD] StringBuilder pathBuilder = new StringBuilder();\\n[ADD] \\n[ADD] if (!this.sourcePartitionPrefix.isEmpty()) {\\n[ADD] pathBuilder.append(this.sourcePartitionPrefix);\\n[ADD] pathBuilder.append(Path.SEPARATOR);\\n[ADD] }\\n[ADD] \\n[ADD] pathBuilder.append(this.partitionPatternFormatter.print(date));\\n[ADD] \\n[ADD] if (!this.sourcePartitionSuffix.isEmpty()) {\\n[ADD] pathBuilder.append(Path.SEPARATOR);\\n[ADD] pathBuilder.append(this.sourcePartitionSuffix);\\n[ADD] }\\n[ADD] \\n[ADD] return new Path(this.sourceDir, pathBuilder.toString());\\n   }\\n[DEL] \\n[ADD] \\n   /**\\n[DEL] * Gets the LWM for this job runs. The new LWM is the HWM of the previous run + 1 unit (day,hour,minute..etc).\\n[ADD] * Gets the LWM for this job runs. The new LWM is the HWM of the previous run + 1 unit (day,hour,minute..etc).\\n    * If there was no previous execution then it is set to the given lowWaterMark + 1 unit.\\n    */\\n   private long getLowWaterMark(Iterable<WorkUnitState> previousStates, String lowWaterMark) {',\n",
              " '   }\\n \\n   @Override\\n[DEL] public D readRecord(D reuse) throws DataRecordException, IOException {\\n[ADD] public List<Tag<?>> generateTags(State state) {\\n[ADD] List<Tag<?>> tags = super.generateTags(state);\\n[ADD] tags.add(new Tag<String>(\"topic\", state.getProp(KafkaSource.TOPIC_NAME)));\\n[ADD] tags.add(new Tag<Integer>(\"partition\", state.getPropAsInt(KafkaSource.PARTITION_ID)));\\n[ADD] return tags;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public D readRecordImpl(D reuse) throws DataRecordException, IOException {\\n     if (this.nextWatermark >= this.highWatermark) {\\n       return null;\\n     }',\n",
              " ' import org.mule.tck.testmodels.mule.TestMessageDispatcher;\\n import org.mule.tck.testmodels.mule.TestMessageDispatcherFactory;\\n \\n[ADD] import org.junit.Ignore;\\n import org.junit.Test;\\n \\n /**',\n",
              " '     ClassTree enumTree = (ClassTree) tree;\\n     for (Tree member : enumTree.members()) {\\n       if (member.is(Tree.Kind.VARIABLE)) {\\n[DEL] ModifiersTree modifiers = ((VariableTree) member).modifiers();\\n[ADD] VariableTree variableTree = (VariableTree) member;\\n[ADD] ModifiersTree modifiers = variableTree.modifiers();\\n         ModifierKeywordTree publicModifier = ModifiersUtils.getModifier(modifiers, Modifier.PUBLIC);\\n[DEL] if (publicModifier != null && !ModifiersUtils.hasModifier(modifiers, Modifier.STATIC)\\n[DEL] // FIXME SONARJAVA-1604 final mutable field should raise issues\\n[DEL] && !ModifiersUtils.hasModifier(modifiers, Modifier.FINAL)) {\\n[ADD] if (publicModifier != null && (isNotStaticOrFinal(variableTree,variableTree.modifiers())|| isMutableFinalMember(variableTree))) {\\n           reportIssue(publicModifier, \"Lower the visibility of this field.\");\\n         }\\n       } else if (member.is(Tree.Kind.METHOD)) {',\n",
              " ' \\n import com.fasterxml.jackson.databind.ObjectMapper;\\n import com.google.api.services.healthcare.v1beta1.model.Message;\\n[ADD] import com.google.api.services.healthcare.v1beta1.model.ParsedData;\\n import com.google.api.services.healthcare.v1beta1.model.SchematizedData;\\n import java.io.IOException;\\n import java.util.Map;\\n[ADD] import java.util.Objects;\\n import javax.annotation.Nullable;\\n \\n /** The type HL7v2 message to wrap the {@link Message} model. */\\n public class HL7v2Message {\\n[DEL] private final String name;\\n[DEL] private final String messageType;\\n[DEL] private final String sendTime;\\n[DEL] private final String createTime;\\n[DEL] private final String data;\\n[DEL] private final String sendFacility;\\n[DEL] private String schematizedData;\\n[DEL] private final Map<String, String> labels;\\n[ADD] private String name;\\n[ADD] private String messageType;\\n[ADD] private String sendTime;\\n[ADD] private String createTime;\\n[ADD] private String data;\\n[ADD] private String sendFacility;\\n[ADD] @Nullable private ParsedData parsedData;\\n[ADD] @Nullable private String schematizedData;\\n[ADD] @Nullable private Map<String, String> labels;\\n \\n   @Override\\n   public String toString() {',\n",
              " '           types.put(typeInformation.getName(), typeInformation);\\n         }\\n       }\\n[DEL] // Return the list ordered by the schema fields.\\n[DEL] return schema\\n[DEL] .getFields()\\n[DEL] .stream()\\n[DEL] .map(f -> types.get(f.getName()))\\n[DEL] .collect(Collectors.toList());\\n[ADD] return Lists.newArrayList(types.values());\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public List<FieldValueTypeInformation> get(Class<?> clazz, Schema schema) {\\n[ADD] return StaticSchemaInference.sortBySchema(get(clazz), schema);\\n     }\\n   }\\n ',\n",
              " ' import org.graylog2.indexer.searches.Searches;\\n import org.graylog2.indexer.searches.timeranges.TimeRange;\\n \\n[DEL] import javax.annotation.Nullable;\\n import java.util.Map;\\n \\n import static com.google.common.base.Strings.isNullOrEmpty;\\n \\n[DEL] public class SearchResultChartWidget extends DashboardWidget {\\n[ADD] public class SearchResultChartWidget extends ChartWidget {\\n \\n     private final String query;\\n     private final TimeRange timeRange;\\n[DEL] private final Searches.DateHistogramInterval interval;\\n[DEL] @Nullable\\n[DEL] private final String streamId;\\n[ADD] \\n     private final Searches searches;\\n \\n     public SearchResultChartWidget(MetricRegistry metricRegistry, Searches searches, String id, String description, WidgetCacheTime cacheTime, Map<String, Object> config, String query, TimeRange timeRange, String creatorUserId) {',\n",
              " '             }\\n         });\\n     }\\n[ADD] \\n[ADD] private static class TestFutureCallback implements FutureCallback<HttpResponse>\\n[ADD] {\\n[ADD] \\n[ADD] private int statusCode = -1;\\n[ADD] private String reasonPhrase;\\n[ADD] \\n[ADD] @Override\\n[ADD] public void cancelled()\\n[ADD] {\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public void completed(HttpResponse response)\\n[ADD] {\\n[ADD] this.statusCode = response.getStatusLine().getStatusCode();\\n[ADD] this.reasonPhrase = response.getStatusLine().getReasonPhrase();\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public void failed(Exception ex)\\n[ADD] {\\n[ADD] }\\n[ADD] \\n[ADD] public int getStatusCode()\\n[ADD] {\\n[ADD] return statusCode;\\n[ADD] }\\n[ADD] \\n[ADD] public String getReasonPhrase()\\n[ADD] {\\n[ADD] return reasonPhrase;\\n[ADD] }\\n[ADD] }\\n }',\n",
              " '  */\\n package org.apache.cloudstack.agent.directdownload;\\n \\n[DEL] import org.apache.cloudstack.storage.to.PrimaryDataStoreTO;\\n[DEL] \\n import java.util.Map;\\n \\n[ADD] import org.apache.cloudstack.storage.to.PrimaryDataStoreTO;\\n[ADD] \\n public class HttpDirectDownloadCommand extends DirectDownloadCommand {\\n \\n[DEL] public HttpDirectDownloadCommand(String url, Long templateId, PrimaryDataStoreTO destPool, String checksum, Map<String, String> headers) {\\n[ADD] public HttpDirectDownloadCommand(String url, Long templateId, PrimaryDataStoreTO destPool, String checksum, Map<String, String> headers, int connectTimeout, int soTimeout) {\\n         super(url, templateId, destPool, checksum, headers);\\n[ADD] setConnectTimeout(connectTimeout);\\n[ADD] setSoTimeout(soTimeout);\\n     }\\n \\n }',\n",
              " '   private static class IdentityConverter extends Converter<Object, Object, Object, Object> {\\n \\n     @Override\\n[DEL] public Object convertSchema(Object inputSchema, WorkUnitState workUnit)\\n[DEL] throws SchemaConversionException {\\n[ADD] public Object convertSchema(Object inputSchema, WorkUnitState workUnit) throws SchemaConversionException {\\n       return inputSchema;\\n     }\\n ',\n",
              " '                 RequestsFunctionNamePrompt(this, e);\\n         }\\n \\n[ADD] \\n[ADD] public event Action<DesignOptionNamePromptEventArgs> RequestDesignOptionNamePrompt;\\n[ADD] public void OnRequestsDesignOptionNamePrompt(DesignOptionNamePromptEventArgs e)\\n[ADD] {\\n[ADD] if (RequestDesignOptionNamePrompt != null)\\n[ADD] RequestDesignOptionNamePrompt(e);\\n[ADD] }\\n         public event WorkspaceHandler WorkspaceSaved;\\n         internal void OnWorkspaceSaved(WorkspaceModel model)\\n         {',\n",
              " ' \\n     private final class KeyIterator implements Iterator<K>\\n     {\\n[DEL] private int current;\\n[DEL] private K[] keyArray;\\n[ADD] private int current = -1;\\n[ADD] private int lastRemovalIndex = current;\\n[ADD] private List<K> keys;\\n \\n         @SuppressWarnings(\"unchecked\")\\n         public KeyIterator()\\n         {\\n[DEL] keyArray = (K[]) core.keySet().toArray();\\n[ADD] keys = new LinkedList<K>(core.keySet());\\n         }\\n \\n         public boolean hasNext()\\n         {\\n[DEL] return current < keyArray.length;\\n[ADD] return current < keys.size() -1;\\n         }\\n \\n         public K next()\\n         {\\n[DEL] return keyArray[current++];\\n[ADD] try\\n[ADD] {\\n[ADD] return keys.get(++current);\\n[ADD] }\\n[ADD] catch (IndexOutOfBoundsException e)\\n[ADD] {\\n[ADD] throw new NoSuchElementException();\\n[ADD] }\\n         }\\n \\n         public void remove()\\n         {\\n[DEL] CopyOnWriteCaseInsensitiveMap.this.remove(keyArray[current]);\\n[ADD] if (current == -1) {\\n[ADD] throw new IllegalStateException(\"Cannot remove element before first invoking next()\");\\n[ADD] }\\n[ADD] \\n[ADD] if (current == lastRemovalIndex) {\\n[ADD] throw new IllegalStateException(\"Remove can only be called once per call to next()\");\\n[ADD] }\\n[ADD] \\n[ADD] CopyOnWriteCaseInsensitiveMap.this.remove(keys.get(current));\\n[ADD] keys.remove(current);\\n[ADD] lastRemovalIndex = current;\\n         }\\n     }\\n ',\n",
              " '     }\\n \\n     @PUT\\n[DEL] @Path(\"tables\")\\n[ADD] @Path(\"tables/{idOrName}\")\\n     @AuditEvent(type = AuditEventTypes.LOOKUP_TABLE_UPDATE)\\n     @ApiOperation(value = \"Update the given lookup table\")\\n[DEL] public LookupTableApi updateTable(@Valid @ApiParam LookupTableApi toUpdate) {\\n[ADD] public LookupTableApi updateTable(@ApiParam(name = \"idOrName\") @PathParam(\"idOrName\") @NotEmpty String idOrName,\\n[ADD] @Valid @ApiParam LookupTableApi toUpdate) {\\n[ADD] checkLookupTableId(idOrName, toUpdate);\\n[ADD] checkPermission(RestPermissions.LOOKUP_TABLES_EDIT, toUpdate.id());\\n         LookupTableDto saved = dbTableService.save(toUpdate.toDto());\\n         clusterBus.post(LookupTablesUpdated.create(saved));\\n ',\n",
              " '     return getWorkUnitForTopicPartition(partition, offsets, topicSpecificState);\\n   }\\n \\n[DEL] \\n   private long getPreviousOffsetForPartition(KafkaPartition partition, SourceState state)\\n[DEL] throws PreviousOffsetNotFoundException {\\n[ADD] throws PreviousOffsetNotFoundException {\\n \\n     getAllPreviousOffsets(state);\\n ',\n",
              " '         } else {\\n             this.context = new ZookeeperWindowsContext();\\n         }\\n[ADD] \\n[ADD] // initialize the context\\n[ADD] this.context.setUnpackedDirectory(UNPACKED_DIRECTORY);\\n[ADD] this.context.setSourceFile(TARGET_FILE_PATH);\\n     }\\n \\n[ADD] private static final Logger logger = LoggerFactory.getLogger(ZookeeperRegistryCenter.class);\\n[ADD] \\n     /**\\n      * The OS type.\\n      */',\n",
              " ' import static org.mule.runtime.core.api.transaction.TransactionCoordination.getInstance;\\n import static org.mule.runtime.core.internal.processor.strategy.AbstractProcessingStrategyTestCase.Mode.FLOW;\\n import static org.mule.runtime.core.internal.processor.strategy.AbstractProcessingStrategyTestCase.Mode.SOURCE;\\n[ADD] import static org.mule.tck.junit4.matcher.Eventually.eventually;\\n[ADD] import static org.mule.tck.util.CollectableReference.collectedByGc;\\n import static org.mule.tck.util.MuleContextUtils.getNotificationDispatcher;\\n import static org.mule.test.allure.AllureConstants.ProcessingStrategiesFeature.PROCESSING_STRATEGIES;\\n import static org.mule.test.allure.AllureConstants.ProcessingStrategiesFeature.ProcessingStrategiesStory.DEFAULT;\\n import static reactor.util.concurrent.Queues.XS_BUFFER_SIZE;\\n \\n[ADD] import io.qameta.allure.Issue;\\n[ADD] import org.junit.Test;\\n[ADD] import org.mule.runtime.api.exception.MuleException;\\n[ADD] import org.mule.runtime.api.util.concurrent.Latch;\\n import org.mule.runtime.core.api.MuleContext;\\n[ADD] import org.mule.runtime.core.api.event.CoreEvent;\\n[ADD] import org.mule.runtime.core.api.processor.Processor;\\n import org.mule.runtime.core.api.processor.strategy.ProcessingStrategy;\\n import org.mule.runtime.core.internal.processor.strategy.AbstractProcessingStrategyTestCase.TransactionAwareProcessingStrategyTestCase;\\n import org.mule.runtime.core.internal.processor.strategy.ProactorStreamEmitterProcessingStrategyFactory.ProactorStreamEmitterProcessingStrategy;\\n import org.mule.tck.testmodels.mule.TestTransaction;\\n[ADD] import org.mule.tck.util.CollectableReference;\\n \\n import java.util.Collection;\\n ',\n",
              " '     // ParquetInputFormat.setFilterPredicate(job, predicate);\\n     // clearOutExistingPredicate(job);\\n     // }\\n[DEL] return super.getRecordReader(split, job, reporter);\\n[ADD] \\n[ADD] final Path finalPath = ((FileSplit) split).getPath();\\n[ADD] FileSystem fileSystem = finalPath.getFileSystem(conf);\\n[ADD] FileStatus curFileStatus = fileSystem.getFileStatus(finalPath);\\n[ADD] \\n[ADD] HoodieTableMetaClient metadata;\\n[ADD] try {\\n[ADD] metadata = getTableMetaClient(finalPath.getFileSystem(conf),\\n[ADD] curFileStatus.getPath().getParent());\\n[ADD] } catch (DatasetNotFoundException | InvalidDatasetException e) {\\n[ADD] LOG.info(\"Handling a non-hoodie path \" + curFileStatus.getPath());\\n[ADD] return super.getRecordReader(split, job, reporter);\\n[ADD] }\\n[ADD] \\n[ADD] if (LOG.isDebugEnabled()) {\\n[ADD] LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metadata);\\n[ADD] }\\n[ADD] String tableName = metadata.getTableConfig().getTableName();\\n[ADD] String mode = HoodieHiveUtil.readMode(Job.getInstance(job), tableName);\\n[ADD] \\n[ADD] if (HoodieHiveUtil.INCREMENTAL_SCAN_MODE.equals(mode)) {\\n[ADD] return super.getRecordReader(split, job, reporter);\\n[ADD] } else {\\n[ADD] List<String> partitions = FSUtils.getAllFoldersWithPartitionMetaFile(metadata.getFs(), metadata.getBasePath());\\n[ADD] String fileId = FSUtils.getFileId(finalPath.getName());\\n[ADD] List<FileStatus> fileStatuses = new ArrayList<>();\\n[ADD] \\n[ADD] for (String partition : partitions) {\\n[ADD] FileStatus[] fileStatus = metadata.getFs().listStatus(new Path(metadata.getBasePath() + \"/\" + partition));\\n[ADD] for (FileStatus fileStatu : fileStatus) {\\n[ADD] String fileName = fileStatu.getPath().getName();\\n[ADD] if (fileStatu.isFile() && fileName.endsWith(\".parquet\") && FSUtils.getFileId(fileName).equals(fileId)) {\\n[ADD] fileStatuses.add(fileStatu);\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] // add file to view\\n[ADD] HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metadata,\\n[ADD] metadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants(),\\n[ADD] fileStatuses.toArray(new FileStatus[fileStatuses.size()]));\\n[ADD] // get lastest files\\n[ADD] List<HoodieDataFile> latestFiles = fsView.getLatestDataFiles().collect(Collectors.toList());\\n[ADD] \\n[ADD] for (HoodieDataFile latestFile : latestFiles) {\\n[ADD] if (latestFile.getFileName().equals(finalPath.getName())) {\\n[ADD] return super.getRecordReader(split, job, reporter);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] return new NoneParquetRecordReaderWrapper();\\n[ADD] }\\n   }\\n \\n   /**',\n",
              " '           // Task was created and may have been registered, but not submitted, so call the\\n           // task state tracker task run completion directly since the task cancel does nothing if not submitted\\n           this.taskStateTracker.onTaskRunCompletion(task);\\n[ADD] areAllTaskSubmitted = false;\\n           log.error(\"Could not submit task for workunit {}\", workUnit, e);\\n         } else {\\n           // task was created and submitted, but failed later, so cancel the task to decrement the CountDownLatch',\n",
              " ' import static org.mule.runtime.core.api.util.FileUtils.deleteTree;\\n import static org.mule.runtime.core.api.util.FileUtils.newFile;\\n import static org.mule.tck.MuleTestUtils.APPLE_FLOW;\\n[DEL] import static org.mule.tck.MuleTestUtils.getTestFlow;\\n import static org.mule.tck.junit4.TestsLogConfigurationHelper.clearLoggingConfig;\\n import static org.mule.tck.junit4.TestsLogConfigurationHelper.configureLoggingForTest;\\n import static org.slf4j.LoggerFactory.getLogger;\\n[ADD] \\n import org.mule.runtime.api.exception.MuleException;\\n import org.mule.runtime.api.message.Message;\\n import org.mule.runtime.api.metadata.DataType;',\n",
              " ' \\n   @VisibleForTesting static final int GCS_UPLOAD_BUFFER_SIZE_BYTES_DEFAULT = 1024 * 1024;\\n \\n[DEL] @VisibleForTesting static final String PIPELINE_FILE_FORMAT = \"pipeline-%s.pb\";\\n[DEL] @VisibleForTesting static final String DATAFLOW_GRAPH_FILE_FORMAT = \"dataflow_graph-%s.json\";\\n[ADD] @VisibleForTesting static final String PIPELINE_FILE_NAME = \"pipeline.pb\";\\n[ADD] @VisibleForTesting static final String DATAFLOW_GRAPH_FILE_NAME = \"dataflow_graph.json\";\\n \\n   private static final ObjectMapper MAPPER = new ObjectMapper();\\n ',\n",
              " '   }\\n \\n   @Test\\n[DEL] @Ignore(\"[BEAM-436] DirectRunner tempLocation configuration insufficient\")\\n   public void testBatchWritePrimitiveDisplayData() throws IOException, InterruptedException {\\n     testWritePrimitiveDisplayData(/* streaming: */ false);\\n   }\\n \\n   @Test\\n[DEL] @Ignore(\"[BEAM-436] DirectRunner tempLocation configuration insufficient\")\\n   public void testStreamingWritePrimitiveDisplayData() throws IOException, InterruptedException {\\n     testWritePrimitiveDisplayData(/* streaming: */ true);\\n   }',\n",
              " '     }\\n #endif\\n \\n[DEL] #ifdef INTL_ICU\\n[DEL] static int CompareStringICU(_In_opt_count_(langtagLen) const char16 *langtag, _In_ charcount_t langtagLen, _In_z_ const char16 *left, _In_ charcount_t cchLeft, _In_z_ const char16 *right, _In_ charcount_t cchRight,\\n[DEL] _In_ CollatorSensitivity sensitivity, _In_ bool ignorePunctuation, _In_ bool numeric, _In_ CollatorCaseFirst caseFirst, _Out_ HRESULT *hr)\\n[ADD] Var IntlEngineInterfaceExtensionObject::EntryIntl_LocaleCompare(RecyclableObject* function, CallInfo callInfo, ...)\\n     {\\n[DEL] ASSERT_ENUM(CollatorSensitivity, sensitivity);\\n[DEL] ASSERT_ENUM(CollatorCaseFirst, caseFirst);\\n[DEL] Assert(left != nullptr && right != nullptr && hr != nullptr);\\n[DEL] int ret = 0;\\n[DEL] *hr = E_INVALIDARG;\\n[DEL] UErrorCode error = U_ZERO_ERROR;\\n[ADD] #ifdef INTL_WINGLOB\\n[ADD] AssertOrFailFastMsg(false, \"platform.localeCompare should not be called in Intl-WinGlob\");\\n[ADD] return nullptr;\\n[ADD] #else\\n[ADD] EngineInterfaceObject_CommonFunctionProlog(function, callInfo);\\n[ADD] INTL_CHECK_ARGS(args.Info.Count == 4 && JavascriptString::Is(args[1]) && JavascriptString::Is(args[2]) && DynamicObject::Is(args[3]));\\n \\n[DEL] char localeID[ULOC_FULLNAME_CAPACITY] = { 0 };\\n[DEL] int32_t length = 0;\\n[DEL] if (langtag == nullptr)\\n[ADD] JavascriptString *left = JavascriptString::UnsafeFromVar(args[1]);\\n[ADD] JavascriptString *right = JavascriptString::UnsafeFromVar(args[2]);\\n[ADD] DynamicObject *state = DynamicObject::UnsafeFromVar(args[3]);\\n[ADD] \\n[ADD] // Below, we lazy-initialize the backing UCollator on the first call to localeCompare\\n[ADD] // On subsequent calls, the UCollator will be cached in state.hiddenObject\\n[ADD] // TODO(jahorto): Make these property IDs sane, so that hiddenObject doesn\\'t have different meanings in different contexts\\n[ADD] Var hiddenObject = nullptr;\\n[ADD] FinalizableUCollator *coll = nullptr;\\n[ADD] UErrorCode status = U_ZERO_ERROR;\\n[ADD] if (state->GetInternalProperty(state, Js::InternalPropertyIds::HiddenObject, &hiddenObject, nullptr, scriptContext))\\n         {\\n[DEL] length = uloc_getName(nullptr, localeID, _countof(localeID), &error);\\n[ADD] coll = reinterpret_cast<FinalizableUCollator *>(hiddenObject);\\n[ADD] INTL_TRACE(\"Using previously cached UCollator (0x%x)\", coll);\\n         }\\n         else\\n         {\\n[DEL] char langtag8[ULOC_FULLNAME_CAPACITY] = { 0 };\\n[DEL] AssertOrFailFast(utf8::WideStringToNarrowNoAlloc(langtag, langtagLen, langtag8, _countof(langtag8)) == S_OK);\\n[DEL] uloc_forLanguageTag(langtag8, localeID, _countof(localeID), &length, &error);\\n[DEL] }\\n[DEL] ICU_ASSERT(error, length > 0);\\n[ADD] // the object key is locale for legacy compat, but its more accurately a BCP47 Language Tag, not an ICU LocaleID\\n[ADD] JavascriptString *langtag = AssertProperty<JavascriptString>(state, PropertyIds::locale);\\n[ADD] CollatorSensitivity sensitivity = static_cast<CollatorSensitivity>(AssertIntegerProperty(state, PropertyIds::sensitivityEnum));\\n[ADD] bool ignorePunctuation = AssertBooleanProperty(state, PropertyIds::ignorePunctuation);\\n[ADD] bool numeric = AssertBooleanProperty(state, PropertyIds::numeric);\\n[ADD] CollatorCaseFirst caseFirst = static_cast<CollatorCaseFirst>(AssertIntegerProperty(state, PropertyIds::caseFirstEnum));\\n \\n[DEL] ScopedUCollator collator(ucol_open(localeID, &error));\\n[DEL] ICU_ASSERT(error, true);\\n[ADD] ASSERT_ENUM(CollatorSensitivity, sensitivity);\\n[ADD] ASSERT_ENUM(CollatorCaseFirst, caseFirst);\\n \\n[DEL] if (sensitivity == CollatorSensitivity::Base)\\n[DEL] {\\n[DEL] ucol_setStrength(collator, UCOL_PRIMARY);\\n[DEL] }\\n[DEL] else if (sensitivity == CollatorSensitivity::Accent)\\n[DEL] {\\n[DEL] ucol_setStrength(collator, UCOL_SECONDARY);\\n[DEL] }\\n[DEL] else if (sensitivity == CollatorSensitivity::Case)\\n[DEL] {\\n[DEL] // see \"description\" for the caseLevel default option: http://userguide.icu-project.org/collation/customization\\n[DEL] ucol_setStrength(collator, UCOL_PRIMARY);\\n[DEL] ucol_setAttribute(collator, UCOL_CASE_LEVEL, UCOL_ON, &error);\\n[DEL] ICU_ASSERT(error, true);\\n[DEL] }\\n[DEL] else if (sensitivity == CollatorSensitivity::Variant)\\n[DEL] {\\n[DEL] ucol_setStrength(collator, UCOL_TERTIARY);\\n[DEL] }\\n[DEL] else\\n[DEL] {\\n[DEL] AssertOrFailFastMsg(false, \"sensitivity is not one of the CollatorSensitivity values\");\\n[DEL] }\\n[ADD] char localeID[ULOC_FULLNAME_CAPACITY] = { 0 };\\n[ADD] BCP47_TO_ICU(langtag->GetSz(), langtag->GetLength(), localeID, _countof(localeID));\\n \\n[DEL] if (ignorePunctuation)\\n[DEL] {\\n[DEL] // see http://userguide.icu-project.org/collation/customization/ignorepunct\\n[DEL] ucol_setAttribute(collator, UCOL_ALTERNATE_HANDLING, UCOL_SHIFTED, &error);\\n[DEL] ICU_ASSERT(error, true);\\n[DEL] }\\n[ADD] coll = FinalizableUCollator::New(scriptContext->GetRecycler(), ucol_open(localeID, &status));\\n[ADD] ICU_ASSERT(status, true);\\n \\n[DEL] if (numeric)\\n[DEL] {\\n[DEL] ucol_setAttribute(collator, UCOL_NUMERIC_COLLATION, UCOL_ON, &error);\\n[DEL] ICU_ASSERT(error, true);\\n[DEL] }\\n[ADD] // REVIEW(jahorto): Anything that requires a status will no-op if its in a failure state\\n[ADD] // Thus, we can ICU_ASSERT the status once after all of the properties are set for simplicity\\n[ADD] if (sensitivity == CollatorSensitivity::Base)\\n[ADD] {\\n[ADD] ucol_setStrength(*coll, UCOL_PRIMARY);\\n[ADD] }\\n[ADD] else if (sensitivity == CollatorSensitivity::Accent)\\n[ADD] {\\n[ADD] ucol_setStrength(*coll, UCOL_SECONDARY);\\n[ADD] }\\n[ADD] else if (sensitivity == CollatorSensitivity::Case)\\n[ADD] {\\n[ADD] // see \"description\" for the caseLevel default option: http://userguide.icu-project.org/collation/customization\\n[ADD] ucol_setStrength(*coll, UCOL_PRIMARY);\\n[ADD] ucol_setAttribute(*coll, UCOL_CASE_LEVEL, UCOL_ON, &status);\\n[ADD] }\\n[ADD] else if (sensitivity == CollatorSensitivity::Variant)\\n[ADD] {\\n[ADD] ucol_setStrength(*coll, UCOL_TERTIARY);\\n[ADD] }\\n \\n[DEL] if (caseFirst == CollatorCaseFirst::Upper)\\n[DEL] {\\n[DEL] ucol_setAttribute(collator, UCOL_CASE_FIRST, UCOL_UPPER_FIRST, &error);\\n[DEL] ICU_ASSERT(error, true);\\n[DEL] }\\n[DEL] else if (caseFirst == CollatorCaseFirst::Lower)\\n[DEL] {\\n[DEL] ucol_setAttribute(collator, UCOL_CASE_FIRST, UCOL_LOWER_FIRST, &error);\\n[DEL] ICU_ASSERT(error, true);\\n[DEL] }\\n[ADD] if (ignorePunctuation)\\n[ADD] {\\n[ADD] // see http://userguide.icu-project.org/collation/customization/ignorepunct\\n[ADD] ucol_setAttribute(*coll, UCOL_ALTERNATE_HANDLING, UCOL_SHIFTED, &status);\\n[ADD] }\\n \\n[DEL] *hr = S_OK;\\n[DEL] UCollationResult result = ucol_strcoll(collator, reinterpret_cast<const UChar *>(left), cchLeft, reinterpret_cast<const UChar *>(right), cchRight);\\n[DEL] if (result == UCOL_LESS)\\n[DEL] {\\n[DEL] ret = 1;\\n[DEL] }\\n[DEL] else if (result == UCOL_EQUAL)\\n[DEL] {\\n[DEL] ret = 2;\\n[DEL] }\\n[DEL] else if (result == UCOL_GREATER)\\n[DEL] {\\n[DEL] ret = 3;\\n[DEL] }\\n[DEL] else\\n[DEL] {\\n[DEL] *hr = E_FAIL;\\n[DEL] ret = 0;\\n[ADD] if (numeric)\\n[ADD] {\\n[ADD] ucol_setAttribute(*coll, UCOL_NUMERIC_COLLATION, UCOL_ON, &status);\\n[ADD] }\\n[ADD] \\n[ADD] if (caseFirst == CollatorCaseFirst::Upper)\\n[ADD] {\\n[ADD] ucol_setAttribute(*coll, UCOL_CASE_FIRST, UCOL_UPPER_FIRST, &status);\\n[ADD] }\\n[ADD] else if (caseFirst == CollatorCaseFirst::Lower)\\n[ADD] {\\n[ADD] ucol_setAttribute(*coll, UCOL_CASE_FIRST, UCOL_LOWER_FIRST, &status);\\n[ADD] }\\n[ADD] \\n[ADD] // Ensure that collator configuration was successfull\\n[ADD] ICU_ASSERT(status, true);\\n[ADD] \\n[ADD] INTL_TRACE(\\n[ADD] \"Caching UCollator (0x%x) with langtag = %s, sensitivity = %d, caseFirst = %b, ignorePunctuation = %b, and numeric = %b\",\\n[ADD] coll,\\n[ADD] langtag->GetSz(),\\n[ADD] sensitivity,\\n[ADD] caseFirst,\\n[ADD] ignorePunctuation,\\n[ADD] numeric\\n[ADD] );\\n[ADD] \\n[ADD] // cache coll for later use (so that the condition that brought us here returns true for future calls)\\n[ADD] state->SetInternalProperty(\\n[ADD] InternalPropertyIds::HiddenObject,\\n[ADD] coll,\\n[ADD] PropertyOperationFlags::PropertyOperation_None,\\n[ADD] nullptr\\n[ADD] );\\n         }\\n \\n[DEL] return ret;\\n[DEL] }\\n[ADD] static_assert(UCOL_LESS == -1 && UCOL_EQUAL == 0 && UCOL_GREATER == 1, \"ucol_strcoll should return values compatible with localeCompare\");\\n[ADD] return JavascriptNumber::ToVar(ucol_strcoll(\\n[ADD] *coll,\\n[ADD] reinterpret_cast<const UChar *>(left->GetSz()),\\n[ADD] left->GetLength(),\\n[ADD] reinterpret_cast<const UChar *>(right->GetSz()),\\n[ADD] right->GetLength()\\n[ADD] ), scriptContext);\\n #endif\\n[ADD] }\\n \\n     Var IntlEngineInterfaceExtensionObject::EntryIntl_CompareString(RecyclableObject* function, CallInfo callInfo, ...)\\n     {\\n[ADD] #ifdef INTL_ICU\\n[ADD] AssertOrFailFastMsg(false, \"EntryIntl_CompareString should not be called in Intl-ICU\");\\n[ADD] return nullptr;\\n[ADD] #else\\n         EngineInterfaceObject_CommonFunctionProlog(function, callInfo);\\n         INTL_CHECK_ARGS(args.Info.Count >= 3 && JavascriptString::Is(args[1]) && JavascriptString::Is(args[2]));\\n \\n[DEL] #ifdef INTL_WINGLOB\\n         const char16 *locale = nullptr; // args[3]\\n         char16 defaultLocale[LOCALE_NAME_MAX_LENGTH] = { 0 };\\n[DEL] #else\\n[DEL] const char16 *langtag = nullptr;\\n[DEL] charcount_t langtagLen = 0;\\n[DEL] #endif\\n[ADD] \\n         CollatorSensitivity sensitivity = CollatorSensitivity::Default; // args[4]\\n         bool ignorePunctuation = false; // args[5]\\n         bool numeric = false; // args[6]',\n",
              " ' import org.fao.geonet.constants.Geonet;\\n import org.fao.geonet.constants.Geonet.Namespaces;\\n import org.fao.geonet.constants.Params;\\n[DEL] import org.fao.geonet.domain.*;\\n[ADD] import org.fao.geonet.domain.Constants;\\n[ADD] import org.fao.geonet.domain.Group;\\n[ADD] import org.fao.geonet.domain.ISODate;\\n[ADD] import org.fao.geonet.domain.InspireAtomFeed;\\n[ADD] import org.fao.geonet.domain.Metadata;\\n[ADD] import org.fao.geonet.domain.MetadataCategory;\\n[ADD] import org.fao.geonet.domain.MetadataDataInfo;\\n[ADD] import org.fao.geonet.domain.MetadataDataInfo_;\\n[ADD] import org.fao.geonet.domain.MetadataFileUpload;\\n[ADD] import org.fao.geonet.domain.MetadataFileUpload_;\\n[ADD] import org.fao.geonet.domain.MetadataHarvestInfo;\\n[ADD] import org.fao.geonet.domain.MetadataRatingByIp;\\n[ADD] import org.fao.geonet.domain.MetadataRatingByIpId;\\n[ADD] import org.fao.geonet.domain.MetadataSourceInfo;\\n[ADD] import org.fao.geonet.domain.MetadataStatus;\\n[ADD] import org.fao.geonet.domain.MetadataStatusId;\\n[ADD] import org.fao.geonet.domain.MetadataStatusId_;\\n[ADD] import org.fao.geonet.domain.MetadataStatus_;\\n[ADD] import org.fao.geonet.domain.MetadataType;\\n[ADD] import org.fao.geonet.domain.MetadataValidation;\\n[ADD] import org.fao.geonet.domain.MetadataValidationId;\\n[ADD] import org.fao.geonet.domain.MetadataValidationStatus;\\n[ADD] import org.fao.geonet.domain.Metadata_;\\n[ADD] import org.fao.geonet.domain.OperationAllowed;\\n[ADD] import org.fao.geonet.domain.OperationAllowedId;\\n[ADD] import org.fao.geonet.domain.OperationAllowedId_;\\n[ADD] import org.fao.geonet.domain.Pair;\\n[ADD] import org.fao.geonet.domain.Profile;\\n[ADD] import org.fao.geonet.domain.ReservedGroup;\\n[ADD] import org.fao.geonet.domain.ReservedOperation;\\n[ADD] import org.fao.geonet.domain.SchematronRequirement;\\n[ADD] import org.fao.geonet.domain.User;\\n[ADD] import org.fao.geonet.domain.UserGroup;\\n[ADD] import org.fao.geonet.domain.UserGroupId;\\n[ADD] import org.fao.geonet.domain.userfeedback.UserFeedback;\\n import org.fao.geonet.events.md.MetadataIndexCompleted;\\n import org.fao.geonet.exceptions.JeevesException;\\n import org.fao.geonet.exceptions.NoSchemaMatchesException;',\n",
              " ' \\n   protected static Latch undeployLatch = new Latch();\\n \\n[ADD] private boolean useMockedListeners = true;\\n \\n   @BeforeClass\\n   public static void beforeClass() throws URISyntaxException, IllegalAccessException {',\n",
              " '     metaClient = HoodieTableMetaClient.reload(metaClient);\\n     final HoodieCopyOnWriteTable newTable = new HoodieCopyOnWriteTable(config, jsc);\\n     List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\\n[DEL] return newTable.handleUpdate(newCommitTime, updatedRecord1.getCurrentLocation().getFileId(),\\n[DEL] updatedRecords.iterator());\\n[ADD] return newTable.handleUpdate(newCommitTime, updatedRecord1.getPartitionPath(),\\n[ADD] updatedRecord1.getCurrentLocation().getFileId(), updatedRecords.iterator());\\n     }).flatMap(x -> HoodieClientTestUtils.collectStatuses(x).iterator()).collect();\\n \\n     // Check the updated file',\n",
              " ' \\n         #region Properties\\n \\n[ADD] /// <summary>\\n[ADD] /// Int value indicates where statement started.\\n[ADD] /// E.g. a+5 StartLine will be 1.\\n[ADD] /// </summary>\\n         public int StartLine { get; private set; }\\n[ADD] \\n[ADD] /// <summary>\\n[ADD] /// Int value indicates where statement ended.\\n[ADD] /// E.g.\\n[ADD] /// a+5\\n[ADD] /// +6+3;\\n[ADD] /// Endline will be 2.\\n[ADD] /// </summary>\\n         public int EndLine { get; private set; }\\n \\n         public Variable FirstDefinedVariable',\n",
              " ' \\t\\t\\t\"required capabilities must be empty when all capabilities are allowed by a wildcard\"))\\n \\t}\\n \\n[ADD] if len(scc.Volumes) > 1 {\\n[ADD] hasNone := false\\n[ADD] for _, fsType := range scc.Volumes {\\n[ADD] if fsType == api.FSTypeNone {\\n[ADD] hasNone = true\\n[ADD] break\\n[ADD] }\\n[ADD] }\\n[ADD] if hasNone {\\n[ADD] allErrs = append(allErrs, field.Invalid(field.NewPath(\"volumes\"), scc.Volumes,\\n[ADD] \"if \\'none\\' is specified, no other values are allowed\"))\\n[ADD] }\\n[ADD] }\\n[ADD] \\n \\treturn allErrs\\n }\\n ',\n",
              " '     UtilitiesTestBase.Helpers.savePropsToDFS(invalidProps, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_INVALID);\\n \\n     TypedProperties props1 = new TypedProperties();\\n[DEL] populateCommonProps(props1);\\n[ADD] populateCommonProps(props1, true, true);\\n     UtilitiesTestBase.Helpers.savePropsToDFS(props1, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_SOURCE1);\\n \\n     TypedProperties properties = new TypedProperties();',\n",
              " '                                     String compactionCommitTime) {\\n     this.context.setJobStatus(this.getClass().getSimpleName(), \"Collect compaction write status and commit compaction\");\\n     List<HoodieWriteStat> writeStats = writeStatuses.map(WriteStatus::getStat).collect();\\n[DEL] writeTableMetadata(table, metadata, new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, compactionCommitTime));\\n[ADD] writeTableMetadata(table, metadata, new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, compactionCommitTime),\\n[ADD] true);\\n     // commit to data table after committing to metadata table.\\n     finalizeWrite(table, compactionCommitTime, writeStats);\\n     LOG.info(\"Committing Compaction \" + compactionCommitTime + \". Finished with result \" + metadata);',\n",
              " ' \\t\\t\\tconnection.registerListener(listener);\\n \\t\\t}\\n \\t\\tif (listener != null || this.isSingleUse()) {\\n[DEL] if (this.getSoTimeout() <= 0) {\\n[ADD] if (this.getSoTimeout() < 0) {\\n \\t\\t\\t\\ttry {\\n[ADD] logger.warn(\"Default so-timeout may go to infinity in a future release, currently 10 seconds\");\\n \\t\\t\\t\\t\\tsocket.setSoTimeout(DEFAULT_REPLY_TIMEOUT);\\n \\t\\t\\t\\t} catch (SocketException e) {\\n \\t\\t\\t\\t\\tlogger.error(\"Error setting default reply timeout\", e);',\n",
              " '     return pnode->nop == knopName && pnode->sxPid.PropertyIdFromNameNode() == Js::PropertyIds::Array;\\n }\\n \\n[ADD] #define STARTSTATEMENET_IFTOPLEVEL(isTopLevel, pnode) \\\\\\n[ADD] if ((isTopLevel)) \\\\\\n[ADD] { \\\\\\n[ADD] byteCodeGenerator->StartStatement(pnode); \\\\\\n[ADD] }\\n[ADD] \\n[ADD] #define ENDSTATEMENET_IFTOPLEVEL(isTopLevel, pnode) \\\\\\n[ADD] if ((isTopLevel)) \\\\\\n[ADD] { \\\\\\n[ADD] byteCodeGenerator->EndStatement(pnode); \\\\\\n[ADD] }\\n[ADD] \\n BOOL MayHaveSideEffectOnNode(ParseNode *pnode, ParseNode *pnodeSE)\\n {\\n     // Try to determine whether pnodeSE may kill the named var represented by pnode.',\n",
              " ' import java.util.stream.Collectors;\\n import java.util.stream.Stream;\\n \\n[ADD] import scala.Tuple3;\\n[ADD] \\n import static org.apache.hudi.common.testutils.HoodieTestTable.makeIncrementalCommitTimes;\\n import static org.apache.hudi.common.testutils.HoodieTestTable.makeNewCommitTime;\\n import static org.apache.hudi.common.testutils.HoodieTestUtils.DEFAULT_PARTITION_PATHS;',\n",
              " '   // Enable the internal Metadata Table which saves file listings\\n   public static final ConfigProperty<Boolean> ENABLE = ConfigProperty\\n       .key(METADATA_PREFIX + \".enable\")\\n[DEL] .defaultValue(true)\\n[ADD] .defaultValue(false)\\n       .sinceVersion(\"0.7.0\")\\n       .withDocumentation(\"Enable the internal metadata table which serves table metadata like level file listings\");\\n ',\n",
              " '         };\\n     }\\n \\n[DEL] private boolean shouldRetryRemotelyClosed(Exception exception, int retryCount)\\n[ADD] private boolean shouldRetryRemotelyClosed(Exception exception, int retryCount, String httpMethod)\\n     {\\n[DEL] boolean shouldRetry = exception instanceof IOException && containsIgnoreCase(exception.getMessage(), REMOTELY_CLOSED) && retryCount > 0;\\n[ADD] boolean shouldRetry = IDEMPOTENT_METHODS.contains(httpMethod) && exception instanceof IOException && containsIgnoreCase(exception.getMessage(), REMOTELY_CLOSED) && retryCount > 0;\\n         if(shouldRetry)\\n         {\\n             logger.warn(\"Sending HTTP message failed with `\" + IOException.class.getCanonicalName() + \": \" + REMOTELY_CLOSED',\n",
              " '   @Test\\n   public void deploysAppZipWithPrivilegedExtensionPlugin() throws Exception {\\n     ApplicationFileBuilder applicationFileBuilder = new ApplicationFileBuilder(\"privilegedPluginApp\")\\n[DEL] .definedBy(APP_WITH_PRIVILEGED_EXTENSION_PLUGIN_CONFIG).containingPlugin(privilegedExtensionPlugin);\\n[ADD] .definedBy(APP_WITH_PRIVILEGED_EXTENSION_PLUGIN_CONFIG).dependingOn(privilegedExtensionPlugin);\\n     addPackedAppFromBuilder(applicationFileBuilder);\\n \\n     startDeployment();',\n",
              " '         Long.toString(datasetOriginTimestamp), Long.toString(datasetUpstreamTimestamp));\\n   }\\n \\n[ADD] private List<CommitStep> getCommitSequence(Collection<WorkUnitState> workUnits, Class<?> baseClass)\\n[ADD] throws IOException {\\n[ADD] List<CommitStepCopyEntity> steps = Lists.newArrayList();\\n[ADD] for (WorkUnitState wus : workUnits) {\\n[ADD] if (baseClass.isAssignableFrom(CopySource.copyEntityClass(wus))) {\\n[ADD] CommitStepCopyEntity step = (CommitStepCopyEntity) CopySource.deserializeCopyEntity(wus);\\n[ADD] steps.add(step);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] Comparator<CommitStepCopyEntity> commitStepSorter = new Comparator<CommitStepCopyEntity>() {\\n[ADD] @Override public int compare(CommitStepCopyEntity o1, CommitStepCopyEntity o2) {\\n[ADD] return Integer.compare(o1.getPriority(), o2.getPriority());\\n[ADD] }\\n[ADD] };\\n[ADD] \\n[ADD] Collections.sort(steps, commitStepSorter);\\n[ADD] List<CommitStep> sequence = Lists.newArrayList();\\n[ADD] for (CommitStepCopyEntity entity : steps) {\\n[ADD] sequence.add(entity.getStep());\\n[ADD] }\\n[ADD] \\n[ADD] return sequence;\\n[ADD] }\\n[ADD] \\n[ADD] private void executeCommitSequence(List<CommitStep> steps) throws IOException {\\n[ADD] for (CommitStep step : steps) {\\n[ADD] step.execute();\\n[ADD] }\\n[ADD] }\\n[ADD] \\n   private Path findPathRoot(Path path) {\\n     while (path.getParent() != null) {\\n       path = path.getParent();',\n",
              " '                     params.getValidate().validate(dataMan, context, md);\\n                     return (Element) md.detach();\\n                 } catch (Exception e) {\\n[DEL] log.info(\"Skipping metadata that does not validate. Remote id : \" + ri.id);\\n[ADD] log.debug(\"Skipping metadata that does not validate. Remote id : \" + ri.id);\\n                     result.doesNotValidate++;\\n                 }\\n         }',\n",
              " \" \\t * Property names starting with an underscore ('_') are ignored. These are\\n \\t * reserved for properties that cause an unwanted side effect when expanded\\n \\t * unnecessary\\n[DEL] *\\n[ADD] *\\n \\t * @return A new Properties with the flattened values\\n \\t */\\n \\tpublic Properties getFlattenedProperties(boolean ignoreInstructions) {\",\n",
              " '             .allowTopLevelDefinition(false)\\n             .build());\\n \\n[DEL] configuration.onDefaultParameterGroup()\\n[ADD] params\\n         .withOptionalParameter(\"dynamicConfigExpiration\")\\n         .describedAs(DYNAMIC_CONFIG_EXPIRATION_DESCRIPTION)\\n         .ofType(new DynamicConfigExpirationTypeBuilder().buildDynamicConfigExpirationType())',\n",
              " ' \\n public class FileMessageReceiver extends AbstractPollingMessageReceiver\\n {\\n[ADD] \\n     public static final String COMPARATOR_CLASS_NAME_PROPERTY = \"comparator\";\\n     public static final String COMPARATOR_REVERSE_ORDER_PROPERTY = \"reverseOrder\";\\n     public static final String MULE_TRANSPORT_FILE_SINGLEPOLLINSTANCE = \"mule.transport.file.singlepollinstance\";\\n[ADD] public static final String NOT_PROCESS_EMPTY_FILES_PROPERTY = SYSTEM_PROPERTY_PREFIX + \"transport.file.notProcessEmptyFiles\";\\n \\n[ADD] private final Boolean NOT_PROCESS_EMPTY_FILES = getBoolean(NOT_PROCESS_EMPTY_FILES_PROPERTY);\\n     private FileConnector fileConnector = null;\\n     private String readDir = null;\\n     private String moveDir = null;',\n",
              " ' import org.junit.Rule;\\n import org.junit.Test;\\n import org.junit.rules.TestName;\\n[ADD] import org.osgi.framework.Version;\\n \\n import aQute.bnd.build.Project;\\n import aQute.bnd.build.Workspace;\\n import aQute.bnd.header.Attrs;\\n[ADD] import aQute.bnd.osgi.About;\\n[ADD] import aQute.bnd.osgi.Constants;\\n[ADD] import aQute.bnd.osgi.Processor;\\n import aQute.lib.io.IO;\\n import aQute.lib.strings.Strings;\\n ',\n",
              " '       ContextKey.named(\"thread-propagation-locations\");\\n \\n   private static final boolean THREAD_PROPAGATION_DEBUGGER =\\n[DEL] Boolean.getBoolean(\"otel.threadPropagationDebugger\");\\n[ADD] Boolean.getBoolean(\"otel.javaagent.experimental.thread-propagation-debugger.enabled\");\\n   private static final boolean FAIL_ON_CONTEXT_LEAK =\\n[DEL] Boolean.getBoolean(\"otel.internal.failOnContextLeak\");\\n[ADD] Boolean.getBoolean(\"otel.javaagent.testing.fail-on-context-leak\");\\n \\n   public static boolean isThreadPropagationDebuggerEnabled() {\\n     return THREAD_PROPAGATION_DEBUGGER;',\n",
              " ' \\t@SuppressWarnings(\"unchecked\")\\n \\tpublic synchronized void serviceChanged(ServiceEvent event) {\\n \\t\\tif (event.getType() == ServiceEvent.REGISTERED) {\\n[ADD] trace(\"service even \" + event);\\n \\t\\t\\tfinal Object service = systemBundle.getBundleContext().getService(event.getServiceReference());\\n \\t\\t\\tString[] objectclasses = (String[]) event.getServiceReference().getProperty(Constants.OBJECTCLASS);\\n ',\n",
              " ' import org.apache.hudi.io.HoodieKeyLocationFetchHandle;\\n import org.apache.hudi.table.HoodieTable;\\n \\n[DEL] import avro.shaded.com.google.common.collect.Lists;\\n[DEL] \\n import java.util.HashMap;\\n import java.util.LinkedList;\\n import java.util.List;',\n",
              " '     // look in site members\\n     for (JavaSymbol symbol : site.getSymbol().members().lookup(name)) {\\n       if (symbol.kind == JavaSymbol.MTH) {\\n[DEL] JavaSymbol best = selectBest(env, callSite, argTypes, symbol, bestSoFar.symbol, autoboxing);\\n[ADD] JavaSymbol best = selectBest(env, callSite, argTypes, typeParams, symbol, bestSoFar.symbol, autoboxing);\\n         if(best == symbol) {\\n           bestSoFar = Resolution.resolution(best);\\n[DEL] bestSoFar.type = resolveTypeSubstitution(((JavaType.MethodJavaType) best.type).resultType, site);\\n[DEL] JavaSymbol.MethodJavaSymbol methodSymbol = (JavaSymbol.MethodJavaSymbol) best;\\n[DEL] bestSoFar.type = handleTypeArguments(typeParams, bestSoFar.type, methodSymbol);\\n[ADD] if (!isConstructor(symbol)) {\\n[ADD] bestSoFar.type = typeSubstitutionSolver.getReturnType((JavaSymbol.MethodJavaSymbol) best, site, typeParams, argTypes);\\n[ADD] }\\n         }\\n       }\\n     }\\n     //look in supertypes for more specialized method (overloading).\\n     if (superclass != null) {\\n       Resolution method = findMethod(env, callSite, superclass, name, argTypes, typeParams);\\n[DEL] method.type = resolveTypeSubstitution(resolveTypeSubstitution(method.type, superclass), site);\\n[DEL] JavaSymbol best = selectBest(env, callSite, argTypes, method.symbol, bestSoFar.symbol, autoboxing);\\n[ADD] if (!isConstructor(method.symbol)) {\\n[ADD] method.type = typeSubstitutionSolver.applySiteSubstitution(typeSubstitutionSolver.applySiteSubstitution(method.type, superclass), site);\\n[ADD] }\\n[ADD] JavaSymbol best = selectBest(env, callSite, argTypes, typeParams, method.symbol, bestSoFar.symbol, autoboxing);\\n       if(best == method.symbol) {\\n         bestSoFar = method;\\n       }\\n     }\\n     for (JavaType interfaceType : site.getSymbol().getInterfaces()) {\\n       Resolution method = findMethod(env, callSite, interfaceType, name, argTypes, typeParams);\\n[DEL] method.type = resolveTypeSubstitution(resolveTypeSubstitution(method.type, interfaceType), site);\\n[DEL] JavaSymbol best = selectBest(env, callSite, argTypes, method.symbol, bestSoFar.symbol, autoboxing);\\n[ADD] if (!isConstructor(method.symbol)) {\\n[ADD] method.type = typeSubstitutionSolver.applySiteSubstitution(typeSubstitutionSolver.applySiteSubstitution(method.type, interfaceType), site);\\n[ADD] }\\n[ADD] JavaSymbol best = selectBest(env, callSite, argTypes, typeParams, method.symbol, bestSoFar.symbol, autoboxing);\\n       if(best == method.symbol) {\\n         bestSoFar = method;\\n       }',\n",
              " '       return Stream.empty();\\n     }\\n     return currentNode.learnedConstraints()\\n[DEL] .filter(lc -> lc.symbolicValue().equals(symbolicValue))\\n[ADD] .filter(lc -> lc.symbolicValue().equals(symbolicValue) && (lc.constraint == null || domains.stream().anyMatch(d -> d.isAssignableFrom(lc.constraint.getClass()))))\\n       .map(LearnedConstraint::constraint)\\n       .peek(lc -> learnedConstraintFlow(lc, currentNode, parent).forEach(flow::add));\\n   }',\n",
              " ' package org.mule.runtime.module.deployment.impl.internal.policy;\\n \\n import static java.lang.String.format;\\n[ADD] import static java.util.Collections.emptySet;\\n import static java.util.stream.Collectors.toList;\\n import static java.util.stream.Collectors.toSet;\\n import static org.mule.runtime.api.util.Preconditions.checkArgument;\\n import static org.mule.runtime.deployment.model.internal.DefaultRegionPluginClassLoadersFactory.getArtifactPluginId;\\n import static org.mule.runtime.module.artifact.api.classloader.DefaultArtifactClassLoaderFilter.NULL_CLASSLOADER_FILTER;\\n import static org.mule.runtime.module.deployment.impl.internal.artifact.ArtifactFactoryUtils.validateArtifactLicense;\\n[ADD] \\n import org.mule.runtime.deployment.model.api.application.Application;\\n import org.mule.runtime.deployment.model.api.plugin.ArtifactPlugin;\\n import org.mule.runtime.deployment.model.api.plugin.ArtifactPluginDescriptor;',\n",
              " '     writer.close();\\n     out.close();\\n \\n[DEL] return tmpFileName;\\n[ADD] return tmpFile;\\n[ADD] }\\n[ADD] \\n[ADD] private static TextSource prepareSource(\\n[ADD] TemporaryFolder temporaryFolder, byte[] data, byte[] delimiter) throws IOException {\\n[ADD] Path path = temporaryFolder.newFile().toPath();\\n[ADD] Files.write(path, data);\\n[ADD] return new TextSource(\\n[ADD] ValueProvider.StaticValueProvider.of(path.toString()),\\n[ADD] EmptyMatchTreatment.DISALLOW,\\n[ADD] delimiter);\\n   }\\n \\n[DEL] @Test\\n[DEL] @Category(NeedsRunner.class)\\n[DEL] public void testTxtRead() throws Exception {\\n[DEL] // Files with non-compressed extensions should work in AUTO and UNCOMPRESSED modes.\\n[DEL] for (Compression type : new Compression[] {AUTO, UNCOMPRESSED}) {\\n[DEL] assertReadingCompressedFileMatchesExpected(emptyTxt, type, EMPTY);\\n[DEL] assertReadingCompressedFileMatchesExpected(tinyTxt, type, TINY);\\n[DEL] assertReadingCompressedFileMatchesExpected(largeTxt, type, LARGE);\\n[ADD] private static String getFileSuffix(Compression compression) {\\n[ADD] switch (compression) {\\n[ADD] case UNCOMPRESSED:\\n[ADD] return \".txt\";\\n[ADD] case GZIP:\\n[ADD] return \".gz\";\\n[ADD] case BZIP2:\\n[ADD] return \".bz2\";\\n[ADD] case ZIP:\\n[ADD] return \".zip\";\\n[ADD] case DEFLATE:\\n[ADD] return \".deflate\";\\n[ADD] default:\\n[ADD] return \"\";\\n     }\\n[DEL] p.run();\\n   }\\n \\n[DEL] @Test\\n[DEL] @Category(NeedsRunner.class)\\n[DEL] public void testGzipCompressedRead() throws Exception {\\n[DEL] // Files with the right extensions should work in AUTO and GZIP modes.\\n[DEL] for (Compression type : new Compression[] {AUTO, GZIP}) {\\n[DEL] assertReadingCompressedFileMatchesExpected(emptyGz, type, EMPTY);\\n[DEL] assertReadingCompressedFileMatchesExpected(tinyGz, type, TINY);\\n[DEL] assertReadingCompressedFileMatchesExpected(largeGz, type, LARGE);\\n[ADD] /** Tests for reading from different size of files with various Compression. */\\n[ADD] @RunWith(Parameterized.class)\\n[ADD] public static class CompressedReadTest {\\n[ADD] @Rule public TemporaryFolder tempFolder = new TemporaryFolder();\\n[ADD] @Rule public TestPipeline p = TestPipeline.create();\\n[ADD] \\n[ADD] @Parameterized.Parameters(name = \"{index}: {1}\")\\n[ADD] public static Iterable<Object[]> data() {\\n[ADD] return ImmutableList.<Object[]>builder()\\n[ADD] .add(new Object[] {EMPTY, UNCOMPRESSED})\\n[ADD] .add(new Object[] {EMPTY, GZIP})\\n[ADD] .add(new Object[] {EMPTY, BZIP2})\\n[ADD] .add(new Object[] {EMPTY, ZIP})\\n[ADD] .add(new Object[] {EMPTY, DEFLATE})\\n[ADD] .add(new Object[] {TINY, UNCOMPRESSED})\\n[ADD] .add(new Object[] {TINY, GZIP})\\n[ADD] .add(new Object[] {TINY, BZIP2})\\n[ADD] .add(new Object[] {TINY, ZIP})\\n[ADD] .add(new Object[] {TINY, DEFLATE})\\n[ADD] .add(new Object[] {LARGE, UNCOMPRESSED})\\n[ADD] .add(new Object[] {LARGE, GZIP})\\n[ADD] .add(new Object[] {LARGE, BZIP2})\\n[ADD] .add(new Object[] {LARGE, ZIP})\\n[ADD] .add(new Object[] {LARGE, DEFLATE})\\n[ADD] .build();\\n     }\\n \\n[DEL] // Sanity check that we\\'re properly testing compression.\\n[DEL] assertThat(largeTxt.length(), greaterThan(largeGz.length()));\\n[ADD] @Parameterized.Parameter(0)\\n[ADD] public List<String> lines;\\n \\n[DEL] // GZIP files with non-gz extension should work in GZIP mode.\\n[DEL] File gzFile = writeToFile(TINY, \"tiny_gz_no_extension\", GZIP);\\n[DEL] assertReadingCompressedFileMatchesExpected(gzFile, GZIP, TINY);\\n[DEL] p.run();\\n[DEL] }\\n[ADD] @Parameterized.Parameter(1)\\n[ADD] public Compression compression;\\n \\n[DEL] @Test\\n[DEL] @Category(NeedsRunner.class)\\n[DEL] public void testBzip2CompressedRead() throws Exception {\\n[DEL] // Files with the right extensions should work in AUTO and BZIP2 modes.\\n[DEL] for (Compression type : new Compression[] {AUTO, BZIP2}) {\\n[DEL] assertReadingCompressedFileMatchesExpected(emptyBzip2, type, EMPTY);\\n[DEL] assertReadingCompressedFileMatchesExpected(tinyBzip2, type, TINY);\\n[DEL] assertReadingCompressedFileMatchesExpected(largeBzip2, type, LARGE);\\n[ADD] /** Tests reading from a small, compressed file with no extension. */\\n[ADD] @Test\\n[ADD] @Category(NeedsRunner.class)\\n[ADD] public void testCompressedReadWithoutExtension() throws Exception {\\n[ADD] String fileName = lines.size() + \"_\" + compression + \"_no_extension\";\\n[ADD] File fileWithNoExtension = writeToFile(lines, tempFolder, fileName, compression);\\n[ADD] assertReadingCompressedFileMatchesExpected(fileWithNoExtension, compression, lines, p);\\n[ADD] p.run();\\n     }\\n \\n[DEL] // Sanity check that we\\'re properly testing compression.\\n[DEL] assertThat(largeTxt.length(), greaterThan(largeBzip2.length()));\\n[ADD] @Test\\n[ADD] @Category(NeedsRunner.class)\\n[ADD] public void testCompressedReadWithExtension() throws Exception {\\n[ADD] String fileName =\\n[ADD] lines.size() + \"_\" + compression + \"_no_extension\" + getFileSuffix(compression);\\n[ADD] File fileWithExtension = writeToFile(lines, tempFolder, fileName, compression);\\n[ADD] \\n[ADD] // Sanity check that we\\'re properly testing compression.\\n[ADD] if (lines.size() == LINES_NUMBER_FOR_LARGE && !compression.equals(UNCOMPRESSED)) {\\n[ADD] File uncompressedFile = writeToFile(lines, tempFolder, \"large.txt\", UNCOMPRESSED);\\n[ADD] assertThat(uncompressedFile.length(), greaterThan(fileWithExtension.length()));\\n[ADD] }\\n[ADD] \\n[ADD] assertReadingCompressedFileMatchesExpected(fileWithExtension, compression, lines, p);\\n[ADD] p.run();\\n[ADD] }\\n \\n[DEL] // BZ2 files with non-bz2 extension should work in BZIP2 mode.\\n[DEL] File bz2File = writeToFile(TINY, \"tiny_bz2_no_extension\", BZIP2);\\n[DEL] assertReadingCompressedFileMatchesExpected(bz2File, BZIP2, TINY);\\n[DEL] p.run();\\n[ADD] @Test\\n[ADD] @Category(NeedsRunner.class)\\n[ADD] public void testReadWithAuto() throws Exception {\\n[ADD] // Files with non-compressed extensions should work in AUTO and UNCOMPRESSED modes.\\n[ADD] String fileName =\\n[ADD] lines.size() + \"_\" + compression + \"_no_extension\" + getFileSuffix(compression);\\n[ADD] File fileWithExtension = writeToFile(lines, tempFolder, fileName, compression);\\n[ADD] assertReadingCompressedFileMatchesExpected(fileWithExtension, AUTO, lines, p);\\n[ADD] p.run();\\n[ADD] }\\n   }\\n \\n[DEL] @Test\\n[DEL] @Category(NeedsRunner.class)\\n[DEL] public void testZipCompressedRead() throws Exception {\\n[DEL] // Files with the right extensions should work in AUTO and ZIP modes.\\n[DEL] for (Compression type : new Compression[] {AUTO, ZIP}) {\\n[DEL] assertReadingCompressedFileMatchesExpected(emptyZip, type, EMPTY);\\n[DEL] assertReadingCompressedFileMatchesExpected(tinyZip, type, TINY);\\n[DEL] assertReadingCompressedFileMatchesExpected(largeZip, type, LARGE);\\n[ADD] /** Tests for reading files with various delimiters. */\\n[ADD] @RunWith(Parameterized.class)\\n[ADD] public static class ReadWithDelimiterTest {\\n[ADD] private static final ImmutableList<String> EXPECTED = ImmutableList.of(\"asdf\", \"hjkl\", \"xyz\");\\n[ADD] @Rule public TemporaryFolder tempFolder = new TemporaryFolder();\\n[ADD] @Parameterized.Parameters(name = \"{index}: {0}\")\\n[ADD] public static Iterable<Object[]> data() {\\n[ADD] return ImmutableList.<Object[]>builder()\\n[ADD] .add(new Object[] {\"\\\\n\\\\n\\\\n\", ImmutableList.of(\"\", \"\", \"\")})\\n[ADD] .add(new Object[] {\"asdf\\\\nhjkl\\\\nxyz\\\\n\", EXPECTED})\\n[ADD] .add(new Object[] {\"asdf\\\\rhjkl\\\\rxyz\\\\r\", EXPECTED})\\n[ADD] .add(new Object[] {\"asdf\\\\r\\\\nhjkl\\\\r\\\\nxyz\\\\r\\\\n\", EXPECTED})\\n[ADD] .add(new Object[] {\"asdf\\\\rhjkl\\\\r\\\\nxyz\\\\n\", EXPECTED})\\n[ADD] .add(new Object[] {\"asdf\\\\nhjkl\\\\nxyz\", EXPECTED})\\n[ADD] .add(new Object[] {\"asdf\\\\rhjkl\\\\rxyz\", EXPECTED})\\n[ADD] .add(new Object[] {\"asdf\\\\r\\\\nhjkl\\\\r\\\\nxyz\", EXPECTED})\\n[ADD] .add(new Object[] {\"asdf\\\\rhjkl\\\\r\\\\nxyz\", EXPECTED})\\n[ADD] .build();\\n     }\\n \\n[DEL] // Sanity check that we\\'re properly testing compression.\\n[DEL] assertThat(largeTxt.length(), greaterThan(largeZip.length()));\\n[ADD] @Parameterized.Parameter(0)\\n[ADD] public String line;\\n \\n[DEL] // Zip files with non-zip extension should work in ZIP mode.\\n[DEL] File zipFile = writeToFile(TINY, \"tiny_zip_no_extension\", ZIP);\\n[DEL] assertReadingCompressedFileMatchesExpected(zipFile, ZIP, TINY);\\n[DEL] p.run();\\n[DEL] }\\n[ADD] @Parameterized.Parameter(1)\\n[ADD] public ImmutableList<String> expected;\\n \\n[DEL] @Test\\n[DEL] @Category(NeedsRunner.class)\\n[DEL] public void testDeflateCompressedRead() throws Exception {\\n[DEL] // Files with the right extensions should work in AUTO and ZIP modes.\\n[DEL] for (Compression type : new Compression[] {AUTO, DEFLATE}) {\\n[DEL] assertReadingCompressedFileMatchesExpected(emptyDeflate, type, EMPTY);\\n[DEL] assertReadingCompressedFileMatchesExpected(tinyDeflate, type, TINY);\\n[DEL] assertReadingCompressedFileMatchesExpected(largeDeflate, type, LARGE);\\n[ADD] @Test\\n[ADD] public void testReadLinesWithDelimiter() throws Exception {\\n[ADD] runTestReadWithData(line.getBytes(UTF_8), expected);\\n     }\\n \\n[DEL] // Sanity check that we\\'re properly testing compression.\\n[DEL] assertThat(largeTxt.length(), greaterThan(largeDeflate.length()));\\n[DEL] \\n[DEL] // Deflate files with non-deflate extension should work in DEFLATE mode.\\n[DEL] File deflateFile = writeToFile(TINY, \"tiny_deflate_no_extension\", DEFLATE);\\n[DEL] assertReadingCompressedFileMatchesExpected(deflateFile, DEFLATE, TINY);\\n[DEL] p.run();\\n[DEL] }\\n[ADD] @Test\\n[ADD] public void testSplittingSource() throws Exception {\\n[ADD] TextSource source = prepareSource(line.getBytes(UTF_8));\\n[ADD] SourceTestUtils.assertSplitAtFractionExhaustive(source, PipelineOptionsFactory.create());\\n[ADD] }\\n \\n[DEL] /**\\n[DEL] * Tests a zip file with no entries. This is a corner case not tested elsewhere as the default\\n[DEL] * test zip files have a single entry.\\n[DEL] */\\n[DEL] @Test\\n[DEL] @Category(NeedsRunner.class)\\n[DEL] public void testZipCompressedReadWithNoEntries() throws Exception {\\n[DEL] String filename = createZipFile(new ArrayList<String>(), \"empty zip file\");\\n[DEL] assertReadingCompressedFileMatchesExpected(new File(filename), ZIP, EMPTY);\\n[DEL] p.run();\\n[DEL] }\\n[ADD] private TextSource prepareSource(byte[] data) throws IOException {\\n[ADD] return TextIOReadTest.prepareSource(tempFolder, data, null);\\n[ADD] }\\n \\n[DEL] /**\\n[DEL] * Tests a zip file with multiple entries. This is a corner case not tested elsewhere as the\\n[DEL] * default test zip files have a single entry.\\n[DEL] */\\n[DEL] @Test\\n[DEL] @Category(NeedsRunner.class)\\n[DEL] public void testZipCompressedReadWithMultiEntriesFile() throws Exception {\\n[DEL] String[] entry0 = new String[] {\"first\", \"second\", \"three\"};\\n[DEL] String[] entry1 = new String[] {\"four\", \"five\", \"six\"};\\n[DEL] String[] entry2 = new String[] {\"seven\", \"eight\", \"nine\"};\\n[DEL] \\n[DEL] List<String> expected = new ArrayList<>();\\n[DEL] \\n[DEL] String filename = createZipFile(expected, \"multiple entries\", entry0, entry1, entry2);\\n[DEL] assertReadingCompressedFileMatchesExpected(new File(filename), ZIP, expected);\\n[DEL] p.run();\\n[ADD] private void runTestReadWithData(byte[] data, List<String> expectedResults) throws Exception {\\n[ADD] TextSource source = prepareSource(data);\\n[ADD] List<String> actual = SourceTestUtils.readFromSource(source, PipelineOptionsFactory.create());\\n[ADD] assertThat(\\n[ADD] actual, containsInAnyOrder(new ArrayList<>(expectedResults).toArray(new String[0])));\\n[ADD] }\\n   }\\n \\n[DEL] /**\\n[DEL] * Read a ZIP compressed file containing data, multiple empty entries, and then more data. We\\n[DEL] * expect just the data back.\\n[DEL] */\\n[DEL] @Test\\n[DEL] @Category(NeedsRunner.class)\\n[DEL] public void testZipCompressedReadWithComplexEmptyAndPresentEntries() throws Exception {\\n[DEL] String filename =\\n[DEL] createZipFile(\\n[DEL] new ArrayList<String>(),\\n[DEL] \"complex empty and present entries\",\\n[DEL] new String[] {\"cat\"},\\n[DEL] new String[] {},\\n[DEL] new String[] {},\\n[DEL] new String[] {\"dog\"});\\n[DEL] \\n[DEL] assertReadingCompressedFileMatchesExpected(\\n[DEL] new File(filename), ZIP, Arrays.asList(\"cat\", \"dog\"));\\n[DEL] p.run();\\n[DEL] }\\n[ADD] /** Tests for some basic operations in {@link TextIO.Read}. */\\n[ADD] @RunWith(JUnit4.class)\\n[ADD] public static class BasicIOTest {\\n[ADD] @Rule public TemporaryFolder tempFolder = new TemporaryFolder();\\n[ADD] @Rule public TestPipeline p = TestPipeline.create();\\n \\n[DEL] @Test\\n[DEL] public void testTextIOGetName() {\\n[DEL] assertEquals(\"TextIO.Read\", TextIO.read().from(\"somefile\").getName());\\n[DEL] assertEquals(\"TextIO.Read\", TextIO.read().from(\"somefile\").toString());\\n[DEL] }\\n[ADD] private void runTestRead(String[] expected) throws Exception {\\n[ADD] File tmpFile = tempFolder.newFile();\\n[ADD] String filename = tmpFile.getPath();\\n \\n[DEL] @Test\\n[DEL] public void testProgressEmptyFile() throws IOException {\\n[DEL] try (BoundedReader<String> reader =\\n[DEL] prepareSource(new byte[0], null).createReader(PipelineOptionsFactory.create())) {\\n[DEL] // Check preconditions before starting.\\n[DEL] assertEquals(0.0, reader.getFractionConsumed(), 1e-6);\\n[DEL] assertEquals(0, reader.getSplitPointsConsumed());\\n[DEL] assertEquals(BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());\\n[ADD] try (PrintStream writer = new PrintStream(new FileOutputStream(tmpFile))) {\\n[ADD] for (String elem : expected) {\\n[ADD] byte[] encodedElem = CoderUtils.encodeToByteArray(StringUtf8Coder.of(), elem);\\n[ADD] String line = new String(encodedElem);\\n[ADD] writer.println(line);\\n[ADD] }\\n[ADD] }\\n \\n[DEL] // Assert empty\\n[DEL] assertFalse(reader.start());\\n[ADD] TextIO.Read read = TextIO.read().from(filename);\\n[ADD] PCollection<String> output = p.apply(read);\\n \\n[DEL] // Check postconditions after finishing\\n[DEL] assertEquals(1.0, reader.getFractionConsumed(), 1e-6);\\n[DEL] assertEquals(0, reader.getSplitPointsConsumed());\\n[DEL] assertEquals(0, reader.getSplitPointsRemaining());\\n[ADD] PAssert.that(output).containsInAnyOrder(expected);\\n[ADD] p.run();\\n     }\\n[DEL] }\\n \\n[DEL] @Test\\n[DEL] public void testProgressTextFile() throws IOException {\\n[DEL] String file = \"line1\\\\nline2\\\\nline3\";\\n[DEL] try (BoundedReader<String> reader =\\n[DEL] prepareSource(file.getBytes(), null).createReader(PipelineOptionsFactory.create())) {\\n[DEL] // Check preconditions before starting\\n[DEL] assertEquals(0.0, reader.getFractionConsumed(), 1e-6);\\n[DEL] assertEquals(0, reader.getSplitPointsConsumed());\\n[DEL] assertEquals(BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());\\n[DEL] \\n[DEL] // Line 1\\n[DEL] assertTrue(reader.start());\\n[DEL] assertEquals(0, reader.getSplitPointsConsumed());\\n[DEL] assertEquals(BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());\\n[DEL] \\n[DEL] // Line 2\\n[DEL] assertTrue(reader.advance());\\n[DEL] assertEquals(1, reader.getSplitPointsConsumed());\\n[DEL] assertEquals(BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());\\n[DEL] \\n[DEL] // Line 3\\n[DEL] assertTrue(reader.advance());\\n[DEL] assertEquals(2, reader.getSplitPointsConsumed());\\n[DEL] assertEquals(1, reader.getSplitPointsRemaining());\\n[DEL] \\n[DEL] // Check postconditions after finishing\\n[DEL] assertFalse(reader.advance());\\n[DEL] assertEquals(1.0, reader.getFractionConsumed(), 1e-6);\\n[DEL] assertEquals(3, reader.getSplitPointsConsumed());\\n[DEL] assertEquals(0, reader.getSplitPointsRemaining());\\n[ADD] @Test\\n[ADD] public void testDelimiterSelfOverlaps(){\\n[ADD] assertFalse(TextIO.Read.isSelfOverlapping(new byte[]{\\'a\\', \\'b\\', \\'c\\'}));\\n[ADD] assertFalse(TextIO.Read.isSelfOverlapping(new byte[]{\\'c\\', \\'a\\', \\'b\\', \\'d\\', \\'a\\', \\'b\\'}));\\n[ADD] assertFalse(TextIO.Read.isSelfOverlapping(new byte[]{\\'a\\', \\'b\\', \\'c\\', \\'a\\', \\'b\\', \\'d\\'}));\\n[ADD] assertTrue(TextIO.Read.isSelfOverlapping(new byte[]{\\'a\\', \\'b\\', \\'a\\'}));\\n[ADD] assertTrue(TextIO.Read.isSelfOverlapping(new byte[]{\\'a\\', \\'b\\', \\'c\\', \\'a\\', \\'b\\'}));\\n     }\\n[DEL] }\\n \\n[DEL] @Test\\n[DEL] public void testProgressAfterSplitting() throws IOException {\\n[DEL] String file = \"line1\\\\nline2\\\\nline3\";\\n[DEL] BoundedSource<String> source = prepareSource(file.getBytes());\\n[DEL] BoundedSource<String> remainder;\\n[DEL] \\n[DEL] // Create the remainder, verifying properties pre- and post-splitting.\\n[DEL] try (BoundedReader<String> readerOrig = source.createReader(PipelineOptionsFactory.create())) {\\n[DEL] // Preconditions.\\n[DEL] assertEquals(0.0, readerOrig.getFractionConsumed(), 1e-6);\\n[DEL] assertEquals(0, readerOrig.getSplitPointsConsumed());\\n[DEL] assertEquals(BoundedReader.SPLIT_POINTS_UNKNOWN, readerOrig.getSplitPointsRemaining());\\n[DEL] \\n[DEL] // First record, before splitting.\\n[DEL] assertTrue(readerOrig.start());\\n[DEL] assertEquals(0, readerOrig.getSplitPointsConsumed());\\n[DEL] assertEquals(BoundedReader.SPLIT_POINTS_UNKNOWN, readerOrig.getSplitPointsRemaining());\\n[DEL] \\n[DEL] // Split. 0.1 is in line1, so should now be able to detect last record.\\n[DEL] remainder = readerOrig.splitAtFraction(0.1);\\n[DEL] System.err.println(readerOrig.getCurrentSource());\\n[DEL] assertNotNull(remainder);\\n[DEL] \\n[DEL] // First record, after splitting.\\n[DEL] assertEquals(0, readerOrig.getSplitPointsConsumed());\\n[DEL] assertEquals(1, readerOrig.getSplitPointsRemaining());\\n[DEL] \\n[DEL] // Finish and postconditions.\\n[DEL] assertFalse(readerOrig.advance());\\n[DEL] assertEquals(1.0, readerOrig.getFractionConsumed(), 1e-6);\\n[DEL] assertEquals(1, readerOrig.getSplitPointsConsumed());\\n[DEL] assertEquals(0, readerOrig.getSplitPointsRemaining());\\n[DEL] }\\n[DEL] \\n[DEL] // Check the properties of the remainder.\\n[DEL] try (BoundedReader<String> reader = remainder.createReader(PipelineOptionsFactory.create())) {\\n[DEL] // Preconditions.\\n[DEL] assertEquals(0.0, reader.getFractionConsumed(), 1e-6);\\n[DEL] assertEquals(0, reader.getSplitPointsConsumed());\\n[DEL] assertEquals(BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());\\n[DEL] \\n[DEL] // First record should be line 2.\\n[DEL] assertTrue(reader.start());\\n[DEL] assertEquals(0, reader.getSplitPointsConsumed());\\n[DEL] assertEquals(BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());\\n[DEL] \\n[DEL] // Second record is line 3\\n[DEL] assertTrue(reader.advance());\\n[DEL] assertEquals(1, reader.getSplitPointsConsumed());\\n[DEL] assertEquals(1, reader.getSplitPointsRemaining());\\n[DEL] \\n[DEL] // Check postconditions after finishing\\n[DEL] assertFalse(reader.advance());\\n[DEL] assertEquals(1.0, reader.getFractionConsumed(), 1e-6);\\n[DEL] assertEquals(2, reader.getSplitPointsConsumed());\\n[DEL] assertEquals(0, reader.getSplitPointsRemaining());\\n[ADD] @Test\\n[ADD] @Category(NeedsRunner.class)\\n[ADD] public void testReadStringsWithCustomDelimiter() throws Exception {\\n[ADD] final String[] inputStrings =\\n[ADD] new String[] {\\n[ADD] // incomplete delimiter\\n[ADD] \"To be, or not to be: that |is the question: \",\\n[ADD] // incomplete delimiter\\n[ADD] \"To be, or not to be: that *is the question: \",\\n[ADD] // complete delimiter\\n[ADD] \"Whether \\'tis nobler in the mind to suffer |*\",\\n[ADD] // truncated delimiter\\n[ADD] \"The slings and arrows of outrageous fortune,|\"\\n[ADD] };\\n[ADD] \\n[ADD] File tmpFile = tempFolder.newFile(\"tmpfile.txt\");\\n[ADD] String filename = tmpFile.getPath();\\n[ADD] \\n[ADD] try (FileWriter writer = new FileWriter(tmpFile)) {\\n[ADD] writer.write(Joiner.on(\"\").join(inputStrings));\\n[ADD] }\\n[ADD] \\n[ADD] PAssert.that(p.apply(TextIO.read().from(filename).withDelimiter(new byte[] {\\'|\\', \\'*\\'})))\\n[ADD] .containsInAnyOrder(\\n[ADD] \"To be, or not to be: that |is the question: To be, or not to be: \"\\n[ADD] + \"that *is the question: Whether \\'tis nobler in the mind to suffer \",\\n[ADD] \"The slings and arrows of outrageous fortune,|\");\\n[ADD] p.run();\\n     }\\n[DEL] }\\n \\n[DEL] @Test\\n[DEL] public void testReadEmptyLines() throws Exception {\\n[DEL] runTestReadWithData(\"\\\\n\\\\n\\\\n\".getBytes(StandardCharsets.UTF_8), ImmutableList.of(\"\", \"\", \"\"));\\n[DEL] }\\n[ADD] @Test\\n[ADD] public void testSplittingSourceWithCustomDelimiter() throws Exception {\\n[ADD] List<String> testCases = Lists.newArrayList();\\n[ADD] String infix = \"first|*second|*|*third\";\\n[ADD] String[] affixes = new String[] {\"\", \"|\", \"*\", \"|*\"};\\n[ADD] for (String prefix : affixes) {\\n[ADD] for (String suffix : affixes) {\\n[ADD] testCases.add(prefix + infix + suffix);\\n[ADD] }\\n[ADD] }\\n[ADD] for (String testCase : testCases) {\\n[ADD] SourceTestUtils.assertSplitAtFractionExhaustive(\\n[ADD] TextIOReadTest.prepareSource(\\n[ADD] tempFolder, testCase.getBytes(UTF_8), new byte[] {\\'|\\', \\'*\\'}),\\n[ADD] PipelineOptionsFactory.create());\\n[ADD] }\\n[ADD] }\\n \\n[DEL] @Test\\n[DEL] public void testReadFileWithLineFeedDelimiter() throws Exception {\\n[DEL] runTestReadWithData(\\n[DEL] \"asdf\\\\nhjkl\\\\nxyz\\\\n\".getBytes(StandardCharsets.UTF_8),\\n[DEL] ImmutableList.of(\"asdf\", \"hjkl\", \"xyz\"));\\n[DEL] }\\n[ADD] @Test\\n[ADD] @Category(NeedsRunner.class)\\n[ADD] public void testReadStrings() throws Exception {\\n[ADD] runTestRead(LINES_ARRAY);\\n[ADD] }\\n \\n[DEL] @Test\\n[DEL] public void testReadFileWithCarriageReturnDelimiter() throws Exception {\\n[DEL] runTestReadWithData(\\n[DEL] \"asdf\\\\rhjkl\\\\rxyz\\\\r\".getBytes(StandardCharsets.UTF_8),\\n[DEL] ImmutableList.of(\"asdf\", \"hjkl\", \"xyz\"));\\n[DEL] }\\n[ADD] @Test\\n[ADD] @Category(NeedsRunner.class)\\n[ADD] public void testReadEmptyStrings() throws Exception {\\n[ADD] runTestRead(NO_LINES_ARRAY);\\n[ADD] }\\n \\n[DEL] @Test\\n[DEL] public void testReadFileWithCarriageReturnAndLineFeedDelimiter() throws Exception {\\n[DEL] runTestReadWithData(\\n[DEL] \"asdf\\\\r\\\\nhjkl\\\\r\\\\nxyz\\\\r\\\\n\".getBytes(StandardCharsets.UTF_8),\\n[DEL] ImmutableList.of(\"asdf\", \"hjkl\", \"xyz\"));\\n[DEL] }\\n[ADD] @Test\\n[ADD] public void testReadNamed() throws Exception {\\n[ADD] File emptyFile = tempFolder.newFile();\\n[ADD] p.enableAbandonedNodeEnforcement(false);\\n \\n[DEL] @Test\\n[DEL] public void testReadFileWithMixedDelimiters() throws Exception {\\n[DEL] runTestReadWithData(\\n[DEL] \"asdf\\\\rhjkl\\\\r\\\\nxyz\\\\n\".getBytes(StandardCharsets.UTF_8),\\n[DEL] ImmutableList.of(\"asdf\", \"hjkl\", \"xyz\"));\\n[DEL] }\\n[ADD] assertEquals(\"TextIO.Read/Read.out\", p.apply(TextIO.read().from(\"somefile\")).getName());\\n[ADD] assertEquals(\\n[ADD] \"MyRead/Read.out\", p.apply(\"MyRead\", TextIO.read().from(emptyFile.getPath())).getName());\\n[ADD] }\\n \\n[DEL] @Test\\n[DEL] public void testReadFileWithLineFeedDelimiterAndNonEmptyBytesAtEnd() throws Exception {\\n[DEL] runTestReadWithData(\\n[DEL] \"asdf\\\\nhjkl\\\\nxyz\".getBytes(StandardCharsets.UTF_8),\\n[DEL] ImmutableList.of(\"asdf\", \"hjkl\", \"xyz\"));\\n[DEL] }\\n[ADD] @Test\\n[ADD] public void testReadDisplayData() {\\n[ADD] TextIO.Read read = TextIO.read().from(\"foo.*\").withCompression(BZIP2);\\n \\n[DEL] @Test\\n[DEL] public void testReadFileWithCarriageReturnDelimiterAndNonEmptyBytesAtEnd() throws Exception {\\n[DEL] runTestReadWithData(\\n[DEL] \"asdf\\\\rhjkl\\\\rxyz\".getBytes(StandardCharsets.UTF_8),\\n[DEL] ImmutableList.of(\"asdf\", \"hjkl\", \"xyz\"));\\n[DEL] }\\n[ADD] DisplayData displayData = DisplayData.from(read);\\n \\n[DEL] @Test\\n[DEL] public void testReadFileWithCarriageReturnAndLineFeedDelimiterAndNonEmptyBytesAtEnd()\\n[DEL] throws Exception {\\n[DEL] runTestReadWithData(\\n[DEL] \"asdf\\\\r\\\\nhjkl\\\\r\\\\nxyz\".getBytes(StandardCharsets.UTF_8),\\n[DEL] ImmutableList.of(\"asdf\", \"hjkl\", \"xyz\"));\\n[DEL] }\\n[ADD] assertThat(displayData, hasDisplayItem(\"filePattern\", \"foo.*\"));\\n[ADD] assertThat(displayData, hasDisplayItem(\"compressionType\", BZIP2.toString()));\\n[ADD] }\\n \\n[DEL] @Test\\n[DEL] public void testReadFileWithMixedDelimitersAndNonEmptyBytesAtEnd() throws Exception {\\n[DEL] runTestReadWithData(\\n[DEL] \"asdf\\\\rhjkl\\\\r\\\\nxyz\".getBytes(StandardCharsets.UTF_8),\\n[DEL] ImmutableList.of(\"asdf\", \"hjkl\", \"xyz\"));\\n[DEL] }\\n[ADD] @Test\\n[ADD] @Category(ValidatesRunner.class)\\n[ADD] public void testPrimitiveReadDisplayData() {\\n[ADD] DisplayDataEvaluator evaluator = DisplayDataEvaluator.create();\\n \\n[DEL] private void runTestReadWithData(byte[] data, List<String> expectedResults) throws Exception {\\n[DEL] TextSource source = prepareSource(data);\\n[DEL] List<String> actual = SourceTestUtils.readFromSource(source, PipelineOptionsFactory.create());\\n[DEL] assertThat(actual, containsInAnyOrder(new ArrayList<>(expectedResults).toArray(new String[0])));\\n[DEL] }\\n[ADD] TextIO.Read read = TextIO.read().from(\"foobar\");\\n \\n[DEL] @Test\\n[DEL] public void testSplittingSourceWithEmptyLines() throws Exception {\\n[DEL] TextSource source = prepareSource(\"\\\\n\\\\n\\\\n\".getBytes(UTF_8));\\n[DEL] SourceTestUtils.assertSplitAtFractionExhaustive(source, PipelineOptionsFactory.create());\\n[DEL] }\\n[ADD] Set<DisplayData> displayData = evaluator.displayDataForPrimitiveSourceTransforms(read);\\n[ADD] assertThat(\\n[ADD] \"TextIO.Read should include the file prefix in its primitive display data\",\\n[ADD] displayData,\\n[ADD] hasItem(hasDisplayItem(hasValue(startsWith(\"foobar\")))));\\n[ADD] }\\n \\n[DEL] @Test\\n[DEL] public void testSplittingSourceWithLineFeedDelimiter() throws Exception {\\n[DEL] TextSource source = prepareSource(\"asdf\\\\nhjkl\\\\nxyz\\\\n\".getBytes(UTF_8));\\n[DEL] SourceTestUtils.assertSplitAtFractionExhaustive(source, PipelineOptionsFactory.create());\\n[DEL] }\\n[ADD] /** Options for testing. */\\n[ADD] public interface RuntimeTestOptions extends PipelineOptions {\\n[ADD] ValueProvider<String> getInput();\\n[ADD] void setInput(ValueProvider<String> value);\\n[ADD] }\\n \\n[DEL] @Test\\n[DEL] public void testSplittingSourceWithCarriageReturnDelimiter() throws Exception {\\n[DEL] TextSource source = prepareSource(\"asdf\\\\rhjkl\\\\rxyz\\\\r\".getBytes(UTF_8));\\n[DEL] SourceTestUtils.assertSplitAtFractionExhaustive(source, PipelineOptionsFactory.create());\\n[DEL] }\\n[ADD] @Test\\n[ADD] public void testRuntimeOptionsNotCalledInApply() throws Exception {\\n[ADD] p.enableAbandonedNodeEnforcement(false);\\n \\n[DEL] @Test\\n[DEL] public void testSplittingSourceWithCarriageReturnAndLineFeedDelimiter() throws Exception {\\n[DEL] TextSource source = prepareSource(\"asdf\\\\r\\\\nhjkl\\\\r\\\\nxyz\\\\r\\\\n\".getBytes(UTF_8));\\n[DEL] SourceTestUtils.assertSplitAtFractionExhaustive(source, PipelineOptionsFactory.create());\\n[DEL] }\\n[ADD] RuntimeTestOptions options =\\n[ADD] PipelineOptionsFactory.as(RuntimeTestOptions.class);\\n \\n[DEL] @Test\\n[DEL] public void testSplittingSourceWithMixedDelimiters() throws Exception {\\n[DEL] TextSource source = prepareSource(\"asdf\\\\rhjkl\\\\r\\\\nxyz\\\\n\".getBytes(UTF_8));\\n[DEL] SourceTestUtils.assertSplitAtFractionExhaustive(source, PipelineOptionsFactory.create());\\n[DEL] }\\n[ADD] p.apply(TextIO.read().from(options.getInput()));\\n[ADD] }\\n \\n[DEL] @Test\\n[DEL] public void testSplittingSourceWithLineFeedDelimiterAndNonEmptyBytesAtEnd() throws Exception {\\n[DEL] TextSource source = prepareSource(\"asdf\\\\nhjkl\\\\nxyz\".getBytes(UTF_8));\\n[DEL] SourceTestUtils.assertSplitAtFractionExhaustive(source, PipelineOptionsFactory.create());\\n[DEL] }\\n[ADD] @Test\\n[ADD] public void testCompressionIsSet() throws Exception {\\n[ADD] TextIO.Read read = TextIO.read().from(\"/tmp/test\");\\n[ADD] assertEquals(AUTO, read.getCompression());\\n[ADD] read = TextIO.read().from(\"/tmp/test\").withCompression(GZIP);\\n[ADD] assertEquals(GZIP, read.getCompression());\\n[ADD] }\\n \\n[DEL] @Test\\n[DEL] public void testSplittingSourceWithCarriageReturnDelimiterAndNonEmptyBytesAtEnd()\\n[DEL] throws Exception {\\n[DEL] TextSource source = prepareSource(\"asdf\\\\rhjkl\\\\rxyz\".getBytes(UTF_8));\\n[DEL] SourceTestUtils.assertSplitAtFractionExhaustive(source, PipelineOptionsFactory.create());\\n[DEL] }\\n[ADD] /**\\n[ADD] * Tests reading from a small, uncompressed file with .gz extension. This must work in AUTO or\\n[ADD] * GZIP modes. This is needed because some network file systems / HTTP clients will\\n[ADD] * transparently decompress gzipped content.\\n[ADD] */\\n[ADD] @Test\\n[ADD] @Category(NeedsRunner.class)\\n[ADD] public void testSmallCompressedGzipReadActuallyUncompressed() throws Exception {\\n[ADD] File smallGzNotCompressed =\\n[ADD] writeToFile(TINY, tempFolder, \"tiny_uncompressed.gz\", UNCOMPRESSED);\\n[ADD] // Should work with GZIP compression set.\\n[ADD] assertReadingCompressedFileMatchesExpected(smallGzNotCompressed, GZIP, TINY, p);\\n[ADD] p.run();\\n[ADD] }\\n \\n[DEL] @Test\\n[DEL] public void testSplittingSourceWithCarriageReturnAndLineFeedDelimiterAndNonEmptyBytesAtEnd()\\n[DEL] throws Exception {\\n[DEL] TextSource source = prepareSource(\"asdf\\\\r\\\\nhjkl\\\\r\\\\nxyz\".getBytes(UTF_8));\\n[DEL] SourceTestUtils.assertSplitAtFractionExhaustive(source, PipelineOptionsFactory.create());\\n[DEL] }\\n[ADD] /**\\n[ADD] * Tests reading from a small, uncompressed file with .gz extension. This must work in AUTO or\\n[ADD] * GZIP modes. This is needed because some network file systems / HTTP clients will\\n[ADD] * transparently decompress gzipped content.\\n[ADD] */\\n[ADD] @Test\\n[ADD] @Category(NeedsRunner.class)\\n[ADD] public void testSmallCompressedAutoReadActuallyUncompressed() throws Exception {\\n[ADD] File smallGzNotCompressed =\\n[ADD] writeToFile(TINY, tempFolder, \"tiny_uncompressed.gz\", UNCOMPRESSED);\\n[ADD] // Should also work with AUTO mode set.\\n[ADD] assertReadingCompressedFileMatchesExpected(smallGzNotCompressed, AUTO, TINY, p);\\n[ADD] p.run();\\n[ADD] }\\n \\n[DEL] @Test\\n[DEL] public void testSplittingSourceWithMixedDelimitersAndNonEmptyBytesAtEnd() throws Exception {\\n[DEL] TextSource source = prepareSource(\"asdf\\\\rhjkl\\\\r\\\\nxyz\".getBytes(UTF_8));\\n[DEL] SourceTestUtils.assertSplitAtFractionExhaustive(source, PipelineOptionsFactory.create());\\n[DEL] }\\n[ADD] /**\\n[ADD] * Tests a zip file with no entries. This is a corner case not tested elsewhere as the default\\n[ADD] * test zip files have a single entry.\\n[ADD] */\\n[ADD] @Test\\n[ADD] @Category(NeedsRunner.class)\\n[ADD] public void testZipCompressedReadWithNoEntries() throws Exception {\\n[ADD] File filename = createZipFile(new ArrayList<String>(), tempFolder, \"empty zip file\");\\n[ADD] assertReadingCompressedFileMatchesExpected(filename, ZIP, EMPTY, p);\\n[ADD] p.run();\\n[ADD] }\\n \\n[DEL] private TextSource prepareSource(byte[] data) throws IOException {\\n[DEL] return prepareSource(data, null /* default delimiters */);\\n[DEL] }\\n[ADD] /**\\n[ADD] * Tests a zip file with multiple entries. This is a corner case not tested elsewhere as the\\n[ADD] * default test zip files have a single entry.\\n[ADD] */\\n[ADD] @Test\\n[ADD] @Category(NeedsRunner.class)\\n[ADD] public void testZipCompressedReadWithMultiEntriesFile() throws Exception {\\n[ADD] String[] entry0 = new String[] {\"first\", \"second\", \"three\"};\\n[ADD] String[] entry1 = new String[] {\"four\", \"five\", \"six\"};\\n[ADD] String[] entry2 = new String[] {\"seven\", \"eight\", \"nine\"};\\n[ADD] \\n[ADD] List<String> expected = new ArrayList<>();\\n[ADD] \\n[ADD] File filename =\\n[ADD] createZipFile(expected, tempFolder, \"multiple entries\", entry0, entry1, entry2);\\n[ADD] assertReadingCompressedFileMatchesExpected(filename, ZIP, expected, p);\\n[ADD] p.run();\\n[ADD] }\\n \\n[DEL] private TextSource prepareSource(byte[] data, byte[] delimiter) throws IOException {\\n[DEL] Path path = Files.createTempFile(tempFolder, \"tempfile\", \"ext\");\\n[DEL] Files.write(path, data);\\n[DEL] return new TextSource(ValueProvider.StaticValueProvider.of(path.toString()),\\n[DEL] EmptyMatchTreatment.DISALLOW, delimiter);\\n[DEL] }\\n[ADD] /**\\n[ADD] * Read a ZIP compressed file containing data, multiple empty entries, and then more data. We\\n[ADD] * expect just the data back.\\n[ADD] */\\n[ADD] @Test\\n[ADD] @Category(NeedsRunner.class)\\n[ADD] public void testZipCompressedReadWithComplexEmptyAndPresentEntries() throws Exception {\\n[ADD] File filename =\\n[ADD] createZipFile(\\n[ADD] new ArrayList<String>(),\\n[ADD] tempFolder,\\n[ADD] \"complex empty and present entries\",\\n[ADD] new String[] {\"cat\"},\\n[ADD] new String[] {},\\n[ADD] new String[] {},\\n[ADD] new String[] {\"dog\"});\\n[ADD] \\n[ADD] assertReadingCompressedFileMatchesExpected(\\n[ADD] filename, ZIP, Arrays.asList(\"cat\", \"dog\"), p);\\n[ADD] p.run();\\n[ADD] }\\n \\n[DEL] @Test\\n[DEL] public void testInitialSplitAutoModeTxt() throws Exception {\\n[DEL] PipelineOptions options = TestPipeline.testingPipelineOptions();\\n[DEL] long desiredBundleSize = 1000;\\n[ADD] @Test\\n[ADD] public void testTextIOGetName() {\\n[ADD] assertEquals(\"TextIO.Read\", TextIO.read().from(\"somefile\").getName());\\n[ADD] assertEquals(\"TextIO.Read\", TextIO.read().from(\"somefile\").toString());\\n[ADD] }\\n \\n[DEL] // Sanity check: file is at least 2 bundles long.\\n[DEL] assertThat(largeTxt.length(), greaterThan(2 * desiredBundleSize));\\n[ADD] private TextSource prepareSource(byte[] data) throws IOException {\\n[ADD] return TextIOReadTest.prepareSource(tempFolder, data, null);\\n[ADD] }\\n \\n[DEL] FileBasedSource<String> source = TextIO.read().from(largeTxt.getPath()).getSource();\\n[DEL] List<? extends FileBasedSource<String>> splits = source.split(desiredBundleSize, options);\\n[ADD] @Test\\n[ADD] public void testProgressEmptyFile() throws IOException {\\n[ADD] try (BoundedSource.BoundedReader<String> reader =\\n[ADD] prepareSource(new byte[0]).createReader(PipelineOptionsFactory.create())) {\\n[ADD] // Check preconditions before starting.\\n[ADD] assertEquals(0.0, reader.getFractionConsumed(), 1e-6);\\n[ADD] assertEquals(0, reader.getSplitPointsConsumed());\\n[ADD] assertEquals(\\n[ADD] BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());\\n[ADD] \\n[ADD] // Assert empty\\n[ADD] assertFalse(reader.start());\\n[ADD] \\n[ADD] // Check postconditions after finishing\\n[ADD] assertEquals(1.0, reader.getFractionConsumed(), 1e-6);\\n[ADD] assertEquals(0, reader.getSplitPointsConsumed());\\n[ADD] assertEquals(0, reader.getSplitPointsRemaining());\\n[ADD] }\\n[ADD] }\\n \\n[DEL] // At least 2 splits and they are equal to reading the whole file.\\n[DEL] assertThat(splits, hasSize(greaterThan(1)));\\n[DEL] SourceTestUtils.assertSourcesEqualReferenceSource(source, splits, options);\\n[DEL] }\\n[ADD] @Test\\n[ADD] public void testProgressTextFile() throws IOException {\\n[ADD] String file = \"line1\\\\nline2\\\\nline3\";\\n[ADD] try (BoundedSource.BoundedReader<String> reader =\\n[ADD] prepareSource(file.getBytes()).createReader(PipelineOptionsFactory.create())) {\\n[ADD] // Check preconditions before starting\\n[ADD] assertEquals(0.0, reader.getFractionConsumed(), 1e-6);\\n[ADD] assertEquals(0, reader.getSplitPointsConsumed());\\n[ADD] assertEquals(\\n[ADD] BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());\\n[ADD] \\n[ADD] // Line 1\\n[ADD] assertTrue(reader.start());\\n[ADD] assertEquals(0, reader.getSplitPointsConsumed());\\n[ADD] assertEquals(\\n[ADD] BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());\\n[ADD] \\n[ADD] // Line 2\\n[ADD] assertTrue(reader.advance());\\n[ADD] assertEquals(1, reader.getSplitPointsConsumed());\\n[ADD] assertEquals(\\n[ADD] BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());\\n[ADD] \\n[ADD] // Line 3\\n[ADD] assertTrue(reader.advance());\\n[ADD] assertEquals(2, reader.getSplitPointsConsumed());\\n[ADD] assertEquals(1, reader.getSplitPointsRemaining());\\n[ADD] \\n[ADD] // Check postconditions after finishing\\n[ADD] assertFalse(reader.advance());\\n[ADD] assertEquals(1.0, reader.getFractionConsumed(), 1e-6);\\n[ADD] assertEquals(3, reader.getSplitPointsConsumed());\\n[ADD] assertEquals(0, reader.getSplitPointsRemaining());\\n[ADD] }\\n[ADD] }\\n \\n[DEL] @Test\\n[DEL] public void testInitialSplitAutoModeGz() throws Exception {\\n[DEL] long desiredBundleSize = 1000;\\n[DEL] PipelineOptions options = TestPipeline.testingPipelineOptions();\\n[ADD] @Test\\n[ADD] public void testProgressAfterSplitting() throws IOException {\\n[ADD] String file = \"line1\\\\nline2\\\\nline3\";\\n[ADD] BoundedSource<String> source = prepareSource(file.getBytes());\\n[ADD] BoundedSource<String> remainder;\\n[ADD] \\n[ADD] // Create the remainder, verifying properties pre- and post-splitting.\\n[ADD] try (BoundedSource.BoundedReader<String> readerOrig =\\n[ADD] source.createReader(PipelineOptionsFactory.create())) {\\n[ADD] // Preconditions.\\n[ADD] assertEquals(0.0, readerOrig.getFractionConsumed(), 1e-6);\\n[ADD] assertEquals(0, readerOrig.getSplitPointsConsumed());\\n[ADD] assertEquals(\\n[ADD] BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, readerOrig.getSplitPointsRemaining());\\n[ADD] \\n[ADD] // First record, before splitting.\\n[ADD] assertTrue(readerOrig.start());\\n[ADD] assertEquals(0, readerOrig.getSplitPointsConsumed());\\n[ADD] assertEquals(\\n[ADD] BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, readerOrig.getSplitPointsRemaining());\\n[ADD] \\n[ADD] // Split. 0.1 is in line1, so should now be able to detect last record.\\n[ADD] remainder = readerOrig.splitAtFraction(0.1);\\n[ADD] System.err.println(readerOrig.getCurrentSource());\\n[ADD] assertNotNull(remainder);\\n[ADD] \\n[ADD] // First record, after splitting.\\n[ADD] assertEquals(0, readerOrig.getSplitPointsConsumed());\\n[ADD] assertEquals(1, readerOrig.getSplitPointsRemaining());\\n[ADD] \\n[ADD] // Finish and postconditions.\\n[ADD] assertFalse(readerOrig.advance());\\n[ADD] assertEquals(1.0, readerOrig.getFractionConsumed(), 1e-6);\\n[ADD] assertEquals(1, readerOrig.getSplitPointsConsumed());\\n[ADD] assertEquals(0, readerOrig.getSplitPointsRemaining());\\n[ADD] }\\n \\n[DEL] // Sanity check: file is at least 2 bundles long.\\n[DEL] assertThat(largeGz.length(), greaterThan(2 * desiredBundleSize));\\n[ADD] // Check the properties of the remainder.\\n[ADD] try (BoundedSource.BoundedReader<String> reader =\\n[ADD] remainder.createReader(PipelineOptionsFactory.create())) {\\n[ADD] // Preconditions.\\n[ADD] assertEquals(0.0, reader.getFractionConsumed(), 1e-6);\\n[ADD] assertEquals(0, reader.getSplitPointsConsumed());\\n[ADD] assertEquals(\\n[ADD] BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());\\n[ADD] \\n[ADD] // First record should be line 2.\\n[ADD] assertTrue(reader.start());\\n[ADD] assertEquals(0, reader.getSplitPointsConsumed());\\n[ADD] assertEquals(\\n[ADD] BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());\\n[ADD] \\n[ADD] // Second record is line 3\\n[ADD] assertTrue(reader.advance());\\n[ADD] assertEquals(1, reader.getSplitPointsConsumed());\\n[ADD] assertEquals(1, reader.getSplitPointsRemaining());\\n[ADD] \\n[ADD] // Check postconditions after finishing\\n[ADD] assertFalse(reader.advance());\\n[ADD] assertEquals(1.0, reader.getFractionConsumed(), 1e-6);\\n[ADD] assertEquals(2, reader.getSplitPointsConsumed());\\n[ADD] assertEquals(0, reader.getSplitPointsRemaining());\\n[ADD] }\\n[ADD] }\\n \\n[DEL] FileBasedSource<String> source = TextIO.read().from(largeGz.getPath()).getSource();\\n[DEL] List<? extends FileBasedSource<String>> splits = source.split(desiredBundleSize, options);\\n[ADD] @Test\\n[ADD] public void testInitialSplitAutoModeTxt() throws Exception {\\n[ADD] PipelineOptions options = TestPipeline.testingPipelineOptions();\\n[ADD] long desiredBundleSize = 1000;\\n[ADD] File largeTxt = writeToFile(LARGE, tempFolder, \"large.txt\", UNCOMPRESSED);\\n \\n[DEL] // Exactly 1 split, even in AUTO mode, since it is a gzip file.\\n[DEL] assertThat(splits, hasSize(equalTo(1)));\\n[DEL] SourceTestUtils.assertSourcesEqualReferenceSource(source, splits, options);\\n[DEL] }\\n[ADD] // Sanity check: file is at least 2 bundles long.\\n[ADD] assertThat(largeTxt.length(), greaterThan(2 * desiredBundleSize));\\n \\n[DEL] @Test\\n[DEL] public void testInitialSplitGzipModeTxt() throws Exception {\\n[DEL] PipelineOptions options = TestPipeline.testingPipelineOptions();\\n[DEL] long desiredBundleSize = 1000;\\n[ADD] FileBasedSource<String> source = TextIO.read().from(largeTxt.getPath()).getSource();\\n[ADD] List<? extends FileBasedSource<String>> splits = source.split(desiredBundleSize, options);\\n \\n[DEL] // Sanity check: file is at least 2 bundles long.\\n[DEL] assertThat(largeTxt.length(), greaterThan(2 * desiredBundleSize));\\n[ADD] // At least 2 splits and they are equal to reading the whole file.\\n[ADD] assertThat(splits, hasSize(greaterThan(1)));\\n[ADD] SourceTestUtils.assertSourcesEqualReferenceSource(source, splits, options);\\n[ADD] }\\n \\n[DEL] FileBasedSource<String> source =\\n[DEL] TextIO.read().from(largeTxt.getPath()).withCompression(GZIP).getSource();\\n[DEL] List<? extends FileBasedSource<String>> splits = source.split(desiredBundleSize, options);\\n[ADD] @Test\\n[ADD] public void testInitialSplitAutoModeGz() throws Exception {\\n[ADD] PipelineOptions options = TestPipeline.testingPipelineOptions();\\n[ADD] long desiredBundleSize = 1000;\\n[ADD] File largeGz = writeToFile(LARGE, tempFolder, \"large.gz\", GZIP);\\n[ADD] // Sanity check: file is at least 2 bundles long.\\n[ADD] assertThat(largeGz.length(), greaterThan(2 * desiredBundleSize));\\n \\n[DEL] // Exactly 1 split, even though splittable text file, since using GZIP mode.\\n[DEL] assertThat(splits, hasSize(equalTo(1)));\\n[DEL] SourceTestUtils.assertSourcesEqualReferenceSource(source, splits, options);\\n[DEL] }\\n[ADD] FileBasedSource<String> source = TextIO.read().from(largeGz.getPath()).getSource();\\n[ADD] List<? extends FileBasedSource<String>> splits = source.split(desiredBundleSize, options);\\n \\n[DEL] @Test\\n[DEL] public void testInitialSplitGzipModeGz() throws Exception {\\n[DEL] PipelineOptions options = TestPipeline.testingPipelineOptions();\\n[DEL] long desiredBundleSize = 1000;\\n[ADD] // Exactly 1 split, even in AUTO mode, since it is a gzip file.\\n[ADD] assertThat(splits, hasSize(equalTo(1)));\\n[ADD] SourceTestUtils.assertSourcesEqualReferenceSource(source, splits, options);\\n[ADD] }\\n \\n[DEL] // Sanity check: file is at least 2 bundles long.\\n[DEL] assertThat(largeGz.length(), greaterThan(2 * desiredBundleSize));\\n[ADD] @Test\\n[ADD] public void testInitialSplitGzipModeTxt() throws Exception {\\n[ADD] PipelineOptions options = TestPipeline.testingPipelineOptions();\\n[ADD] long desiredBundleSize = 1000;\\n[ADD] File largeTxt = writeToFile(LARGE, tempFolder, \"large.txt\", UNCOMPRESSED);\\n[ADD] // Sanity check: file is at least 2 bundles long.\\n[ADD] assertThat(largeTxt.length(), greaterThan(2 * desiredBundleSize));\\n \\n[DEL] FileBasedSource<String> source =\\n[DEL] TextIO.read().from(largeGz.getPath()).withCompression(GZIP).getSource();\\n[DEL] List<? extends FileBasedSource<String>> splits = source.split(desiredBundleSize, options);\\n[ADD] FileBasedSource<String> source =\\n[ADD] TextIO.read().from(largeTxt.getPath()).withCompression(GZIP).getSource();\\n[ADD] List<? extends FileBasedSource<String>> splits = source.split(desiredBundleSize, options);\\n \\n[DEL] // Exactly 1 split using .gz extension and using GZIP mode.\\n[DEL] assertThat(splits, hasSize(equalTo(1)));\\n[DEL] SourceTestUtils.assertSourcesEqualReferenceSource(source, splits, options);\\n[DEL] }\\n[ADD] // Exactly 1 split, even though splittable text file, since using GZIP mode.\\n[ADD] assertThat(splits, hasSize(equalTo(1)));\\n[ADD] SourceTestUtils.assertSourcesEqualReferenceSource(source, splits, options);\\n[ADD] }\\n \\n[DEL] @Test\\n[DEL] @Category(NeedsRunner.class)\\n[DEL] public void testReadAll() throws IOException {\\n[DEL] writeToFile(TINY, \"readAllTiny1.zip\", ZIP);\\n[DEL] writeToFile(TINY, \"readAllTiny2.txt\", UNCOMPRESSED);\\n[DEL] writeToFile(LARGE, \"readAllLarge1.zip\", ZIP);\\n[DEL] writeToFile(LARGE, \"readAllLarge2.txt\", UNCOMPRESSED);\\n[DEL] PCollection<String> lines =\\n[ADD] @Test\\n[ADD] @Category(NeedsRunner.class)\\n[ADD] public void testReadAll() throws IOException {\\n[ADD] Path tempFolderPath = tempFolder.getRoot().toPath();\\n[ADD] writeToFile(TINY, tempFolder, \"readAllTiny1.zip\", ZIP);\\n[ADD] writeToFile(TINY, tempFolder, \"readAllTiny2.txt\", UNCOMPRESSED);\\n[ADD] writeToFile(LARGE, tempFolder, \"readAllLarge1.zip\", ZIP);\\n[ADD] writeToFile(LARGE, tempFolder, \"readAllLarge2.txt\", UNCOMPRESSED);\\n[ADD] PCollection<String> lines =\\n         p.apply(\\n[DEL] Create.of(\\n[DEL] tempFolder.resolve(\"readAllTiny*\").toString(),\\n[DEL] tempFolder.resolve(\"readAllLarge*\").toString()))\\n[DEL] .apply(TextIO.readAll().withCompression(AUTO));\\n[DEL] PAssert.that(lines).containsInAnyOrder(Iterables.concat(TINY, TINY, LARGE, LARGE));\\n[DEL] p.run();\\n[DEL] }\\n[ADD] Create.of(\\n[ADD] tempFolderPath.resolve(\"readAllTiny*\").toString(),\\n[ADD] tempFolderPath.resolve(\"readAllLarge*\").toString()))\\n[ADD] .apply(TextIO.readAll().withCompression(AUTO));\\n[ADD] PAssert.that(lines).containsInAnyOrder(Iterables.concat(TINY, TINY, LARGE, LARGE));\\n[ADD] p.run();\\n[ADD] }\\n \\n[DEL] @Test\\n[DEL] @Category(NeedsRunner.class)\\n[DEL] public void testReadFiles() throws IOException {\\n[DEL] writeToFile(TINY, \"readAllTiny1.zip\", ZIP);\\n[DEL] writeToFile(TINY, \"readAllTiny2.txt\", UNCOMPRESSED);\\n[DEL] writeToFile(LARGE, \"readAllLarge1.zip\", ZIP);\\n[DEL] writeToFile(LARGE, \"readAllLarge2.txt\", UNCOMPRESSED);\\n[DEL] PCollection<String> lines =\\n[ADD] @Test\\n[ADD] @Category(NeedsRunner.class)\\n[ADD] public void testReadFiles() throws IOException {\\n[ADD] Path tempFolderPath = tempFolder.getRoot().toPath();\\n[ADD] writeToFile(TINY, tempFolder, \"readAllTiny1.zip\", ZIP);\\n[ADD] writeToFile(TINY, tempFolder, \"readAllTiny2.txt\", UNCOMPRESSED);\\n[ADD] writeToFile(LARGE, tempFolder, \"readAllLarge1.zip\", ZIP);\\n[ADD] writeToFile(LARGE, tempFolder, \"readAllLarge2.txt\", UNCOMPRESSED);\\n[ADD] PCollection<String> lines =\\n         p.apply(\\n[DEL] Create.of(\\n[DEL] tempFolder.resolve(\"readAllTiny*\").toString(),\\n[DEL] tempFolder.resolve(\"readAllLarge*\").toString()))\\n[DEL] .apply(FileIO.matchAll())\\n[DEL] .apply(FileIO.readMatches().withCompression(AUTO))\\n[DEL] .apply(TextIO.readFiles().withDesiredBundleSizeBytes(10));\\n[DEL] PAssert.that(lines).containsInAnyOrder(Iterables.concat(TINY, TINY, LARGE, LARGE));\\n[DEL] p.run();\\n[DEL] }\\n[ADD] Create.of(\\n[ADD] tempFolderPath.resolve(\"readAllTiny*\").toString(),\\n[ADD] tempFolderPath.resolve(\"readAllLarge*\").toString()))\\n[ADD] .apply(FileIO.matchAll())\\n[ADD] .apply(FileIO.readMatches().withCompression(AUTO))\\n[ADD] .apply(TextIO.readFiles().withDesiredBundleSizeBytes(10));\\n[ADD] PAssert.that(lines).containsInAnyOrder(Iterables.concat(TINY, TINY, LARGE, LARGE));\\n[ADD] p.run();\\n[ADD] }\\n \\n[DEL] @Test\\n[DEL] @Category({NeedsRunner.class, UsesSplittableParDo.class})\\n[DEL] public void testReadWatchForNewFiles() throws IOException, InterruptedException {\\n[DEL] final Path basePath = tempFolder.resolve(\"readWatch\");\\n[DEL] basePath.toFile().mkdir();\\n[DEL] PCollection<String> lines =\\n[DEL] p.apply(\\n[DEL] TextIO.read()\\n[DEL] .from(basePath.resolve(\"*\").toString())\\n[DEL] // Make sure that compression type propagates into readAll()\\n[DEL] .withCompression(ZIP)\\n[DEL] .watchForNewFiles(\\n[DEL] Duration.millis(100),\\n[DEL] Watch.Growth.<String>afterTimeSinceNewOutput(Duration.standardSeconds(3))));\\n[DEL] \\n[DEL] Thread writer =\\n[ADD] @Test\\n[ADD] @Category({NeedsRunner.class, UsesSplittableParDo.class})\\n[ADD] public void testReadWatchForNewFiles() throws IOException, InterruptedException {\\n[ADD] final Path basePath = tempFolder.getRoot().toPath().resolve(\"readWatch\");\\n[ADD] basePath.toFile().mkdir();\\n[ADD] PCollection<String> lines =\\n[ADD] p.apply(\\n[ADD] TextIO.read()\\n[ADD] .from(basePath.resolve(\"*\").toString())\\n[ADD] // Make sure that compression type propagates into readAll()\\n[ADD] .withCompression(ZIP)\\n[ADD] .watchForNewFiles(\\n[ADD] Duration.millis(100),\\n[ADD] Watch.Growth.<String>afterTimeSinceNewOutput(Duration.standardSeconds(3))));\\n[ADD] \\n[ADD] Thread writer =\\n         new Thread() {\\n           @Override\\n           public void run() {\\n             try {\\n               Thread.sleep(1000);\\n               writeToFile(\\n[DEL] Arrays.asList(\"a.1\", \"a.2\"),\\n[DEL] basePath.resolve(\"fileA\").toString(),\\n[DEL] ZIP);\\n[ADD] Arrays.asList(\"a.1\", \"a.2\"),\\n[ADD] tempFolder,\\n[ADD] basePath.resolve(\"fileA\").toString(),\\n[ADD] ZIP);\\n               Thread.sleep(300);\\n               writeToFile(\\n[DEL] Arrays.asList(\"b.1\", \"b.2\"),\\n[DEL] basePath.resolve(\"fileB\").toString(),\\n[DEL] ZIP);\\n[ADD] Arrays.asList(\"b.1\", \"b.2\"),\\n[ADD] tempFolder,\\n[ADD] basePath.resolve(\"fileB\").toString(),\\n[ADD] ZIP);\\n               Thread.sleep(300);\\n               writeToFile(\\n[DEL] Arrays.asList(\"c.1\", \"c.2\"),\\n[DEL] basePath.resolve(\"fileC\").toString(),\\n[DEL] ZIP);\\n[ADD] Arrays.asList(\"c.1\", \"c.2\"),\\n[ADD] tempFolder,\\n[ADD] basePath.resolve(\"fileC\").toString(),\\n[ADD] ZIP);\\n             } catch (IOException | InterruptedException e) {\\n               throw new RuntimeException(e);\\n             }\\n           }\\n         };\\n[DEL] writer.start();\\n[ADD] writer.start();\\n \\n[DEL] PAssert.that(lines).containsInAnyOrder(\"a.1\", \"a.2\", \"b.1\", \"b.2\", \"c.1\", \"c.2\");\\n[DEL] p.run();\\n[ADD] PAssert.that(lines).containsInAnyOrder(\"a.1\", \"a.2\", \"b.1\", \"b.2\", \"c.1\", \"c.2\");\\n[ADD] p.run();\\n \\n[DEL] writer.join();\\n[ADD] writer.join();\\n[ADD] }\\n   }\\n }',\n",
              " ' \\t\\t}\\n \\t}\\n \\n[ADD] private static void augmentWithSystemProperties(Properties properties) {\\n[ADD] for (String key : LauncherConstants.LAUNCHER_PROPERTY_KEYS) {\\n[ADD] String value = System.getProperty(key);\\n[ADD] if (value == null)\\n[ADD] continue;\\n[ADD] \\n[ADD] properties.put(key, value);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n \\tstatic void load(final InputStream in, Properties properties) throws UnsupportedEncodingException, IOException {\\n \\t\\tInputStreamReader ir = new InputStreamReader(in, \"UTF-8\");\\n \\t\\ttry {',\n",
              " \"    * @private\\n    */\\n   getInitialPageId_(firstPageEl) {\\n[ADD] const pages = this.element.querySelectorAll('amp-story-page');\\n     const isActualPage = pageId =>\\n[DEL] findIndex(this.pages_, page => page.element.id === pageId) >= 0;\\n[ADD] findIndex(pages, page => page.id === pageId) >= 0;\\n[ADD] \\n     const historyPage = /** @type {string} */ (getHistoryState(\\n       this.win,\\n       HistoryState.PAGE_ID\",\n",
              " ' \\t\\t * \\t\\t\\t\\t to avoid extra type checking, the parameter is {@link MethodHandle}. If we need to\\n \\t\\t * \\t\\t\\t\\t handle {@link VarHandle} in the future, the type can be {@link Object}.\\n \\t\\t * @throws IllegalAccessException If the member is not accessible.\\n[ADD] * @throws IllegalAccessError If a handle argument or return type is not accessible.\\n \\t\\t */\\n \\t\\tprivate void checkAccess(Class<?> targetClass, String name, int memberModifiers, MethodHandle handle) throws IllegalAccessException {\\n \\t\\t\\tcheckClassAccess(targetClass);\\n \\n[ADD] /*[IF Sidecar19-SE]*/\\n[ADD] if (null != handle) {\\n[ADD] MethodType type = handle.type();\\n[ADD] Module accessModule = accessClass.getModule();\\n[ADD] \\n[ADD] try {\\n[ADD] checkClassModuleVisibility(accessMode, accessModule, type.returnType);\\n[ADD] for (Class<?> c: type.arguments) {\\n[ADD] checkClassModuleVisibility(accessMode, accessModule, c);\\n[ADD] }\\n[ADD] } catch (IllegalAccessException exc) {\\n[ADD] IllegalAccessError err = new IllegalAccessError(exc.getMessage());\\n[ADD] err.initCause(exc);\\n[ADD] throw err;\\n[ADD] }\\n[ADD] }\\n[ADD] /*[ENDIF]*/\\n \\t\\t\\tif (Modifier.isPublic(memberModifiers)) {\\n \\t\\t\\t\\t/* checkClassAccess already determined that we have more than \"no access\" (public access) */\\n \\t\\t\\t\\treturn;',\n",
              " '  * interface does not guarantee thread safeness. Check each particular implementation\\n  * for information about that\\n  */\\n[DEL] public interface Consumer<T> extends Closeable, ProvidesTotalHint\\n[ADD] public interface Consumer<T> extends Closeable\\n {\\n \\n     /**',\n",
              " '  */\\n package org.rstudio.core.client;\\n \\n[ADD] import java.util.Set;\\n[ADD] import java.util.TreeSet;\\n[ADD] \\n import com.google.gwt.resources.client.ImageResource;\\n import com.google.gwt.safehtml.shared.SafeHtml;\\n import com.google.gwt.safehtml.shared.SafeHtmlBuilder;',\n",
              " '       rnwContext_ = rnwContext;\\n       docDisplay_ = docDisplay;\\n       canAutoPopup_ = canAutoPopup;\\n[ADD] helpStrategy_ = helpStrategy;\\n       \\n       input_.addBlurHandler(new BlurHandler() {\\n          public void onBlur(BlurEvent event)',\n",
              " '             parameters.setPath(hostPath.replaceFirst(\"/\", \"\"));\\n             parameters.setUserInfo(userInfo);\\n         } else if (scheme.equalsIgnoreCase(\"PreSetup\")) {\\n[DEL] if (StringUtils.isNotBlank(hypervisorType) && HypervisorType.getType(hypervisorType).equals(HypervisorType.VMware)) {\\n[ADD] if (hypervisorType.equals(HypervisorType.VMware)) {\\n                 validateVcenterDetails(zoneId, podId, clusterId,storageHost);\\n             }\\n             parameters.setType(StoragePoolType.PreSetup);',\n",
              " '   }\\n \\n   @Subscribe\\n[DEL] public void handleDeleteJobConfigArrival(DeleteJobConfigArrivalEvent deleteJobArrival) {\\n[ADD] public void handleDeleteJobConfigArrival(DeleteJobConfigArrivalEvent deleteJobArrival) throws InterruptedException {\\n     LOGGER.info(\"Received delete for job configuration of job \" + deleteJobArrival.getJobName());\\n     try {\\n       unscheduleJob(deleteJobArrival.getJobName());\\n[ADD] Properties jobConfig = deleteJobArrival.getJobConfig();\\n[ADD] if (PropertiesUtils.getPropAsBoolean(jobConfig, GobblinClusterConfigurationKeys.CANCEL_RUNNING_JOB_ON_DELETE,\\n[ADD] GobblinClusterConfigurationKeys.DEFAULT_CANCEL_RUNNING_JOB_ON_DELETE)) {\\n[ADD] LOGGER.info(\"Cancelling workflow: {}\", deleteJobArrival.getJobName());\\n[ADD] TaskDriver taskDriver = new TaskDriver(this.jobHelixManager);\\n[ADD] taskDriver.waitToStop(deleteJobArrival.getJobName(), this.helixJobStopTimeoutSeconds);\\n[ADD] LOGGER.info(\"Stopped workflow: {}\", deleteJobArrival.getJobName());\\n[ADD] }\\n     } catch (JobException je) {\\n       LOGGER.error(\"Failed to unschedule job \" + deleteJobArrival.getJobName());\\n     }',\n",
              " ' \\n package io.opentelemetry.javaagent.instrumentation.netty.v4_1;\\n \\n[DEL] import static io.opentelemetry.javaagent.instrumentation.netty.v4_1.client.NettyClientSingletons.connectInstrumenter;\\n[ADD] import static io.opentelemetry.javaagent.instrumentation.netty.v4_1.client.NettyClientSingletons.connectionInstrumenter;\\n[ADD] import static net.bytebuddy.matcher.ElementMatchers.isConstructor;\\n import static net.bytebuddy.matcher.ElementMatchers.named;\\n import static net.bytebuddy.matcher.ElementMatchers.takesArgument;\\n[ADD] import static net.bytebuddy.matcher.ElementMatchers.takesArguments;\\n \\n[DEL] import io.netty.channel.ChannelFuture;\\n[ADD] import io.netty.bootstrap.Bootstrap;\\n[ADD] import io.netty.channel.ChannelPromise;\\n[ADD] import io.netty.resolver.AddressResolverGroup;\\n[ADD] import io.netty.resolver.DefaultAddressResolverGroup;\\n import io.opentelemetry.context.Context;\\n import io.opentelemetry.context.Scope;\\n import io.opentelemetry.javaagent.extension.instrumentation.TypeInstrumentation;\\n import io.opentelemetry.javaagent.extension.instrumentation.TypeTransformer;\\n import io.opentelemetry.javaagent.instrumentation.api.Java8BytecodeBridge;\\n[DEL] import io.opentelemetry.javaagent.instrumentation.netty.common.NettyConnectRequest;\\n[ADD] import io.opentelemetry.javaagent.instrumentation.netty.common.NettyConnectionRequest;\\n import io.opentelemetry.javaagent.instrumentation.netty.common.client.ConnectionCompleteListener;\\n[ADD] import io.opentelemetry.javaagent.instrumentation.netty.v4_1.client.InstrumentedAddressResolverGroup;\\n import java.net.SocketAddress;\\n import net.bytebuddy.asm.Advice;\\n import net.bytebuddy.description.type.TypeDescription;',\n",
              " ' \\n             HasUnsavedChanges = true;\\n \\n[ADD] if (node is CodeBlockNodeModel cbn\\n[ADD] && string.IsNullOrEmpty(cbn.Code)) return;\\n[ADD] \\n             RequestRun();\\n         }\\n ',\n",
              " '     thrown.expectMessage(\"must not be mutated\");\\n     pipeline.run();\\n   }\\n[ADD] \\n[ADD] @Test\\n[ADD] public void testAggregators() throws Exception {\\n[ADD] Pipeline pipeline = getPipeline();\\n[ADD] \\n[ADD] CountOddsFn countOdds = new CountOddsFn();\\n[ADD] pipeline\\n[ADD] .apply(Create.of(1, 3, 5, 7, 2, 4, 6, 8, 10, 12, 14, 20, 42, 68, 100))\\n[ADD] .apply(ParDo.of(countOdds));\\n[ADD] PipelineResult result = pipeline.run();\\n[ADD] \\n[ADD] AggregatorValues<Integer> values = result.getAggregatorValues(countOdds.aggregator);\\n[ADD] assertThat(values.getValuesAtSteps(),\\n[ADD] equalTo((Map<String, Integer>) ImmutableMap.<String, Integer>of(\"ParDo(CountOdds)\", 4)));\\n[ADD] }\\n[ADD] \\n[ADD] private static class CountOddsFn extends DoFn<Integer, Void> {\\n[ADD] @Override\\n[ADD] public void processElement(ProcessContext c) throws Exception {\\n[ADD] if (c.element() % 2 == 1) {\\n[ADD] aggregator.addValue(1);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] Aggregator<Integer, Integer> aggregator =\\n[ADD] createAggregator(\"odds\", new SumIntegerFn());\\n[ADD] }\\n }',\n",
              " '   public String excludedTypes = \"\";\\n   private final List<String> excludedTypesList = new ArrayList<>();\\n \\n[ADD] private final Set<TryStatementTree> tryWithResourcesTrees = new HashSet<>();\\n[ADD] private final Set<Tree> knownResources = new HashSet<>();\\n   private Type visitedMethodOwnerType;\\n \\n   private static final String JAVA_IO_AUTO_CLOSEABLE = \"java.lang.AutoCloseable\";',\n",
              " ' \\n     @property\\n     def installed(self):\\n[DEL] return os.path.isdir(self.prefix)\\n[ADD] \\n[ADD] has_prefix = os.path.isdir(self.prefix)\\n[ADD] try:\\n[ADD] # If the spec is in the DB, check the installed\\n[ADD] # attribute of the record\\n[ADD] rec = spack.store.db.get_record(self.spec)\\n[ADD] db_says_installed = rec.installed\\n[ADD] except KeyError:\\n[ADD] # If the spec is not in the DB, the method\\n[ADD] #  above raises a Key error\\n[ADD] db_says_installed = False\\n[ADD] \\n[ADD] return has_prefix and db_says_installed\\n \\n     @property\\n     def prefix(self):',\n",
              " ' \\t}\\n \\n \\tpublic JdbcOutboundGateway(JdbcOperations jdbcOperations, String updateQuery, String selectQuery) {\\n[DEL] \\n \\t\\tAssert.notNull(jdbcOperations, \"\\'jdbcOperations\\' must not be null.\");\\n \\n \\t\\tif (!StringUtils.hasText(updateQuery) && !StringUtils.hasText(selectQuery)) {\\n[DEL] throw new IllegalArgumentException(\"The \\'updateQuery\\' and the \\'selectQuery\\' must not both be null or empty.\");\\n[ADD] throw new IllegalArgumentException(\\n[ADD] \"The \\'updateQuery\\' and the \\'selectQuery\\' must not both be null or empty.\");\\n \\t\\t}\\n \\n \\t\\tif (StringUtils.hasText(selectQuery)) {\\n \\t\\t\\tthis.poller = new JdbcPollingChannelAdapter(jdbcOperations, selectQuery);\\n[DEL] this.poller.setMaxRowsPerPoll(1);\\n[ADD] this.poller.setMaxRows(1);\\n \\t\\t}\\n \\t\\telse {\\n \\t\\t\\tthis.poller = null;',\n",
              " '           int refIndexKey = call.getArgList().get(0);\\n           int refIndexValue = call.getArgList().get(1);\\n \\n[ADD] FieldTypeDescriptor keyDescriptor =\\n[ADD] sourceSchema.getField(refIndexKey).getTypeDescriptor();\\n           BeamSqlInputRefExpression sourceExpKey = new BeamSqlInputRefExpression(\\n[DEL] CalciteUtils.getFieldCalciteType(sourceRowType, refIndexKey), refIndexKey);\\n[ADD] CalciteUtils.toSqlTypeName(keyDescriptor.getType(), keyDescriptor.getMetadata()),\\n[ADD] refIndexKey);\\n[ADD] \\n[ADD] FieldTypeDescriptor valueDescriptor =\\n[ADD] sourceSchema.getField(refIndexValue).getTypeDescriptor();\\n           BeamSqlInputRefExpression sourceExpValue = new BeamSqlInputRefExpression(\\n[DEL] CalciteUtils.getFieldCalciteType(sourceRowType, refIndexValue), refIndexValue);\\n[ADD] CalciteUtils.toSqlTypeName(valueDescriptor.getType(), valueDescriptor.getMetadata()),\\n[ADD] refIndexValue);\\n \\n           sourceFieldExps.add(KV.of(sourceExpKey, sourceExpValue));\\n         } else {\\n           int refIndex = call.getArgList().size() > 0 ? call.getArgList().get(0) : 0;\\n[ADD] FieldTypeDescriptor typeDescriptor = sourceSchema.getField(refIndex).getTypeDescriptor();\\n           BeamSqlInputRefExpression sourceExp = new BeamSqlInputRefExpression(\\n[DEL] CalciteUtils.getFieldCalciteType(sourceRowType, refIndex), refIndex);\\n[ADD] CalciteUtils.toSqlTypeName(typeDescriptor.getType(), typeDescriptor.getMetadata()),\\n[ADD] refIndex);\\n           sourceFieldExps.add(sourceExp);\\n         }\\n \\n[DEL] SqlTypeCoder outFieldType = CalciteUtils.toCoder(call.type.getSqlTypeName());\\n[DEL] fields.add(RowType.newField(call.name, outFieldType));\\n[ADD] FieldTypeDescriptor typeDescriptor = CalciteUtils.toFieldTypeDescriptor(call.type);\\n[ADD] fields.add(Schema.Field.of(call.name, typeDescriptor));\\n \\n         switch (call.getAggregation().getName()) {\\n           case \"COUNT\":',\n",
              " ' \\t\\t\\t\\t\\t\\t\\t\\tthrow (RuntimeException) e;\\n \\t\\t\\t\\t\\t\\t\\t}\\n \\t\\t\\t\\t\\t\\t\\telse {\\n[DEL] throw new MessageHandlingException(new ErrorMessage(e));\\n[ADD] throw new MessageHandlingException(new ErrorMessage(e), null);\\n \\t\\t\\t\\t\\t\\t\\t}\\n \\t\\t\\t\\t\\t\\t}\\n \\t\\t\\t\\t\\t}',\n",
              " ' import org.mule.api.transformer.Transformer;\\n import org.mule.api.transformer.TransformerException;\\n import org.mule.api.transformer.TransformerMessagingException;\\n[DEL] import org.mule.api.transport.OutputHandler;\\n import org.mule.api.transport.PropertyScope;\\n import org.mule.config.MuleManifest;\\n import org.mule.config.i18n.CoreMessages;',\n",
              " '             });\\n          }\\n       };\\n[DEL] \\n[ADD] \\n       view_.getChangelistTable().addSelectionChangeHandler(new SelectionChangeEvent.Handler()\\n       {\\n          @Override',\n",
              " \"     });\\n   }\\n \\n[ADD] /**\\n[ADD] * Sets the pageview state token associated with the slot. Token does not\\n[ADD] * expire.\\n[ADD] * @param {?string} token\\n[ADD] */\\n[ADD] setPageviewStateToken(token) {\\n[ADD] pageviewStateTokens[token] = this;\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Checks for the presence of a pageview token in the module level object\\n[ADD] * and removes it if present.\\n[ADD] */\\n[ADD] removePageviewStateToken() {\\n[ADD] for (const token in pageviewStateTokens) {\\n[ADD] if (pageviewStateTokens[token] == this) {\\n[ADD] delete pageviewStateTokens[token];\\n[ADD] break;\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] \\n   /** @override */\\n   getA4aAnalyticsVars(analyticsTrigger) {\\n     return getCsiAmpAnalyticsVariables(analyticsTrigger, this, this.qqid_);\\n[DEL] }\\n[ADD] }\\n \\n[DEL] /** @override */\\n[DEL] getA4aAnalyticsConfig() {\\n[DEL] return getCsiAmpAnalyticsConfig();\\n[DEL] }\\n[ADD] /** @override */\\n[ADD] getA4aAnalyticsConfig() { return getCsiAmpAnalyticsConfig(); }\\n }\\n \\n AMP.extension(TAG, '0.1', AMP => {\",\n",
              " ' import org.jdom.Namespace;\\n import org.jdom.Text;\\n \\n[DEL] import java.util.ArrayList;\\n[DEL] import java.util.HashMap;\\n[DEL] import java.util.Iterator;\\n[DEL] import java.util.List;\\n[DEL] import java.util.Map;\\n[ADD] import java.util.*;\\n \\n import jeeves.constants.Jeeves;\\n import jeeves.server.ServiceConfig;\\n import jeeves.server.context.ServiceContext;\\n import jeeves.xlink.XLink;\\n \\n[ADD] import static org.fao.geonet.schema.iso19139.ISO19139Namespaces.GCO;\\n[ADD] import static org.fao.geonet.schema.iso19139.ISO19139Namespaces.GMD;\\n[ADD] import static org.fao.geonet.schema.iso19139.ISO19139Namespaces.SRV;\\n[ADD] \\n /**\\n  * Created by francois on 11/03/16.\\n  */',\n",
              " '    * @return the updated configuration.\\n    */\\n   public SchedulerConfig withMaxConcurrentTasks(int maxConcurrentTasks) {\\n[DEL] this.maxConcurrentTasks = maxConcurrentTasks;\\n[DEL] return this;\\n[ADD] return new SchedulerConfig(maxConcurrentTasks, schedulerName, rejectionAction, shutdownTimeoutMillis);\\n   }\\n \\n   /**',\n",
              " '         if (vmDetails != null) {\\n             Map<String, String> resourceDetails = new HashMap<String, String>();\\n             for (UserVmDetailVO userVmDetailVO : vmDetails) {\\n[ADD] if (userVmDetailVO.getName().startsWith(VmDetailConstants.KEY_PAIR_NAMES)) {\\n[ADD] s_logger.info(userVmDetailVO.getValue());\\n[ADD] userVmResponse.setKeyPairNames(userVmDetailVO.getValue());\\n[ADD] }\\n                 if (!userVmDetailVO.getName().startsWith(ApiConstants.PROPERTIES) ||\\n                         (UserVmManager.DisplayVMOVFProperties.value() && userVmDetailVO.getName().startsWith(ApiConstants.PROPERTIES))) {\\n                     resourceDetails.put(userVmDetailVO.getName(), userVmDetailVO.getValue());',\n",
              " '             // This should never happen;\\n             logger.error(\"Got exception when trying to fetch the underlying result from AsyncRpcResult.\", e);\\n         }\\n[DEL] return new AppResponse();\\n[ADD] \\n[ADD] ConsumerModel consumerModel = ApplicationModel.getConsumerModel(invocation.getTargetServiceUniqueName());\\n[ADD] String methodName = invocation.getMethodName();\\n[ADD] String params = ReflectUtils.getDesc(invocation.getParameterTypes());\\n[ADD] MethodDescriptor method = consumerModel.getServiceModel().getMethod(methodName, params);\\n[ADD] return new AppResponse(ReflectUtils.defaultReturn(method.getReturnClass()));\\n     }\\n \\n     /**',\n",
              " '         ManagedChannelFactory.createDefault().forDescriptor(externalPayload.getEndpoint());\\n     BeamFnApi.StartWorkerResponse startWorkerResponse =\\n         BeamFnExternalWorkerPoolGrpc.newBlockingStub(managedChannel)\\n[ADD] .withWaitForReady()\\n             .startWorker(startWorkerRequest);\\n     if (!startWorkerResponse.getError().isEmpty()) {\\n       throw new RuntimeException(startWorkerResponse.getError());',\n",
              " '           .getDefault(DslResolvingContext.getDefault(extensionModels));\\n \\n       ComponentModel rootComponent = new ComponentModel.Builder()\\n[DEL] .setIdentifier(ComponentIdentifier.builder().withNamespace(CORE_PREFIX).withName(CORE_PREFIX).build()).build();\\n[DEL] this.muleComponentModels.add(rootComponent);\\n[ADD] .setIdentifier(ComponentIdentifier.builder()\\n[ADD] .withNamespace(CORE_PREFIX)\\n[ADD] .withName(CORE_PREFIX)\\n[ADD] .build())\\n[ADD] .build();\\n[ADD] \\n[ADD] AtomicBoolean atLeastOneComponentAdded = new AtomicBoolean(false);\\n \\n       artifactDeclaration.getGlobalElements().stream()\\n           .map(e -> elementFactory.create((ElementDeclaration) e))',\n",
              " ' import java.util.Collections;\\n import java.util.Comparator;\\n import java.util.Date;\\n[ADD] import java.util.LinkedList;\\n import java.util.List;\\n import java.util.Map;\\n import java.util.Set;',\n",
              " ' \\n \\t/**\\n \\t * Load the effective set from the properties\\n[DEL] *\\n \\t */\\n \\tMap<String,Set<String>> loadEffectiveSet() {\\n \\t\\tString effective = properties.getProperty(RUN_EFFECTIVE_INSTRUCTION);',\n",
              " '         assertApplicationDeploymentSuccess(applicationDeploymentListener, dummyDomainApp1FileBuilder.getId());\\n     }\\n \\n[ADD] @Test\\n[ADD] public void redeploysDomainZipRefreshesAppsButIfTheyWereStoppedTheyDoNotStart() throws Exception\\n[ADD] {\\n[ADD] addPackedDomainFromBuilder(dummyDomainFileBuilder);\\n[ADD] File dummyDomainFile = new File(domainsDir, dummyDomainFileBuilder.getZipPath());\\n[ADD] long firstFileTimestamp = dummyDomainFile.lastModified();\\n[ADD] \\n[ADD] addPackedAppFromBuilder(dummyDomainApp1FileBuilder);\\n[ADD] \\n[ADD] deploymentService.start();\\n[ADD] \\n[ADD] assertDeploymentSuccess(domainDeploymentListener, dummyDomainFileBuilder.getId());\\n[ADD] assertApplicationDeploymentSuccess(applicationDeploymentListener, dummyDomainApp1FileBuilder.getId());\\n[ADD] \\n[ADD] final Application app = findApp(dummyDomainApp1FileBuilder.getId(), 1);\\n[ADD] app.stop();\\n[ADD] \\n[ADD] reset(domainDeploymentListener);\\n[ADD] reset(applicationDeploymentListener);\\n[ADD] \\n[ADD] addPackedDomainFromBuilder(dummyDomainFileBuilder);\\n[ADD] alterTimestampIfNeeded(dummyDomainFile, firstFileTimestamp);\\n[ADD] \\n[ADD] assertUndeploymentSuccess(applicationDeploymentListener, dummyDomainApp1FileBuilder.getId());\\n[ADD] assertUndeploymentSuccess(domainDeploymentListener, dummyDomainFileBuilder.getId());\\n[ADD] assertDeploymentSuccess(applicationDeploymentListener, dummyDomainApp1FileBuilder.getId());\\n[ADD] assertStatus(dummyDomainApp1FileBuilder.getId(), CREATED);\\n[ADD] }\\n[ADD] \\n     @Test\\n     public void redeploysDomainZipDeployedAfterStartup() throws Exception\\n     {',\n",
              " ' \\t\\t\\tCONNECTION,\\n \\t\\t\\tCONTENT_LENGTH,\\n \\t\\t\\tCONTENT_TYPE,\\n[ADD] MessageHeaders.CONTENT_TYPE,\\n \\t\\t\\tCOOKIE,\\n \\t\\t\\tDATE,\\n \\t\\t\\tEXPECT,',\n",
              " '   }\\n \\n   protected List<FlowStatus> asFlowStatuses(List<FlowExecutionJobStateGrouping> flowExecutionGroupings) {\\n[DEL] return flowExecutionGroupings.stream().map(exec ->\\n[DEL] new FlowStatus(exec.getFlowName(), exec.getFlowGroup(), exec.getFlowExecutionId(),\\n[DEL] asJobStatuses(exec.getJobStates().stream().sorted(\\n[DEL] // rationalized order, to facilitate test assertions\\n[DEL] Comparator.comparing(this::getJobGroup).thenComparing(this::getJobName).thenComparing(this::getJobExecutionId)\\n[DEL] ).collect(Collectors.toList()))))\\n[DEL] .collect(Collectors.toList());\\n[ADD] return flowExecutionGroupings.stream().map(exec -> {\\n[ADD] List<JobStatus> jobStatuses = ImmutableList.copyOf(asJobStatuses(exec.getJobStates().stream().sorted(\\n[ADD] // rationalized order, to facilitate test assertions\\n[ADD] Comparator.comparing(this::getJobGroup).thenComparing(this::getJobName).thenComparing(this::getJobExecutionId)\\n[ADD] ).collect(Collectors.toList())));\\n[ADD] Iterator<JobStatus> jobStatusIterator = jobStatuses.iterator();\\n[ADD] // duplicate copy of the iterator is required due to un re-parsing nature of the Iterator\\n[ADD] Iterator<JobStatus> jobStatusIterator2 = jobStatuses.iterator();\\n[ADD] return new FlowStatus(exec.getFlowName(), exec.getFlowGroup(), exec.getFlowExecutionId(), jobStatusIterator,\\n[ADD] getFlowStatusFromJobStatuses(dagManagerEnabled, jobStatusIterator2));\\n[ADD] }).collect(Collectors.toList());\\n   }\\n \\n   @AllArgsConstructor',\n",
              " ' \\tpublic static final String SEQUENCE_SIZE = \"sequenceSize\";\\n \\n \\tpublic static final String SEQUENCE_DETAILS = \"sequenceDetails\";\\n[ADD] \\n[ADD] public static final String CONTENT_TYPE = \"content-type\";\\n \\n \\n \\tprivate final Map<String, Object> headers;',\n",
              " ' \\n         //creating the command\\n         UploadSslCertCmd uploadCmd = new UploadSslCertCmdExtn();\\n[DEL] Class<?> _class = uploadCmd.getClass().getSuperclass();\\n[ADD] final Class<?> klazz = uploadCmd.getClass().getSuperclass();\\n \\n[DEL] Field certField = _class.getDeclaredField(\"cert\");\\n[ADD] final Field certField = klazz.getDeclaredField(\"cert\");\\n         certField.setAccessible(true);\\n         certField.set(uploadCmd, cert);\\n \\n[DEL] Field keyField = _class.getDeclaredField(\"key\");\\n[ADD] final Field keyField = klazz.getDeclaredField(\"key\");\\n         keyField.setAccessible(true);\\n         keyField.set(uploadCmd, key);\\n \\n[ADD] uploadCmd = Mockito.spy(uploadCmd);\\n         certService.uploadSslCert(uploadCmd);\\n[ADD] Mockito.verify(uploadCmd, Mockito.atLeastOnce()).getAccountName();\\n[ADD] Mockito.verify(uploadCmd, Mockito.times(1)).getCert();\\n     }\\n \\n[DEL] \\n     @Test\\n     public void runUploadSslCertBadChain() throws IOException, IllegalAccessException, NoSuchFieldException {\\n         Assume.assumeTrue(isOpenJdk() || isJCEInstalled());\\n \\n[DEL] String certFile = URLDecoder.decode(getClass().getResource(\"/certs/rsa_ca_signed.crt\").getFile(),Charset.defaultCharset().name());\\n[DEL] String keyFile = URLDecoder.decode(getClass().getResource(\"/certs/rsa_ca_signed.key\").getFile(),Charset.defaultCharset().name());\\n[DEL] String chainFile = URLDecoder.decode(getClass().getResource(\"/certs/rsa_self_signed.crt\").getFile(),Charset.defaultCharset().name());\\n[ADD] final String certFile = URLDecoder.decode(getClass().getResource(\"/certs/rsa_ca_signed.crt\").getFile(),Charset.defaultCharset().name());\\n[ADD] final String keyFile = URLDecoder.decode(getClass().getResource(\"/certs/rsa_ca_signed.key\").getFile(),Charset.defaultCharset().name());\\n[ADD] final String chainFile = URLDecoder.decode(getClass().getResource(\"/certs/rsa_self_signed.crt\").getFile(),Charset.defaultCharset().name());\\n \\n[DEL] String cert = readFileToString(new File(certFile));\\n[DEL] String key = readFileToString(new File(keyFile));\\n[DEL] String chain = readFileToString(new File(chainFile));\\n[ADD] final String cert = readFileToString(new File(certFile));\\n[ADD] final String key = readFileToString(new File(keyFile));\\n[ADD] final String chain = readFileToString(new File(chainFile));\\n \\n[DEL] CertServiceImpl certService = new CertServiceImpl();\\n[ADD] final CertServiceImpl certService = new CertServiceImpl();\\n \\n         //setting mock objects\\n         certService._accountMgr = Mockito.mock(AccountManager.class);\\n[DEL] Account account = new AccountVO(\"testaccount\", 1, \"networkdomain\", (short)0, UUID.randomUUID().toString());\\n[ADD] final Account account = new AccountVO(\"testaccount\", 1, \"networkdomain\", (short)0, UUID.randomUUID().toString());\\n         when(certService._accountMgr.getAccount(anyLong())).thenReturn(account);\\n \\n         certService._domainDao = Mockito.mock(DomainDao.class);\\n[DEL] DomainVO domain = new DomainVO(\"networkdomain\", 1L, 1L, \"networkdomain\");\\n[ADD] final DomainVO domain = new DomainVO(\"networkdomain\", 1L, 1L, \"networkdomain\");\\n         when(certService._domainDao.findByIdIncludingRemoved(anyLong())).thenReturn(domain);\\n \\n         certService._sslCertDao = Mockito.mock(SslCertDao.class);\\n         when(certService._sslCertDao.persist(any(SslCertVO.class))).thenReturn(new SslCertVO());\\n \\n         //creating the command\\n[DEL] UploadSslCertCmd uploadCmd = new UploadSslCertCmdExtn();\\n[DEL] Class<?> _class = uploadCmd.getClass().getSuperclass();\\n[ADD] final UploadSslCertCmd uploadCmd = new UploadSslCertCmdExtn();\\n[ADD] final Class<?> klazz = uploadCmd.getClass().getSuperclass();\\n \\n[DEL] Field certField = _class.getDeclaredField(\"cert\");\\n[ADD] final Field certField = klazz.getDeclaredField(\"cert\");\\n         certField.setAccessible(true);\\n         certField.set(uploadCmd, cert);\\n \\n[DEL] Field keyField = _class.getDeclaredField(\"key\");\\n[ADD] final Field keyField = klazz.getDeclaredField(\"key\");\\n         keyField.setAccessible(true);\\n         keyField.set(uploadCmd, key);\\n \\n[DEL] Field chainField = _class.getDeclaredField(\"chain\");\\n[ADD] final Field chainField = klazz.getDeclaredField(\"chain\");\\n         chainField.setAccessible(true);\\n         chainField.set(uploadCmd, chain);\\n \\n         try {\\n             certService.uploadSslCert(uploadCmd);\\n             fail(\"The chain given is not the correct chain for the certificate\");\\n[DEL] } catch (Exception e) {\\n[ADD] } catch (final Exception e) {\\n             assertTrue(e.getMessage().contains(\"Invalid certificate chain\"));\\n         }\\n     }',\n",
              " ' \\n package gobblin.source.extractor.extract.kafka;\\n \\n[ADD] import gobblin.source.extractor.limiter.LimiterConfigurationKeys;\\n import gobblin.source.workunit.MultiWorkUnit;\\n import java.io.IOException;\\n import java.util.ArrayList;',\n",
              " ' \\t@MongoDbAvailable\\n \\tpublic void validateMessageHandlingWithNamedCollection() throws Exception {\\n \\n[DEL] MongoDbFactory mongoDbFactory = this.prepareMongoFactory();\\n \\t\\tMongoDbStoringMessageHandler handler = new MongoDbStoringMessageHandler(mongoDbFactory);\\n \\t\\thandler.setCollectionNameExpression(new LiteralExpression(\"foo\"));\\n \\t\\thandler.setBeanFactory(mock(BeanFactory.class));',\n",
              " '     return toBuilder().setUdafDefinitions(newUdafs).build();\\n   }\\n \\n[ADD] public SqlTransform withErrorsTransformer(\\n[ADD] PTransform<PCollection<BeamCalcRelError>, POutput> errorsTransformer) {\\n[ADD] return toBuilder().setErrorsTransformer(errorsTransformer).build();\\n[ADD] }\\n[ADD] \\n   abstract Builder toBuilder();\\n \\n   static Builder builder() {',\n",
              " ' import com.google.common.base.Strings;\\n import com.google.common.collect.Lists;\\n \\n[ADD] import static org.apache.avro.SchemaCompatibility.*;\\n[ADD] import static org.apache.avro.SchemaCompatibility.SchemaCompatibilityType.*;\\n[ADD] \\n \\n /**\\n  * A Utils class for dealing with Avro objects',\n",
              " '           + \"record size estimate compute dynamically based on commit metadata. \"\\n           + \" This is critical in computing the insert parallelism and bin-packing inserts into small files.\");\\n \\n[ADD] public static final ConfigProperty<String> ARCHIVE_MAX_FILES = ConfigProperty\\n[ADD] .key(\"hoodie.archive.max.files\")\\n[ADD] .noDefaultValue()\\n[ADD] .withDocumentation(\"The numbers of kept archive files under archived.\");\\n[ADD] \\n[ADD] public static final ConfigProperty<String> ARCHIVE_AUTO_TRIM_ENABLE = ConfigProperty\\n[ADD] .key(\"hoodie.archive.auto.trim.enable\")\\n[ADD] .defaultValue(\"false\")\\n[ADD] .withDocumentation(\"WARNING: do not use this config unless you know what you\\'re doing. \"\\n[ADD] + \"If enabled, Hoodie will keep the most recent \" + ARCHIVE_MAX_FILES.key() + \" archive files and details of older archived instants will be deleted, \"\\n[ADD] + \"resulting in information loss in the archived timeline, which may affect tools like CLI and repair. \"\\n[ADD] + \"Only enable this if you hit severe performance issues for retrieving archived timeline.\");\\n[ADD] \\n[ADD] \\n[ADD] \\n   /** @deprecated Use {@link #CLEANER_POLICY} and its methods instead */\\n   @Deprecated\\n   public static final String CLEANER_POLICY_PROP = CLEANER_POLICY.key();',\n",
              " '     assertFieldOfType(record, \"ID\", testDatabase.getIdFieldMetaDataType());\\n     assertFieldOfType(record, \"POSITION\", testDatabase.getPositionFieldMetaDataType());\\n     assertFieldOfType(record, \"NAME\", typeBuilder.stringType().build());\\n[ADD] //The following assertion will fail in MySQL due that Blob is communicated as a byte[]\\n     assertFieldOfType(record, \"PICTURE\", typeLoader.load(Blob.class));\\n   }\\n ',\n",
              " '     return !( cause != null && SHORT_MESSAGE_EXCEPTIONS.contains( cause.getClass().getName() ) );\\n   }\\n \\n[ADD] /**\\n[ADD] * Returns the column name for a MariaDB field.\\n[ADD] *\\n[ADD] * @param dbMetaData\\n[ADD] * @param rsMetaData\\n[ADD] * @param index\\n[ADD] * @return The column label.\\n[ADD] * @throws KettleDatabaseException\\n[ADD] */\\n[ADD] @Override public String getLegacyColumnName( DatabaseMetaData dbMetaData, ResultSetMetaData rsMetaData, int index ) throws KettleDatabaseException {\\n[ADD] if ( dbMetaData == null ) {\\n[ADD] throw new KettleDatabaseException( BaseMessages.getString( PKG, \"MySQLDatabaseMeta.Exception.LegacyColumnNameNoDBMetaDataException\" ) );\\n[ADD] }\\n[ADD] \\n[ADD] if ( rsMetaData == null ) {\\n[ADD] throw new KettleDatabaseException( BaseMessages.getString( PKG, \"MySQLDatabaseMeta.Exception.LegacyColumnNameNoRSMetaDataException\" ) );\\n[ADD] }\\n[ADD] \\n[ADD] try {\\n[ADD] return rsMetaData.getColumnLabel( index );\\n[ADD] } catch ( Exception e ) {\\n[ADD] throw new KettleDatabaseException( String.format( \"%s: %s\", BaseMessages.getString( PKG, \"MySQLDatabaseMeta.Exception.LegacyColumnNameException\" ), e.getMessage() ), e );\\n[ADD] }\\n[ADD] }\\n }',\n",
              " ' \\n \\t/**\\n \\t * Wait until the JVM\\'s attach API has initialized\\n[DEL] * @return success if true, false if attach API is disabled or error\\n[ADD] * @return success if true, false if attach API is disabled or error, or does not initialize in 100 s.\\n \\t */\\n \\tpublic static boolean waitForAttachApiInitialization() {\\n \\t\\tboolean result = false;\\n[DEL] try {\\n[DEL] Class<?> attachHandlerClass = Class.forName(TargetManager.COM_IBM_TOOLS_ATTACH_TARGET_ATTACH_HANDLER);\\n[DEL] final Method waitForAttachApiInitialization = attachHandlerClass.getMethod(\"waitForAttachApiInitialization\");\\n[DEL] result = (boolean) waitForAttachApiInitialization.invoke(attachHandlerClass);\\n[DEL] } catch (ClassNotFoundException | NoSuchMethodException | SecurityException | IllegalAccessException | IllegalArgumentException | InvocationTargetException e) {\\n[DEL] logger.error(\"error waiting for attach API initialization: \"+e.getMessage());\\n[ADD] int tries = 100;\\n[ADD] RuntimeMXBean bean = (RuntimeMXBean) ManagementFactory.getRuntimeMXBean();\\n[ADD] while (tries > 0) {\\n[ADD] logger.debug(\"Poll attach API status\");\\n[ADD] if (bean.isAttachApiInitialized()) {\\n[ADD] result = true;\\n[ADD] logger.debug(\"attach API initialized\");\\n[ADD] break;\\n[ADD] } else if (bean.isAttachApiTerminated()) {\\n[ADD] logger.debug(\"attach API terminated\");\\n[ADD] result = false;\\n[ADD] break;\\n[ADD] }\\n[ADD] try {\\n[ADD] Thread.sleep(1000);\\n[ADD] } catch (InterruptedException e) {\\n[ADD] break;\\n[ADD] }\\n[ADD] --tries;\\n \\t\\t}\\n \\t\\treturn result;\\n \\t}',\n",
              " '    * Read config from properties file (`--props` option) and cmd line (`--hoodie-conf` option).\\n    */\\n   public static DFSPropertiesConfiguration readConfig(FileSystem fs, Path cfgPath, List<String> overriddenProps) {\\n[DEL] DFSPropertiesConfiguration conf;\\n[DEL] try {\\n[DEL] conf = new DFSPropertiesConfiguration(cfgPath.getFileSystem(fs.getConf()), cfgPath);\\n[DEL] } catch (Exception e) {\\n[DEL] conf = new DFSPropertiesConfiguration();\\n[DEL] LOG.warn(\"Unexpected error read props file at :\" + cfgPath, e);\\n[DEL] }\\n[DEL] \\n[ADD] DFSPropertiesConfiguration conf = new DFSPropertiesConfiguration(fs, cfgPath);\\n     try {\\n       if (!overriddenProps.isEmpty()) {\\n         LOG.info(\"Adding overridden properties to file properties.\");\\n[DEL] conf.addProperties(new BufferedReader(new StringReader(String.join(\"\\\\n\", overriddenProps))));\\n[ADD] conf.addPropsFromStream(new BufferedReader(new StringReader(String.join(\"\\\\n\", overriddenProps))));\\n       }\\n     } catch (IOException ioe) {\\n       throw new HoodieIOException(\"Unexpected error adding config overrides\", ioe);',\n",
              " ' \\n package org.apache.hudi.common.table.timeline;\\n \\n[ADD] import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\\n import org.apache.hudi.common.util.CollectionUtils;\\n import org.apache.hadoop.fs.FileStatus;\\n ',\n",
              " '     sourceResponseSendErrorType = errorTypeRepository.getErrorType(SOURCE_RESPONSE_SEND).get();\\n     sourceErrorResponseGenerateErrorType = errorTypeRepository.getErrorType(SOURCE_ERROR_RESPONSE_GENERATE).get();\\n     sourceErrorResponseSendErrorType = errorTypeRepository.getErrorType(SOURCE_ERROR_RESPONSE_SEND).get();\\n[ADD] \\n[ADD] if (processorInterceptorManager != null) {\\n[ADD] processorInterceptorManager.getInterceptorFactories().stream().forEach(interceptorFactory -> {\\n[ADD] ReactiveInterceptorSourceCallbackAdapter reactiveInterceptorAdapter =\\n[ADD] new ReactiveInterceptorSourceCallbackAdapter(interceptorFactory);\\n[ADD] try {\\n[ADD] muleContext.getInjector().inject(reactiveInterceptorAdapter);\\n[ADD] } catch (MuleException e) {\\n[ADD] throw new MuleRuntimeException(e);\\n[ADD] }\\n[ADD] additionalInterceptors.add(0, reactiveInterceptorAdapter);\\n[ADD] });\\n[ADD] }\\n   }\\n \\n   @Override',\n",
              " ' \\t\\t/**\\n \\t\\t * Set included classes\\n \\t\\t * \\n[DEL] * @param classNames\\n[DEL] *            the included class names\\n[ADD] * @param classNames the included class names\\n[ADD] * @throws IllegalArgumentException if the specified argument is\\n[ADD] *             {@code null}\\n \\t\\t */\\n \\t\\tdefault BundleSpecExportPackage include(String... classNames) {\\n[ADD] requireNonNull(classNames, \"\\'classNames\\' cannot be null\");\\n \\t\\t\\tdirective(\"include\", x().join(classNames));\\n \\t\\t\\treturn this;\\n \\t\\t}',\n",
              " '           + \"files from lake storage, before committing the write. Reduce this value, if the high number of tasks incur delays for smaller tables \"\\n           + \"or low latency writes.\");\\n \\n[ADD] public static final ConfigProperty<String> MARKERS_TYPE = ConfigProperty\\n[ADD] .key(\"hoodie.write.markers.type\")\\n[ADD] .defaultValue(MarkerType.DIRECT.toString())\\n[ADD] .sinceVersion(\"0.9.0\")\\n[ADD] .withDocumentation(\"Marker IO mode to use.  Two modes are supported: \"\\n[ADD] + \"- DIRECT: individual marker file corresponding to each data file is directly \"\\n[ADD] + \"created by writer. \"\\n[ADD] + \"- TIMELINE_SERVER_BASED: marker operations are all handled at the timeline service \"\\n[ADD] + \"which serves as a proxy.  New marker entries are batch processed and stored \"\\n[ADD] + \"in a limited number of underlying files for efficiency.\");\\n[ADD] \\n[ADD] public static final ConfigProperty<Integer> MARKERS_TIMELINE_SERVER_BASED_BATCH_NUM_THREADS = ConfigProperty\\n[ADD] .key(\"hoodie.markers.timeline_server_based.batch.num_threads\")\\n[ADD] .defaultValue(20)\\n[ADD] .sinceVersion(\"0.9.0\")\\n[ADD] .withDocumentation(\"Number of threads to use for batch processing marker \"\\n[ADD] + \"creation requests at the timeline service\");\\n[ADD] \\n[ADD] public static final ConfigProperty<Long> MARKERS_TIMELINE_SERVER_BASED_BATCH_INTERVAL_MS = ConfigProperty\\n[ADD] .key(\"hoodie.markers.timeline_server_based.batch.interval_ms\")\\n[ADD] .defaultValue(50L)\\n[ADD] .sinceVersion(\"0.9.0\")\\n[ADD] .withDocumentation(\"The batch interval in milliseconds for marker creation batch processing\");\\n[ADD] \\n   public static final ConfigProperty<String> MARKERS_DELETE_PARALLELISM = ConfigProperty\\n       .key(\"hoodie.markers.delete.parallelism\")\\n       .defaultValue(\"100\")',\n",
              " ' \\t */\\n \\tpublic static IDATA j9mem_check_tags(VoidPointer memoryPointer, long headerEyecatcher, long footerEyecatcher) throws J9MemTagCheckError \\n \\t{\\n[DEL] J9MemTagPointer headerTagAddress, footerTagAddress = J9MemTagPointer.NULL;\\n[DEL] \\n[DEL] headerTagAddress = j9mem_get_header_tag(memoryPointer);\\n[ADD] J9MemTagPointer headerTagAddress = j9mem_get_header_tag(memoryPointer);\\n[ADD] J9MemTagPointer footerTagAddress = J9MemTagPointer.NULL;\\n \\n \\t\\ttry {\\n \\t\\t\\tfooterTagAddress = j9mem_get_footer_tag(headerTagAddress);',\n",
              " '             addExtraConfig(vm, caller, extraConfig);\\n         }\\n \\n[ADD] if (cmd.getCopyImageTagsToVm()) {\\n[ADD] final ResourceTag.ResourceObjectType templateType = (_templateDao.findById(templateId).getFormat() == ImageFormat.ISO) ? ResourceTag.ResourceObjectType.ISO : ResourceTag.ResourceObjectType.Template;\\n[ADD] final List<? extends ResourceTag> resourceTags = resourceTagDao.listBy(templateId, templateType);\\n[ADD] for (ResourceTag resourceTag : resourceTags) {\\n[ADD] final ResourceTagVO copyTag = new ResourceTagVO(resourceTag.getKey(), resourceTag.getValue(), resourceTag.getAccountId(), resourceTag.getDomainId(), vm.getId(), ResourceTag.ResourceObjectType.UserVm, resourceTag.getCustomer(), vm.getUuid());\\n[ADD] resourceTagDao.persist(copyTag);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n         return vm;\\n     }\\n ',\n",
              " '     {\\n         return getConfigurationProvider()\\n                 .map(provider -> provider.get(event))\\n[DEL] .orElseGet(() -> {\\n[DEL] if (StringUtils.isBlank(configurationProviderName))\\n[DEL] {\\n[DEL] return extensionManager.getConfiguration(extensionModel, event);\\n[DEL] }\\n[DEL] return extensionManager.getConfiguration(configurationProviderName, event);\\n[DEL] });\\n[ADD] .orElseGet(() -> extensionManager.getConfiguration(extensionModel, event));\\n     }\\n \\n     private Optional<ConfigurationProvider<Object>> getConfigurationProvider()\\n     {\\n[DEL] Optional<ConfigurationProvider<Object>> provider = StringUtils.isBlank(configurationProviderName)\\n[DEL] ? extensionManager.getConfigurationProvider(extensionModel)\\n[DEL] : extensionManager.getConfigurationProvider(configurationProviderName);\\n[DEL] return provider;\\n[ADD] return StringUtils.isBlank(configurationProviderName) ? extensionManager.getConfigurationProvider(extensionModel)\\n[ADD] : extensionManager.getConfigurationProvider(configurationProviderName);\\n     }\\n \\n }',\n",
              " '          toolbar.addRightWidget(label);\\n       }\\n    }\\n[DEL] \\n[ADD] \\n[ADD] private ClickHandler[] getColumnViewClickHandlers() {\\n[ADD] ClickHandler handlers[] = {\\n[ADD] new ClickHandler()\\n[ADD] {\\n[ADD] @Override\\n[ADD] public void onClick(ClickEvent event)\\n[ADD] {\\n[ADD] firstColumnPage();\\n[ADD] }\\n[ADD] },\\n[ADD] new ClickHandler()\\n[ADD] {\\n[ADD] @Override\\n[ADD] public void onClick(ClickEvent event)\\n[ADD] {\\n[ADD] prevColumnPage();\\n[ADD] }\\n[ADD] },\\n[ADD] new ClickHandler()\\n[ADD] {\\n[ADD] @Override\\n[ADD] public void onClick(ClickEvent event)\\n[ADD] {\\n[ADD] nextColumnPage();\\n[ADD] }\\n[ADD] },\\n[ADD] new ClickHandler()\\n[ADD] {\\n[ADD] @Override\\n[ADD] public void onClick(ClickEvent event)\\n[ADD] {\\n[ADD] lastColumnPage();\\n[ADD] }\\n[ADD] }\\n[ADD] };\\n[ADD] \\n[ADD] return handlers;\\n[ADD] }\\n[ADD] private void addColumnControls(Toolbar toolbar)\\n[ADD] {\\n[ADD] colsLabel_ =\\n[ADD] new ToolbarLabel(\"Cols:\");\\n[ADD] colsLabel_.addStyleName(ThemeStyles.INSTANCE.toolbarInfoLabel());\\n[ADD] colsLabel_.setVisible(false);\\n[ADD] toolbar.addLeftWidget(colsLabel_);\\n[ADD] \\n[ADD] ClickHandler[] clickHandlers = getColumnViewClickHandlers();\\n[ADD] SimpleButton columnButton;\\n[ADD] columnViewButtons_ = new ArrayList<>();\\n[ADD] \\n[ADD] for (int i = 0; i < COLUMN_VIEW_BUTTONS.length; i++)\\n[ADD] {\\n[ADD] columnButton = new SimpleButton(COLUMN_VIEW_BUTTONS[i]);\\n[ADD] columnButton.addClickHandler(clickHandlers[i]);\\n[ADD] columnButton.setVisible(false);\\n[ADD] toolbar.addLeftWidget(columnButton);\\n[ADD] columnViewButtons_.add(columnButton);\\n[ADD] \\n[ADD] if (i == 1)\\n[ADD] {\\n[ADD] columnTextWidget_ = new DataTableColumnWidget(this::setOffsetAndMaxColumns);\\n[ADD] columnTextWidget_.setVisible(false);\\n[ADD] toolbar.addLeftWidget(columnTextWidget_);\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private void setColumnControlVisibility(boolean visible)\\n[ADD] {\\n[ADD] colsLabel_.setVisible(false);\\n[ADD] for (int i = 0; i < COLUMN_VIEW_BUTTONS.length; i++)\\n[ADD] {\\n[ADD] columnViewButtons_.get(i).setVisible(visible);\\n[ADD] }\\n[ADD] columnTextWidget_.setVisible(visible);\\n[ADD] }\\n[ADD] \\n[ADD] private CommandWith2Args<Integer, Integer> getDataTableColumnCallback()\\n[ADD] {\\n[ADD] return (offset, max) ->\\n[ADD] {\\n[ADD] // This is super weird but the type coming in is throwing crazy exceptions if\\n[ADD] // we treat it like a normal Integer. This cast-cleaning seems to fix it.\\n[ADD] int off = Integer.parseInt(\"\" + offset) + 1;\\n[ADD] int mx = Integer.parseInt(\"\" + max) + off - 1;\\n[ADD] columnTextWidget_.setValue(off + \" - \" + mx);\\n[ADD] \\n[ADD] setColumnControlVisibility(isLimitedColumnFrame());\\n[ADD] };\\n[ADD] }\\n[ADD] \\n    private WindowEx getWindow()\\n    {\\n       IFrameElementEx frameEl = (IFrameElementEx) host_.getDataTableFrame().getElement().cast();',\n",
              " '     // setup the small file handling params\\n     HoodieWriteConfig config = getSmallInsertWriteConfig(insertSplitLimit, true); // hold upto 200 records max\\n     dataGen = new HoodieTestDataGenerator(new String[] {testPartitionPath});\\n[DEL] \\n[DEL] SparkRDDWriteClient client = getHoodieWriteClient(config, false);\\n[ADD] SparkRDDWriteClient client = getHoodieWriteClient(config);\\n \\n     // delete non existent keys\\n     String commitTime1 = \"001\";',\n",
              " '       request\\n           .getAsyncContext()\\n           .addListener(new Listener(listener), request, (HttpServletResponse) response);\\n[DEL] } else {\\n[DEL] request.getAsyncContext().addListener(new Listener(listener));\\n     }\\n   }\\n ',\n",
              " ' \\n     @DB\\n     protected long recalculateAccountResourceCount(final long accountId, final ResourceType type) {\\n[DEL] Long newCount = Transaction.execute(new TransactionCallback<Long>() {\\n[ADD] final Long newCount;\\n[ADD] if (type == Resource.ResourceType.user_vm) {\\n[ADD] newCount = _userVmDao.countAllocatedVMsForAccount(accountId);\\n[ADD] } else if (type == Resource.ResourceType.volume) {\\n[ADD] long virtualRouterCount = _vmDao.findIdsOfAllocatedVirtualRoutersForAccount(accountId).size();\\n[ADD] newCount = _volumeDao.countAllocatedVolumesForAccount(accountId) - virtualRouterCount; // don\\'t count the volumes of virtual router\\n[ADD] } else if (type == Resource.ResourceType.snapshot) {\\n[ADD] newCount = _snapshotDao.countSnapshotsForAccount(accountId);\\n[ADD] } else if (type == Resource.ResourceType.public_ip) {\\n[ADD] newCount = calculatePublicIpForAccount(accountId);\\n[ADD] } else if (type == Resource.ResourceType.template) {\\n[ADD] newCount = _vmTemplateDao.countTemplatesForAccount(accountId);\\n[ADD] } else if (type == Resource.ResourceType.project) {\\n[ADD] newCount = _projectAccountDao.countByAccountIdAndRole(accountId, Role.Admin);\\n[ADD] } else if (type == Resource.ResourceType.network) {\\n[ADD] newCount = _networkDao.countNetworksUserCanCreate(accountId);\\n[ADD] } else if (type == Resource.ResourceType.vpc) {\\n[ADD] newCount = _vpcDao.countByAccountId(accountId);\\n[ADD] } else if (type == Resource.ResourceType.cpu) {\\n[ADD] newCount = countCpusForAccount(accountId);\\n[ADD] } else if (type == Resource.ResourceType.memory) {\\n[ADD] newCount = calculateMemoryForAccount(accountId);\\n[ADD] } else if (type == Resource.ResourceType.primary_storage) {\\n[ADD] List<Long> virtualRouters = _vmDao.findIdsOfAllocatedVirtualRoutersForAccount(accountId);\\n[ADD] newCount = _volumeDao.primaryStorageUsedForAccount(accountId, virtualRouters);\\n[ADD] } else if (type == Resource.ResourceType.secondary_storage) {\\n[ADD] newCount = calculateSecondaryStorageForAccount(accountId);\\n[ADD] } else {\\n[ADD] throw new InvalidParameterValueException(\"Unsupported resource type \" + type);\\n[ADD] }\\n[ADD] \\n[ADD] long oldCount = 0;\\n[ADD] final ResourceCountVO accountRC = _resourceCountDao.findByOwnerAndType(accountId, ResourceOwnerType.Account, type);\\n[ADD] if (accountRC != null) {\\n[ADD] oldCount = accountRC.getCount();\\n[ADD] }\\n[ADD] \\n[ADD] if (newCount == null || !newCount.equals(oldCount)) {\\n[ADD] Transaction.execute(new TransactionCallbackNoReturn() {\\n                 @Override\\n[DEL] public Long doInTransaction(TransactionStatus status) {\\n[DEL] Long newCount = null;\\n[ADD] public void doInTransactionWithoutResult(TransactionStatus status) {\\n                     lockAccountAndOwnerDomainRows(accountId, type);\\n[DEL] ResourceCountVO accountRC = _resourceCountDao.findByOwnerAndType(accountId, ResourceOwnerType.Account, type);\\n[DEL] long oldCount = 0;\\n[DEL] if (accountRC != null)\\n[DEL] oldCount = accountRC.getCount();\\n[DEL] \\n[DEL] if (type == Resource.ResourceType.user_vm) {\\n[DEL] newCount = _userVmDao.countAllocatedVMsForAccount(accountId);\\n[DEL] } else if (type == Resource.ResourceType.volume) {\\n[DEL] newCount = _volumeDao.countAllocatedVolumesForAccount(accountId);\\n[DEL] long virtualRouterCount = _vmDao.findIdsOfAllocatedVirtualRoutersForAccount(accountId).size();\\n[DEL] newCount = newCount - virtualRouterCount; // don\\'t count the volumes of virtual router\\n[DEL] } else if (type == Resource.ResourceType.snapshot) {\\n[DEL] newCount = _snapshotDao.countSnapshotsForAccount(accountId);\\n[DEL] } else if (type == Resource.ResourceType.public_ip) {\\n[DEL] newCount = calculatePublicIpForAccount(accountId);\\n[DEL] } else if (type == Resource.ResourceType.template) {\\n[DEL] newCount = _vmTemplateDao.countTemplatesForAccount(accountId);\\n[DEL] } else if (type == Resource.ResourceType.project) {\\n[DEL] newCount = _projectAccountDao.countByAccountIdAndRole(accountId, Role.Admin);\\n[DEL] } else if (type == Resource.ResourceType.network) {\\n[DEL] newCount = _networkDao.countNetworksUserCanCreate(accountId);\\n[DEL] } else if (type == Resource.ResourceType.vpc) {\\n[DEL] newCount = _vpcDao.countByAccountId(accountId);\\n[DEL] } else if (type == Resource.ResourceType.cpu) {\\n[DEL] newCount = countCpusForAccount(accountId);\\n[DEL] } else if (type == Resource.ResourceType.memory) {\\n[DEL] newCount = calculateMemoryForAccount(accountId);\\n[DEL] } else if (type == Resource.ResourceType.primary_storage) {\\n[DEL] List<Long> virtualRouters = _vmDao.findIdsOfAllocatedVirtualRoutersForAccount(accountId);\\n[DEL] newCount = _volumeDao.primaryStorageUsedForAccount(accountId, virtualRouters);\\n[DEL] } else if (type == Resource.ResourceType.secondary_storage) {\\n[DEL] newCount = calculateSecondaryStorageForAccount(accountId);\\n[DEL] } else {\\n[DEL] throw new InvalidParameterValueException(\"Unsupported resource type \" + type);\\n[DEL] }\\n[DEL] _resourceCountDao.setResourceCount(accountId, ResourceOwnerType.Account, type, (newCount == null) ? 0 : newCount.longValue());\\n[DEL] \\n[DEL] // No need to log message for primary and secondary storage because both are recalculating the\\n[DEL] // resource count which will not lead to any discrepancy.\\n[DEL] if (!Long.valueOf(oldCount).equals(newCount) &&\\n[DEL] (type != Resource.ResourceType.primary_storage && type != Resource.ResourceType.secondary_storage)) {\\n[DEL] s_logger.warn(\"Discrepency in the resource count \" + \"(original count=\" + oldCount + \" correct count = \" + newCount + \") for type \" + type +\\n[DEL] \" for account ID \" + accountId + \" is fixed during resource count recalculation.\");\\n[DEL] }\\n[DEL] return newCount;\\n[ADD] _resourceCountDao.setResourceCount(accountId, ResourceOwnerType.Account, type, (newCount == null) ? 0 : newCount);\\n                 }\\n             });\\n[ADD] }\\n[ADD] \\n[ADD] // No need to log message for primary and secondary storage because both are recalculating the\\n[ADD] // resource count which will not lead to any discrepancy.\\n[ADD] if (newCount != null && !newCount.equals(oldCount) &&\\n[ADD] type != Resource.ResourceType.primary_storage && type != Resource.ResourceType.secondary_storage) {\\n[ADD] s_logger.warn(\"Discrepancy in the resource count \" + \"(original count=\" + oldCount + \" correct count = \" + newCount + \") for type \" + type +\\n[ADD] \" for account ID \" + accountId + \" is fixed during resource count recalculation.\");\\n[ADD] }\\n \\n[DEL] return (newCount == null) ? 0 : newCount.longValue();\\n[ADD] return (newCount == null) ? 0 : newCount;\\n     }\\n \\n     public long countCpusForAccount(long accountId) {',\n",
              " '   protected final Path outputPath;\\n   protected final FileSystem fs;\\n   protected final FsPermission perm;\\n[ADD] protected final boolean deduplicate;\\n \\n   protected MRCompactorJobRunner(State jobProps, FileSystem fs) {\\n     this.jobProps = jobProps;',\n",
              " '     return t;\\n   }\\n \\n[DEL] @SuppressWarnings({\"unchecked\", \"rawtypes\"})\\n   private void addVariableToList(VariableDeclarationExpression e2, List list) {\\n     ModifiersTreeImpl modifiers = convertModifiers(e2.modifiers());\\n     TypeTree type = convertType(e2.getType());\\n \\n     for (int i = 0; i < e2.fragments().size(); i++) {\\n       VariableDeclarationFragment fragment = (VariableDeclarationFragment) e2.fragments().get(i);\\n[DEL] VariableTreeImpl t = new VariableTreeImpl(convertSimpleName(fragment.getName()));\\n[ADD] VariableTreeImpl t = new VariableTreeImpl(createSimpleName(fragment.getName()));\\n       t.completeModifiers(modifiers);\\n       if (fragment.getInitializer() == null) {\\n         t.completeType(type);',\n",
              " ' public class RepairsCommand implements CommandMarker {\\n \\n   private static final Logger LOG = Logger.getLogger(RepairsCommand.class);\\n[ADD] public static final String DEDUPLICATE_RETURN_PREFIX = \"Deduplicated files placed in:  \";\\n \\n   @CliCommand(value = \"repair deduplicate\",\\n       help = \"De-duplicate a partition path contains duplicates & produce repaired files to replace with\")',\n",
              " ' \\n                 // save conf\\n                 try {\\n[DEL] mCol.getDecks().save(mOptions);\\n[ADD] mCol.getDecks().save(mOptionsCopy);\\n                 } catch (RuntimeException e) {\\n                     Timber.e(e, \"DeckOptions - RuntimeException on saving conf\");\\n                     AnkiDroidApp.sendExceptionReport(e, \"DeckOptionsSaveConf\");',\n",
              " '     final int hash = hashCode();\\n     HashBlob blob = new ExtendedHashBlob(hash);\\n \\n[DEL] assertThat(hash, equalTo(ClassUtils.getFieldValue(blob, \"hash\", true)));\\n[ADD] assertThat(hash, is(equalTo(ClassUtils.getFieldValue(blob, \"hash\", true))));\\n   }\\n \\n   @Test(expected = NoSuchFieldException.class)\\n[DEL] public void getUnexistentFieldValueRecursive() throws Exception {\\n[ADD] public void getNotExistentFieldValueRecursive() throws Exception {\\n     ClassUtils.getFieldValue(new ExtendedHashBlob(1), \"fake\", true);\\n   }\\n ',\n",
              " '         }\\n     }\\n \\n[DEL] private Void handleCreateTemplateFromSnapshot(SnapshotInfo snapshotInfo, TemplateInfo templateInfo, AsyncCompletionCallback<CopyCommandResult> callback) {\\n[ADD] private boolean usingBackendSnapshotFor(SnapshotInfo snapshotInfo) {\\n[ADD] String property = getProperty(snapshotInfo.getId(), \"takeSnapshot\");\\n[ADD] \\n[ADD] return Boolean.parseBoolean(property);\\n[ADD] }\\n[ADD] \\n[ADD] private void handleCreateTemplateFromSnapshot(SnapshotInfo snapshotInfo, TemplateInfo templateInfo, AsyncCompletionCallback<CopyCommandResult> callback) {\\n         try {\\n             snapshotInfo.processEvent(Event.CopyingRequested);\\n         }',\n",
              " ' \\n \\t\\t// Fire up the sender.\\n \\n[DEL] SocketTestUtils.testSendStxEtx(port, latch);\\n[ADD] CountDownLatch done = SocketTestUtils.testSendStxEtx(port, latch);\\n \\t\\tlatch.countDown();\\n \\t\\tassertTrue(semaphore.tryAcquire(1, 10000, TimeUnit.MILLISECONDS));\\n \\t\\tassertTrue(semaphore.tryAcquire(1, 10000, TimeUnit.MILLISECONDS));',\n",
              " ' import java.util.Set;\\n import java.util.concurrent.CountDownLatch;\\n import java.util.concurrent.atomic.AtomicInteger;\\n[ADD] import java.util.jar.Attributes;\\n[ADD] import java.util.jar.JarInputStream;\\n[ADD] import java.util.jar.Manifest;\\n[ADD] import java.util.regex.Matcher;\\n[ADD] import java.util.regex.Pattern;\\n[ADD] import java.util.stream.Collectors;\\n[ADD] import java.util.stream.Stream;\\n \\n import org.osgi.framework.Bundle;\\n import org.osgi.framework.BundleContext;',\n",
              " '         }\\n     }\\n \\n[ADD] @VisibleForTesting\\n[ADD] protected Optional<Number> getNumber(Object o) {\\n[ADD] if (o instanceof Number) {\\n[ADD] return Optional.of((Number)o);\\n[ADD] }\\n[ADD] \\n[ADD] try {\\n[ADD] return Optional.of(Double.valueOf(String.valueOf(o)));\\n[ADD] } catch (NumberFormatException e) {\\n[ADD] return Optional.empty();\\n[ADD] }\\n[ADD] }\\n }',\n",
              " ' import org.apache.beam.sdk.coders.MapCoder;\\n import org.apache.beam.sdk.coders.NullableCoder;\\n import org.apache.beam.sdk.coders.StringUtf8Coder;\\n[ADD] import org.apache.beam.sdk.values.TypeDescriptor;\\n \\n /** A coder for PubsubMessage including attributes. */\\n[DEL] public class PubsubMessageWithAttributesCoder extends CustomCoder<PubsubIO.PubsubMessage> {\\n[ADD] public class PubsubMessageWithAttributesCoder extends CustomCoder<PubsubMessage> {\\n   private static final Coder<byte[]> PAYLOAD_CODER =\\n       NullableCoder.of(ByteArrayCoder.of());\\n   private static final Coder<Map<String, String>> ATTRIBUTES_CODER = MapCoder.of(\\n       StringUtf8Coder.of(), StringUtf8Coder.of());\\n \\n[ADD] public static Coder<PubsubMessage> of(TypeDescriptor<PubsubMessage> ignored) {\\n[ADD] return of();\\n[ADD] }\\n[ADD] \\n   public static PubsubMessageWithAttributesCoder of() {\\n     return new PubsubMessageWithAttributesCoder();\\n   }\\n \\n[DEL] public void encode(PubsubIO.PubsubMessage value, OutputStream outStream, Context context)\\n[ADD] public void encode(PubsubMessage value, OutputStream outStream, Context context)\\n       throws IOException {\\n     PAYLOAD_CODER.encode(\\n[DEL] value.getMessage(),\\n[ADD] value.getPayload(),\\n         outStream,\\n         context.nested());\\n     ATTRIBUTES_CODER.encode(value.getAttributeMap(), outStream, context);\\n   }\\n \\n   @Override\\n[DEL] public PubsubIO.PubsubMessage decode(InputStream inStream, Context context) throws IOException {\\n[ADD] public PubsubMessage decode(InputStream inStream, Context context) throws IOException {\\n     byte[] payload = PAYLOAD_CODER.decode(inStream, context.nested());\\n     Map<String, String> attributes = ATTRIBUTES_CODER.decode(inStream, context);\\n[DEL] return new PubsubIO.PubsubMessage(payload, attributes);\\n[ADD] return new PubsubMessage(payload, attributes);\\n   }\\n }',\n",
              " ' import java.util.TreeSet;\\n import java.util.stream.Collectors;\\n import javax.annotation.Nullable;\\n[ADD] import org.sonar.api.utils.AnnotationUtils;\\n import org.sonar.api.utils.log.Logger;\\n import org.sonar.api.utils.log.Loggers;\\n[ADD] import org.sonar.check.Rule;\\n import org.sonar.java.AnalysisError;\\n[ADD] import org.sonar.java.IllegalRuleParameterException;\\n import org.sonar.java.JavaVersionAwareVisitor;\\n import org.sonar.java.SonarComponents;\\n import org.sonar.java.ast.visitors.SonarSymbolTableVisitor;',\n",
              " ' \\n package org.pentaho.di.trans.steps.databaselookup;\\n \\n[DEL] import java.util.ArrayList;\\n import java.util.Arrays;\\n[DEL] import java.util.Collections;\\n[DEL] import java.util.Date;\\n[DEL] import java.util.LinkedHashMap;\\n import java.util.List;\\n \\n import com.google.common.annotations.VisibleForTesting;\\n import org.pentaho.di.core.Const;\\n[DEL] import org.pentaho.di.core.RowMetaAndData;\\n[DEL] import org.pentaho.di.core.TimedRow;\\n import org.pentaho.di.core.database.Database;\\n import org.pentaho.di.core.database.DatabaseMeta;\\n import org.pentaho.di.core.exception.KettleDatabaseException;',\n",
              " '                     profile.setIsolationUri(Networks.IsolationType.Vlan.toUri(publicIp.getVlanTag()));\\n                     profile.setSecurityGroupEnabled(_networkModel.isSecurityGroupSupportedInNetwork(network));\\n                     profile.setName(_networkModel.getNetworkTag(vm.getHypervisorType(), network));\\n[DEL] profile.setNetworId(network.getId());\\n[ADD] profile.setNetworkRate(_networkModel.getNetworkRate(network.getId(), vm.getId()));\\n[ADD] profile.setNetworkId(network.getId());\\n \\n                     guru.updateNicProfile(profile, network);\\n                     vm.addNic(profile);',\n",
              " '     assertThat(metrics.get(\"statements\").intValue()).isEqualTo(12047);\\n     assertThat(metrics.get(\"complexity\").intValue()).isEqualTo(8475 - 80 /* SONAR-3793 */- 2 /* SONAR-3794 */);\\n     assertThat(metrics.get(\"comment_lines\").intValue()).isEqualTo(17908);\\n[DEL] assertThat(metrics.get(\"public_api\").intValue()).isEqualTo(3221);\\n[ADD] // 69: SONARJAVA-861 analyseAccessors property of the measurer is set to true. Getters and setters ignored.\\n[ADD] assertThat(metrics.get(\"public_api\").intValue()).isEqualTo(3221 - 69);\\n     double density = 1.0;\\n     if (metrics.get(\"public_api\").intValue() != 0) {\\n       density = (metrics.get(\"public_api\") - metrics.get(\"public_undocumented_api\")) / metrics.get(\"public_api\");',\n",
              " '     // There is no way to simulate failed commit on the main dataset, hence we simply delete the completed\\n     // instant so that only the inflight is left over.\\n     String commitInstantFileName = HoodieTimeline.makeCommitFileName(\"002\");\\n[DEL] assertTrue(fs.delete(new Path(basePath + Path.SEPARATOR + HoodieTableMetaClient.METAFOLDER_NAME,\\n[ADD] assertTrue(fs().delete(new Path(basePath() + Path.SEPARATOR + HoodieTableMetaClient.METAFOLDER_NAME,\\n         commitInstantFileName), false));\\n     // Next upsert\\n     testTable.doWriteOperation(\"003\", UPSERT, emptyList(), asList(\"p1\", \"p2\"), 1);\\n     // Post rollback commit and metadata should be valid\\n     syncTableMetadata(writeConfig);\\n[DEL] HoodieTableMetaClient metadataMetaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(metadataTableBasePath).build();\\n[ADD] HoodieTableMetaClient metadataMetaClient = HoodieTableMetaClient.builder().setConf(hadoopConf()).setBasePath(metadataTableBasePath).build();\\n     HoodieActiveTimeline timeline = metadataMetaClient.getActiveTimeline();\\n     assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\\n     assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));',\n",
              " ' \\t/**\\n \\t * Wait for service to be registered\\n \\t * \\n[DEL] * @param class1 name of the service\\n[DEL] * @param timeoutInMs timeout in ms\\n[ADD] * @param clazz name of the service\\n[ADD] * @param timeout timeout\\n \\t * @return a service\\n \\t * @throws InterruptedException\\n[ADD] * @throws IllegalArgumentException if any of the specified arguments is\\n[ADD] *             {@code null}\\n \\t */\\n[DEL] public <T> Optional<T> waitForService(Class<T> class1, long timeoutInMs) throws InterruptedException {\\n[ADD] public <T> Optional<T> waitForService(Class<T> clazz, Duration timeout) throws InterruptedException {\\n[ADD] requireNonNull(clazz, \"Service class instance cannot be null\");\\n[ADD] requireNonNull(timeout, \"Timeout cannot be null\");\\n[ADD] \\n \\t\\ttry {\\n[DEL] Optional<ServiceReference<T>> ref = waitForServiceReference(class1, timeoutInMs);\\n[ADD] Optional<ServiceReference> ref = waitForServiceReference(clazz, timeout);\\n \\t\\t\\tBundleContext context = getBundleContext();\\n[DEL] return ref.map(context::getService);\\n[ADD] return ref.map(context::getService).filter(clazz::isInstance).map(clazz::cast);\\n \\t\\t} catch (Exception e) {\\n \\t\\t\\tthrow Exceptions.duck(e);\\n \\t\\t}\\n \\t}\\n \\n \\tObject getService(Injector.Target<Service> param) {\\n[DEL] \\n \\t\\ttry {\\n \\t\\t\\tif (param.type == BundleContext.class) {\\n \\t\\t\\t\\treturn getBundleContext();',\n",
              " ' \\n package org.springframework.integration.gateway;\\n \\n[ADD] import java.util.concurrent.CompletableFuture;\\n import java.util.concurrent.atomic.AtomicLong;\\n \\n[ADD] import org.reactivestreams.Subscriber;\\n[ADD] \\n import org.springframework.core.AttributeAccessor;\\n import org.springframework.integration.MessageTimeoutException;\\n[ADD] import org.springframework.integration.channel.ReactiveStreamsSubscribableChannel;\\n import org.springframework.integration.core.MessagingTemplate;\\n import org.springframework.integration.endpoint.AbstractEndpoint;\\n import org.springframework.integration.endpoint.EventDrivenConsumer;\\n import org.springframework.integration.endpoint.PollingConsumer;\\n[ADD] import org.springframework.integration.endpoint.ReactiveStreamsConsumer;\\n import org.springframework.integration.handler.BridgeHandler;\\n import org.springframework.integration.history.HistoryWritingMessagePostProcessor;\\n import org.springframework.integration.mapping.InboundMessageMapper;',\n",
              " '                     if (nodeListFor2.getLength() > 1) {\\n                         /* Do we need to figure out all the sub elements here and put them in a map? */\\n                     } else {\\n[DEL] String element = nodeListFor.item(index).getTextContent();\\n[ADD] String element = nodeListFor.item(index).getNodeValue();\\n                         myMap.put(rnode, (E) element);\\n                     }\\n                 }',\n",
              " '         if ( isJettyMode() ) {\\n           out.println( \"<link rel=\\\\\"stylesheet\\\\\" type=\\\\\"text/css\\\\\" href=\\\\\"/static/css/carte.css\\\\\" />\" );\\n         } else {\\n[DEL] out.print( StatusServletUtils.getPentahoStyles() );\\n[ADD] out.print( StatusServletUtils.getPentahoStyles( root ) );\\n           out.println( \"<style>\" );\\n           out.println( \".pentaho-table td, tr.cellTableRow, td.gwt-MenuItem, .toolbar-button:not(.toolbar-button-disabled) {\" );\\n           out.println( \"  cursor: pointer;\" );',\n",
              " ' \\t\\t\\tenv = append(env, \"PULUMI_NODEJS_TSCONFIG_PATH=\"+host.tsconfigpath)\\n \\t\\t}\\n \\n[ADD] var nodeargs []string\\n[ADD] if host.nodeargs != \"\" {\\n[ADD] nodeargs = strings.Split(host.nodeargs, \" \")\\n[ADD] }\\n[ADD] nodeargs = append(nodeargs, args...)\\n[ADD] \\n \\t\\tif logging.V(5) {\\n[DEL] commandStr := strings.Join(args, \" \")\\n[ADD] commandStr := strings.Join(nodeargs, \" \")\\n \\t\\t\\tlogging.V(5).Infoln(\"Language host launching process: \", host.nodeBin, commandStr)\\n \\t\\t}\\n \\n \\t\\t// Now simply spawn a process to execute the requested program, wiring up stdout/stderr directly.\\n \\t\\tvar errResult string\\n \\t\\t// #nosec G204\\n[DEL] cmd := exec.Command(host.nodeBin, args...)\\n[ADD] cmd := exec.Command(host.nodeBin, nodeargs...)\\n \\t\\tcmd.Stdout = os.Stdout\\n \\t\\tcmd.Stderr = os.Stderr\\n \\t\\tcmd.Env = env',\n",
              " ' \\tpublic long getTotPhysicalMem() {\\n \\t\\treturn totPhysicalMem;\\n \\t}\\n[DEL] \\n[ADD] \\n \\t/**\\n \\t * @return the system load average in last minute (fraction)\\n \\t */\\n \\tpublic double getSysLoadAvg() {\\n \\t\\treturn sysLoadAvg;\\n \\t}\\n[DEL] \\n[ADD] \\n \\t/**\\n \\t * Returns total amount of time the process has been scheduled or executed so far\\n \\t * in both kernel and user modes.\\n[DEL] *\\n[ADD] *\\n \\t * @return time in 100 ns units, or -1 in the case of an error or if this metric\\n \\t *         is not supported for this operating system.\\n \\t */\\n \\tpublic long getCpuTime() {\\n \\t\\treturn cpuTime;\\n \\t}\\n[DEL] \\n[ADD] \\n \\t/**\\n[DEL] *  Native used to retrieve the heap/OS stats\\n[ADD] * Native method used to retrieve the heap/OS stats.\\n \\t */\\n \\tpublic native void getStats();\\n[DEL] }\\n[ADD] }',\n",
              " '         }\\n \\n         if (storageId != null) {\\n[DEL] sb.and(\"poolId\", sb.entity().getPoolId(), SearchCriteria.Op.EQ);\\n[ADD] StoragePoolVO poolVO = _storagePoolDao.findById((Long) storageId);\\n[ADD] if (poolVO.getPoolType() == Storage.StoragePoolType.DatastoreCluster) {\\n[ADD] sb.and(\"poolId\", sb.entity().getPoolId(), SearchCriteria.Op.IN);\\n[ADD] } else {\\n[ADD] sb.and(\"poolId\", sb.entity().getPoolId(), SearchCriteria.Op.EQ);\\n[ADD] }\\n         }\\n \\n         if (affinityGroupId != null) {',\n",
              " ' import org.sonar.squidbridge.annotations.SqaleConstantRemediation;\\n import org.sonar.squidbridge.annotations.SqaleSubCharacteristic;\\n \\n[DEL] import java.util.List;\\n[DEL] \\n @Rule(\\n   key = \"S1309\",\\n   name = \"The @SuppressWarnings annotation should not be used\",',\n",
              " '  * be passed as the inputs of other PTransforms.\\n  *\\n  * <p>Some root transforms produce bounded {@code PCollections} and others\\n[DEL] * produce unbounded ones.  For example, {@link TextIO.Read} reads a static set\\n[DEL] * of files, so it produces a bounded {@link PCollection}.\\n[DEL] * {@link PubsubIO.Read}, on the other hand, receives a potentially infinite stream\\n[DEL] * of Pubsub messages, so it produces an unbounded {@link PCollection}.\\n[ADD] * produce unbounded ones. For example, {@link CountingInput#upTo} produces a fixed set of integers,\\n[ADD] * so it produces a bounded {@link PCollection}. {@link CountingInput#unbounded} produces all\\n[ADD] * integers as an infinite stream, so it produces an unbounded {@link PCollection}.\\n  *\\n[DEL] * <p>Each element in a {@link PCollection} may have an associated implicit\\n[DEL] * timestamp.  Readers assign timestamps to elements when they create\\n[DEL] * {@link PCollection PCollections}, and other {@link PTransform PTransforms} propagate these\\n[DEL] * timestamps from their input to their output. For example, {@link PubsubIO.Read}\\n[DEL] * assigns pubsub message timestamps to elements, and {@link TextIO.Read} assigns\\n[DEL] * the default value {@link BoundedWindow#TIMESTAMP_MIN_VALUE} to elements. User code can\\n[DEL] * explicitly assign timestamps to elements with\\n[DEL] * {@link org.apache.beam.sdk.transforms.DoFn.Context#outputWithTimestamp}.\\n[ADD] * <p>Each element in a {@link PCollection} has an associated timestamp. Readers assign timestamps\\n[ADD] * to elements when they create {@link PCollection PCollections}, and other\\n[ADD] * {@link PTransform PTransforms} propagate these timestamps from their input to their output. See\\n[ADD] * the documentation on {@link BoundedReader} and {@link UnboundedReader} for more information on\\n[ADD] * how these readers produce timestamps and watermarks.\\n  *\\n  * <p>Additionally, a {@link PCollection} has an associated\\n  * {@link WindowFn} and each element is assigned to a set of windows.',\n",
              " '       }\\n    }\\n \\n[ADD] public void setEnabled(boolean enabled, String sourceColumnName)\\n[ADD] {\\n[ADD] if (enabled != enabled_)\\n[ADD] {\\n[ADD] enabled_ = enabled;\\n[ADD] handlers_.fireEvent(new EnabledChangedEvent(this, sourceColumnName));\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] public void setButtonEnabled(boolean buttonEnabled, String sourceColumnName)\\n[ADD] {\\n[ADD] handlers_.fireEvent(new EnabledChangedEvent(this, sourceColumnName, buttonEnabled));\\n[ADD] }\\n[ADD] \\n    public boolean isVisible()\\n    {\\n       return visible_;',\n",
              " '    {\\n       globalDisplay_.openRStudioLink(\"authoring_presentations\");\\n    }\\n[ADD] \\n[ADD] @Handler\\n[ADD] void onRcppHelp()\\n[ADD] {\\n[ADD] globalDisplay_.openRStudioLink(\"rcpp_help\");\\n[ADD] }\\n      \\n    @Handler\\n    void onKnitToHTML()',\n",
              " '     private WindowFn<Row, IntervalWindow> windowFn;\\n     private int windowFieldIndex;\\n     private List<FieldAggregation> fieldAggregations;\\n[ADD] private int groupSetCount;\\n[ADD] private boolean ignoreValues;\\n \\n     private Transform(\\n         WindowFn<Row, IntervalWindow> windowFn,',\n",
              " ' \\n         reset(applicationDeploymentListener);\\n \\n[DEL] File configFile = new File(appsDir + \"/dummy-app\", \"mule-config.xml\");\\n[ADD] File configFile = new File(appsDir + DUMMY_APP_PATH, MULE_CONFIG_XML_FILE);\\n         configFile.setLastModified(configFile.lastModified() + 1000);\\n \\n         assertDeploymentSuccess(applicationDeploymentListener, DUMMY_APP);',\n",
              " '  */\\n package org.mule.module.launcher;\\n \\n[ADD] import static java.sql.DriverManager.deregisterDriver;\\n[ADD] import static java.sql.DriverManager.getDrivers;\\n[ADD] import static java.sql.DriverManager.registerDriver;\\n[ADD] import static java.util.Collections.list;\\n[ADD] import static org.hamcrest.CoreMatchers.hasItem;\\n[ADD] import static org.hamcrest.CoreMatchers.not;\\n import static org.hamcrest.Matchers.is;\\n import static org.junit.Assert.assertThat;\\n[ADD] import static org.mockito.Mockito.mock;\\n[ADD] \\n import org.mule.module.launcher.artifact.AbstractArtifactClassLoader;\\n[ADD] import org.mule.module.launcher.artifact.DefaultResourceReleaser;\\n import org.mule.module.launcher.artifact.ResourceReleaser;\\n import org.mule.tck.junit4.AbstractMuleTestCase;\\n import org.mule.tck.size.SmallTest;\\n \\n import java.lang.reflect.Method;\\n[ADD] import java.sql.Driver;\\n \\n import org.junit.Test;\\n ',\n",
              " '   public Optional<FileSlice> getLatestFileSliceBeforeOrOn(String maxCommitTime) {\\n     return getAllFileSlices()\\n         .filter(slice ->\\n[DEL] HoodieTimeline.compareTimestamps(slice.getBaseCommitTime(),\\n[ADD] HoodieTimeline.compareTimestamps(slice.getBaseInstantTime(),\\n                 maxCommitTime,\\n                 HoodieTimeline.LESSER_OR_EQUAL))\\n         .findFirst();\\n   }\\n \\n[ADD] /**\\n[ADD] * Obtain the latest file slice, upto a commitTime i.e < maxInstantTime\\n[ADD] * @param maxInstantTime Max Instant Time\\n[ADD] * @return\\n[ADD] */\\n[ADD] public Optional<FileSlice> getLatestFileSliceBefore(String maxInstantTime) {\\n[ADD] return getAllFileSlices()\\n[ADD] .filter(slice ->\\n[ADD] HoodieTimeline.compareTimestamps(slice.getBaseInstantTime(),\\n[ADD] maxInstantTime,\\n[ADD] HoodieTimeline.LESSER))\\n[ADD] .findFirst();\\n[ADD] }\\n[ADD] \\n   public Optional<FileSlice> getLatestFileSliceInRange(List<String> commitRange) {\\n     return getAllFileSlices()\\n[DEL] .filter(slice -> commitRange.contains(slice.getBaseCommitTime()))\\n[ADD] .filter(slice -> commitRange.contains(slice.getBaseInstantTime()))\\n         .findFirst();\\n   }\\n ',\n",
              " '     config.timeBetweenEvictionRunsMillis = poolingProfile.getEvictionCheckIntervalMillis();\\n     GenericObjectPool genericPool = new GenericObjectPool(new ObjectFactoryAdapter(), config);\\n \\n[ADD] applyInitialisationPolicy(genericPool);\\n[ADD] \\n     return genericPool;\\n   }\\n \\n[ADD] protected void applyInitialisationPolicy(GenericObjectPool pool) {\\n[ADD] int initialConnections = 0;\\n[ADD] switch (poolingProfile.getInitialisationPolicy()) {\\n[ADD] case INITIALISE_ONE:\\n[ADD] initialConnections = 1;\\n[ADD] break;\\n[ADD] case INITIALISE_ALL:\\n[ADD] initialConnections = poolingProfile.getMaxActive();\\n[ADD] break;\\n[ADD] }\\n[ADD] \\n[ADD] for (int t = 0; t < initialConnections; t++) {\\n[ADD] try {\\n[ADD] pool.addObject();\\n[ADD] } catch (Exception e) {\\n[ADD] LOGGER.warn(\"Failed to create a connection while applying the pool initialization policy.\", e);\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] \\n   public PoolingProfile getPoolingProfile() {\\n     return poolingProfile;\\n   }',\n",
              " '     return new XmlConfigurationDocumentLoader();\\n   }\\n \\n[ADD] private XmlApplicationParser createApplicationParser() {\\n[ADD] ExtensionManager extensionManager = muleContext.getExtensionManager();\\n[ADD] DslResolvingContext context =\\n[ADD] extensionManager != null ? DslResolvingContext.getDefault(extensionManager.getExtensions()) : null;\\n[ADD] return new XmlApplicationParser(new XmlApplicationServiceRegistry(serviceRegistry, context));\\n[ADD] }\\n[ADD] \\n   private void determineIfOnlyNewParsingMechanismCanBeUsed() {\\n     if (applicationModel.hasSpringConfig()) {\\n       useNewParsingMechanism = false;',\n",
              " '     });\\n     return knownRelations;\\n   }\\n[ADD] \\n[ADD] public ObjectConstraint getConstraintWithStatus(SymbolicValue value, Object aState) {\\n[ADD] final Object constraint = getConstraint(value.wrappedValue());\\n[ADD] if (constraint instanceof ObjectConstraint) {\\n[ADD] ObjectConstraint oConstraint = (ObjectConstraint) constraint;\\n[ADD] if (oConstraint.hasStatus(aState)) {\\n[ADD] return oConstraint;\\n[ADD] }\\n[ADD] }\\n[ADD] return null;\\n[ADD] }\\n }',\n",
              " '     }\\n   }\\n \\n[ADD] /**\\n[ADD] * A translator just to vend the URN. This will need to be moved to runners-core-construction-java\\n[ADD] * once SDF is reorganized appropriately.\\n[ADD] */\\n[ADD] private static class SplittableParDoProcessElementsTranslator\\n[ADD] implements PTransformTranslation.TransformPayloadTranslator<\\n[ADD] SplittableParDoViaKeyedWorkItems.ProcessElements<?, ?, ?, ?>> {\\n[ADD] \\n[ADD] private SplittableParDoProcessElementsTranslator() {}\\n[ADD] \\n[ADD] @Override\\n[ADD] public String getUrn(SplittableParDoViaKeyedWorkItems.ProcessElements<?, ?, ?, ?> transform) {\\n[ADD] return SPLITTABLE_PROCESS_URN;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public RunnerApi.FunctionSpec translate(\\n[ADD] AppliedPTransform<?, ?, SplittableParDoViaKeyedWorkItems.ProcessElements<?, ?, ?, ?>>\\n[ADD] transform,\\n[ADD] SdkComponents components) {\\n[ADD] throw new UnsupportedOperationException(\\n[ADD] String.format(\\n[ADD] \"%s should never be translated\",\\n[ADD] SplittableParDoViaKeyedWorkItems.ProcessElements.class.getCanonicalName()));\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] /** Registers classes specialized to the Flink runner. */\\n[ADD] @AutoService(TransformPayloadTranslatorRegistrar.class)\\n[ADD] public static class FlinkTransformsRegistrar implements TransformPayloadTranslatorRegistrar {\\n[ADD] @Override\\n[ADD] public Map<\\n[ADD] ? extends Class<? extends PTransform>,\\n[ADD] ? extends PTransformTranslation.TransformPayloadTranslator>\\n[ADD] getTransformPayloadTranslators() {\\n[ADD] return ImmutableMap\\n[ADD] .<Class<? extends PTransform>, PTransformTranslation.TransformPayloadTranslator>builder()\\n[ADD] .put(\\n[ADD] CreateStreamingFlinkView.CreateFlinkPCollectionView.class,\\n[ADD] new CreateStreamingFlinkViewPayloadTranslator())\\n[ADD] .put(\\n[ADD] SplittableParDoViaKeyedWorkItems.ProcessElements.class,\\n[ADD] new SplittableParDoProcessElementsTranslator())\\n[ADD] .put(\\n[ADD] SplittableParDoViaKeyedWorkItems.GBKIntoKeyedWorkItems.class,\\n[ADD] new SplittableParDoGbkIntoKeyedWorkItemsPayloadTranslator())\\n[ADD] .build();\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * A translator just to vend the URN. This will need to be moved to runners-core-construction-java\\n[ADD] * once SDF is reorganized appropriately.\\n[ADD] */\\n[ADD] private static class SplittableParDoProcessElementsPayloadTranslator\\n[ADD] implements PTransformTranslation.TransformPayloadTranslator<\\n[ADD] SplittableParDoViaKeyedWorkItems.ProcessElements<?, ?, ?, ?>> {\\n[ADD] \\n[ADD] private SplittableParDoProcessElementsPayloadTranslator() {}\\n[ADD] \\n[ADD] @Override\\n[ADD] public String getUrn(SplittableParDoViaKeyedWorkItems.ProcessElements<?, ?, ?, ?> transform) {\\n[ADD] return SplittableParDo.SPLITTABLE_PROCESS_URN;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public RunnerApi.FunctionSpec translate(\\n[ADD] AppliedPTransform<?, ?, SplittableParDoViaKeyedWorkItems.ProcessElements<?, ?, ?, ?>>\\n[ADD] transform,\\n[ADD] SdkComponents components) {\\n[ADD] throw new UnsupportedOperationException(\\n[ADD] String.format(\\n[ADD] \"%s should never be translated\",\\n[ADD] SplittableParDoViaKeyedWorkItems.ProcessElements.class.getCanonicalName()));\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * A translator just to vend the URN. This will need to be moved to runners-core-construction-java\\n[ADD] * once SDF is reorganized appropriately.\\n[ADD] */\\n[ADD] private static class SplittableParDoGbkIntoKeyedWorkItemsPayloadTranslator\\n[ADD] implements PTransformTranslation.TransformPayloadTranslator<\\n[ADD] SplittableParDoViaKeyedWorkItems.GBKIntoKeyedWorkItems<?, ?>> {\\n[ADD] \\n[ADD] private SplittableParDoGbkIntoKeyedWorkItemsPayloadTranslator() {}\\n[ADD] \\n[ADD] @Override\\n[ADD] public String getUrn(SplittableParDoViaKeyedWorkItems.GBKIntoKeyedWorkItems<?, ?> transform) {\\n[ADD] return SplittableParDo.SPLITTABLE_GBKIKWI_URN;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public RunnerApi.FunctionSpec translate(\\n[ADD] AppliedPTransform<?, ?, SplittableParDoViaKeyedWorkItems.GBKIntoKeyedWorkItems<?, ?>>\\n[ADD] transform,\\n[ADD] SdkComponents components) {\\n[ADD] throw new UnsupportedOperationException(\\n[ADD] String.format(\\n[ADD] \"%s should never be translated\",\\n[ADD] SplittableParDoViaKeyedWorkItems.GBKIntoKeyedWorkItems.class.getCanonicalName()));\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * A translator just to vend the URN.\\n[ADD] */\\n[ADD] private static class CreateStreamingFlinkViewPayloadTranslator\\n[ADD] implements PTransformTranslation.TransformPayloadTranslator<\\n[ADD] CreateStreamingFlinkView.CreateFlinkPCollectionView<?, ?>> {\\n[ADD] \\n[ADD] private CreateStreamingFlinkViewPayloadTranslator() {}\\n[ADD] \\n[ADD] @Override\\n[ADD] public String getUrn(CreateStreamingFlinkView.CreateFlinkPCollectionView<?, ?> transform) {\\n[ADD] return CreateStreamingFlinkView.CREATE_STREAMING_FLINK_VIEW_URN;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public RunnerApi.FunctionSpec translate(\\n[ADD] AppliedPTransform<?, ?, CreateStreamingFlinkView.CreateFlinkPCollectionView<?, ?>>\\n[ADD] transform,\\n[ADD] SdkComponents components) {\\n[ADD] throw new UnsupportedOperationException(\\n[ADD] String.format(\\n[ADD] \"%s should never be translated\",\\n[ADD] CreateStreamingFlinkView.CreateFlinkPCollectionView.class.getCanonicalName()));\\n[ADD] }\\n[ADD] }\\n }',\n",
              " '       throw new IllegalStateException(\"Already disposed\");\\n     }\\n \\n[DEL] TransactionCoordination txCoord = TransactionCoordination.getInstance();\\n[DEL] if (txCoord.runningNestedTransaction()) {\\n[DEL] sinksNestedTx.get(txCoord.getTransaction(), tx -> createSink()).next(event);\\n[DEL] } else {\\n[DEL] sinks.get(currentThread(), t -> createSink()).next(event);\\n[DEL] }\\n[ADD] sinkProvider.accept(this, event);\\n   }\\n \\n   @Override',\n",
              " '  *\\n  * @param <T> the type of the values being transcoded\\n  */\\n[DEL] public class LengthPrefixCoder<T> extends StandardCoder<T> {\\n[ADD] public class LengthPrefixCoder<T> extends CustomCoder<T> {\\n \\n   public static <T> LengthPrefixCoder<T> of(\\n       Coder<T> valueCoder) {',\n",
              " '     }\\n \\n     Region parseRegion(Id mdId, Element extentObj) throws Exception {\\n[ADD] MultiPolygon geometry = parseElement(extentObj);\\n[ADD] \\n[ADD] String id = null;\\n[ADD] if (geometry != null) {\\n[ADD] Element element = extentObj.getChild(\"element\", Geonet.Namespaces.GEONET);\\n[ADD] if (element != null) {\\n[ADD] id = element.getAttributeValue(\"ref\");\\n[ADD] }\\n[ADD] return new MetadataRegion(mdId, id, geometry);\\n[ADD] } else {\\n[ADD] return null;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private MultiPolygon parseElement(Element extentObj) throws Exception {\\n         GeonetContext gc = (GeonetContext) context.getHandlerContext(Geonet.CONTEXT_NAME);\\n         gc.getBean(DataManager.class).getEditLib().removeEditingInfo(extentObj);\\n \\n[DEL] String id = null;\\n[DEL] Geometry geometry = null;\\n[ADD] MultiPolygon geometry = null;\\n         if (\"polygon\".equals(extentObj.getName())) {\\n[DEL] String gml = Xml.getString(extentObj);\\n[DEL] geometry = SpatialIndexWriter.parseGml(parsers[0], gml);\\n[ADD] geometry = parsePolygon(extentObj);\\n         } else if (\"EX_BoundingPolygon\".equals(extentObj.getName())) {\\n[DEL] String gml = Xml.getString(extentObj.getChild(\"polygon\", Geonet.Namespaces.GMD));\\n[DEL] geometry = SpatialIndexWriter.parseGml(parsers[0], gml);\\n[ADD] Element polygon = extentObj.getChild(\"polygon\", Namespaces.GMD);\\n[ADD] geometry = parsePolygon(polygon);\\n         } else if (\"EX_GeographicBoundingBox\".equals(extentObj.getName())) {\\n             double minx = Double.parseDouble(extentObj.getChild(\"westBoundLongitude\", Geonet.Namespaces.GMD).getChildText(\"Decimal\", Geonet.Namespaces.GCO));\\n             double maxx = Double.parseDouble(extentObj.getChild(\"eastBoundLongitude\", Geonet.Namespaces.GMD).getChildText(\"Decimal\", Geonet.Namespaces.GCO));\\n             double miny = Double.parseDouble(extentObj.getChild(\"southBoundLatitude\", Geonet.Namespaces.GMD).getChildText(\"Decimal\", Geonet.Namespaces.GCO));\\n             double maxy = Double.parseDouble(extentObj.getChild(\"northBoundLatitude\", Geonet.Namespaces.GMD).getChildText(\"Decimal\", Geonet.Namespaces.GCO));\\n[DEL] geometry = factory.toGeometry(new Envelope(minx, maxx, miny, maxy));\\n[ADD] Polygon[] polygons = {(Polygon)factory.toGeometry(new Envelope(minx, maxx, miny, maxy))};\\n[ADD] geometry = factory.createMultiPolygon(polygons);\\n         }\\n[ADD] return geometry;\\n[ADD] }\\n \\n[DEL] if (geometry != null) {\\n[DEL] Element element = extentObj.getChild(\"element\", Geonet.Namespaces.GEONET);\\n[DEL] if (element != null) {\\n[DEL] id = element.getAttributeValue(\"ref\");\\n[DEL] }\\n[DEL] return new MetadataRegion(mdId, id, geometry);\\n[ADD] private MultiPolygon parsePolygon(Element extentObj) throws Exception {\\n[ADD] // get gml node (only child)\\n[ADD] List children = extentObj.getChildren();\\n[ADD] if (children.size()==0) return null;\\n[ADD] Element gmlNode = (Element) children.get(0);\\n[ADD] \\n[ADD] // get geometry\\n[ADD] String gml = Xml.getString(gmlNode);\\n[ADD] Parser parser = getParser(gmlNode);\\n[ADD] MultiPolygon geometry = SpatialIndexWriter.parseGml(parser, gml);\\n[ADD] \\n[ADD] // if we have an srs and its not WGS84 then transform to WGS84\\n[ADD] String srs = gmlNode.getAttributeValue(\"srsName\");\\n[ADD] \\n[ADD] CoordinateReferenceSystem sourceCRS;\\n[ADD] if (srs != null && !(srs.equals(\"\"))) {\\n[ADD] sourceCRS = CRS.decode(srs);\\n         } else {\\n[DEL] return null;\\n[ADD] sourceCRS = DefaultGeographicCRS.WGS84;\\n[ADD] }\\n[ADD] \\n[ADD] if (!CRS.equalsIgnoreMetadata(sourceCRS, DefaultGeographicCRS.WGS84)) {\\n[ADD] MathTransform tform = CRS.findMathTransform(sourceCRS, DefaultGeographicCRS.WGS84);\\n[ADD] geometry = (MultiPolygon) JTS.transform(geometry, tform);\\n[ADD] }\\n[ADD] \\n[ADD] return geometry;\\n[ADD] }\\n[ADD] \\n[ADD] private Parser getParser(Element gmlNode) {\\n[ADD] if (gmlNode.getNamespace().equals(Namespaces.GML32)) {\\n[ADD] return parsers[1]; // geotools gml3.2 parser\\n[ADD] } else {\\n[ADD] return parsers[0];\\n         }\\n     }\\n ',\n",
              " ' \\n   @Override\\n   public String toString() {\\n[DEL] return \"ListenerRequestMatcher{\" + \"path=\\'\" + path + \\'\\\\\\'\\' + \", methodRequestMatcher=\" + methodRequestMatcher + \\'}\\';\\n[ADD] return \"PathAndMethodRequestMatcher{\" + \"path=\\'\" + path + \\'\\\\\\'\\' + \", methodRequestMatcher=\" + methodRequestMatcher + \\'}\\';\\n   }\\n }',\n",
              " ' \\n   @Override\\n   public void initialise() throws InitialisationException {\\n[DEL] ClassLoader contextClassLoader = currentThread().getContextClassLoader();\\n[DEL] File attributesFile;\\n[DEL] URL resourceFromClassPath = contextClassLoader.getResource(fileLocation);\\n[DEL] if (resourceFromClassPath != null) {\\n[DEL] attributesFile = toFile(resourceFromClassPath);\\n[DEL] } else {\\n[DEL] attributesFile = new File(fileLocation);\\n[DEL] }\\n[DEL] if (!attributesFile.exists()) {\\n[DEL] throw new ConfigurationPropertiesException(createStaticMessage(String\\n[DEL] .format(\"Couldn\\'t find configuration properties file %s neither on classpath or in file system\", fileLocation)), this);\\n[DEL] }\\n[DEL] if (!attributesFile.getName().endsWith(PROPERTIES_EXTENSION) && !attributesFile.getName().endsWith(YAML_EXTENSION)) {\\n[ADD] if (!fileLocation.endsWith(PROPERTIES_EXTENSION) && !fileLocation.endsWith(YAML_EXTENSION)) {\\n       throw new ConfigurationPropertiesException(createStaticMessage(format(\"Configuration properties file %s must end with yaml or properties extension\",\\n                                                                             fileLocation)),\\n                                                  this);\\n     }\\n \\n[DEL] try (InputStream is = new FileInputStream(attributesFile)) {\\n[DEL] if (attributesFile.getName().endsWith(PROPERTIES_EXTENSION)) {\\n[ADD] InputStream is = null;\\n[ADD] \\n[ADD] try {\\n[ADD] is = resourceProvider.getResourceAsStream(fileLocation);\\n[ADD] if (is == null) {\\n[ADD] File attributesFile = new File(fileLocation);\\n[ADD] if (attributesFile.exists()) {\\n[ADD] is = new FileInputStream(attributesFile);\\n[ADD] }\\n[ADD] }\\n[ADD] } catch (Exception e) {\\n[ADD] // ignore, will detect if is is null\\n[ADD] }\\n[ADD] \\n[ADD] if (is == null) {\\n[ADD] throw new ConfigurationPropertiesException(createStaticMessage(\\n[ADD] String.format(\\n[ADD] \"Couldn\\'t find configuration properties file %s neither on classpath or in file system\",\\n[ADD] fileLocation)),\\n[ADD] this);\\n[ADD] }\\n[ADD] \\n[ADD] try {\\n[ADD] if (fileLocation.endsWith(PROPERTIES_EXTENSION)) {\\n         Properties properties = new Properties();\\n         properties.load(is);\\n         properties.keySet().stream().map(key -> {',\n",
              " '          }\\n       };\\n       \\n[ADD] prefs_.helpFontSizePoints().bind(new CommandWithArg<Double>()\\n[ADD] {\\n[ADD] public void execute(Double value)\\n[ADD] {\\n[ADD] refresh();\\n[ADD] }\\n[ADD] });\\n       \\n       ensureWidget();\\n    }',\n",
              " '     return getAll(storeName, Predicates.<String>alwaysTrue());\\n   }\\n \\n[ADD] @Override\\n[ADD] public List<String> getStateNames(String storeName, Predicate<String> predicate) throws IOException {\\n[ADD] List<String> names = Lists.newArrayList();\\n[ADD] String path = formPath(storeName);\\n[ADD] \\n[ADD] List<String> children = propStore.getChildNames(path, 0);\\n[ADD] \\n[ADD] if (children != null) {\\n[ADD] for (String c : children) {\\n[ADD] if (predicate.apply(c)) {\\n[ADD] names.add(c);\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] return names;\\n[ADD] }\\n   @Override\\n   public void createAlias(String storeName, String original, String alias) throws IOException {\\n     String pathOriginal = formPath(storeName, original);\\n[DEL] String pathAlias = formPath(storeName, alias);\\n     byte[] data;\\n \\n     if (!propStore.exists(pathOriginal, 0)) {',\n",
              " '  */\\n package com.ning.http.client.providers.grizzly;\\n \\n[DEL] import com.ning.http.client.AsyncHandler;\\n[DEL] import com.ning.http.client.ProxyServer;\\n[DEL] import com.ning.http.client.Request;\\n[DEL] import com.ning.http.client.providers.grizzly.events.GracefulCloseEvent;\\n[DEL] import com.ning.http.client.uri.Uri;\\n[DEL] import com.ning.http.client.ws.WebSocket;\\n[DEL] import com.ning.http.util.AsyncHttpProviderUtils;\\n[DEL] import com.ning.http.util.ProxyUtils;\\n[DEL] \\n import java.io.IOException;\\n[DEL] import java.util.HashSet;\\n[DEL] import java.util.Set;\\n \\n import org.glassfish.grizzly.CloseListener;\\n import org.glassfish.grizzly.CloseType;',\n",
              " ' import gobblin.config.store.api.ConfigStoreWithResolution;\\n \\n /**\\n[DEL] * ConfigStoreBackedValueInspector always query the underline {@link ConfigStore} to get the freshest\\n[ADD] * ConfigStoreBackedValueInspector always query the underline {@link ConfigStore} to get the freshest\\n  * {@link com.typesafe.config.Config}\\n  * @author mitu\\n  *',\n",
              " '             }\\n             set\\n             {\\n[DEL] isFrozenExplicitly = value;\\n[DEL] OnNodeModified();\\n[ADD] isFrozenExplicitly = value;\\n[ADD] RaisePropertyChanged(\"IsFrozen\");\\n[ADD] //If the node is Unfreezed then Mark all the downstream nodes as\\n[ADD] // modified. This is essential recompiling the AST.\\n[ADD] if (!value)\\n[ADD] {\\n[ADD] MarkDownStreamNodesAsModified(this);\\n[ADD] OnNodeModified();\\n[ADD] }\\n[ADD] //If the node is frozen, then do not execute the graph immediately.\\n[ADD] else\\n[ADD] {\\n[ADD] OnToggleNodeFreeze();\\n[ADD] }\\n             }\\n         }\\n        ',\n",
              " \"         return network;\\n     }\\n \\n[ADD] @Override\\n[ADD] public boolean isSharedNetworkWithoutSpecifyVlan(NetworkOffering offering) {\\n[ADD] if (offering == null || offering.getTrafficType() != TrafficType.Guest || offering.getGuestType() != GuestType.Shared) {\\n[ADD] return false;\\n[ADD] }\\n[ADD] if (! offering.isSpecifyVlan()) {\\n[ADD] return true;\\n[ADD] }\\n[ADD] return false;\\n[ADD] }\\n[ADD] \\n[ADD] private boolean isPrivateGatewayWithoutSpecifyVlan(NetworkOffering ntwkOff) {\\n[ADD] return ntwkOff.getId() == _networkOfferingDao.findByUniqueName(NetworkOffering.SystemPrivateGatewayNetworkOfferingWithoutVlan).getId();\\n[ADD] }\\n[ADD] \\n     /**\\n      * Encodes VLAN/VXLAN ID into a Broadcast URI according to the isolation method from the Physical Network.\\n      * @return Broadcast URI, e.g. 'vlan://vlan_ID' or 'vxlan://vlxan_ID'\",\n",
              " '         case Js::OpCode::StatementBoundary:\\n             // This instruction is merely to help convey source info through the IR\\n             // and eventually generate the nativeOffset maps.\\n[DEL] break;\\n[ADD] #if DBG_DUMP && DBG\\n[ADD] // If we have a JITStatementBreakpoint, then we should break on this statement\\n[ADD] {\\n[ADD] uint32 statementIndex = instr->AsPragmaInstr()->m_statementIndex;\\n[ADD] if (Js::Configuration::Global.flags.StatementDebugBreak.Contains(instr->m_func->GetFunctionNumber(), statementIndex))\\n[ADD] {\\n[ADD] IR::Instr* tempinstr = instr;\\n[ADD] // go past any labels, and then add a debug breakpoint\\n[ADD] while (tempinstr->m_next != nullptr && tempinstr->m_next->m_opcode == Js::OpCode::Label)\\n[ADD] {\\n[ADD] tempinstr = tempinstr->m_next;\\n[ADD] }\\n[ADD] this->m_lowererMD.GenerateDebugBreak(tempinstr);\\n[ADD] }\\n[ADD] }\\n[ADD] #endif\\n[ADD] break;\\n \\n         case Js::OpCode::BailOnNotPolymorphicInlinee:\\n             instrPrev = LowerBailOnNotPolymorphicInlinee(instr);',\n",
              " ' import org.apache.hudi.common.config.SerializableConfiguration;\\n import org.apache.hudi.common.model.HoodieRecord;\\n import org.apache.hudi.common.model.WriteOperationType;\\n[ADD] import org.apache.hudi.common.util.ObjectSizeCalculator;\\n import org.apache.hudi.index.HoodieIndex;\\n import org.apache.hudi.operator.event.BatchWriteSuccessEvent;\\n import org.apache.hudi.table.action.commit.FlinkWriteHelper;',\n",
              " '   @Test\\n   public void testMatchOnNoArgs() throws Exception {\\n     ReflectionEntryPointResolver resolver = new ReflectionEntryPointResolver();\\n[DEL] final Event event = Event.builder(DefaultEventContext.create(flowConstruct, TEST_CONNECTOR)).message(of(null)).build();\\n[ADD] final Event event =\\n[ADD] Event.builder(DefaultEventContext.create(flowConstruct, fromSingleComponent(TEST_CONNECTOR))).message(of(null)).build();\\n     // This should fail because the Kiwi.bite() method has a void return type, and by default\\n     // void methods are ignorred\\n     MuleEventContext eventContext = new DefaultMuleEventContext(flowConstruct, event);',\n",
              " ' import javax.xml.bind.DatatypeConverter;\\n \\n /**\\n[DEL] * Unique ID for task, worker, function...\\n[ADD] * Unique ID for task, worker, function.\\n  */\\n public class UniqueID implements Serializable {\\n ',\n",
              " '     return message;\\n   }\\n \\n[ADD] /**\\n[ADD] * {@inheritDoc}\\n[ADD] */\\n[ADD] @Override\\n[ADD] public Error getError() {\\n[ADD] return error;\\n[ADD] }\\n[ADD] \\n   @Override\\n   public byte[] getMessageAsBytes(MuleContext muleContext) throws DefaultMuleException {\\n     try {',\n",
              " '       return false;\\n     }\\n     HoodieLogFile that = (HoodieLogFile) o;\\n[DEL] return path != null ? path.equals(that.path) : that.path == null;\\n[ADD] return Objects.equals(pathStr, that.pathStr);\\n   }\\n \\n   @Override\\n   public int hashCode() {\\n[DEL] return path != null ? path.hashCode() : 0;\\n[ADD] return Objects.hash(pathStr);\\n   }\\n \\n   @Override\\n   public String toString() {\\n[DEL] return \"HoodieLogFile {\" + path + \\'}\\';\\n[ADD] return \"HoodieLogFile{\"\\n[ADD] + \"pathStr=\\'\" + pathStr + \\'\\\\\\'\\'\\n[ADD] + \", fileLen=\" + fileLen\\n[ADD] + \\'}\\';\\n   }\\n }',\n",
              " '       this.defaultMetadata.addTransferEncoding(c.getTag());\\n     }\\n \\n[DEL] String partitionPath = builder.getPartitionPath(properties);\\n[ADD] this.partitionKey = builder.getPartitionPath(properties);\\n     if (builder.getPartitionPath(properties) != null) {\\n[DEL] properties.setProp(ConfigurationKeys.WRITER_PARTITION_PATH_KEY + builder.getWriterId(), partitionPath);\\n[ADD] properties.setProp(ConfigurationKeys.WRITER_PARTITION_PATH_KEY + builder.getWriterId(), partitionKey);\\n     }\\n   }\\n ',\n",
              " \" \\t\\t\\t\\t\\tquot1 = res;\\n \\n \\t\\t\\t\\t\\t// Write the digit into the correct position\\n[DEL] /*[IF Sidecar19-SE]*/\\n[DEL] helpers.putCharInArrayByIndex(value, index1--, (char) ('0' - rem));\\n[DEL] /*[ELSE]*/\\n \\t\\t\\t\\t\\tvalue[index1--] = (char) ('0' - rem);\\n[DEL] /*[ENDIF]*/\\n \\t\\t\\t\\t} while (quot1 != 0);\\n \\n \\t\\t\\t\\tif (v1 < 0) {\\n[DEL] /*[IF Sidecar19-SE]*/\\n[DEL] helpers.putCharInArrayByIndex(value, index1--, (char) '-');\\n[DEL] /*[ELSE]*/\\n \\t\\t\\t\\t\\tvalue[index1--] = '-';\\n[DEL] /*[ENDIF]*/\\n \\t\\t\\t\\t}\\n \\t\\t\\t\\t\\n \\t\\t\\t\\tinitCompressionFlag();\\n \\t\\t\\t}\\n \\t\\t} else {\\n[DEL] /*[IF Sidecar19-SE]*/\\n[DEL] value = new byte[len * 2];\\n[DEL] /*[ELSE]*/\\n \\t\\t\\tvalue = new char[len];\\n[DEL] /*[ENDIF]*/\\n \\t\\t\\tcount = len;\\n \\n \\t\\t\\tint start = len;\\n \\n \\t\\t\\t// Copy in s3 contents\\n \\t\\t\\tstart = start - s3len;\\n[DEL] /*[IF Sidecar19-SE]*/\\n[DEL] decompressedArrayCopy(s3.value, 0, value, start, s3len);\\n[DEL] /*[ELSE]*/\\n \\t\\t\\tSystem.arraycopy(s3.value, 0, value, start, s3len);\\n[DEL] /*[ENDIF]*/\\n \\n \\t\\t\\t// Copy in s2 contents\\n \\t\\t\\tstart = start - s2len;\\n[DEL] /*[IF Sidecar19-SE]*/\\n[DEL] decompressedArrayCopy(s2.value, 0, value, start, s2len);\\n[DEL] /*[ELSE]*/\\n \\t\\t\\tSystem.arraycopy(s2.value, 0, value, start, s2len);\\n[DEL] /*[ENDIF]*/\\n \\n \\t\\t\\t// Copy in v2\\n \\t\\t\\tint index2 = start - 1;\",\n",
              " '             .build()).withAutoCommit(false).withProperties(properties).build();\\n     // Create the first commit\\n     createCommitWithInserts(cfg, getHoodieWriteClient(cfg), \"000\", \"001\", 200);\\n[DEL] try {\\n[DEL] ExecutorService executors = Executors.newFixedThreadPool(2);\\n[DEL] SparkRDDWriteClient client1 = getHoodieWriteClient(cfg);\\n[DEL] SparkRDDWriteClient client2 = getHoodieWriteClient(cfg);\\n[DEL] Future future1 = executors.submit(() -> {\\n[DEL] String newCommitTime = \"004\";\\n[DEL] int numRecords = 100;\\n[DEL] String commitTimeBetweenPrevAndNew = \"002\";\\n[DEL] try {\\n[DEL] createCommitWithUpserts(cfg, client1, \"002\", commitTimeBetweenPrevAndNew, newCommitTime, numRecords);\\n[DEL] } catch (Exception e1) {\\n[DEL] assertTrue(e1 instanceof HoodieWriteConflictException);\\n[DEL] throw new RuntimeException(e1);\\n[DEL] }\\n[DEL] });\\n[DEL] Future future2 = executors.submit(() -> {\\n[DEL] String newCommitTime = \"005\";\\n[DEL] int numRecords = 100;\\n[DEL] String commitTimeBetweenPrevAndNew = \"002\";\\n[DEL] try {\\n[DEL] createCommitWithUpserts(cfg, client2, \"002\", commitTimeBetweenPrevAndNew, newCommitTime, numRecords);\\n[DEL] } catch (Exception e2) {\\n[DEL] assertTrue(e2 instanceof HoodieWriteConflictException);\\n[DEL] throw new RuntimeException(e2);\\n[DEL] }\\n[DEL] });\\n[DEL] future1.get();\\n[DEL] future2.get();\\n[DEL] fail(\"Should not reach here, this means concurrent writes were handled incorrectly\");\\n[DEL] } catch (Exception e) {\\n[DEL] // Expected to fail due to overlapping commits\\n[DEL] }\\n[ADD] ExecutorService executors = Executors.newFixedThreadPool(2);\\n[ADD] SparkRDDWriteClient client1 = getHoodieWriteClient(cfg);\\n[ADD] SparkRDDWriteClient client2 = getHoodieWriteClient(cfg);\\n[ADD] Future future1 = executors.submit(() -> {\\n[ADD] String newCommitTime = \"004\";\\n[ADD] int numRecords = 100;\\n[ADD] String commitTimeBetweenPrevAndNew = \"002\";\\n[ADD] try {\\n[ADD] createCommitWithUpserts(cfg, client1, \"002\", commitTimeBetweenPrevAndNew, newCommitTime, numRecords);\\n[ADD] } catch (Exception e1) {\\n[ADD] assertTrue(e1 instanceof HoodieWriteConflictException);\\n[ADD] throw new RuntimeException(e1);\\n[ADD] }\\n[ADD] });\\n[ADD] Future future2 = executors.submit(() -> {\\n[ADD] String newCommitTime = \"005\";\\n[ADD] int numRecords = 100;\\n[ADD] String commitTimeBetweenPrevAndNew = \"002\";\\n[ADD] try {\\n[ADD] createCommitWithUpserts(cfg, client2, \"002\", commitTimeBetweenPrevAndNew, newCommitTime, numRecords);\\n[ADD] } catch (Exception e2) {\\n[ADD] assertTrue(e2 instanceof HoodieWriteConflictException);\\n[ADD] throw new RuntimeException(e2);\\n[ADD] }\\n[ADD] });\\n[ADD] future1.get();\\n[ADD] future2.get();\\n   }\\n \\n   @Test',\n",
              " '     @Override\\n     public NextAction handleRead(FilterChainContext ctx) throws IOException\\n     {\\n[DEL] ctx.getAttributes().setAttribute(HTTPS.getScheme(), true);\\n[DEL] NextAction nextAction = super.handleRead(ctx);\\n[DEL] ctx.getAttributes().setAttribute(SSL_SESSION_ATTRIBUTE_KEY, getSslSession(ctx));\\n[DEL] return nextAction;\\n[ADD] try\\n[ADD] {\\n[ADD] ctx.getAttributes().setAttribute(HTTPS.getScheme(), true);\\n[ADD] NextAction nextAction = super.handleRead(ctx);\\n[ADD] ctx.getAttributes().setAttribute(SSL_SESSION_ATTRIBUTE_KEY, getSslSession(ctx));\\n[ADD] return nextAction;\\n[ADD] }\\n[ADD] catch (SSLHandshakeException e)\\n[ADD] {\\n[ADD] logger.error(\"SSL handshake error: \" + e.getMessage());\\n[ADD] throw e;\\n[ADD] }\\n     }\\n \\n     private SSLSession getSslSession(FilterChainContext ctx) throws SSLPeerUnverifiedException',\n",
              " ' \\t\\tthis.lifecycleLock.lock();\\n \\t\\ttry {\\n \\t\\t\\treturn this.running;\\n[DEL] } finally {\\n[ADD] }\\n[ADD] finally {\\n \\t\\t\\tthis.lifecycleLock.unlock();\\n \\t\\t}\\n \\t}\\n \\n \\tpublic int getPhase() {\\n[DEL] return phase;\\n[ADD] return this.phase;\\n \\t}\\n \\n \\tpublic void setPhase(int phase) {\\n \\t\\tthis.phase = phase;\\n \\t}\\n \\n[DEL] \\n \\tpublic boolean isAutoStartup() {\\n[DEL] return autoStartup;\\n[ADD] return this.autoStartup;\\n \\t}\\n[DEL] \\n[DEL] public void setAutoStartup(boolean autostartup) {\\n[DEL] this.autoStartup = autostartup;\\n[ADD] \\n[ADD] public void setAutoStartup(boolean autoStartup) {\\n[ADD] this.autoStartup = autoStartup;\\n \\t}\\n \\n \\tpublic void stop(Runnable callback) {',\n",
              " ' \\n       if (status == REJECT) {\\n         if (LOGGER.isDebugEnabled()) {\\n[DEL] itemId = pollItem.getItemId().orElseGet(() -> pollItem.getResult().getAttributes().map(Object::toString).orElse(\"\"));\\n[ADD] itemId = getItemId(pollItem);\\n           LOGGER.debug(\"Source in flow \\'{}\\' is skipping item \\'{}\\' because it was rejected by the watermark\", flowName, itemId);\\n         }\\n       }',\n",
              " '         // loop through and remove from queues\\n         Cursor cur = null;\\n         try {\\n[DEL] cur = mCol.getDb().getDatabase().query(String.format(Locale.US,\\n[DEL] \"select id, queue from cards where nid=%d and id!=%d \"+\\n[DEL] \"and (queue=\" + Consts.QUEUE_TYPE_NEW + \" or (queue=\" + Consts.QUEUE_TYPE_REV + \" and due<=%d))\", card.getNid(), card.getId(), mToday), null);\\n[ADD] cur = mCol.getDb().getDatabase().query(\\n[ADD] \"select id, queue from cards where nid=? and id!=? \"+\\n[ADD] \"and (queue=\" + Consts.QUEUE_TYPE_NEW + \" or (queue=\" + Consts.QUEUE_TYPE_REV + \" and due<=?))\",\\n[ADD] new Object[] {card.getNid(), card.getId(), mToday});\\n             while (cur.moveToNext()) {\\n                 long cid = cur.getLong(0);\\n                 int queue = cur.getInt(1);',\n",
              " ' \\n     @Override\\n     public boolean revertSnapshot(SnapshotInfo snapshot) {\\n[DEL] throw new CloudRuntimeException(\"revert Snapshot is not supported\");\\n[ADD] if (canHandle(snapshot,SnapshotOperation.REVERT) == StrategyPriority.CANT_HANDLE) {\\n[ADD] throw new UnsupportedOperationException(\"Reverting not supported. Create a template or volume based on the snapshot instead.\");\\n[ADD] }\\n[ADD] \\n[ADD] SnapshotVO snapshotVO = snapshotDao.acquireInLockTable(snapshot.getId());\\n[ADD] \\n[ADD] if (snapshotVO == null) {\\n[ADD] throw new CloudRuntimeException(\"Failed to get lock on snapshot:\" + snapshot.getId());\\n[ADD] }\\n[ADD] \\n[ADD] try {\\n[ADD] VolumeInfo volumeInfo = snapshot.getBaseVolume();\\n[ADD] StoragePool store = (StoragePool)volumeInfo.getDataStore();\\n[ADD] \\n[ADD] if (store != null && store.getStatus() != StoragePoolStatus.Up) {\\n[ADD] snapshot.processEvent(Event.OperationFailed);\\n[ADD] \\n[ADD] throw new CloudRuntimeException(\"store is not in up state\");\\n[ADD] }\\n[ADD] \\n[ADD] volumeInfo.stateTransit(Volume.Event.RevertSnapshotRequested);\\n[ADD] \\n[ADD] boolean result = false;\\n[ADD] \\n[ADD] try {\\n[ADD] result =  snapshotSvr.revertSnapshot(snapshot);\\n[ADD] \\n[ADD] if (!result) {\\n[ADD] s_logger.debug(\"Failed to revert snapshot: \" + snapshot.getId());\\n[ADD] \\n[ADD] throw new CloudRuntimeException(\"Failed to revert snapshot: \" + snapshot.getId());\\n[ADD] }\\n[ADD] } finally {\\n[ADD] if (result) {\\n[ADD] volumeInfo.stateTransit(Volume.Event.OperationSucceeded);\\n[ADD] } else {\\n[ADD] volumeInfo.stateTransit(Volume.Event.OperationFailed);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] return result;\\n[ADD] } finally {\\n[ADD] if (snapshotVO != null) {\\n[ADD] snapshotDao.releaseFromLockTable(snapshot.getId());\\n[ADD] }\\n[ADD] }\\n     }\\n \\n     @Override',\n",
              " '         String ip6addr = null;\\n         //Isolated network can exist in Basic zone only, so no need to verify the zone type\\n         if (network.getGuestType() == Network.GuestType.Isolated) {\\n[DEL] try {\\n[DEL] if (ipv4Address != null) {\\n[DEL] ipaddr = _ipAddrMgr.allocateGuestIP(network, ipv4Address);\\n[DEL] }\\n[DEL] if (ipv6Address != null) {\\n[DEL] ip6addr = ipv6AddrMgr.allocateGuestIpv6(network, ipv6Address);\\n[DEL] }\\n[DEL] } catch (InsufficientAddressCapacityException e) {\\n[DEL] throw new InvalidParameterValueException(\"Allocating guest ip for nic failed\");\\n[ADD] if ((ipv4Address != null || NetUtils.isIpv4(network.getGateway()) && org.apache.commons.lang3.StringUtils.isBlank(ipv6Address))) {\\n[ADD] ipaddr = _ipAddrMgr.allocateGuestIP(network, ipv4Address);\\n[ADD] }\\n[ADD] if (ipv6Address != null) {\\n[ADD] ip6addr = ipv6AddrMgr.allocateGuestIpv6(network, ipv6Address);\\n             }\\n         } else if (network.getGuestType() == Network.GuestType.Shared) {\\n             //for basic zone, need to provide the podId to ensure proper ip alloation',\n",
              " '                 .apply(\\n                     \"ReadFiles\",\\n                     ParDo.of(\\n[DEL] new DoFn<String, TableRow>() {\\n[ADD] new DoFn<String, T>() {\\n                               @ProcessElement\\n                               public void processElement(ProcessContext c) throws Exception {\\n                                 TableSchema schema =\\n                                     BigQueryHelpers.fromJsonString(\\n                                         c.sideInput(schemaView), TableSchema.class);\\n                                 String jobUuid = c.sideInput(jobIdTokenView);\\n[DEL] BigQuerySourceBase source = createSource(jobUuid);\\n[DEL] List<BoundedSource<TableRow>> sources =\\n[ADD] BigQuerySourceBase<T> source = createSource(jobUuid, coder);\\n[ADD] List<BoundedSource<T>> sources =\\n                                     source.createSources(\\n                                         ImmutableList.of(\\n                                             FileSystems.matchNewResource(\\n                                                 c.element(), false /* is directory */)),\\n                                         schema);\\n                                 checkArgument(sources.size() == 1, \"Expected exactly one source.\");\\n[DEL] BoundedSource<TableRow> avroSource = sources.get(0);\\n[DEL] BoundedSource.BoundedReader<TableRow> reader =\\n[ADD] BoundedSource<T> avroSource = sources.get(0);\\n[ADD] BoundedSource.BoundedReader<T> reader =\\n                                     avroSource.createReader(c.getPipelineOptions());\\n                                 for (boolean more = reader.start(); more; more = reader.advance()) {\\n                                   c.output(reader.getCurrent());',\n",
              " '       return toBuilder().setHintMatchesManyFiles(true).build();\\n     }\\n \\n[ADD] /**\\n[ADD] * Allows the user the opt out of getting recordNums associated with each record.\\n[ADD] *\\n[ADD] * <p>ContextualTextIO uses a shuffle step to assemble the recordNums for each record which may\\n[ADD] * result in some performance loss.\\n[ADD] *\\n[ADD] * <p>Use this when metadata like fileNames are required and their position/order can be\\n[ADD] * ignored.\\n[ADD] */\\n[ADD] public Read withoutLineNumMetadata() {\\n[ADD] return toBuilder().setWithoutLineNumMetadata(true).build();\\n[ADD] }\\n[ADD] \\n     /** See {@link MatchConfiguration#withEmptyMatchTreatment}. */\\n     public Read withEmptyMatchTreatment(EmptyMatchTreatment treatment) {\\n       return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));',\n",
              " '  *\\n  * Also this class supports sending multiple records in a batch manner.\\n  *\\n[DEL] * This class use Base64 to encode any byte array, so the consumer side should use the corresponding\\n[DEL] * decoder to recover the original bytes.\\n  */\\n @Slf4j\\n public class EventhubDataWriter implements SyncDataWriter<byte[]>, BatchAsyncDataWriter<byte[]> {',\n",
              " '    * <p>For example, when reading from \"gs://bucket/path\", the scheme \"gs\" is\\n    * used to lookup the appropriate factory.\\n    */\\n[ADD] @VisibleForTesting\\n   public static void setIOFactory(String scheme, IOChannelFactory factory) {\\n     FACTORY_MAP.put(scheme, factory);\\n   }',\n",
              " '     activeTimeline.createNewInstant(instant2);\\n     // create replace metadata only with replaced file Ids (no new files created)\\n     activeTimeline.saveAsComplete(instant2,\\n[DEL] Option.of(getReplaceCommitMetadata(basePath, ts2, replacePartition,0, newFilePartition,3, Collections.emptyMap())));\\n[ADD] Option.of(getReplaceCommitMetadata(basePath, ts2, replacePartition, 0, newFilePartition, 3, Collections.emptyMap())));\\n     metaClient.reloadActiveTimeline();\\n     partitions = TimelineUtils.getAffectedPartitions(metaClient.getActiveTimeline().findInstantsAfter(\"1\", 10));\\n     assertEquals(1, partitions.size());',\n",
              " ' import org.apache.beam.sdk.testing.PAssert;\\n import org.apache.beam.sdk.testing.TestPipeline;\\n import org.apache.beam.sdk.transforms.Create;\\n[DEL] import org.apache.beam.sdk.transforms.ParDo;\\n import org.apache.beam.sdk.values.PCollection;\\n import org.apache.beam.sdk.values.PCollectionTuple;\\n import org.apache.beam.sdk.values.Row;\\n import org.apache.beam.sdk.values.TimestampedValue;\\n[DEL] import org.apache.beam.sdk.values.TupleTagList;\\n import org.apache.beam.vendor.calcite.v1_20_0.com.google.common.collect.ImmutableMap;\\n import org.apache.beam.vendor.calcite.v1_20_0.com.google.common.collect.ImmutableSet;\\n import org.joda.time.DateTime;\\n import org.joda.time.Instant;\\n[ADD] import org.junit.Assert;\\n import org.junit.Rule;\\n import org.junit.Test;\\n ',\n",
              " '                      }\\n                   });\\n          }\\n[DEL] }));\\n[ADD] };\\n[ADD] \\n[ADD] dependencyManager_.withRMarkdown(\"R Notebook\", \"Creating R Notebooks\", createNotebookCmd);\\n    }\\n    \\n    @Inject',\n",
              " '                 requestParams.addContent(new Element(e.getKey()).setText(e.getValue()));\\n             }\\n         }\\n[ADD] \\n[ADD] if (StringUtils.isNotEmpty(transformation)) {\\n[ADD] requestParams.addContent(new Element(\"transformation\", transformation));\\n[ADD] }\\n[ADD] \\n         root.addContent(requestParams);\\n         root.addContent(descKeys);\\n         root.addContent(gui);',\n",
              " '       ConfigurationKeys.CONFIG_BASED_PREFIX + \".dataset.common.root\";\\n \\n   // In addition to the white/blacklist tags, this configuration let the user to black/whitelist some datasets\\n[DEL] // in the job-level configuration, which is not in configStore\\n[DEL] // as to have easier approach to black/whitelist some datasets.\\n[ADD] // in the job-level configuration, which is not specified in configStore\\n[ADD] // as to have easier approach to black/whitelist some datasets on operation side.\\n   // The semantics keep still as tag, which the blacklist override whitelist if any dataset in common.\\n   public static final String JOB_LEVEL_BLACKLIST = CopyConfiguration.COPY_PREFIX + \".configBased.blacklist\" ;\\n   public static final String JOB_LEVEL_WHITELIST = CopyConfiguration.COPY_PREFIX + \".configBased.whitelist\" ;',\n",
              " '             }\\n             if (BuildConfig.DEBUG) {\\n                 Timber.i(\"Debug mode, option for showing onboarding walkthrough\");\\n[DEL] android.preference.CheckBoxPreference onboardingPreference = new android.preference.CheckBoxPreference(requireContext());\\n[ADD] CheckBoxPreference onboardingPreference = new CheckBoxPreference(requireContext());\\n                 onboardingPreference.setKey(\"showOnboarding\");\\n                 onboardingPreference.setTitle(R.string.show_onboarding);\\n                 onboardingPreference.setSummary(R.string.show_onboarding_desc);',\n",
              " '       throw new CopyNotSupportedException(convertedRecord + \" is not copyable\");\\n     }\\n \\n[DEL] for (int i = 0; i < branches; i++) {\\n[DEL] if (this.forks.get(i).isPresent() && forkedRecords.get(i)) {\\n[DEL] this.forks.get(i).get().processRecord(makesCopy ? ((Copyable) convertedRecord).copy() : convertedRecord);\\n[ADD] // If the record has been successfully put into the queues of every forks\\n[ADD] boolean allPutsSucceeded = false;\\n[ADD] // Use an array of primitive boolean type to avoid unnecessary boxing/unboxing\\n[ADD] boolean[] succeededPuts = new boolean[branches];\\n[ADD] // Put the record into the record queue of each fork. A put may timeout and return a false, in which\\n[ADD] // case the put needs to be retried in the next iteration along with other failed puts. This goes on\\n[ADD] // until all puts succeed, at which point the task moves to the next record.\\n[ADD] while (!allPutsSucceeded) {\\n[ADD] allPutsSucceeded = true;\\n[ADD] for (int i = 0; i < branches && !succeededPuts[i]; i++) {\\n[ADD] if (this.forks.get(i).isPresent() && forkedRecords.get(i)) {\\n[ADD] boolean succeeded =\\n[ADD] this.forks.get(i).get().putRecord(makesCopy ? ((Copyable) convertedRecord).copy() : convertedRecord);\\n[ADD] succeededPuts[i] = succeeded;\\n[ADD] if (!succeeded) {\\n[ADD] allPutsSucceeded = false;\\n[ADD] }\\n[ADD] } else {\\n[ADD] succeededPuts[i] = true;\\n[ADD] }\\n       }\\n     }\\n   }',\n",
              " ' \\n   private final KafkaOffsetGen offsetGen;\\n \\n[ADD] private final String useCustomDeserializerProp = \"hoodie.deltastreamer.kafka.custom.avro.deserializer\";\\n[ADD] \\n   public AvroKafkaSource(TypedProperties props, JavaSparkContext sparkContext, SparkSession sparkSession,\\n       SchemaProvider schemaProvider) {\\n     super(props, sparkContext, sparkSession, schemaProvider);\\n[ADD] boolean useCustomDeserializer = props.getBoolean(useCustomDeserializerProp, false);\\n     props.put(\"key.deserializer\", StringDeserializer.class);\\n[DEL] props.put(\"value.deserializer\", KafkaAvroDeserializer.class);\\n[ADD] props.put(\"value.deserializer\", useCustomDeserializer ? HoodieAvroKafkaDeserializer.class : KafkaAvroDeserializer.class);\\n     offsetGen = new KafkaOffsetGen(props);\\n   }\\n ',\n",
              " ' \\n     @Override\\n     public boolean isUp() {\\n[DEL] boolean result = true;\\n[DEL] for (MongoIndexSet indexSet : findAllMongoIndexSets()) {\\n[DEL] if (indexSet.getConfig().isWritable()) {\\n[DEL] result = result && indexSet.isUp();\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] return result;\\n[ADD] return findAllMongoIndexSets().stream()\\n[ADD] .filter(indexSet -> indexSet.getConfig().isWritable())\\n[ADD] .allMatch(MongoIndexSet::isUp);\\n     }\\n \\n     @Override',\n",
              " '   }\\n \\n   public void addMetadata(String metaKey, String value) {\\n[DEL] extraMetadataMap.put(metaKey, value);\\n[ADD] extraMetadata.put(metaKey, value);\\n   }\\n \\n   public List<HoodieWriteStat> getWriteStats(String partitionPath) {',\n",
              " ' import static org.assertj.core.api.Assertions.assertThatThrownBy;\\n import static org.mockito.Mockito.mock;\\n import static org.mockito.Mockito.when;\\n[DEL] import static org.objectweb.asm.Opcodes.*;\\n[ADD] import static org.objectweb.asm.Opcodes.DADD;\\n[ADD] import static org.objectweb.asm.Opcodes.DDIV;\\n[ADD] import static org.objectweb.asm.Opcodes.DMUL;\\n[ADD] import static org.objectweb.asm.Opcodes.DREM;\\n[ADD] import static org.objectweb.asm.Opcodes.DSUB;\\n[ADD] import static org.objectweb.asm.Opcodes.GOTO;\\n[ADD] import static org.objectweb.asm.Opcodes.ICONST_0;\\n[ADD] import static org.objectweb.asm.Opcodes.ILOAD;\\n[ADD] import static org.objectweb.asm.Opcodes.ISTORE;\\n[ADD] import static org.objectweb.asm.Opcodes.LADD;\\n[ADD] import static org.objectweb.asm.Opcodes.LAND;\\n[ADD] import static org.objectweb.asm.Opcodes.LDIV;\\n[ADD] import static org.objectweb.asm.Opcodes.LMUL;\\n[ADD] import static org.objectweb.asm.Opcodes.LOR;\\n[ADD] import static org.objectweb.asm.Opcodes.LREM;\\n[ADD] import static org.objectweb.asm.Opcodes.LSHL;\\n[ADD] import static org.objectweb.asm.Opcodes.LSHR;\\n[ADD] import static org.objectweb.asm.Opcodes.LSUB;\\n[ADD] import static org.objectweb.asm.Opcodes.LUSHR;\\n[ADD] import static org.objectweb.asm.Opcodes.LXOR;\\n[ADD] import static org.objectweb.asm.Opcodes.RETURN;\\n \\n public class BytecodeEGWalkerExecuteTest {\\n ',\n",
              " '             }\\n         }\\n         counter = getInvokeCounter(runs, LeastActiveLoadBalance.NAME);\\n[DEL] for (Invoker minvoker : counter.keySet()) {\\n[DEL] Long count = counter.get(minvoker).get();\\n[DEL] }\\n         Assert.assertEquals(runs, counter.get(invoker1).intValue());\\n         Assert.assertEquals(0, counter.get(invoker2).intValue());\\n         Assert.assertEquals(0, counter.get(invoker3).intValue());',\n",
              " ' \\t\\t\\t * as it is not set up for lookup objects by default.\\n \\t\\t\\t */\\n \\t\\t\\tint fullAccessMode = FULL_ACCESS_MASK | MODULE | UNCONDITIONAL;\\n[ADD] \\n[ADD] switch(dropMode) {\\n[ADD] case PUBLIC:\\n[ADD] case MODULE:\\n[ADD] case PACKAGE:\\n[ADD] case PRIVATE:\\n[ADD] case PROTECTED:\\n[ADD] case UNCONDITIONAL:\\n[ADD] /* dropMode is OK */\\n[ADD] break;\\n[ADD] default:\\n[ADD] /*[MSG \"K065R\", \"The requested lookup mode: 0x{0} is not one of the existing access modes: 0x{1}\"]*/\\n[ADD] throw new IllegalArgumentException(com.ibm.oti.util.Msg.getString(\"K065R\", Integer.toHexString(dropMode), Integer.toHexString(fullAccessMode))); //$NON-NLS-1$\\n[ADD] }\\n \\n[ADD] /*[IF Java14]*/\\n[ADD] /* The lookup object has to discard the protected access by default */\\n[ADD] int newAccessMode = accessMode & ~PROTECTED;\\n[ADD] /*[ELSE]*/\\n \\t\\t\\t/* The lookup object has to discard the protected and unconditional access by default */\\n \\t\\t\\tint newAccessMode = accessMode & ~(PROTECTED | UNCONDITIONAL);\\n[ADD] /*[ENDIF] Java14*/\\n \\t\\t\\t\\n[DEL] switch (dropMode) {\\n[ADD] /* The access mode to be dropped must exist in the current access mode;\\n[ADD] * otherwise, the new access mode remains unchanged.\\n[ADD] */\\n[ADD] switch (dropMode & newAccessMode) {\\n \\t\\t\\tcase PUBLIC:\\n \\t\\t\\t\\tnewAccessMode = NO_ACCESS;\\n \\t\\t\\t\\tbreak;\\n[DEL] case MODULE:\\n[DEL] newAccessMode &= ~(MODULE | PACKAGE | PRIVATE);\\n[DEL] break;\\n \\t\\t\\tcase PACKAGE:\\n \\t\\t\\t\\tnewAccessMode &= ~(PACKAGE | PRIVATE);\\n \\t\\t\\t\\tbreak;\\n \\t\\t\\tcase PRIVATE:\\n \\t\\t\\t\\tnewAccessMode &= ~PRIVATE;\\n \\t\\t\\t\\tbreak;\\n[DEL] case PROTECTED:\\n \\t\\t\\tcase UNCONDITIONAL:\\n[ADD] /*[IF Java14]*/\\n[ADD] newAccessMode = NO_ACCESS;\\n[ADD] /*[ENDIF] Java14*/\\n \\t\\t\\t\\tbreak;\\n \\t\\t\\tdefault:\\n[DEL] /*[MSG \"K065R\", \"The requested lookup mode: 0x{0} is not one of the existing access modes: 0x{1}\"]*/\\n[DEL] throw new IllegalArgumentException(com.ibm.oti.util.Msg.getString(\"K065R\", Integer.toHexString(dropMode), Integer.toHexString(fullAccessMode))); //$NON-NLS-1$\\n[ADD] /* no change in the access mode */\\n[ADD] }\\n[ADD] \\n[ADD] /* The exception is MODULE in which case all access bits involved must be dropped\\n[ADD] * whether or not the MODULE bit exists in the access mode.\\n[ADD] */\\n[ADD] if ((dropMode == MODULE) || ((dropMode & newAccessMode) == MODULE)) {\\n[ADD] newAccessMode &= ~(MODULE | PACKAGE | PRIVATE);\\n[ADD] }\\n[ADD] \\n[ADD] /*[IF Java14]*/\\n[ADD] /* There is no previous lookup class for the requested lookup class\\n[ADD] * if the MODULE or UNCONDITIONAL bit is set in the new access mode.\\n[ADD] */\\n[ADD] Class<?> newPrevAccessClass = prevAccessClass;\\n[ADD] if (((newAccessMode & MODULE) == MODULE)\\n[ADD] || ((newAccessMode & UNCONDITIONAL) == UNCONDITIONAL)\\n[ADD] ) {\\n[ADD] newPrevAccessClass = null;\\n \\t\\t\\t}\\n \\t\\t\\t\\n[DEL] return new Lookup(accessClass, newAccessMode);\\n[ADD] return new Lookup(accessClass, newPrevAccessClass, newAccessMode);\\n[ADD] /*[ELSE]*/\\n[ADD] return new Lookup(accessClass, newAccessMode);\\n[ADD] /*[ENDIF] Java14*/\\n \\t\\t}\\n \\t\\t\\n \\t\\t/**',\n",
              " '     return new SampleAnyCombineFn<>(sampleSize);\\n   }\\n \\n[ADD] /**\\n[ADD] * Returns a {@link CombineFn} that computes a single and potentially non-uniform sample value of\\n[ADD] * its inputs.\\n[ADD] */\\n[ADD] public static <T> CombineFn<T, ?, T> anyValueCombineFn() {\\n[ADD] return new AnyValueCombineFn<>();\\n[ADD] }\\n[ADD] \\n   /**\\n    * {@code Sample#any(long)} takes a {@code PCollection<T>} and a limit, and produces a new {@code\\n    * PCollection<T>} containing up to limit elements of the input {@code PCollection}.',\n",
              " '             return volume;\\n \\n         } catch (Exception e) {\\n[DEL] throw new CloudRuntimeException(\"Exception caught during resize volume operation of volume UUID: \" + volume.getUuid(), e);\\n[ADD] throw new CloudRuntimeException(String.format(\"Failed to resize volume operation of volume UUID: [%s] due to - %s\", volume.getUuid(), e.getMessage()));\\n         }\\n     }\\n ',\n",
              " '     Mockito.verifyZeroInteractions(fs);\\n   }\\n \\n[ADD] @Test\\n[ADD] void display_warning_for_missing_bytecode_when_libraries_empty_and_have_java_sources() {\\n[ADD] javaTestClasspath = createJavaClasspath();\\n[ADD] javaTestClasspath.init();\\n[ADD] assertThat(javaTestClasspath.getFilesFromProperty(ClasspathProperties.SONAR_JAVA_TEST_LIBRARIES)).isEmpty();\\n[ADD] assertThat(javaTestClasspath.hasJavaSources()).isTrue();\\n[ADD] \\n[ADD] javaTestClasspath.logSuspiciousEmptyLibraries();\\n[ADD] \\n[ADD] String warning = \"Dependencies/libraries were not provided for analysis of test files. The \\'sonar.java.test.libraries\\' property is empty. \"\\n[ADD] + \"Verify your configuration, as you might end up with less precise results.\";\\n[ADD] assertThat(logTester.logs(LoggerLevel.WARN))\\n[ADD] .hasSize(1)\\n[ADD] .contains(warning);\\n[ADD] }\\n[ADD] \\n[ADD] @Test\\n[ADD] void no_warning_for_missing_bytecode_when_libraries_empty_and_have_no_java_sources() {\\n[ADD] javaTestClasspath = new ClasspathForTest(settings.asConfig(), new DefaultFileSystem(new File(\"src/test/files/classpath/\")));\\n[ADD] javaTestClasspath.init();\\n[ADD] assertThat(javaTestClasspath.getFilesFromProperty(ClasspathProperties.SONAR_JAVA_TEST_LIBRARIES)).isEmpty();\\n[ADD] assertThat(javaTestClasspath.hasJavaSources()).isFalse();\\n[ADD] \\n[ADD] javaTestClasspath.logSuspiciousEmptyLibraries();\\n[ADD] \\n[ADD] assertThat(logTester.logs(LoggerLevel.WARN)).isEmpty();\\n[ADD] }\\n[ADD] \\n   @Test\\n   void libraries_should_accept_path_ending_with_wildcard() {\\n     settings.setProperty(ClasspathProperties.SONAR_JAVA_TEST_LIBRARIES, \"lib/*\");',\n",
              " '     }\\n \\n \\n[ADD] @VisibleForTesting\\n[ADD] void rescheduleWithoutValidation(long[] selectedCardIds, Integer newDays) {\\n[ADD] CollectionTask.launchCollectionTask(DISMISS_MULTI,\\n[ADD] rescheduleCardHandler(),\\n[ADD] new TaskData(new Object[]{selectedCardIds, Collection.DismissType.RESCHEDULE_CARDS, newDays}));\\n[ADD] }\\n[ADD] \\n[ADD] \\n     private void showChangeDeckDialog() {\\n         if (!hasSelectedCards()) {\\n             Timber.i(\"Not showing Change Deck - No Cards\");',\n",
              " '       return false;\\n     }\\n \\n[DEL] private boolean isNullCheck(Tree tree) {\\n[DEL] if (tree.is(Tree.Kind.EQUAL_TO, Tree.Kind.NOT_EQUAL_TO)) {\\n[DEL] BinaryExpressionTree bet = (BinaryExpressionTree) tree;\\n[DEL] return ExpressionUtils.isNullLiteral(bet.leftOperand()) || ExpressionUtils.isNullLiteral(bet.rightOperand());\\n[DEL] }\\n[DEL] return false;\\n[DEL] }\\n[DEL] \\n     private List<JavaFileScannerContext.Location> methodInvocationFlow(Constraint learnedConstraint, ExplodedGraph.Edge edge) {\\n       ExplodedGraph.Node parent = edge.parent;\\n       MethodInvocationTree mit = (MethodInvocationTree) parent.programPoint.syntaxTree();',\n",
              " '     public abstract Map<String, Object> parameters();\\n \\n     @JsonProperty(\"in_grace\")\\n[ADD] @Nullable\\n[ADD] @JsonInclude(JsonInclude.Include.NON_NULL)\\n     public abstract Boolean inGrace();\\n \\n     @JsonProperty(\"title\")',\n",
              " '   @Override\\n   void addFileGroupsInPendingClustering(Stream<Pair<HoodieFileGroupId, HoodieInstant>> fileGroups) {\\n     fileGroups.forEach(fileGroupInstantPair -> {\\n[DEL] ValidationUtils.checkArgument(fgIdToPendingClustering.containsKey(fileGroupInstantPair.getLeft()),\\n[ADD] ValidationUtils.checkArgument(!fgIdToPendingClustering.containsKey(fileGroupInstantPair.getLeft()),\\n           \"Trying to add a FileGroupId which is already in pending clustering operation. FgId :\"\\n               + fileGroupInstantPair.getLeft() + \", new instant: \" + fileGroupInstantPair.getRight() + \", existing instant \"\\n               + fgIdToPendingClustering.get(fileGroupInstantPair.getLeft()));',\n",
              " ' \\treturn a.listPipeline(srv.Context(), request, srv.Send)\\n }\\n \\n[ADD] func (a *apiServer) latestJobState(ctx context.Context, info *pps.PipelineInfo) error {\\n[ADD] // most recently updated first\\n[ADD] jobSortOptions := &col.Options{Order: col.SortDescend, Target: col.SortByModRevision}\\n[ADD] \\n[ADD] // fill in most recent job state\\n[ADD] var job pps.JobInfo\\n[ADD] return a.jobs.ReadOnly(ctx).GetByIndex(\\n[ADD] ppsdb.JobsPipelineIndex,\\n[ADD] info.Pipeline.Name,\\n[ADD] &job,\\n[ADD] jobSortOptions, func(_ string) error {\\n[ADD] info.LastJobState = job.State\\n[ADD] return errutil.ErrBreak\\n[ADD] })\\n[ADD] }\\n[ADD] \\n func (a *apiServer) listPipeline(ctx context.Context, request *pps.ListPipelineRequest, f func(*pps.PipelineInfo) error) error {\\n \\tvar jqCode *gojq.Code\\n \\tvar enc serde.Encoder',\n",
              " ' import javax.inject.Inject;\\n import java.time.LocalDateTime;\\n import java.util.Date;\\n[ADD] import java.util.HashMap;\\n import java.util.List;\\n[ADD] import java.util.Map;\\n[ADD] import java.util.Objects;\\n \\n public abstract class PetStoreConnectionProvider<T extends PetStoreClient> implements ConnectionProvider<T>, Lifecycle {\\n ',\n",
              " '         \"liveFixedField is null, MergeSuccBlocksInfo might have not initialized it?\"\\n     );\\n \\n[DEL] if (instr->IsStFldVariant())\\n[ADD] // StFld LazyBailOut tag removal optimization. Given that this instr is a StFld variant and\\n[ADD] // that there already is an BailOutOnImplicitCall tag on this instr, we can remove the\\n[ADD] // LazyBailOut tag on this instr if the StFld is writing to a live fixed field. We cannot\\n[ADD] // perform this optimization if a BailOutOnImplicitCall tag is abscent because writing to\\n[ADD] // a property can result in an implicit call that then can result in a lazy bailout.\\n[ADD] if (instr->IsStFldVariant() && BailOutInfo::IsBailOutOnImplicitCalls(instr->GetBailOutKind()))\\n     {\\n         Assert(instr->GetDst());\\n[DEL] Js::PropertyId id = instr->GetDst()->GetSym()->AsPropertySym()->m_propertyId;\\n[DEL] \\n[DEL] // We only need to protect against SetFld if it is setting to one of the live fixed fields\\n[DEL] return this->currentBlock->liveFixedFields->Test(id);\\n[ADD] // We only need to protect against StFld if it is setting to one of the live fixed fields.\\n[ADD] return currentBlock->liveFixedFields->Test(instr->GetDst()->GetSym()->AsPropertySym()->m_propertyId);\\n     }\\n \\n[ADD] // If no more fixed fields exist at this point in the block it is safe to assume that any field marked as\\n[ADD] // a fixed field has been verified to have not been modified and thus a LazyBailOut tag is not necessary.\\n     return !this->currentBlock->liveFixedFields->IsEmpty();\\n }\\n ',\n",
              " ' \\t\\tsuper.doStop();\\n \\t}\\n \\n[DEL] \\n \\t@Override\\n \\tprotected void handleMessage(Message<?> message) {\\n[DEL] this.handler.handleMessage(message);\\n[ADD] Deque<ExecutorChannelInterceptor> interceptorStack = null;\\n[ADD] try {\\n[ADD] if (this.channelInterceptors != null\\n[ADD] && ((ExecutorChannelInterceptorAware) this.inputChannel).hasExecutorInterceptors()) {\\n[ADD] interceptorStack = new ArrayDeque<ExecutorChannelInterceptor>();\\n[ADD] message = applyBeforeHandle(message, interceptorStack);\\n[ADD] if (message == null) {\\n[ADD] return;\\n[ADD] }\\n[ADD] }\\n[ADD] this.handler.handleMessage(message);\\n[ADD] if (!CollectionUtils.isEmpty(interceptorStack)) {\\n[ADD] triggerAfterMessageHandled(message, null, interceptorStack);\\n[ADD] }\\n[ADD] }\\n[ADD] catch (Exception ex) {\\n[ADD] if (!CollectionUtils.isEmpty(interceptorStack)) {\\n[ADD] triggerAfterMessageHandled(message, ex, interceptorStack);\\n[ADD] }\\n[ADD] if (ex instanceof MessagingException) {\\n[ADD] throw (MessagingException) ex;\\n[ADD] }\\n[ADD] String description = \"Failed to handle \" + message + \" to \" + this + \" in \" + this.handler;\\n[ADD] throw new MessageDeliveryException(message, description, ex);\\n[ADD] }\\n[ADD] catch (Error ex) {\\n[ADD] if (!CollectionUtils.isEmpty(interceptorStack)) {\\n[ADD] String description = \"Failed to handle \" + message + \" to \" + this + \" in \" + this.handler;\\n[ADD] triggerAfterMessageHandled(message,\\n[ADD] new MessageDeliveryException(message, description, ex),\\n[ADD] interceptorStack);\\n[ADD] }\\n[ADD] throw ex;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private Message<?> applyBeforeHandle(Message<?> message, Deque<ExecutorChannelInterceptor> interceptorStack) {\\n[ADD] for (ChannelInterceptor interceptor : this.channelInterceptors) {\\n[ADD] if (interceptor instanceof ExecutorChannelInterceptor) {\\n[ADD] ExecutorChannelInterceptor executorInterceptor = (ExecutorChannelInterceptor) interceptor;\\n[ADD] message = executorInterceptor.beforeHandle(message, this.inputChannel, this.handler);\\n[ADD] if (message == null) {\\n[ADD] if (logger.isDebugEnabled()) {\\n[ADD] logger.debug(executorInterceptor.getClass().getSimpleName()\\n[ADD] + \" returned null from beforeHandle, i.e. precluding the send.\");\\n[ADD] }\\n[ADD] triggerAfterMessageHandled(null, null, interceptorStack);\\n[ADD] return null;\\n[ADD] }\\n[ADD] interceptorStack.add(executorInterceptor);\\n[ADD] }\\n[ADD] }\\n[ADD] return message;\\n[ADD] }\\n[ADD] \\n[ADD] private void triggerAfterMessageHandled(Message<?> message, Exception ex,\\n[ADD] Deque<ExecutorChannelInterceptor> interceptorStack) {\\n[ADD] Iterator<ExecutorChannelInterceptor> iterator = interceptorStack.descendingIterator();\\n[ADD] while (iterator.hasNext()) {\\n[ADD] ExecutorChannelInterceptor interceptor = iterator.next();\\n[ADD] try {\\n[ADD] interceptor.afterMessageHandled(message, this.inputChannel, this.handler, ex);\\n[ADD] }\\n[ADD] catch (Throwable ex2) {\\n[ADD] logger.error(\"Exception from afterMessageHandled in \" + interceptor, ex2);\\n[ADD] }\\n[ADD] }\\n \\t}\\n \\n \\t@Override\\n \\tprotected Message<?> receiveMessage() {\\n[DEL] Message<?> message = (this.receiveTimeout >= 0)\\n[ADD] return (this.receiveTimeout >= 0)\\n \\t\\t\\t\\t? this.inputChannel.receive(this.receiveTimeout)\\n \\t\\t\\t\\t: this.inputChannel.receive();\\n[DEL] return message;\\n \\t}\\n \\n \\t@Override',\n",
              " '   private String getReplyDestinationName(Destination destination, ConsumerType replyConsumerType) throws JMSException {\\n     return replyConsumerType.isTopic() ? ((Topic) destination).getTopicName() : ((Queue) destination).getQueueName();\\n   }\\n[DEL] \\n }',\n",
              " ' import java.lang.reflect.Field;\\n import java.lang.reflect.Method;\\n import java.lang.reflect.Modifier;\\n[DEL] /*[IF Sidecar19-SE]\\n[DEL] import jdk.internal.misc.Unsafe;\\n[DEL] /*[ELSE]*/\\n[DEL] import sun.misc.Unsafe;\\n[DEL] /*[ENDIF]*/\\n import com.ibm.oti.vm.VM;\\n[ADD] import com.ibm.jit.JITHelpers;\\n \\n /**\\n  * PrimitiveHandle is a subclass of MethodHandle used for grouping MethodHandles that directly refer a Java-level method. ',\n",
              " '         // TODO MULE-14734 Review collection parameter in splitAggregate\\n         .withRole(ParameterRole.BEHAVIOUR)\\n         .withExpressionSupport(REQUIRED)\\n[DEL] .describedAs(\"Expression that defines the collection of parts to be processed in parallel\");\\n[ADD] .describedAs(\"Expression that defines the collection of parts to be processed in parallel.\");\\n     splitAggregate.onDefaultParameterGroup()\\n         .withOptionalParameter(\"timeout\")\\n         .ofType(typeLoader.load(Long.class))',\n",
              " '       throws IOException, InterruptedException {\\n     int numVals = 0;\\n \\n[ADD] AvroValue<GenericRecord> valueToRetain = null;\\n[ADD] \\n     for (AvroValue<GenericRecord> value : values) {\\n[DEL] this.outKey.datum(value.datum());\\n[ADD] if (valueToRetain == null) {\\n[ADD] valueToRetain = value;\\n[ADD] } else if (this.deltaComparatorOptional.isPresent()) {\\n[ADD] valueToRetain = this.deltaComparatorOptional.get().compare(valueToRetain, value) >= 0 ? valueToRetain : value;\\n[ADD] }\\n       numVals++;\\n     }\\n[ADD] this.outKey.datum(valueToRetain.datum());\\n \\n     if (numVals > 1) {\\n       context.getCounter(EVENT_COUNTER.MORE_THAN_1).increment(1);',\n",
              " '         }\\n     }\\n \\n[DEL] protected void initialize(String muleHome)\\n[DEL] {\\n[DEL] this.muleHome = muleHome;\\n[DEL] this.muleBin = getMuleBin();\\n[DEL] this.domainsDir = new File(muleHome + \"/domains\");\\n[DEL] this.appsDir = new File(muleHome + \"/apps/\");\\n[DEL] this.libsDir = new File(muleHome + \"/lib/user\");\\n[DEL] }\\n[DEL] \\n     protected int runSync(String command, String... args)\\n     {\\n         Map<Object, Object> newEnv = copyEnvironmentVariables();\\n[DEL] return executeSyncCommand(command, args, newEnv, TIMEOUT);\\n[ADD] return executeSyncCommand(command, args, newEnv, timeout);\\n     }\\n \\n     private int executeSyncCommand(String command, String[] args, Map<Object, Object> newEnv, long timeout)',\n",
              " '    * Generate the partitions based on the lists specified by the user in job config\\n    */\\n   private List<Partition> createUserSpecifiedPartitions() {\\n[ADD] \\n     List<Partition> partitions = new ArrayList<>();\\n \\n     List<String> watermarkPoints = state.getPropAsList(USER_SPECIFIED_PARTITIONS);\\n[ADD] boolean isEarlyStop = state.getPropAsBoolean(IS_EARLY_STOP);\\n[ADD] \\n[ADD] if (isEarlyStop && isEarlyStopEnabled() && isFullDump()) {\\n[ADD] throw new UnsupportedOperationException(\"We found early stop is required for this source, but full dump doesn\\'t support this mode.\");\\n[ADD] }\\n[ADD] \\n     if (watermarkPoints == null || watermarkPoints.size() == 0 ) {\\n       LOG.info(\"There should be some partition points\");\\n       long defaultWatermark = ConfigurationKeys.DEFAULT_WATERMARK_VALUE;',\n",
              " '             .submit();\\n       }\\n     }\\n[ADD] // Execute cleanup staging table commands\\n[ADD] executeQueries(cleanupCommands);\\n[ADD] }\\n[ADD] \\n[ADD] private void executeQueries(String queries) {\\n[ADD] if (StringUtils.isBlank(queries)) {\\n[ADD] return;\\n[ADD] }\\n[ADD] try {\\n[ADD] this.hiveJdbcConnector.executeStatements(queries);\\n[ADD] } catch (SQLException e) {\\n[ADD] throw new RuntimeException(e);\\n[ADD] }\\n   }\\n \\n   @Override',\n",
              " ' /*[INCLUDE-IF Sidecar18-SE]*/\\n /*******************************************************************************\\n[DEL] * Copyright (c) 2012, 2017 IBM Corp. and others\\n[ADD] * Copyright (c) 2012, 2020 IBM Corp. and others\\n  *\\n  * This program and the accompanying materials are made available under\\n  * the terms of the Eclipse Public License 2.0 which accompanies this',\n",
              " ' import org.junit.jupiter.api.Test;\\n import org.mockito.Mockito;\\n \\n[ADD] /**\\n[ADD] * {@link HoodieTestSuiteWriter}. Helps to test writing a DFS file.\\n[ADD] */\\n public class TestDFSHoodieTestSuiteWriterAdapter extends UtilitiesTestBase {\\n \\n   private FilebasedSchemaProvider schemaProvider;',\n",
              " ' import org.junit.Test;\\n import org.junit.rules.ExpectedException;\\n \\n[ADD] import org.springframework.beans.DirectFieldAccessor;\\n import org.springframework.beans.factory.BeanFactory;\\n import org.springframework.expression.spel.SpelCompilerMode;\\n import org.springframework.expression.spel.SpelEvaluationException;\\n[ADD] import org.springframework.expression.spel.SpelParserConfiguration;\\n import org.springframework.integration.annotation.ServiceActivator;\\n import org.springframework.integration.gateway.GatewayProxyFactoryBean;\\n import org.springframework.integration.gateway.RequestReplyExchanger;\\n import org.springframework.integration.support.MessageBuilder;\\n[ADD] import org.springframework.integration.test.util.TestUtils;\\n import org.springframework.integration.util.MessagingMethodInvokerHelper;\\n import org.springframework.messaging.Message;\\n import org.springframework.messaging.MessageHandlingException;',\n",
              " '   public static final boolean DEFAULT_USE_SCHEMA_FILE = false;\\n   public static final String SCHEMA_FILE_NAME = \"schema.file.name\";\\n   public static final String DEFAULT_SCHEMA_FILE_NAME = \"_schema.avsc\";\\n[ADD] public static final String SCHEMA_TEMP_FILE_NAME = \"schema.temp.file.name\";\\n[ADD] public static final String DEFAULT_SCHEMA_TEMP_FILE_NAME = \"_schema_temp.avsc\";\\n[ADD] public static final String USE_SCHEMA_TEMP_FILE = \"use.schema.temp.file\";\\n[ADD] public static final boolean DEFAULT_USE_SCHEMA_TEMP_FILE = false;\\n   public static final String SCHEMA_LITERAL_LENGTH_LIMIT = \"schema.literal.length.limit\";\\n   public static final int DEFAULT_SCHEMA_LITERAL_LENGTH_LIMIT = 4000;\\n   public static final String HIVE_SPEC_SCHEMA_READING_TIMER = \"hiveAvroSerdeManager.schemaReadTimer\";',\n",
              " '         return false;\\n     }\\n \\n[ADD] @Nonnull\\n[ADD] @SuppressWarnings(\"unchecked\")\\n[ADD] @Override\\n[ADD] public Set<String> getRoleIds() {\\n[ADD] final List<ObjectId> roles = (List<ObjectId>) fields.get(ROLES);\\n[ADD] if (roles == null) {\\n[ADD] return Sets.newHashSet();\\n[ADD] }\\n[ADD] return Sets.newHashSet(Collections2.transform(roles, new ObjectIdStringFunction()));\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public void setRoleIds(Set<String> roles) {\\n[ADD] fields.put(ROLES, Lists.newArrayList(Collections2.transform(roles, new StringObjectIdFunction())));\\n[ADD] }\\n[ADD] \\n     @Override\\n     public void setStartpage(final String type, final String id) {\\n         final Map<String, String> startpage = Maps.newHashMap();',\n",
              " '                    FileDialogs fileDialogs,\\n                    RemoteFileSystemContext fileSystemContext)\\n    {\\n[DEL] spellingService_ = spellingService;\\n       globalDisplay_= globalDisplay;\\n       fileDialogs_ = fileDialogs;\\n       fileSystemContext_ = fileSystemContext;',\n",
              " '             sc.setParameters(\"keyPairName\", keyPairName);\\n         }\\n \\n[DEL] if (cmd instanceof ListVMsCmdByAdmin) {\\n[DEL] ListVMsCmdByAdmin aCmd = (ListVMsCmdByAdmin)cmd;\\n[DEL] if (aCmd.getPodId() != null) {\\n[ADD] if (_accountMgr.isRootAdmin(caller.getId())) {\\n[ADD] if (cmd.getPodId() != null) {\\n                 sc.setParameters(\"podId\", pod);\\n \\n                 if (state == null) {',\n",
              " ' \\n         [Test]\\n         [Category(\"ExpressionInterpreterRunner\")]\\n[DEL] [Category(\"Failure\")]\\n         public void TestWatchExpression2()\\n         {\\n             // Execute and verify the main script in a debug session',\n",
              " '     {\\n         addExplodedAppFromBuilder(springPropertyAppFileBuilder);\\n \\n[DEL] deploymentService.start();\\n[ADD] startDeployment();\\n         assertApplicationDeploymentSuccess(applicationDeploymentListener, springPropertyAppFileBuilder.getId());\\n \\n         final Application app = findApp(springPropertyAppFileBuilder.getId(), 1);',\n",
              " '     if (rule == null) {\\n       throw new IllegalStateException(\"No rule was created for \" + ruleClass + \" in \" + repository.key());\\n     }\\n[DEL] String metadataKey = ruleMetadata(ruleClass, rule);\\n[ADD] String rspecKey = rspecKey(ruleClass, rule);\\n[ADD] RuleMetadata ruleMetadata = readRuleMetadata(rspecKey);\\n[ADD] addMetadata(rule, ruleMetadata);\\n[ADD] String ruleHtmlDescription = readRuleHtmlDescription(rspecKey);\\n[ADD] if (ruleHtmlDescription != null) {\\n[ADD] rule.setHtmlDescription(ruleHtmlDescription);\\n[ADD] }\\n     // \\'setActivatedByDefault\\' is used by SonarLint standalone, to define which rules will be active\\n[DEL] rule.setActivatedByDefault(profile.ruleKeys.contains(ruleKey) || profile.ruleKeys.contains(metadataKey));\\n[ADD] boolean activatedInProfile = profile.ruleKeys.contains(ruleKey) || profile.ruleKeys.contains(rspecKey);\\n[ADD] boolean isSecurityHotspot = ruleMetadata != null && ruleMetadata.isSecurityHotspot();\\n[ADD] rule.setActivatedByDefault(activatedInProfile && !isSecurityHotspot);\\n     rule.setTemplate(TEMPLATE_RULE_KEY.contains(ruleKey));\\n   }\\n \\n[DEL] private String ruleMetadata(Class<?> ruleClass, NewRule rule) {\\n[DEL] String metadataKey = rule.key();\\n[ADD] private static String rspecKey(Class<?> ruleClass, NewRule rule) {\\n     org.sonar.java.RspecKey rspecKeyAnnotation = AnnotationUtils.getAnnotation(ruleClass, org.sonar.java.RspecKey.class);\\n     if (rspecKeyAnnotation != null) {\\n[DEL] metadataKey = rspecKeyAnnotation.value();\\n[DEL] rule.setInternalKey(metadataKey);\\n[ADD] String rspecKey = rspecKeyAnnotation.value();\\n[ADD] rule.setInternalKey(rspecKey);\\n[ADD] return rspecKey;\\n[ADD] } else {\\n[ADD] return rule.key();\\n     }\\n[DEL] addHtmlDescription(rule, metadataKey);\\n[DEL] addMetadata(rule, metadataKey);\\n[DEL] return metadataKey;\\n   }\\n \\n[DEL] private void addMetadata(NewRule rule, String metadataKey) {\\n[ADD] @Nullable\\n[ADD] static RuleMetadata readRuleMetadata(String metadataKey) {\\n     URL resource = JavaRulesDefinition.class.getResource(RESOURCE_BASE_PATH + \"/\" + metadataKey + \"_java.json\");\\n[DEL] if (resource != null) {\\n[DEL] RuleMetadata metadata = gson.fromJson(readResource(resource), RuleMetadata.class);\\n[DEL] rule.setSeverity(metadata.defaultSeverity.toUpperCase(Locale.US));\\n[DEL] rule.setName(metadata.title);\\n[DEL] rule.addTags(metadata.tags);\\n[DEL] rule.setType(RuleType.valueOf(metadata.type));\\n[DEL] rule.setStatus(RuleStatus.valueOf(metadata.status.toUpperCase(Locale.US)));\\n[DEL] if(metadata.remediation != null) {\\n[DEL] rule.setDebtRemediationFunction(metadata.remediation.remediationFunction(rule.debtRemediationFunctions()));\\n[DEL] rule.setGapDescription(metadata.remediation.linearDesc);\\n[DEL] }\\n[DEL] }\\n[ADD] return resource != null ? GSON.fromJson(readResource(resource), RuleMetadata.class) : null;\\n   }\\n \\n[DEL] private static void addHtmlDescription(NewRule rule, String metadataKey) {\\n[ADD] private static String readRuleHtmlDescription(String metadataKey) {\\n     URL resource = JavaRulesDefinition.class.getResource(RESOURCE_BASE_PATH + \"/\" + metadataKey + \"_java.html\");\\n     if (resource != null) {\\n[DEL] rule.setHtmlDescription(readResource(resource));\\n[ADD] return readResource(resource);\\n[ADD] }\\n[ADD] return null;\\n[ADD] }\\n[ADD] \\n[ADD] private void addMetadata(NewRule rule, @Nullable RuleMetadata metadata) {\\n[ADD] if (metadata == null) {\\n[ADD] return;\\n[ADD] }\\n[ADD] rule.setSeverity(metadata.defaultSeverity.toUpperCase(Locale.US));\\n[ADD] rule.setName(metadata.title);\\n[ADD] rule.addTags(metadata.tags);\\n[ADD] if (metadata.isSecurityHotspot() && !supportsSecurityHotspots) {\\n[ADD] rule.setType(RuleType.VULNERABILITY);\\n[ADD] } else {\\n[ADD] rule.setType(RuleType.valueOf(metadata.type));\\n[ADD] }\\n[ADD] rule.setStatus(RuleStatus.valueOf(metadata.status.toUpperCase(Locale.US)));\\n[ADD] if (metadata.remediation != null) {\\n[ADD] rule.setDebtRemediationFunction(metadata.remediation.remediationFunction(rule.debtRemediationFunctions()));\\n[ADD] rule.setGapDescription(metadata.remediation.linearDesc);\\n     }\\n   }\\n ',\n",
              " ' \\n import javax.annotation.Nullable;\\n import java.util.Arrays;\\n[ADD] import java.util.Collections;\\n[ADD] import java.util.List;\\n import java.util.Objects;\\n \\n final class JType implements Type, Type.ArrayType {',\n",
              " '     props.put(KeyGeneratorOptions.RECORDKEY_FIELD_OPT_KEY, \"_row_key\");\\n     props.put(KeyGeneratorOptions.PARTITIONPATH_FIELD_OPT_KEY, \"current_date\");\\n     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\\n[DEL] env.setParallelism(2);\\n[ADD] env.setParallelism(1);\\n \\n     SimpleTestSinkFunction.valuesList.clear();\\n     env.fromCollection(recordStr)',\n",
              " '     @Param(description = \"If the network has redundant routers enabled\", since = \"4.11.1\")\\n     private Boolean redundantRouter;\\n \\n[ADD] @SerializedName(ApiConstants.ACL_NAME)\\n[ADD] @Param(description = \"ACL name associated with the VPC network\")\\n[ADD] private String aclName;\\n[ADD] \\n     public Boolean getDisplayNetwork() {\\n         return displayNetwork;\\n     }',\n",
              " '             if (typeMappings.stream().map(SubTypeMapping::baseType).distinct().collect(toList()).size() != typeMappings.size())\\n             {\\n                 throw new IllegalModelDefinitionException(String.format(\"There should be only one SubtypeMapping for any given base type in extension [%s].\" +\\n[DEL] \" Duplicated base types are not allowed\", declaration.getExtensionDeclaration().getName()));\\n[ADD] \" Duplicated base types are not allowed\", declaration.getDeclaration().getName()));\\n             }\\n \\n             Map<MetadataType, List<MetadataType>> subTypesMap = typeMappings.stream().collect(',\n",
              " '     return toBuilder().setFieldInsertionOrder(true).build();\\n   }\\n \\n[DEL] /** Return the field ids accessed. Should not be called until after {@link #resolve} is called. */\\n[DEL] public Set<Integer> fieldIdsAccessed() {\\n[ADD] /**\\n[ADD] * Return the field ids accessed. Should not be called until after {@link #resolve} is called.\\n[ADD] * Iteration order is consistent with {@link #getFieldsAccessed}.\\n[ADD] */\\n[ADD] public List<Integer> fieldIdsAccessed() {\\n     return getFieldsAccessed().stream()\\n         .map(FieldDescriptor::getFieldId)\\n[DEL] .collect(Collectors.toSet());\\n[ADD] .collect(Collectors.toList());\\n   }\\n \\n   /**',\n",
              " '             }\\n             // mark card using javascript\\n             if (url.startsWith(\"signal:mark_current_card\")) {\\n[DEL] executeCommand(COMMAND_MARK);\\n[ADD] if (jsApiListMap.get(\"markCard\")) {\\n[ADD] executeCommand(COMMAND_MARK);\\n[ADD] } else {\\n[ADD] showDeveloperContact();\\n[ADD] }\\n                 return true;\\n             }\\n             // flag card (blue, green, orange, red) using javascript from AnkiDroid webview\\n             if (url.startsWith(\"signal:flag_\")) {\\n[DEL] String mFlag = url.replaceFirst(\"signal:flag_\",\"\");\\n[DEL] switch (mFlag) {\\n[DEL] case \"none\": executeCommand(COMMAND_UNSET_FLAG);\\n[DEL] return true;\\n[DEL] case \"red\": executeCommand(COMMAND_TOGGLE_FLAG_RED);\\n[DEL] return true;\\n[DEL] case \"orange\": executeCommand(COMMAND_TOGGLE_FLAG_ORANGE);\\n[DEL] return true;\\n[DEL] case \"green\": executeCommand(COMMAND_TOGGLE_FLAG_GREEN);\\n[DEL] return true;\\n[DEL] case \"blue\": executeCommand(COMMAND_TOGGLE_FLAG_BLUE);\\n[DEL] return true;\\n[DEL] default:\\n[DEL] Timber.d(\"No such Flag found.\");\\n[DEL] return true;\\n[ADD] if (jsApiListMap.get(\"toggleFlag\")) {\\n[ADD] String mFlag = url.replaceFirst(\"signal:flag_\",\"\");\\n[ADD] switch (mFlag) {\\n[ADD] case \"none\": executeCommand(COMMAND_UNSET_FLAG);\\n[ADD] return true;\\n[ADD] case \"red\": executeCommand(COMMAND_TOGGLE_FLAG_RED);\\n[ADD] return true;\\n[ADD] case \"orange\": executeCommand(COMMAND_TOGGLE_FLAG_ORANGE);\\n[ADD] return true;\\n[ADD] case \"green\": executeCommand(COMMAND_TOGGLE_FLAG_GREEN);\\n[ADD] return true;\\n[ADD] case \"blue\": executeCommand(COMMAND_TOGGLE_FLAG_BLUE);\\n[ADD] return true;\\n[ADD] default:\\n[ADD] Timber.d(\"No such Flag found.\");\\n[ADD] return true;\\n[ADD] }\\n                 }\\n[ADD] showDeveloperContact();\\n[ADD] return true;\\n             }\\n \\n             // Show toast using JS',\n",
              " ' package org.mule.module.extension.internal;\\n \\n import static org.hamcrest.CoreMatchers.equalTo;\\n[ADD] import static org.hamcrest.CoreMatchers.instanceOf;\\n import static org.hamcrest.CoreMatchers.is;\\n import static org.hamcrest.CoreMatchers.notNullValue;\\n import static org.hamcrest.Matchers.empty;',\n",
              " '       return new TypedWithoutMetadata<>(this);\\n     }\\n \\n[ADD] private Schema fetchAvroSchema(String schemaRegistryUrl, String subject) {\\n[ADD] SchemaRegistryClient registryClient;\\n[ADD] if (getSchemaRegistryClientFactoryFn() != null) {\\n[ADD] registryClient = getSchemaRegistryClientFactoryFn().apply(schemaRegistryUrl);\\n[ADD] } else {\\n[ADD] registryClient = new CachedSchemaRegistryClient(schemaRegistryUrl, Integer.MAX_VALUE);\\n[ADD] }\\n[ADD] \\n[ADD] SchemaMetadata latestSchemaMetadata;\\n[ADD] try {\\n[ADD] latestSchemaMetadata = registryClient.getLatestSchemaMetadata(subject);\\n[ADD] } catch (IOException | RestClientException e) {\\n[ADD] throw new IllegalArgumentException(\\n[ADD] \"Unable to get latest schema metadata for subject: \" + subject, e);\\n[ADD] }\\n[ADD] \\n[ADD] final Schema avroSchema = new Schema.Parser().parse(latestSchemaMetadata.getSchema());\\n[ADD] checkArgument(avroSchema != null, \"Avro schema can\\'t be null\");\\n[ADD] return avroSchema;\\n[ADD] }\\n[ADD] \\n     @Override\\n     public PCollection<KafkaRecord<K, V>> expand(PBegin input) {\\n       checkArgument(',\n",
              " ' import org.apache.hadoop.fs.FileStatus;\\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;\\n[ADD] import org.apache.hadoop.hive.serde2.io.DateWritable;\\n[ADD] import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\\n[ADD] import org.apache.hadoop.io.BooleanWritable;\\n[ADD] import org.apache.hadoop.io.ByteWritable;\\n[ADD] import org.apache.hadoop.io.BytesWritable;\\n[ADD] import org.apache.hadoop.io.DoubleWritable;\\n[ADD] import org.apache.hadoop.io.FloatWritable;\\n[ADD] import org.apache.hadoop.io.IntWritable;\\n[ADD] import org.apache.hadoop.io.LongWritable;\\n[ADD] import org.apache.hadoop.io.ShortWritable;\\n[ADD] import org.apache.hadoop.io.Text;\\n[ADD] import org.apache.hadoop.io.WritableComparable;\\n import org.apache.hadoop.mapreduce.Job;\\n import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\\n import org.apache.orc.OrcFile;',\n",
              " '         }\\n \\n         public String getMethodParameter(String method, String key, String defaultValue) {\\n[DEL] if (methodParams == null) {\\n[ADD] if (CollectionUtils.isNotEmptyMap(methodParams)) {\\n                 methodParams = URL.toMethodParameters(params);\\n                 consumerMethodParams = URL.toMethodParameters(consumerParams);\\n             }\\n \\n             String value = getMethodParameter(method, key, consumerMethodParams);\\n[DEL] if (value != null) {\\n[ADD] if (StringUtils.isNotEmpty(value)) {\\n                 return value;\\n             }\\n             value = getMethodParameter(method, key, methodParams);\\n[DEL] return value == null ? defaultValue : value;\\n[ADD] return StringUtils.isEmpty(value) ? defaultValue : value;\\n         }\\n \\n         private String getMethodParameter(String method, String key, Map<String, Map<String, String>> map) {',\n",
              " '         // Use the default bootstrap index class.\\n         hoodieConfig.setDefaultValue(BOOTSTRAP_INDEX_CLASS_NAME, getDefaultBootstrapIndexClass(properties));\\n       }\\n[ADD] if (hoodieConfig.contains(TIMELINE_TIMEZONE)) {\\n[ADD] // set commit timeline timezone to utc\\n[ADD] HoodieInstantTimeGenerator.setCommitTimeZone(hoodieConfig.getString(TIMELINE_TIMEZONE));\\n[ADD] }\\n       hoodieConfig.getProps().store(outputStream, \"Properties saved on \" + new Date(System.currentTimeMillis()));\\n     }\\n   }',\n",
              " ' import org.junit.Test;\\n \\n @SmallTest\\n[DEL] public class TransformationGraphLookupStrategyTestCase extends AbstractMuleTestCase {\\n[ADD] public class TransformationGraphLookupStrategyTestCase extends AbstractTransformationGraphTestCase {\\n \\n[DEL] private static final DataType XML_DATA_TYPE = mock(DataType.class, \"XML_DATA_TYPE\");\\n[DEL] private static final DataType JSON_DATA_TYPE = mock(DataType.class, \"JSON_DATA_TYPE\");\\n[DEL] private static final DataType INPUT_STREAM_DATA_TYPE = mock(DataType.class, \"INPUT_STREAM_DATA_TYPE\");\\n[DEL] private static final DataType STRING_DATA_TYPE = mock(DataType.class, \"STRING_DATA_TYPE\");\\n \\n   private TransformationGraph graph = new TransformationGraph();\\n   private TransformationGraphLookupStrategy lookupStrategyTransformation = new TransformationGraphLookupStrategy(graph);\\n \\n[ADD] \\n   @Test\\n   public void lookupTransformersNoSourceInGraph() throws Exception {\\n     Converter inputStreamToXml = new MockConverterBuilder().from(INPUT_STREAM_DATA_TYPE).to(XML_DATA_TYPE).build();',\n",
              " '    public void doRestart(Session session)\\n    {\\n       prepareForQuit(\\n[DEL] \"Restarting RStudio\",\\n[ADD] constants_.restartRStudio(),\\n             saveChanges -> {\\n                String project = session.getSessionInfo().getActiveProjectFile();\\n                if (project == null)',\n",
              " '     assertThat(reader.advance()).isFalse();\\n   }\\n \\n[ADD] @Test\\n[ADD] public void watermarkDoesNotChangeWhenToFewSampleRecords()\\n[ADD] throws IOException, TransientKinesisException {\\n[ADD] final long timestampMs = 1000L;\\n[ADD] \\n[ADD] KinesisRecord firstRecord = prepareRecordMockWithArrivalTimestamp(timestampMs);\\n[ADD] OngoingStubbing<CustomOptional<KinesisRecord>> firstIteratorStubbing =\\n[ADD] when(firstIterator.next()).thenReturn(CustomOptional.of(firstRecord));\\n[ADD] for (int i = 0; i < 5; i++) {\\n[ADD] KinesisRecord record = prepareRecordMockWithArrivalTimestamp(timestampMs + i);\\n[ADD] firstIteratorStubbing = firstIteratorStubbing.thenReturn(CustomOptional.of(record));\\n[ADD] }\\n[ADD] firstIteratorStubbing.thenReturn(CustomOptional.<KinesisRecord>absent());\\n[ADD] when(secondIterator.next()).thenReturn(CustomOptional.<KinesisRecord>absent());\\n[ADD] \\n[ADD] reader.start();\\n[ADD] \\n[ADD] assertThat(reader.getWatermark()).isEqualTo(BoundedWindow.TIMESTAMP_MIN_VALUE);\\n[ADD] while (reader.advance()) {\\n[ADD] assertThat(reader.getWatermark()).isEqualTo(BoundedWindow.TIMESTAMP_MIN_VALUE);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] @Test\\n[ADD] public void watermarkAdvancesWhenEnoughRecordsReadRecently()\\n[ADD] throws IOException, TransientKinesisException {\\n[ADD] long timestampMs = 1000L;\\n[ADD] \\n[ADD] KinesisRecord firstRecord = prepareRecordMockWithArrivalTimestamp(timestampMs);\\n[ADD] OngoingStubbing<CustomOptional<KinesisRecord>> firstIteratorStubbing =\\n[ADD] when(firstIterator.next()).thenReturn(CustomOptional.of(firstRecord));\\n[ADD] for (int i = 0; i < KinesisReader.MIN_WATERMARK_MESSAGES; i++) {\\n[ADD] KinesisRecord record = prepareRecordMockWithArrivalTimestamp(timestampMs + i);\\n[ADD] firstIteratorStubbing = firstIteratorStubbing.thenReturn(CustomOptional.of(record));\\n[ADD] }\\n[ADD] firstIteratorStubbing.thenReturn(CustomOptional.<KinesisRecord>absent());\\n[ADD] when(secondIterator.next()).thenReturn(CustomOptional.<KinesisRecord>absent());\\n[ADD] \\n[ADD] reader.start();\\n[ADD] \\n[ADD] assertThat(reader.getWatermark()).isEqualTo(BoundedWindow.TIMESTAMP_MIN_VALUE);\\n[ADD] for (int i = 0; i < KinesisReader.MIN_WATERMARK_MESSAGES; i++) {\\n[ADD] reader.advance();\\n[ADD] }\\n[ADD] assertThat(reader.getWatermark()).isEqualTo(new Instant(timestampMs));\\n[ADD] }\\n[ADD] \\n[ADD] @Test\\n[ADD] public void watermarkMonotonicallyIncreases()\\n[ADD] throws IOException, TransientKinesisException {\\n[ADD] long timestampMs = 1000L;\\n[ADD] \\n[ADD] KinesisRecord firstRecord = prepareRecordMockWithArrivalTimestamp(timestampMs);\\n[ADD] OngoingStubbing<CustomOptional<KinesisRecord>> firstIteratorStubbing =\\n[ADD] when(firstIterator.next()).thenReturn(CustomOptional.of(firstRecord));\\n[ADD] for (int i = KinesisReader.MIN_WATERMARK_MESSAGES;\\n[ADD] i > -KinesisReader.MIN_WATERMARK_MESSAGES; i--) {\\n[ADD] KinesisRecord record = prepareRecordMockWithArrivalTimestamp(timestampMs + i);\\n[ADD] firstIteratorStubbing = firstIteratorStubbing.thenReturn(CustomOptional.of(record));\\n[ADD] }\\n[ADD] firstIteratorStubbing.thenReturn(CustomOptional.<KinesisRecord>absent());\\n[ADD] when(secondIterator.next()).thenReturn(CustomOptional.<KinesisRecord>absent());\\n[ADD] \\n[ADD] reader.start();\\n[ADD] \\n[ADD] Instant lastWatermark = reader.getWatermark();\\n[ADD] assertThat(lastWatermark).isEqualTo(BoundedWindow.TIMESTAMP_MIN_VALUE);\\n[ADD] while (reader.advance()) {\\n[ADD] Instant currentWatermark = reader.getWatermark();\\n[ADD] assertThat(currentWatermark).isGreaterThanOrEqualTo(lastWatermark);\\n[ADD] lastWatermark = currentWatermark;\\n[ADD] }\\n[ADD] assertThat(reader.advance()).isFalse();\\n[ADD] }\\n[ADD] \\n[ADD] private KinesisRecord prepareRecordMockWithArrivalTimestamp(long timestampMs) {\\n[ADD] KinesisRecord record = mock(KinesisRecord.class);\\n[ADD] when(record.getApproximateArrivalTimestamp()).thenReturn(new Instant(timestampMs));\\n[ADD] return record;\\n[ADD] }\\n[ADD] \\n }',\n",
              " ' import java.net.URI;\\n import java.util.Collection;\\n import java.util.List;\\n[ADD] import java.util.Map;\\n import java.util.Set;\\n \\n import org.apache.hadoop.conf.Configuration;',\n",
              " ' \\t\\tassertEquals(\"payload is \\'hello\\'\", result.toString());\\n \\t}\\n \\n[ADD] @Test\\n[ADD] public void testInt3166GroovyScriptExecutingMessageProcessorPerformance() throws Exception {\\n[ADD] final Message<?> message = new GenericMessage<Object>(\"test\");\\n[ADD] \\n[ADD] final AtomicInteger var1 = new AtomicInteger();\\n[ADD] final AtomicInteger var2 = new AtomicInteger();\\n[ADD] \\n[ADD] String script =\\n[ADD] \"var1.incrementAndGet(); Thread.sleep(100); var2.set(Math.max(var1.get(), var2.get())); var1.decrementAndGet()\";\\n[ADD] \\n[ADD] ScriptSource scriptSource = new StaticScriptSource(script, Script.class.getName());\\n[ADD] final MessageProcessor<Object> processor =\\n[ADD] new GroovyScriptExecutingMessageProcessor(scriptSource, new ScriptVariableGenerator() {\\n[ADD] @Override\\n[ADD] public Map<String, Object> generateScriptVariables(Message<?> message) {\\n[ADD] Map<String, Object> variables = new HashMap<String, Object>(2);\\n[ADD] variables.put(\"var1\", var1);\\n[ADD] variables.put(\"var2\", var2);\\n[ADD] return variables;\\n[ADD] }\\n[ADD] });\\n[ADD] \\n[ADD] ExecutorService executor = Executors.newFixedThreadPool(10);\\n[ADD] for (int i = 0; i < 10; i++) {\\n[ADD] executor.execute(new Runnable() {\\n[ADD] @Override\\n[ADD] public void run() {\\n[ADD] processor.processMessage(message);\\n[ADD] }\\n[ADD] });\\n[ADD] }\\n[ADD] executor.shutdown();\\n[ADD] assertTrue(executor.awaitTermination(3, TimeUnit.SECONDS));\\n[ADD] \\n[ADD] assertTrue(var2.get() > 1);\\n[ADD] \\n[ADD] }\\n \\n \\tprivate static class TestResource extends AbstractResource {\\n ',\n",
              " ' import org.springframework.integration.config.GlobalChannelInterceptor;\\n import org.springframework.integration.config.IntegrationConverter;\\n import org.springframework.integration.config.SpelFunctionFactoryBean;\\n[ADD] import org.springframework.integration.context.IntegrationContextUtils;\\n import org.springframework.integration.core.MessageSource;\\n import org.springframework.integration.core.MessagingTemplate;\\n import org.springframework.integration.endpoint.AbstractEndpoint;',\n",
              " '       log.error(String.format(\"Failed to commit dataset state for dataset %s of job %s\", this.datasetUrn,\\n           this.jobContext.getJobId()), ioe);\\n       throw new RuntimeException(ioe);\\n[ADD] } catch (Throwable t) {\\n[ADD] log.error(\"Error\", t);\\n     } finally {\\n       try {\\n         finalizeDatasetState(datasetState, datasetUrn);',\n",
              " ' \\t\\t\\t\\thandle = new InstanceFieldVarHandle(field, field.getDeclaringClass(), field.getType());\\n \\t\\t\\t}\\n \\t\\t\\t\\n[DEL] checkAccess(handle);\\n[ADD] checkAccess(handle, true);\\n \\t\\t\\tcheckSecurity(handle.getDefiningClass(), handle.getDefiningClass(), handle.modifiers);\\n \\t\\t\\t\\n \\t\\t\\treturn handle;',\n",
              " ' \\n   /**\\n    * Parse min/max statistics stored in parquet footers for all columns.\\n[ADD] * ParquetRead.readFooter is not a thread safe method.\\n[ADD] *\\n[ADD] * @param conf hadoop conf.\\n[ADD] * @param parquetFilePath file to be read.\\n[ADD] * @param cols cols which need to collect statistics.\\n[ADD] * @return a HoodieColumnRangeMetadata instance.\\n    */\\n[DEL] public Collection<HoodieColumnRangeMetadata<Comparable>> readRangeFromParquetMetadata(Configuration conf, Path parquetFilePath, List<String> cols) {\\n[ADD] public Collection<HoodieColumnRangeMetadata<Comparable>> readRangeFromParquetMetadata(\\n[ADD] Configuration conf,\\n[ADD] Path parquetFilePath,\\n[ADD] List<String> cols) {\\n     ParquetMetadata metadata = readMetadata(conf, parquetFilePath);\\n     // collect stats from all parquet blocks\\n     Map<String, List<HoodieColumnRangeMetadata<Comparable>>> columnToStatsListMap = metadata.getBlocks().stream().flatMap(blockMetaData -> {\\n[DEL] return blockMetaData.getColumns().stream().filter(f -> cols.contains(f.getPath().toDotString())).map(columnChunkMetaData ->\\n[DEL] new HoodieColumnRangeMetadata<>(parquetFilePath.getName(), columnChunkMetaData.getPath().toDotString(),\\n[DEL] columnChunkMetaData.getStatistics().genericGetMin(),\\n[DEL] columnChunkMetaData.getStatistics().genericGetMax(),\\n[DEL] columnChunkMetaData.getStatistics().getNumNulls(),\\n[DEL] columnChunkMetaData.getPrimitiveType().stringifier()));\\n[ADD] return blockMetaData.getColumns().stream().filter(f -> cols.contains(f.getPath().toDotString())).map(columnChunkMetaData -> {\\n[ADD] String minAsString;\\n[ADD] String maxAsString;\\n[ADD] if (columnChunkMetaData.getPrimitiveType().getOriginalType() == OriginalType.DATE) {\\n[ADD] synchronized (lock) {\\n[ADD] minAsString = columnChunkMetaData.getStatistics().minAsString();\\n[ADD] maxAsString = columnChunkMetaData.getStatistics().maxAsString();\\n[ADD] }\\n[ADD] } else {\\n[ADD] minAsString = columnChunkMetaData.getStatistics().minAsString();\\n[ADD] maxAsString = columnChunkMetaData.getStatistics().maxAsString();\\n[ADD] }\\n[ADD] return new HoodieColumnRangeMetadata<>(parquetFilePath.getName(), columnChunkMetaData.getPath().toDotString(),\\n[ADD] columnChunkMetaData.getStatistics().genericGetMin(),\\n[ADD] columnChunkMetaData.getStatistics().genericGetMax(),\\n[ADD] columnChunkMetaData.getStatistics().getNumNulls(),\\n[ADD] minAsString, maxAsString);\\n[ADD] });\\n     }).collect(Collectors.groupingBy(e -> e.getColumnName()));\\n \\n     // we only intend to keep file level statistics.',\n",
              " '                     // load up the card selected on the list\\n                     long clickedCardId = getCards().get(position).getId();\\n                     openNoteEditorForCard(clickedCardId);\\n[ADD] mOldCardId = clickedCardId;\\n[ADD] mOldCardTopOffset = calculateTopOffset(position);\\n                 }\\n             }\\n         });',\n",
              " '                 .addParameter(MonitorService.CONCURRENT, 1)\\n                 .addParameter(MonitorService.MAX_CONCURRENT, 1);\\n         monitor.collect(statistics);\\n[ADD] monitor.send();\\n         while (lastStatistics == null) {\\n             Thread.sleep(10);\\n         }\\n[DEL] Assert.assertEquals(lastStatistics.getParameter(MonitorService.APPLICATION), \"morgan\");\\n[DEL] Assert.assertEquals(lastStatistics.getProtocol(), \"dubbo\");\\n[DEL] Assert.assertEquals(lastStatistics.getHost(), \"10.20.153.10\");\\n[DEL] Assert.assertEquals(lastStatistics.getParameter(MonitorService.APPLICATION), \"morgan\");\\n[DEL] Assert.assertEquals(lastStatistics.getParameter(MonitorService.INTERFACE), \"MemberService\");\\n[DEL] Assert.assertEquals(lastStatistics.getParameter(MonitorService.METHOD), \"findPerson\");\\n[DEL] Assert.assertEquals(lastStatistics.getParameter(MonitorService.CONSUMER), \"10.20.153.11\");\\n[DEL] Assert.assertEquals(lastStatistics.getParameter(MonitorService.SUCCESS), \"1\");\\n[DEL] Assert.assertEquals(lastStatistics.getParameter(MonitorService.FAILURE), \"0\");\\n[DEL] Assert.assertEquals(lastStatistics.getParameter(MonitorService.ELAPSED), \"3\");\\n[DEL] Assert.assertEquals(lastStatistics.getParameter(MonitorService.MAX_ELAPSED), \"3\");\\n[DEL] Assert.assertEquals(lastStatistics.getParameter(MonitorService.CONCURRENT), \"1\");\\n[DEL] Assert.assertEquals(lastStatistics.getParameter(MonitorService.MAX_CONCURRENT), \"1\");\\n[ADD] Assert.assertEquals(\"morgan\", lastStatistics.getParameter(MonitorService.APPLICATION));\\n[ADD] Assert.assertEquals(\"dubbo\", lastStatistics.getProtocol());\\n[ADD] Assert.assertEquals(\"10.20.153.10\", lastStatistics.getHost());\\n[ADD] Assert.assertEquals(\"morgan\", lastStatistics.getParameter(MonitorService.APPLICATION));\\n[ADD] Assert.assertEquals(\"MemberService\", lastStatistics.getParameter(MonitorService.INTERFACE));\\n[ADD] Assert.assertEquals(\"findPerson\", lastStatistics.getParameter(MonitorService.METHOD));\\n[ADD] Assert.assertEquals(\"10.20.153.11\", lastStatistics.getParameter(MonitorService.CONSUMER));\\n[ADD] Assert.assertEquals(\"1\", lastStatistics.getParameter(MonitorService.SUCCESS));\\n[ADD] Assert.assertEquals(\"0\", lastStatistics.getParameter(MonitorService.FAILURE));\\n[ADD] Assert.assertEquals(\"3\", lastStatistics.getParameter(MonitorService.ELAPSED));\\n[ADD] Assert.assertEquals(\"3\", lastStatistics.getParameter(MonitorService.MAX_ELAPSED));\\n[ADD] Assert.assertEquals(\"1\", lastStatistics.getParameter(MonitorService.CONCURRENT));\\n[ADD] Assert.assertEquals(\"1\", lastStatistics.getParameter(MonitorService.MAX_CONCURRENT));\\n         monitor.destroy();\\n     }\\n ',\n",
              " ' \\n     this.baseUrl = state.getProp(KAFKA_AUDIT_REST_BASE_URL);\\n     this.maxNumTries = state.getPropAsInt(KAFKA_AUDIT_REST_MAX_TRIES, 5);\\n[ADD] this.retryBackOffSecs = state.getPropAsInt(KAFKA_AUDIT_REST_BACKOFF_INTERVAL_SECONDS, KAFKA_AUDIT_REST_BACKOFF_INTERVAL_SECONDS_DEFAULT);\\n[ADD] this.topicQueryString = state.getProp(KAFKA_AUDIT_REST_TOPIC_QUERYSTRING_KEY, KAFKA_AUDIT_REST_TOPIC_QUERYSTRING_DEFAULT);\\n     this.startQueryString = state.getProp(KAFKA_AUDIT_REST_START_QUERYSTRING_KEY, KAFKA_AUDIT_REST_START_QUERYSTRING_DEFAULT);\\n     this.endQueryString = state.getProp(KAFKA_AUDIT_REST_END_QUERYSTRING_KEY, KAFKA_AUDIT_REST_END_QUERYSTRING_DEFAULT);\\n   }\\n \\n \\n[DEL] public Map<String, Long> fetch (String datasetName, long start, long end)  throws IOException {\\n[ADD] public Map<String, Long> fetch (String topic, long start, long end)  throws IOException {\\n     String fullUrl =\\n[DEL] (this.baseUrl.endsWith(\"/\") ? this.baseUrl : this.baseUrl + \"/\") + StringUtils.replaceChars(datasetName, \\'/\\', \\'.\\')\\n[DEL] + \"?\" + this.startQueryString + \"=\" + start + \"&\" + this.endQueryString + \"=\" + end;\\n[ADD] (this.baseUrl.endsWith(\"/\") ? this.baseUrl.substring(0, this.baseUrl.length() - 1) : this.baseUrl) + \"?\" + this.topicQueryString + \"=\" + topic\\n[ADD] + \"&\" + this.startQueryString + \"=\" + start + \"&\" + this.endQueryString + \"=\" + end;\\n     log.info(\"Full URL is \" + fullUrl);\\n     String response = getHttpResponse(fullUrl);\\n[DEL] return parseResponse (fullUrl, response, datasetName);\\n[ADD] return parseResponse(fullUrl, response, topic);\\n   }\\n \\n ',\n",
              " '             return obj.DsasmValue;\\n         }\\n \\n[DEL] public bool CompareArrays(DsasmArray dsArray, List<Object> expected, System.Type type)\\n[DEL] {\\n[DEL] if (dsArray.members.Length != expected.Count)\\n[DEL] return false;\\n[DEL] \\n[DEL] for (int i = 0; i < dsArray.members.Length; ++i)\\n[DEL] {\\n[DEL] List<Object> subExpected = expected[i] as List<Object>;\\n[DEL] DsasmArray subArray = dsArray.members[i].Payload as DsasmArray;\\n[DEL] \\n[DEL] if ((subExpected != null) && (subArray != null)) {\\n[DEL] \\n[DEL] if (!CompareArrays(subArray, subExpected, type))\\n[DEL] return false;\\n[DEL] }\\n[DEL] else if ((subExpected == null) && (subArray == null))\\n[DEL] {\\n[DEL] if (type == typeof(Int64))\\n[DEL] {\\n[DEL] if (Convert.ToInt64(dsArray.members[i].Payload) != Convert.ToInt64(expected[i]))\\n[DEL] return false;\\n[DEL] }\\n[DEL] else if (type == typeof(Double))\\n[DEL] {\\n[DEL] // can\\'t use Double.Episilion, according to msdn, it is smaller than most\\n[DEL] // errors.\\n[DEL] if (Math.Abs(Convert.ToDouble(dsArray.members[i].Payload) - Convert.ToDouble(expected[i])) > 0.000001)\\n[DEL] return false;\\n[DEL] }\\n[DEL] else if (type == typeof(Boolean))\\n[DEL] {\\n[DEL] if (Convert.ToBoolean(dsArray.members[i].Payload) != Convert.ToBoolean(expected[i]))\\n[DEL] return false;\\n[DEL] }\\n[DEL] else if (type == typeof(Char))\\n[DEL] {\\n[DEL] object payload = dsArray.members[i].Payload;\\n[DEL] return Convert.ToChar(Convert.ToInt64(payload)) == Convert.ToChar(expected[i]);\\n[DEL] }\\n[DEL] else if (type == typeof(String))\\n[DEL] {\\n[DEL] return Convert.ToString(dsArray.members[i].Payload) == Convert.ToString(expected[i]);\\n[DEL] }\\n[DEL] else\\n[DEL] {\\n[DEL] throw new NotImplementedException(\"Test comparison not implemented: {EBAFAE6C-BCBF-42B8-B99C-49CFF989F0F0}\");\\n[DEL] }\\n[DEL] }\\n[DEL] else\\n[DEL] {\\n[DEL] return false;\\n[DEL] }\\n[DEL] }\\n[DEL] return true;\\n[DEL] }\\n[DEL] \\n[DEL] public bool CompareArrays(string mirrorObj, List<Object> expected, System.Type type, int blockIndex = 0)\\n[DEL] {\\n[DEL] DsasmArray computedArray = GetValue(mirrorObj, blockIndex).Payload as DsasmArray;\\n[DEL] return CompareArrays(computedArray, expected, type);\\n[DEL] }\\n[DEL] \\n         public bool EqualDotNetObject(Obj dsObj, object dotNetObj)\\n         {\\n             // check for null first',\n",
              " '      */\\n     public void send(@Connection SenderConnection connection,\\n                      @UseConfig SMTPConfiguration configuration,\\n[DEL] @Optional(defaultValue = PAYLOAD) EmailContent content,\\n[ADD] @Optional(defaultValue = PAYLOAD) EmailContent content, // TODO: create a transformer from string to EmailContent when the sdk have support for it - MULE-9181.\\n                      @Optional(defaultValue = \"[No Subject]\") String subject,\\n                      List<String> toAddresses,\\n                      @Optional List<String> ccAddresses,',\n",
              " ' \\n     //Test hive writer re-write operation can de-register old partitions and register new one\\n     try {\\n[DEL] Assert.assertTrue(client.getPartition(\"hivedb\", \"testTable\",Lists.newArrayList(\"2020-03-17-00\")) != null);\\n[ADD] Assert.assertTrue(client.getPartition(\"hivedb\", \"testTable\",Lists.newArrayList(\"2020-03-17-00\")) != null);\\n[ADD] // Test additional table been registered\\n[ADD] Assert.assertTrue(client.tableExists(dedupedDbName, \"testTable\"));\\n     } catch (TException e) {\\n       throw new IOException(e);\\n     }',\n",
              " ' \\n     auto alloc = scriptContext->SourceCodeAllocator();\\n     bool isLibraryCode = ((scriptFlags & fscrIsLibraryCode) == fscrIsLibraryCode);\\n[DEL] int builtInPropertyCount = isLibraryCode ? PropertyIds::_countJSOnlyProperty : TotalNumberOfBuiltInProperties;\\n[DEL] auto reader = Anew(alloc, ByteCodeBufferReader, scriptContext, buffer, isLibraryCode, builtInPropertyCount);\\n[ADD] bool isJsBuiltInCode = ((scriptFlags & fscrJsBuiltIn) == fscrJsBuiltIn);\\n[ADD] bool isLibraryOrJsBuiltInCode = isLibraryCode || isJsBuiltInCode;\\n[ADD] int builtInPropertyCount = isLibraryCode || isJsBuiltInCode ? PropertyIds::_countJSOnlyProperty : TotalNumberOfBuiltInProperties;\\n[ADD] auto reader = Anew(alloc, ByteCodeBufferReader, scriptContext, buffer, isLibraryOrJsBuiltInCode, builtInPropertyCount);\\n     auto hr = reader->ReadHeader();\\n     if (FAILED(hr))\\n     {',\n",
              " '         });\\n       }\\n     });\\n[DEL] ProcessorInterceptor interceptor2 = spy(new ProcessorInterceptor() {});\\n[ADD] ProcessorInterceptor interceptor2 = prepareInterceptor(new ProcessorInterceptor() {});\\n     startFlowWithInterceptors(interceptor1, interceptor2);\\n \\n     Event result = process(flow, eventBuilder().message(Message.of(\"\")).build());\\n     assertThat(result.getMessage().getPayload().getValue(), is(TEST_PAYLOAD));\\n     assertThat(result.getError().isPresent(), is(false));\\n \\n[DEL] InOrder inOrder = inOrder(processor, interceptor1, interceptor2);\\n[ADD] if (useMockInterceptor) {\\n[ADD] InOrder inOrder = inOrder(processor, interceptor1, interceptor2);\\n \\n[DEL] inOrder.verify(interceptor1).before(any(), mapArgWithEntry(\"param\", \"\"), argThat(interceptionHasPayloadValue(\"\")));\\n[DEL] inOrder.verify(interceptor1).around(any(), mapArgWithEntry(\"param\", \"\"), any(), any());\\n[DEL] inOrder.verify(interceptor2).before(any(), mapArgWithEntry(\"param\", \"\"), argThat(interceptionHasPayloadValue(\"\")));\\n[DEL] inOrder.verify(interceptor2).around(any(), mapArgWithEntry(\"param\", \"\"), argThat(interceptionHasPayloadValue(\"\")), any());\\n[DEL] inOrder.verify(processor).process(argThat(hasPayloadValue(\"\")));\\n[DEL] inOrder.verify(interceptor2).after(any(), argThat(interceptionHasPayloadValue(\"\")), eq(empty()));\\n[DEL] inOrder.verify(interceptor1).after(any(), argThat(interceptionHasPayloadValue(TEST_PAYLOAD)), eq(empty()));\\n[ADD] inOrder.verify(interceptor1).before(any(), mapArgWithEntry(\"param\", \"\"), argThat(interceptionHasPayloadValue(\"\")));\\n[ADD] inOrder.verify(interceptor1).around(any(), mapArgWithEntry(\"param\", \"\"), any(), any());\\n[ADD] inOrder.verify(interceptor2).before(any(), mapArgWithEntry(\"param\", \"\"), argThat(interceptionHasPayloadValue(\"\")));\\n[ADD] inOrder.verify(interceptor2).around(any(), mapArgWithEntry(\"param\", \"\"), argThat(interceptionHasPayloadValue(\"\")), any());\\n[ADD] inOrder.verify(processor).process(argThat(hasPayloadValue(\"\")));\\n[ADD] inOrder.verify(interceptor2).after(any(), argThat(interceptionHasPayloadValue(\"\")), eq(empty()));\\n[ADD] inOrder.verify(interceptor1).after(any(), argThat(interceptionHasPayloadValue(TEST_PAYLOAD)), eq(empty()));\\n[ADD] \\n[ADD] assertThat(result.getParameters().entrySet(), hasSize(0));\\n[ADD] verifyParametersResolvedAndDisposed(times(1));\\n[ADD] }\\n   }\\n \\n   @Test\\n   public void secondInterceptorMutatesEventAroundAfterProceed() throws Exception {\\n[DEL] ProcessorInterceptor interceptor1 = spy(new ProcessorInterceptor() {});\\n[DEL] ProcessorInterceptor interceptor2 = spy(new ProcessorInterceptor() {\\n[ADD] ProcessorInterceptor interceptor1 = prepareInterceptor(new ProcessorInterceptor() {});\\n[ADD] ProcessorInterceptor interceptor2 = prepareInterceptor(new ProcessorInterceptor() {\\n \\n       @Override\\n       public CompletableFuture<InterceptionEvent> around(ComponentLocation location, Map<String, Object> parameters,',\n",
              " '     try {\\n       process(flow, eventBuilder().message(Message.of(\"\")).build());\\n     } finally {\\n[DEL] InOrder inOrder = inOrder(processor, interceptor1, interceptor2);\\n[DEL] \\n[DEL] inOrder.verify(interceptor1).before(any(), any(), any());\\n[DEL] inOrder.verify(interceptor1).around(any(), any(), any(), any());\\n[DEL] inOrder.verify(interceptor2).before(any(), any(), any());\\n[DEL] inOrder.verify(interceptor2).around(any(), any(), any(), any());\\n[DEL] inOrder.verify(processor).process(any());\\n[DEL] inOrder.verify(interceptor2).after(any(), any(), eq(empty()));\\n[DEL] inOrder.verify(interceptor1).after(any(), any(), eq(of(expectedException)));\\n[ADD] if (useMockInterceptor) {\\n[ADD] InOrder inOrder = inOrder(processor, interceptor1, interceptor2);\\n[ADD] \\n[ADD] inOrder.verify(interceptor1).before(any(), any(), any());\\n[ADD] inOrder.verify(interceptor1).around(any(), any(), any(), any());\\n[ADD] inOrder.verify(interceptor2).before(any(), any(), any());\\n[ADD] inOrder.verify(interceptor2).around(any(), any(), any(), any());\\n[ADD] inOrder.verify(processor).process(any());\\n[ADD] inOrder.verify(interceptor2).after(any(), any(), eq(empty()));\\n[ADD] inOrder.verify(interceptor1).after(any(), any(), eq(of(expectedException)));\\n[ADD] \\n[ADD] verifyParametersResolvedAndDisposed(times(1));\\n[ADD] }\\n     }\\n   }\\n \\n   @Test\\n   public void firstInterceptorThrowsExceptionAround() throws Exception {\\n     RuntimeException expectedException = new RuntimeException(\"Some Error\");\\n[DEL] ProcessorInterceptor interceptor1 = spy(new ProcessorInterceptor() {\\n[ADD] ProcessorInterceptor interceptor1 = prepareInterceptor(new ProcessorInterceptor() {\\n \\n       @Override\\n       public CompletableFuture<InterceptionEvent> around(ComponentLocation location, Map<String, Object> parameters,',\n",
              " ' import java.util.List;\\n import java.util.Properties;\\n \\n[ADD] import org.apache.gobblin.configuration.State;\\n[ADD] import org.apache.gobblin.destination.DestinationDatasetHandlerService;\\n[ADD] import org.apache.gobblin.source.workunit.BasicWorkUnitStream;\\n import org.apache.hadoop.fs.FileStatus;\\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;',\n",
              " ' \\n \\tprivate static class DBObjectToUUIDConverter implements Converter<DBObject, UUID> {\\n \\t\\tpublic UUID convert(DBObject source) {\\n[DEL] UUID id = UUID.fromString((String) source.get(\"_value\"));\\n[DEL] return id;\\n[ADD] return UUID.fromString((String) source.get(\"_value\"));\\n \\t\\t}\\n \\t}\\n ',\n",
              " '         }\\n     }\\n \\n[DEL] protected void addRedeliveredProperty(Message jmsMessage, Map<String, Object> messageProperties)\\n[ADD] protected void addRedeliveredProperty(Message jmsMessage, Map<String, Serializable> messageProperties)\\n     {\\n         try\\n         {',\n",
              " '   private DataSource dataSource;\\n \\n   @Override\\n[DEL] public final DbConnection connect() throws ConnectionException {\\n[ADD] public DbConnection connect() throws ConnectionException {\\n     try {\\n       Connection jdbcConnection = jdbcConnectionFactory.createConnection(dataSource, resolvedCustomTypes);\\n ',\n",
              " '                   translateTimerSpec(getTimerSpecOrThrow(timer.getValue(), doFn), newComponents);\\n               timerSpecs.put(timer.getKey(), spec);\\n             }\\n[ADD] \\n[ADD] for (Map.Entry<String, DoFnSignature.TimerFamilyDeclaration> timerFamily :\\n[ADD] signature.timerFamilyDeclarations().entrySet()) {\\n[ADD] RunnerApi.TimerSpec spec =\\n[ADD] translateTimerSpec(\\n[ADD] DoFnSignatures.getTimerFamilySpecOrThrow(timerFamily.getValue(), doFn),\\n[ADD] newComponents);\\n[ADD] timerSpecs.put(timerFamily.getKey(), spec);\\n[ADD] }\\n[ADD] \\n             return timerSpecs;\\n           }\\n ',\n",
              " '  */\\n package org.mule.test.core.context.notification.processors;\\n \\n[DEL] import static org.junit.Assert.assertNotNull;\\n[ADD] import static org.hamcrest.Matchers.not;\\n[ADD] import static org.hamcrest.Matchers.nullValue;\\n[ADD] import static org.junit.Assert.assertThat;\\n import static org.mule.runtime.core.api.config.MuleProperties.MULE_DEFAULT_PROCESSING_STRATEGY;\\n import static org.mule.runtime.core.util.ProcessingStrategyUtils.NON_BLOCKING_PROCESSING_STRATEGY;\\n[DEL] \\n[DEL] import org.mule.runtime.core.api.client.MuleClient;\\n[ADD] import static org.mule.service.http.api.HttpConstants.Method.GET;\\n[ADD] \\n[ADD] import org.mule.runtime.api.lifecycle.InitialisationException;\\n[ADD] import org.mule.runtime.core.api.registry.RegistrationException;\\n[ADD] import org.mule.service.http.api.HttpService;\\n[ADD] import org.mule.service.http.api.client.HttpClient;\\n[ADD] import org.mule.service.http.api.client.HttpClientConfiguration;\\n[ADD] import org.mule.service.http.api.domain.entity.ByteArrayHttpEntity;\\n[ADD] import org.mule.service.http.api.domain.message.request.HttpRequest;\\n import org.mule.tck.junit4.rule.DynamicPort;\\n import org.mule.tck.junit4.rule.SystemProperty;\\n import org.mule.test.core.context.notification.Node;\\n import org.mule.test.core.context.notification.RestrictedNode;\\n import org.mule.test.runner.RunnerDelegateTo;\\n \\n[ADD] import java.io.IOException;\\n import java.util.Arrays;\\n import java.util.Collection;\\n \\n[ADD] import org.junit.After;\\n[ADD] import org.junit.Before;\\n import org.junit.Rule;\\n import org.junit.Test;\\n import org.junit.runners.Parameterized;',\n",
              " '     return new JobState();\\n   }\\n \\n[DEL] /**\\n[DEL] * Get the list of {@link WorkUnitState}s in the given previous {@link JobState}.\\n[DEL] */\\n[DEL] private List<WorkUnitState> getPreviousWorkUnitStates(JobState previousJobState) {\\n[DEL] List<WorkUnitState> previousWorkUnitStates = Lists.newArrayList();\\n[DEL] for (TaskState taskState : previousJobState.getTaskStates()) {\\n[DEL] WorkUnitState workUnitState = new WorkUnitState(taskState.getWorkunit());\\n[DEL] workUnitState.setId(taskState.getId());\\n[DEL] workUnitState.addAll(taskState);\\n[DEL] previousWorkUnitStates.add(workUnitState);\\n[DEL] }\\n[DEL] \\n[DEL] return previousWorkUnitStates;\\n[DEL] }\\n[DEL] \\n   /**\\n    * Try acquiring the job lock and return whether the lock is successfully locked.\\n    */',\n",
              " '     return responseMap;\\n   }\\n \\n[DEL] private boolean isCompileSuccessful(Map<String, AddSpecResponse> responseMap) {\\n[ADD] public static boolean isCompileSuccessful(Map<String, AddSpecResponse> responseMap) {\\n     AddSpecResponse<String> addSpecResponse = responseMap.getOrDefault(ServiceConfigKeys.GOBBLIN_SERVICE_JOB_SCHEDULER_LISTENER_CLASS, new AddSpecResponse<>(\"\"));\\n \\n[DEL] return addSpecResponse != null\\n[DEL] && addSpecResponse.getValue() != null\\n[DEL] && !addSpecResponse.getValue().contains(\"ConfigException\");\\n[ADD] return isCompileSuccessful(addSpecResponse.getValue());\\n[ADD] }\\n[ADD] \\n[ADD] public static boolean isCompileSuccessful(String dag) {\\n[ADD] return dag != null && !dag.contains(ConfigException.class.getSimpleName());\\n   }\\n \\n   @Override',\n",
              " ' \\tprivate void makeDirectories(String path, Session<F> session) throws IOException {\\n \\t\\tif (!session.exists(path)){\\n \\n[DEL] int nextSeparatorIndex = path.lastIndexOf(remoteFileSeparator);\\n[ADD] int nextSeparatorIndex = path.lastIndexOf(this.remoteFileSeparator);\\n \\n \\t\\t\\tif (nextSeparatorIndex > -1){\\n \\t\\t\\t\\tList<String> pathsToCreate = new LinkedList<String>();\\n \\t\\t\\t\\twhile (nextSeparatorIndex > -1){\\n \\t\\t\\t\\t\\tString pathSegment = path.substring(0, nextSeparatorIndex);\\n[DEL] if (session.exists(pathSegment)){\\n[ADD] if (pathSegment.length() == 0 || session.exists(pathSegment)) {\\n \\t\\t\\t\\t\\t\\t// no more paths to create\\n \\t\\t\\t\\t\\t\\tbreak;\\n \\t\\t\\t\\t\\t}\\n \\t\\t\\t\\t\\telse {\\n \\t\\t\\t\\t\\t\\tpathsToCreate.add(0, pathSegment);\\n[DEL] nextSeparatorIndex = pathSegment.lastIndexOf(remoteFileSeparator);\\n[ADD] nextSeparatorIndex = pathSegment.lastIndexOf(this.remoteFileSeparator);\\n \\t\\t\\t\\t\\t}\\n \\t\\t\\t\\t}\\n ',\n",
              " '     }\\n   }\\n \\n[ADD] @Deprecated\\n[ADD] protected HiveWorkUnit workUnitForTable(HiveDataset hiveDataset) throws IOException {\\n[ADD] return this.workUnitForTable(hiveDataset, false);\\n[ADD] }\\n[ADD] \\n   protected HiveWorkUnit workUnitForTable(HiveDataset hiveDataset, boolean disableAvroCheck) throws IOException {\\n     HiveWorkUnit hiveWorkUnit = new HiveWorkUnit(hiveDataset);\\n     if (disableAvroCheck || isAvro(hiveDataset.getTable())) {',\n",
              " '                     \" group by cards.did\";\\n             Timber.d(\"AdvancedStatistics.TodayStats query: %s\", query);\\n \\n[DEL] try (Cursor cur = db.query(query)) {\\n[ADD] try (Cursor mCur = mDb.query(query)) {\\n \\n[DEL] while(cur.moveToNext()) {\\n[DEL] nLearnedPerDeckId.put(cur.getLong(0), cur.getInt(1));\\n[ADD] while(mCur.moveToNext()) {\\n[ADD] mNLearnedPerDeckId.put(mCur.getLong(0), mCur.getInt(1));\\n                 }\\n             }\\n         }\\n \\n         public int getNLearned(long did) {\\n[DEL] if(nLearnedPerDeckId.containsKey(did)) {\\n[DEL] return nLearnedPerDeckId.get(did);\\n[ADD] if(mNLearnedPerDeckId.containsKey(did)) {\\n[ADD] return mNLearnedPerDeckId.get(did);\\n             }\\n             else {\\n                 return 0;',\n",
              " '     Either<CredentialsProvider, SshSessionFactory> providerSessionFactoryEither;\\n     boolean isSshWithPublicKeyEnabled = ConfigUtils.getBoolean(config, ConfigurationKeys.GIT_MONITOR_SSH_WITH_PUBLIC_KEY_ENABLED, false);\\n     if (isSshWithPublicKeyEnabled) {\\n[DEL] this.privateKeyPath = ConfigUtils.getString(config, ConfigurationKeys.GIT_MONITOR_PRIVATE_KEY_PATH, null);\\n[DEL] if (Strings.isNullOrEmpty(this.privateKeyPath)) {\\n[DEL] throw new RuntimeException(\"Path to private key must be provided\");\\n[ADD] this.privateKeyPath = ConfigUtils.getString(config, ConfigurationKeys.GIT_MONITOR_SSH_PRIVATE_KEY_PATH, null);\\n[ADD] String privateKeyBase64Encoded = ConfigUtils.getString(config, ConfigurationKeys.GIT_MONITOR_SSH_PRIVATE_KEY_BASE64_ENCODED, null);\\n[ADD] String publicKeyBase64Encoded = ConfigUtils.getString(config, ConfigurationKeys.GIT_MONITOR_SSH_PUBLIC_KEY_BASE64_ENCODED, null);\\n[ADD] \\n[ADD] if ((Strings.isNullOrEmpty(this.privateKeyPath)) && ((Strings.isNullOrEmpty(privateKeyBase64Encoded)) || (Strings.isNullOrEmpty(publicKeyBase64Encoded)))) {\\n[ADD] throw new RuntimeException(\"Path to private key or private/public key strings must be provided\");\\n[ADD] }\\n[ADD] \\n[ADD] if (!Strings.isNullOrEmpty(privateKeyBase64Encoded)) {\\n[ADD] this.privateKey = Base64.decodeBase64(privateKeyBase64Encoded);\\n[ADD] this.publicKey = Base64.decodeBase64(publicKeyBase64Encoded);\\n       }\\n[ADD] \\n       String passPhraseEnc = ConfigUtils.getString(config, ConfigurationKeys.GIT_MONITOR_SSH_PASSPHRASE, null);\\n[DEL] if (passPhraseEnc != null) {\\n[ADD] if (!Strings.isNullOrEmpty(passPhraseEnc)) {\\n         this.passphrase = passwordManager.readPassword(passPhraseEnc);\\n       }\\n       providerSessionFactoryEither = Either.right(getSshSessionFactory());\\n       this.isJschLoggerEnabled = ConfigUtils.getBoolean(config, ConfigurationKeys.GIT_MONITOR_JSCH_LOGGER_ENABLED, false);\\n[ADD] this.strictHostKeyCheckingEnabled = ConfigUtils.getBoolean(config, ConfigurationKeys.GIT_MONITOR_SSH_STRICT_HOST_KEY_CHECKING_ENABLED,\\n[ADD] true);\\n[ADD] this.knownHosts = ConfigUtils.getString(config, ConfigurationKeys.GIT_MONITOR_SSH_KNOWN_HOSTS, null);\\n[ADD] this.knownHostsFile = ConfigUtils.getString(config, ConfigurationKeys.GIT_MONITOR_SSH_KNOWN_HOSTS_FILE, null);\\n[ADD] if (strictHostKeyCheckingEnabled && Strings.isNullOrEmpty(knownHostsFile) && Strings.isNullOrEmpty(knownHosts)) {\\n[ADD] throw new RuntimeException(\"Either StrictHostKeyChecking should be disabled or a knownHostFile or knownHosts string must be provided\");\\n[ADD] }\\n     } else { //Use CredentialsProvider\\n       String username = ConfigUtils.getString(config, ConfigurationKeys.GIT_MONITOR_USERNAME, null);\\n       String passwordEnc = ConfigUtils.getString(config, ConfigurationKeys.GIT_MONITOR_PASSWORD, null);',\n",
              " ' \\t\\t}\\n \\t}\\n \\n[ADD] @Override\\n \\tpublic void stop() {\\n \\t\\tsynchronized (this.lifecycleMonitor) {\\n \\t\\t\\tif (this.active) {',\n",
              " '     when(mockMuleMessage.getInboundProperty(\"errorMessage\")).thenReturn(\"ERROR!!!\");\\n     String expectedExpression = IOUtils.getResourceAsString(LOCATION, this.getClass());\\n \\n[DEL] when(mockMuleMessage.getPayload()).thenReturn(TypedValue.of(\"Parsed\"));\\n[DEL] when(mockMuleMessage.getAttributes()).thenReturn(TypedValue.of(new HashMap<>()));\\n[ADD] when(mockMuleMessage.getPayload()).thenReturn(of(\"Parsed\"));\\n[ADD] when(mockMuleMessage.getAttributes()).thenReturn(of(new HashMap<>()));\\n     when(mockExpressionManager.parseLogTemplate(eq(expectedExpression), eq(event), any(), any())).thenReturn(\"Parsed\");\\n \\n     CoreEvent response = parseTemplateProcessor.process(event);',\n",
              " ' \\n /**\\n  * ScriptRouter\\n[DEL] *\\n  */\\n public class ScriptRouter implements Router {\\n ',\n",
              " ' \\t\\treturn name.substring(\"_org.springframework.integration\".length() + 1);\\n \\t}\\n \\n[DEL] private MessageSourceMetrics enhanceSourceMonitor(SimpleMessageSourceMetrics monitor) {\\n[ADD] private MessageSourceMetrics enhanceSourceMonitor(MessageSourceMetrics monitor) {\\n \\n \\t\\tMessageSourceMetrics result = monitor;\\n \\n[DEL] if (monitor.getName() != null && monitor.getSource() != null) {\\n[ADD] if (monitor.getManagedName() != null) {\\n \\t\\t\\treturn monitor;\\n \\t\\t}\\n ',\n",
              " ' import java.net.InetAddress;\\n import java.net.InetSocketAddress;\\n import java.net.MulticastSocket;\\n[ADD] import java.net.SocketAddress;\\n[ADD] import java.net.URI;\\n \\n[ADD] import org.springframework.expression.Expression;\\n import org.springframework.messaging.Message;\\n import org.springframework.messaging.MessageHandler;\\n ',\n",
              " '    * Copy information from the meta-data input to the dialog fields.\\n    */\\n   public void getData() {\\n[DEL] wInput.setText( Const.NVL( input.getInputFileField(), \"\" ) );\\n[DEL] wOutput.setText( Const.NVL( input.getOutputFileField(), \"\" ) );\\n[ADD] wInputField.setText( Const.NVL( input.getInputFileField(), \"\" ) );\\n[ADD] wOutputField.setText( Const.NVL( input.getOutputFileField(), \"\" ) );\\n[ADD] \\n[ADD] wInput.setText( Const.NVL( input.getInputFile(), \"\" ) );\\n[ADD] wOutput.setText( Const.NVL( input.getOutputFile(), \"\" ) );\\n[ADD] \\n[ADD] wParentFolder.setSelection( input.getCreateParentfolder() );\\n[ADD] wUseValuesFromFields.setSelection( input.getUseValuesFromFields() );\\n[ADD] \\n[ADD] setActiveFields();\\n \\n     for ( String name : input.getParameterFieldMap().keySet() ) {\\n       String field = input.getParameterFieldMap().get( name );',\n",
              " '     }\\n     return deletePaths;\\n   }\\n[ADD] \\n[ADD] /**\\n[ADD] * This method finds the files to be cleaned based on the number of hours. If {@code config.getCleanerHoursRetained()} is set to 5,\\n[ADD] * all the files with commit time earlier than 5 hours will be removed. Also the latest file for any file group is retained.\\n[ADD] * This policy gives much more flexibility to users for retaining data for running incremental queries as compared to\\n[ADD] * KEEP_LATEST_COMMITS cleaning policy. The default number of hours is 5.\\n[ADD] * @param partitionPath partition path to check\\n[ADD] * @return list of files to clean\\n[ADD] */\\n[ADD] private List<CleanFileInfo> getFilesToCleanKeepingLatestHours(String partitionPath) {\\n[ADD] int commitsToRetain = 0;\\n[ADD] return getFilesToCleanKeepingLatestCommits(partitionPath, commitsToRetain, HoodieCleaningPolicy.KEEP_LATEST_BY_HOURS);\\n[ADD] }\\n   \\n   private List<CleanFileInfo> getReplacedFilesEligibleToClean(List<String> savepointedFiles, String partitionPath, Option<HoodieInstant> earliestCommitToRetain) {\\n     final Stream<HoodieFileGroup> replacedGroups;',\n",
              " '  */\\n package org.mule.routing.filters;\\n \\n[ADD] import static org.mule.util.ClassUtils.equal;\\n[ADD] import static org.mule.util.ClassUtils.hash;\\n import org.mule.api.MuleMessage;\\n import org.mule.api.routing.filter.Filter;\\n import org.mule.api.routing.filter.ObjectFilter;',\n",
              " ' \\n import androidx.test.ext.junit.runners.AndroidJUnit4;\\n \\n[ADD] /* @RequiresApi(api = Build.VERSION_CODES.O)\\n[ADD] * java.nio.file.Paths#\\n[ADD] */\\n @RunWith(AndroidJUnit4.class)\\n @Config(sdk = { 21, 26 })\\n public class CompatCopyFileTest {',\n",
              " '     }\\n     return timestamp;\\n   }\\n[ADD] \\n[ADD] @Override\\n[ADD] public Option<HoodieCompactionPlan> execute() {\\n[ADD] if (!config.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()\\n[ADD] && !config.getFailedWritesCleanPolicy().isLazy()) {\\n[ADD] // if there are inflight writes, their instantTime must not be less than that of compaction instant time\\n[ADD] table.getActiveTimeline().getCommitsTimeline().filterPendingExcludingCompaction().firstInstant()\\n[ADD] .ifPresent(earliestInflight -> ValidationUtils.checkArgument(\\n[ADD] HoodieTimeline.compareTimestamps(earliestInflight.getTimestamp(), HoodieTimeline.GREATER_THAN, instantTime),\\n[ADD] \"Earliest write inflight instant time must be later than compaction time. Earliest :\" + earliestInflight\\n[ADD] + \", Compaction scheduled at \" + instantTime));\\n[ADD] // Committed and pending compaction instants should have strictly lower timestamps\\n[ADD] List<HoodieInstant> conflictingInstants = table.getActiveTimeline()\\n[ADD] .getWriteTimeline().filterCompletedAndCompactionInstants().getInstants()\\n[ADD] .filter(instant -> HoodieTimeline.compareTimestamps(\\n[ADD] instant.getTimestamp(), HoodieTimeline.GREATER_THAN_OR_EQUALS, instantTime))\\n[ADD] .collect(Collectors.toList());\\n[ADD] ValidationUtils.checkArgument(conflictingInstants.isEmpty(),\\n[ADD] \"Following instants have timestamps >= compactionInstant (\" + instantTime + \") Instants :\"\\n[ADD] + conflictingInstants);\\n[ADD] }\\n[ADD] \\n[ADD] HoodieCompactionPlan plan = scheduleCompaction();\\n[ADD] if (plan != null && (plan.getOperations() != null) && (!plan.getOperations().isEmpty())) {\\n[ADD] extraMetadata.ifPresent(plan::setExtraMetadata);\\n[ADD] HoodieInstant compactionInstant =\\n[ADD] new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, instantTime);\\n[ADD] try {\\n[ADD] table.getActiveTimeline().saveToCompactionRequested(compactionInstant,\\n[ADD] TimelineMetadataUtils.serializeCompactionPlan(plan));\\n[ADD] } catch (IOException ioe) {\\n[ADD] throw new HoodieIOException(\"Exception scheduling compaction\", ioe);\\n[ADD] }\\n[ADD] return Option.of(plan);\\n[ADD] }\\n[ADD] return Option.empty();\\n[ADD] }\\n }',\n",
              " ' \\t\\t\\tif (value.isEmpty() || Constants.EMPTY_HEADER.equals(value))\\n \\t\\t\\t\\tcontinue;\\n \\n[DEL] char c = value.charAt(0);\\n[ADD] char c = key.charAt(0);\\n \\t\\t\\tif (Character.isUpperCase(c))\\n[DEL] application.putValue(key, value);\\n[ADD] mainAttributes.putValue(key, value);\\n \\t\\t}\\n \\t\\tInstructions instructions = new Instructions(project.mergeProperties(REMOVEHEADERS));\\n[DEL] Collection<Object> result = instructions.select(application.keySet(), false);\\n[DEL] application.keySet()\\n[ADD] Collection<Object> result = instructions.select(mainAttributes.keySet(), false);\\n[ADD] mainAttributes.keySet()\\n \\t\\t\\t.removeAll(result);\\n \\t}\\n \\n[DEL] private void set(Attributes application, String key, String... values) {\\n[DEL] if (application.getValue(key) != null)\\n[ADD] // puts the values to the attributes if no other value is set by this key\\n[ADD] private void set(Attributes mainAttributes, String key, String... values) {\\n[ADD] if (mainAttributes.getValue(key) != null)\\n \\t\\t\\treturn;\\n \\n \\t\\tfor (String value : values) {\\n \\t\\t\\tif (value != null) {\\n[DEL] application.putValue(key, value);\\n[ADD] mainAttributes.putValue(key, value);\\n \\t\\t\\t\\treturn;\\n \\t\\t\\t}\\n \\t\\t}\\n[DEL] \\n \\t}\\n[DEL] \\n[DEL] }\\n[ADD] }',\n",
              " '     private static final Map<String, ContextAwareMeter> groupSuccessfulMeters = Maps.newConcurrentMap();\\n     private static final Map<String, ContextAwareMeter> groupFailureMeters = Maps.newConcurrentMap();\\n \\n[DEL] private JobStatusRetriever jobStatusRetriever;\\n[DEL] private DagStateStore dagStateStore;\\n[DEL] private DagStateStore failedDagStateStore;\\n[DEL] private BlockingQueue<Dag<JobExecutionPlan>> queue;\\n[DEL] private BlockingQueue<String> cancelQueue;\\n[DEL] private BlockingQueue<String> resumeQueue;\\n[ADD] private final JobStatusRetriever jobStatusRetriever;\\n[ADD] private final DagStateStore dagStateStore;\\n[ADD] private final DagStateStore failedDagStateStore;\\n[ADD] private final BlockingQueue<Dag<JobExecutionPlan>> queue;\\n[ADD] private final BlockingQueue<String> cancelQueue;\\n[ADD] private final BlockingQueue<String> resumeQueue;\\n \\n     /**\\n      * Constructor.',\n",
              " '             JOB_STATUS_RETRIEVER_CLASS_KEY, FsJobStatusRetriever.class.getName()));\\n \\n     if (serviceConfig.isRestLIServerEnabled()) {\\n[DEL] binder.bind(EmbeddedRestliServer.class).toProvider(EmbeddedRestliServerProvider.class).in(Singleton.class);\\n[ADD] binder.bind(EmbeddedRestliServer.class).toProvider(EmbeddedRestliServerProvider.class);\\n     }\\n \\n     binder.bind(GobblinServiceManager.class);\\n \\n[DEL] binder.bind(MultiContextIssueRepository.class).to(InMemoryMultiContextIssueRepository.class);\\n[ADD] binder.bind(ServiceDatabaseProvider.class).to(ServiceDatabaseProviderImpl.class);\\n[ADD] binder.bind(ServiceDatabaseProviderImpl.Configuration.class);\\n[ADD] \\n[ADD] binder.bind(ServiceDatabaseManager.class);\\n[ADD] \\n[ADD] binder.bind(MultiContextIssueRepository.class)\\n[ADD] .to(getClassByNameOrAlias(MultiContextIssueRepository.class, serviceConfig.getInnerConfig(),\\n[ADD] ServiceConfigKeys.ISSUE_REPO_CLASS, MySqlMultiContextIssueRepository.class.getName()));\\n[ADD] \\n[ADD] binder.bind(MySqlMultiContextIssueRepository.Configuration.class);\\n[ADD] binder.bind(InMemoryMultiContextIssueRepository.Configuration.class);\\n \\n     binder.bind(JobIssueEventHandler.class);\\n ',\n",
              " ' \\n   /**\\n    * Commit the {@code HoodieRecord}s to Metadata Table as a new delta-commit.\\n[DEL] *\\n[DEL] * @param records The list of records to be written.\\n[ADD] *  @param records The list of records to be written.\\n    * @param partitionName The partition to which the records are to be written.\\n    * @param instantTime The timestamp to use for the deltacommit.\\n[ADD] * @param canTriggerTableService true if table services can be scheduled and executed. false otherwise.\\n    */\\n[DEL] protected abstract void commit(List<HoodieRecord> records, String partitionName, String instantTime);\\n[ADD] protected abstract void commit(List<HoodieRecord> records, String partitionName, String instantTime, boolean canTriggerTableService);\\n \\n   /**\\n    *  Perform a compaction on the Metadata Table.',\n",
              " ' import org.rstudio.studio.client.common.rnw.RnwWeave;\\n import org.rstudio.studio.client.common.rnw.RnwWeaveRegistry;\\n import org.rstudio.studio.client.common.satellite.Satellite;\\n[DEL] import org.rstudio.studio.client.common.synctex.Synctex;\\n[DEL] import org.rstudio.studio.client.common.synctex.events.SynctexStatusChangedEvent;\\n import org.rstudio.studio.client.events.GetEditorContextEvent;\\n[DEL] import org.rstudio.studio.client.events.GetEditorContextEvent.DocumentSelection;\\n import org.rstudio.studio.client.events.ReplaceRangesEvent;\\n import org.rstudio.studio.client.events.ReplaceRangesEvent.ReplacementData;\\n[ADD] import org.rstudio.studio.client.palette.model.CommandPaletteEntrySource;\\n[ADD] import org.rstudio.studio.client.palette.model.CommandPaletteItem;\\n import org.rstudio.studio.client.events.SetSelectionRangesEvent;\\n[DEL] import org.rstudio.studio.client.rmarkdown.model.RmdChosenTemplate;\\n[DEL] import org.rstudio.studio.client.rmarkdown.model.RmdFrontMatter;\\n[DEL] import org.rstudio.studio.client.rmarkdown.model.RmdOutputFormat;\\n[DEL] import org.rstudio.studio.client.rmarkdown.model.RmdTemplateData;\\n import org.rstudio.studio.client.server.ServerError;\\n import org.rstudio.studio.client.server.ServerRequestCallback;\\n import org.rstudio.studio.client.server.VoidServerRequestCallback;',\n",
              " '     return result;\\n   }\\n \\n[DEL] public NewArrayTreeImpl newArrayInitializer(AstNode openBraceTokenAstNode, Optional<List<AstNode>> rests, AstNode closeBraceTokenAstNode) {\\n[ADD] public NewArrayTreeImpl newArrayInitializer(InternalSyntaxToken openBraceToken, Optional<List<AstNode>> rests, InternalSyntaxToken closeBraceToken) {\\n     ImmutableList.Builder<ExpressionTree> initializers = ImmutableList.builder();\\n     List<AstNode> children = Lists.newArrayList();\\n \\n[DEL] InternalSyntaxToken openBraceToken = InternalSyntaxToken.create(openBraceTokenAstNode);\\n[DEL] InternalSyntaxToken closeBraceToken = InternalSyntaxToken.create(closeBraceTokenAstNode);\\n[DEL] \\n     children.add(openBraceToken);\\n     if (rests.isPresent()) {\\n       for (AstNode rest : rests.get()) {',\n",
              " ' \\tpublic <T> T resolve(boolean failOnChanges, boolean writeOnChanges,\\n \\t\\tConverter<T, Collection<? extends HeaderClause>> runbundlesFormatter) throws Exception {\\n \\n[DEL] RunResolution resolution = resolve();\\n[ADD] RunResolution resolution = resolve(true);\\n \\n[DEL] if (isOk()) {\\n[ADD] if (resolution.exception == null) {\\n \\t\\t\\tupdate(resolution, failOnChanges, writeOnChanges);\\n \\t\\t\\treturn runbundlesFormatter.convert(model.getRunBundles());\\n[DEL] } else {\\n[DEL] logger.info(getErrors().toString());\\n \\t\\t}\\n[DEL] return runbundlesFormatter.convert(Collections.emptyList());\\n[ADD] \\n[ADD] throw resolution.exception;\\n \\t}\\n \\n \\tpublic RunResolution resolve(ResolutionCallback... callbacks) throws Exception {\\n[ADD] return resolve(false, callbacks);\\n[ADD] }\\n[ADD] \\n[ADD] public RunResolution resolve(boolean sparseErrors, ResolutionCallback... callbacks) throws Exception {\\n \\t\\tRunResolution resolution = RunResolution.resolve(this, this, Arrays.asList(callbacks));\\n[DEL] if (resolution.exception != null) {\\n[ADD] if (!sparseErrors && (resolution.exception != null)) {\\n \\t\\t\\tif (resolution.exception instanceof ResolutionException) {\\n \\t\\t\\t\\tResolutionException re = (ResolutionException) resolution.exception;\\n \\t\\t\\t\\tFilterParser filterParser = new FilterParser();',\n",
              " ' \\n   private <T extends ParameterizedModel> void enrichElementModel(T model, DslElementSyntax elementDsl,\\n                                                                  ComponentAst configuration,\\n[DEL] DslElementModel.Builder<T> builder) {\\n[ADD] DslElementModel.Builder builder) {\\n[ADD] // Have to keep the order of the elements consistent so the generated metadata keys are backwards compatible\\n[ADD] populateConnectionProviderElements(model, elementDsl, builder, configuration);\\n     populateParameterizedElements(model, elementDsl, builder, configuration);\\n[DEL] if (model instanceof ComposableModel) {\\n[DEL] populateComposableElements((ComposableModel) model, elementDsl, builder, configuration);\\n[DEL] }\\n[ADD] populateComposableElements(model, elementDsl, builder, configuration);\\n \\n     if (model instanceof SourceModel) {\\n       ((SourceModel) model).getSuccessCallback()',\n",
              " '  */\\n public class DubboClassPathBeanDefinitionScanner extends ClassPathBeanDefinitionScanner {\\n \\n[ADD] private final ConcurrentMap<String, Set<BeanDefinition>> beanDefinitionMap = new ConcurrentHashMap<>();\\n \\n     public DubboClassPathBeanDefinitionScanner(BeanDefinitionRegistry registry, boolean useDefaultFilters, Environment environment,\\n                                                ResourceLoader resourceLoader) {',\n",
              " '       .defaultValue(20)// default min 20 commits\\n       .withDescription(\"Min number of commits to keep before archiving older commits into a sequential log, default 20\");\\n \\n[ADD] // ------------------------------------------------------------------------\\n[ADD] //  Clustering Options\\n[ADD] // ------------------------------------------------------------------------\\n[ADD] \\n[ADD] public static final ConfigOption<Boolean> CLUSTERING_SCHEDULE_ENABLED = ConfigOptions\\n[ADD] .key(\"clustering.schedule.enabled\")\\n[ADD] .booleanType()\\n[ADD] .defaultValue(false) // default false for pipeline\\n[ADD] .withDescription(\"Async clustering, default false for pipeline\");\\n[ADD] \\n[ADD] public static final ConfigOption<Integer> CLUSTERING_TASKS = ConfigOptions\\n[ADD] .key(\"clustering.tasks\")\\n[ADD] .intType()\\n[ADD] .defaultValue(10)\\n[ADD] .withDescription(\"Parallelism of tasks that do actual clustering, default is 10\");\\n[ADD] \\n[ADD] public static final ConfigOption<Integer> CLUSTERING_TARGET_PARTITIONS = ConfigOptions\\n[ADD] .key(\"clustering.plan.strategy.daybased.lookback.partitions\")\\n[ADD] .intType()\\n[ADD] .defaultValue(2)\\n[ADD] .withDescription(\"Number of partitions to list to create ClusteringPlan\");\\n[ADD] \\n[ADD] public static final ConfigOption<String> CLUSTERING_PLAN_STRATEGY_CLASS = ConfigOptions\\n[ADD] .key(\"clustering.plan.strategy.class\")\\n[ADD] .stringType()\\n[ADD] .defaultValue(\"org.apache.hudi.client.clustering.plan.strategy.FlinkRecentDaysClusteringPlanStrategy\")\\n[ADD] .withDescription(\"Config to provide a strategy class (subclass of ClusteringPlanStrategy) to create clustering plan \"\\n[ADD] + \"i.e select what file groups are being clustered. Default strategy, looks at the last N (determined by \"\\n[ADD] + CLUSTERING_TARGET_PARTITIONS.key() + \") day based partitions picks the small file slices within those partitions.\");\\n[ADD] \\n[ADD] public static final ConfigOption<Integer> CLUSTERING_PLAN_STRATEGY_TARGET_FILE_MAX_BYTES = ConfigOptions\\n[ADD] .key(\"clustering.plan.strategy.target.file.max.bytes\")\\n[ADD] .intType()\\n[ADD] .defaultValue(1024 * 1024 * 1024) // default 1 GB\\n[ADD] .withDescription(\"Each group can produce \\'N\\' (CLUSTERING_MAX_GROUP_SIZE/CLUSTERING_TARGET_FILE_SIZE) output file groups, default 1 GB\");\\n[ADD] \\n[ADD] public static final ConfigOption<Integer> CLUSTERING_PLAN_STRATEGY_SMALL_FILE_LIMIT = ConfigOptions\\n[ADD] .key(\"clustering.plan.strategy.small.file.limit\")\\n[ADD] .intType()\\n[ADD] .defaultValue(600) // default 600 MB\\n[ADD] .withDescription(\"Files smaller than the size specified here are candidates for clustering, default 600 MB\");\\n[ADD] \\n[ADD] public static final ConfigOption<Integer> CLUSTERING_PLAN_STRATEGY_SKIP_PARTITIONS_FROM_LATEST = ConfigOptions\\n[ADD] .key(\"clustering.plan.strategy.daybased.skipfromlatest.partitions\")\\n[ADD] .intType()\\n[ADD] .defaultValue(0)\\n[ADD] .withDescription(\"Number of partitions to skip from latest when choosing partitions to create ClusteringPlan\");\\n[ADD] \\n[ADD] public static final ConfigOption<String> CLUSTERING_SORT_COLUMNS = ConfigOptions\\n[ADD] .key(\"clustering.plan.strategy.sort.columns\")\\n[ADD] .stringType()\\n[ADD] .noDefaultValue()\\n[ADD] .withDescription(\"Columns to sort the data by when clustering\");\\n[ADD] \\n[ADD] public static final ConfigOption<Integer> CLUSTERING_MAX_NUM_GROUPS = ConfigOptions\\n[ADD] .key(\"clustering.plan.strategy.max.num.groups\")\\n[ADD] .intType()\\n[ADD] .defaultValue(30)\\n[ADD] .withDescription(\"Maximum number of groups to create as part of ClusteringPlan. Increasing groups will increase parallelism\");\\n[ADD] \\n   // ------------------------------------------------------------------------\\n   //  Hive Sync Options\\n   // ------------------------------------------------------------------------',\n",
              " ' \\t\\t}\\n \\n \\t\\tpublic void postSend(Message<?> message, MessageChannel channel, boolean sent) {\\n[DEL] if (logger.isDebugEnabled()) {\\n[DEL] logger.debug(\"postSend (sent=\" + sent + \") on channel \\'\" + channel + \"\\', message: \" + message);\\n[DEL] }\\n \\t\\t\\tif (this.interceptors.size() > 0) {\\n \\t\\t\\t\\tfor (ChannelInterceptor interceptor : interceptors) {\\n \\t\\t\\t\\t\\tinterceptor.postSend(message, channel, sent);',\n",
              " '[DEL] /*******************************************************************************\\n[ADD] /*! ******************************************************************************\\n  *\\n  * Pentaho Data Integration\\n  *\\n[DEL] * Copyright (C) 2002-2012 by Pentaho : http://www.pentaho.com\\n[ADD] * Copyright (C) 2002-2013 by Pentaho : http://www.pentaho.com\\n  *\\n  *******************************************************************************\\n  *',\n",
              " '         functionRegistry = new FunctionRegistry(functions);\\n     }\\n \\n[ADD] @Test\\n[ADD] public void string_concat(){\\n[ADD] final Rule rule = parser.parseRule(ruleForTest(), false);\\n[ADD] final Message message = evaluateRule(rule, new Message(\"Dummy Message\", \"test\", Tools.nowUTC()));\\n[ADD] \\n[ADD] assertThat(message.hasField(\"result\")).isTrue();\\n[ADD] assertThat(message.getField(\"result\")).isEqualTo(\"aabbcc\");\\n[ADD] }\\n[ADD] \\n     @Test\\n     public void jsonpath() {\\n         final String json = \"{\\\\n\" +',\n",
              " '   public ExpectedException expectedException = none();\\n \\n   private DependencyResolver dependencyResolver;\\n[ADD] private ArtifactClassificationTypeResolver artifactClassificationTypeResolver;\\n   private ClassPathClassifierContext context;\\n   private AetherClassPathClassifier classifier;\\n ',\n",
              " '    * @param event the current {@link Event\\n    * @param config the current {@link ValidationExtension} that serves as config\\n    */\\n[ADD] @Throws(ValidationErrorTypeProvider.class)\\n[ADD] @DisplayName(\"Is IP\")\\n   public void isIp(String ip, @ParameterGroup(ERROR_GROUP) ValidationOptions options, Event event,\\n                    @UseConfig ValidationExtension config)\\n       throws Exception {',\n",
              " '             for (StoragePoolAllocator allocator : _storagePoolAllocators) {\\n                 final List<StoragePool> suitablePools = allocator.allocateToPool(diskProfile, vmProfile, plan, avoid, returnUpTo);\\n                 if (suitablePools != null && !suitablePools.isEmpty()) {\\n[DEL] suitableVolumeStoragePools.put(toBeCreated, suitablePools);\\n[ADD] List<StoragePool> pools = new ArrayList<>();\\n[ADD] Optional<StoragePool> storagePool = getPreferredStoragePool(suitablePools, vmProfile.getVirtualMachine());\\n[ADD] storagePool.ifPresent(pools::add);\\n[ADD] \\n[ADD] pools.addAll(suitablePools);\\n[ADD] suitableVolumeStoragePools.put(toBeCreated, pools);\\n                     foundPotentialPools = true;\\n                     break;\\n                 }',\n",
              " ' import org.mule.api.transport.DispatchException;\\n import org.mule.api.transport.MessageDispatcher;\\n import org.mule.config.i18n.MessageFactory;\\n[ADD] import org.mule.construct.Flow;\\n import org.mule.service.ServiceAsyncReplyCompositeMessageSource;\\n \\n import java.util.List;\\n \\n[ADD] import javax.resource.spi.work.Work;\\n[ADD] import javax.resource.spi.work.WorkException;\\n[ADD] import javax.resource.spi.work.WorkListener;\\n[ADD] \\n /**\\n  * Abstract implementation of an outbound channel adaptors. Outbound channel adaptors send messages over over\\n  * a specific transport. Different implementations may support different Message Exchange Patterns.',\n",
              " ' \\n     void setTitle(String title);\\n \\n[ADD] Map<String, Map<String, Integer>> getPositions();\\n[ADD] \\n[ADD] void setPostions(Map<String, Map<String, Object>> positions);\\n[ADD] \\n     String getDescription();\\n \\n     void setDescription(String description);',\n",
              " '           .setFlowName(flowProps.getProperty(ConfigurationKeys.FLOW_NAME_KEY)))\\n           .setProperties(flowPropsAsStringMap);\\n     }\\n[ADD] \\n[ADD] public static int maxFlowSpecUri() {\\n[ADD] return URI_SCHEME.length() + \":\".length() // URI separator\\n[ADD] + URI_PATH_SEPARATOR.length() + ServiceConfigKeys.MAX_FLOW_NAME + URI_PATH_SEPARATOR.length() + ServiceConfigKeys.MAX_FLOW_GROUP;\\n[ADD] }\\n   }\\n }',\n",
              " '                     DEFAULT_GROUP_ID,\\n                     \"Name of the consumer group the Kafka input belongs to\",\\n                     ConfigurationField.Optional.OPTIONAL));\\n[ADD] cr.addField(new TextField(\\n[ADD] CK_CUSTOM_PROPERTIES,\\n[ADD] \"Custom Kafka properties\",\\n[ADD] \"\",\\n[ADD] \"A newline separated list of Kafka properties. (e.g.: \\\\\"ssl.keystore.location=/etc/graylog/server/kafka.keystore.jks\\\\\").\",\\n[ADD] ConfigurationField.Optional.OPTIONAL,\\n[ADD] 110,\\n[ADD] TextField.Attribute.TEXTAREA\\n[ADD] ));\\n \\n             return cr;\\n         }\\n     }\\n }\\n[ADD] ',\n",
              " ' import org.testng.Assert;\\n import org.testng.annotations.Test;\\n \\n[ADD] import com.google.common.collect.Maps;\\n[ADD] \\n \\n public class AvroUtilsTest {\\n   private static final String AVRO_DIR = \"gobblin-utility/src/test/resources/avroDirParent/\";',\n",
              " '       // unique value\\n       .thenReturn( expectedResult );\\n \\n[ADD] when( databaseDialog.getDatabaseMeta() ).thenReturn( createDefaultDatabase() );\\n \\n     BaseStepDialog dialog = mock( BaseStepDialog.class );\\n     dialog.databaseDialog = databaseDialog;',\n",
              " '                     .executorsCreatedBy(new ReflectiveOperationExecutorFactory<>(actingClass, method));\\n \\n             declareOperationParameters(method, operation);\\n[DEL] \\n             calculateExtendedTypes(actingClass, method, operation);\\n[ADD] declareOperationCapabilities(context, operation, method);\\n         }\\n     }\\n ',\n",
              " ' import io.opentelemetry.context.Context;\\n import io.opentelemetry.instrumentation.api.db.SqlStatementInfo;\\n import io.opentelemetry.instrumentation.api.db.SqlStatementSanitizer;\\n[DEL] import io.opentelemetry.javaagent.instrumentation.api.CallDepthThreadLocalMap;\\n[ADD] import io.opentelemetry.javaagent.instrumentation.api.CallDepth;\\n import io.opentelemetry.javaagent.instrumentation.api.ContextStore;\\n import java.util.Arrays;\\n import java.util.HashSet;',\n",
              " ' \\t\\tIO.close(domain);\\n \\t}\\n \\n[ADD] @Test\\n[ADD] public void testCaching() throws Exception {\\n[ADD] Map<String, String> config = new HashMap<>();\\n[ADD] config.put(\"index\", null);\\n[ADD] config.put(\"source\", \"commons-cli:commons-cli:1.4-SNAPSHOT\");\\n[ADD] \\n[ADD] config(config);\\n[ADD] \\n[ADD] File f = repo.get(\"commons-cli:commons-cli\", new Version(\"1.4.0.SNAPSHOT\"), null);\\n[ADD] System.out.println(\"got a file \" + f + \" \" + f.isFile());\\n[ADD] assertThat(f).isFile();\\n[ADD] \\n[ADD] }\\n[ADD] \\n \\t@Test\\n \\tpublic void testAutomaticSources() throws Exception {\\n \\t\\tconfig(null);',\n",
              " ' \\n     return sb.toString();\\n   }\\n[ADD] \\n[ADD] private static Calendar getCalendarUTCInstance() {\\n[ADD] return Calendar.getInstance(TimeZone.getTimeZone(\"UTC\"));\\n[ADD] }\\n }',\n",
              " '     }\\n   }\\n \\n[ADD] /**\\n[ADD] * Load all {@link FlowSpec}s from {@link FlowCatalog} as one of the initialization step,\\n[ADD] * and make schedulers be aware of that.\\n[ADD] *\\n[ADD] */\\n[ADD] private void scheduleSpecsFromCatalog() {\\n[ADD] Iterator<URI> specUris = null;\\n[ADD] long startTime = System.currentTimeMillis();\\n[ADD] \\n[ADD] try {\\n[ADD] specUris = this.flowCatalog.get().getSpecURIs();\\n[ADD] } catch (SpecSerDeException ssde) {\\n[ADD] throw new RuntimeException(\"Failed to get the iterator of all Spec URIS\", ssde);\\n[ADD] }\\n[ADD] \\n[ADD] \\n[ADD] try {\\n[ADD] while (specUris.hasNext()) {\\n[ADD] Spec spec = null;\\n[ADD] try {\\n[ADD] spec = this.flowCatalog.get().getSpec(specUris.next());\\n[ADD] } catch (SpecNotFoundException snfe) {\\n[ADD] _log.error(String.format(\"The URI %s discovered in SpecStore is missing in FlowCatlog\"\\n[ADD] + \", suspecting current modification on SpecStore\", specUris.next()), snfe);\\n[ADD] }\\n[ADD] \\n[ADD] //Disable FLOW_RUN_IMMEDIATELY on service startup or leadership change\\n[ADD] if (spec instanceof FlowSpec) {\\n[ADD] Spec modifiedSpec = disableFlowRunImmediatelyOnStart((FlowSpec) spec);\\n[ADD] onAddSpec(modifiedSpec);\\n[ADD] } else {\\n[ADD] onAddSpec(spec);\\n[ADD] }\\n[ADD] }\\n[ADD] } finally {\\n[ADD] flowSpecInitFinished.countDown();\\n[ADD] this.flowCatalog.get().getMetrics().updateGetSpecTime(startTime);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n   @VisibleForTesting\\n   protected static Spec disableFlowRunImmediatelyOnStart(FlowSpec spec) {\\n     Properties properties = spec.getConfigAsProperties();',\n",
              " '         DataSourceWriteOptions.DEFAULT_HIVE_USE_JDBC_OPT_VAL()));\\n     hiveSyncConfig.autoCreateDatabase = Boolean.valueOf(props.getString(DataSourceWriteOptions.HIVE_AUTO_CREATE_DATABASE_OPT_KEY(),\\n         DataSourceWriteOptions.DEFAULT_HIVE_AUTO_CREATE_DATABASE_OPT_KEY()));\\n[ADD] hiveSyncConfig.ignoreConnectException = Boolean.valueOf(props.getString(DataSourceWriteOptions.HIVE_IGNORE_CONNECT_EXCEPTION_OPT_KEY(),\\n[ADD] DataSourceWriteOptions.DEFAULT_HIVE_IGNORE_CONNECT_EXCEPTION_OPT_KEY()));\\n     hiveSyncConfig.skipROSuffix = Boolean.valueOf(props.getString(DataSourceWriteOptions.HIVE_SKIP_RO_SUFFIX(),\\n         DataSourceWriteOptions.DEFAULT_HIVE_SKIP_RO_SUFFIX_VAL()));\\n     hiveSyncConfig.supportTimestamp = Boolean.valueOf(props.getString(DataSourceWriteOptions.HIVE_SUPPORT_TIMESTAMP(),',\n",
              " \"         mCurrentlyDrawing = false;\\n         PathMeasure pm = new PathMeasure(mPath, false);\\n         mPath.lineTo(mX, mY);\\n[DEL] if (pm.getLength() > 0) {\\n[DEL] mCanvas.drawPath(mPath, mPaint);\\n[DEL] mUndo.add(mPath);\\n[DEL] } else {\\n[DEL] mCanvas.drawPoint(mX, mY, mPaint);\\n[DEL] mUndo.add(mX, mY);\\n[DEL] }\\n[ADD] Paint paint = new Paint(mPaint);\\n[ADD] WhiteboardAction action = pm.getLength() > 0 ? new DrawPath(new Path(mPath), paint) : new DrawPoint(mX, mY, paint);\\n[ADD] action.apply(mCanvas);\\n[ADD] mUndo.add(action);\\n         mUndoModeActive = true;\\n         // kill the path so we don't double draw\\n         mPath.reset();\",\n",
              " ' import gobblin.fork.CopyNotSupportedException;\\n import gobblin.fork.Copyable;\\n import gobblin.fork.ForkOperator;\\n[ADD] import gobblin.instrumented.extractor.InstrumentedExtractorDecorator;\\n[ADD] import gobblin.metrics.MetricContext;\\n import gobblin.qualitychecker.row.RowLevelPolicyCheckResults;\\n import gobblin.qualitychecker.row.RowLevelPolicyChecker;\\n[ADD] import gobblin.runtime.util.TaskMetrics;\\n import gobblin.source.extractor.Extractor;\\n \\n ',\n",
              " '     }\\n   }\\n \\n[ADD] /**\\n[ADD] * Checks the source data are written as expected.\\n[ADD] *\\n[ADD] * <p>Note: Replace it with the Flink reader when it is supported.\\n[ADD] *\\n[ADD] * @param basePath   The file base to check, should be a directory\\n[ADD] * @param expected   The expected results mapping, the key should be the partition path\\n[ADD] */\\n[ADD] public static void checkWrittenFullData(\\n[ADD] File basePath,\\n[ADD] Map<String, List<String>> expected) throws IOException {\\n[ADD] \\n[ADD] // 1. init flink table\\n[ADD] HoodieTableMetaClient metaClient = HoodieTestUtils.init(basePath.getAbsolutePath());\\n[ADD] HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath.getAbsolutePath()).build();\\n[ADD] FlinkTaskContextSupplier supplier = new FlinkTaskContextSupplier(null);\\n[ADD] HoodieFlinkEngineContext context = new HoodieFlinkEngineContext(supplier);\\n[ADD] HoodieFlinkTable table = HoodieFlinkTable.create(config, context, metaClient);\\n[ADD] \\n[ADD] // 2. check each partition data\\n[ADD] expected.forEach((partition, partitionDataSet) -> {\\n[ADD] \\n[ADD] List<String> readBuffer = new ArrayList<>();\\n[ADD] \\n[ADD] table.getFileSystemView().getAllFileGroups(partition)\\n[ADD] .forEach(v -> v.getLatestDataFile().ifPresent(baseFile -> {\\n[ADD] String path = baseFile.getPath();\\n[ADD] try {\\n[ADD] ParquetReader<GenericRecord> reader = AvroParquetReader.<GenericRecord>builder(new Path(path)).build();\\n[ADD] GenericRecord nextRecord = reader.read();\\n[ADD] while (nextRecord != null) {\\n[ADD] readBuffer.add(filterOutVariables(nextRecord));\\n[ADD] nextRecord = reader.read();\\n[ADD] }\\n[ADD] } catch (IOException e) {\\n[ADD] throw new RuntimeException(e);\\n[ADD] }\\n[ADD] }));\\n[ADD] \\n[ADD] assertTrue(partitionDataSet.size() == readBuffer.size() && partitionDataSet.containsAll(readBuffer));\\n[ADD] \\n[ADD] });\\n[ADD] \\n[ADD] }\\n[ADD] \\n   /**\\n    * Filter out the variables like file name.\\n    */',\n",
              " '         .allowingTopLevelDefinition()\\n         .withStereotype(FLOW);\\n \\n[ADD] flow.onDefaultParameterGroup().withRequiredParameter(\"name\").asComponentId()\\n[ADD] .ofType(BaseTypeBuilder.create(JAVA).stringType().build());\\n     flow.onDefaultParameterGroup().withOptionalParameter(\"initialState\").defaultingTo(\"started\")\\n         .ofType(BaseTypeBuilder.create(JAVA).stringType().enumOf(\"started\", \"stopped\").build());\\n     flow.onDefaultParameterGroup().withOptionalParameter(\"maxConcurrency\")',\n",
              " '     options.getGcsUtil().copy(toFilenames(srcResourceIds), toFilenames(destResourceIds));\\n   }\\n \\n[ADD] /**\\n[ADD] * Expands a pattern into {@link MatchResult}.\\n[ADD] *\\n[ADD] * <p>{@code gcsPattern} is expected to contain globs.\\n[ADD] */\\n[ADD] @VisibleForTesting\\n[ADD] MatchResult expand(GcsPath gcsPattern) throws IOException {\\n[ADD] String prefix = GcsUtil.getGlobPrefix(gcsPattern.getObject());\\n[ADD] Pattern p = Pattern.compile(GcsUtil.globToRegexp(gcsPattern.getObject()));\\n[ADD] \\n[ADD] LOG.debug(\"matching files in bucket {}, prefix {} against pattern {}\", gcsPattern.getBucket(),\\n[ADD] prefix, p.toString());\\n[ADD] \\n[ADD] String pageToken = null;\\n[ADD] List<Metadata> results = new LinkedList<>();\\n[ADD] do {\\n[ADD] Objects objects = options.getGcsUtil().listObjects(gcsPattern.getBucket(), prefix, pageToken);\\n[ADD] if (objects.getItems() == null) {\\n[ADD] break;\\n[ADD] }\\n[ADD] \\n[ADD] // Filter objects based on the regex.\\n[ADD] for (StorageObject o : objects.getItems()) {\\n[ADD] String name = o.getName();\\n[ADD] // Skip directories, which end with a slash.\\n[ADD] if (p.matcher(name).matches() && !name.endsWith(\"/\")) {\\n[ADD] LOG.debug(\"Matched object: {}\", name);\\n[ADD] results.add(toMetadata(o));\\n[ADD] }\\n[ADD] }\\n[ADD] pageToken = objects.getNextPageToken();\\n[ADD] } while (pageToken != null);\\n[ADD] return MatchResult.create(Status.OK, results.toArray(new Metadata[results.size()]));\\n[ADD] }\\n[ADD] \\n   private List<String> toFilenames(Collection<GcsResourceId> resources) {\\n     return FluentIterable.from(resources)\\n         .transform(',\n",
              " ' \\n package org.apache.hudi.common.config;\\n \\n[DEL] import org.apache.hadoop.conf.Configuration;\\n[DEL] import org.apache.hadoop.fs.FileSystem;\\n[DEL] import org.apache.hadoop.fs.Path;\\n import org.apache.hudi.common.fs.FSUtils;\\n import org.apache.hudi.common.util.Option;\\n import org.apache.hudi.common.util.StringUtils;\\n import org.apache.hudi.common.util.ValidationUtils;\\n[ADD] import org.apache.hudi.exception.HoodieIOException;\\n[ADD] \\n[ADD] import org.apache.hadoop.conf.Configuration;\\n[ADD] import org.apache.hadoop.fs.FileSystem;\\n[ADD] import org.apache.hadoop.fs.Path;\\n import org.apache.log4j.LogManager;\\n import org.apache.log4j.Logger;\\n \\n import javax.annotation.Nonnull;\\n import javax.annotation.Nullable;\\n[ADD] \\n import java.io.BufferedReader;\\n import java.io.File;\\n import java.io.IOException;',\n",
              " ' \\n import static io.opentelemetry.javaagent.instrumentation.jedis.v3_0.JedisClientTracer.TRACER;\\n import static java.util.Collections.singletonMap;\\n[ADD] import static net.bytebuddy.matcher.ElementMatchers.is;\\n import static net.bytebuddy.matcher.ElementMatchers.isMethod;\\n import static net.bytebuddy.matcher.ElementMatchers.named;\\n import static net.bytebuddy.matcher.ElementMatchers.takesArgument;\\n[ADD] import static net.bytebuddy.matcher.ElementMatchers.takesArguments;\\n \\n import com.google.auto.service.AutoService;\\n import io.opentelemetry.context.Scope;\\n[DEL] import io.opentelemetry.javaagent.instrumentation.api.CallDepthThreadLocalMap;\\n[ADD] import io.opentelemetry.javaagent.instrumentation.jedis.v3_0.JedisClientTracer.CommandWithArgs;\\n import io.opentelemetry.javaagent.tooling.Instrumenter;\\n import io.opentelemetry.trace.Span;\\n import java.util.Map;',\n",
              " ' import java.util.Collection;\\n import java.util.Collections;\\n import java.util.HashMap;\\n[ADD] import java.util.List;\\n import java.util.Map;\\n import java.util.Map.Entry;\\n ',\n",
              " ' \\t\\t\\t\\t}\\n \\t\\t\\t\\tsaveCursor = walkState.bp.subOffset(saveOffset);\\n \\n[DEL] if (TRBuildFlags.host_POWER) { /*defined(TR_SHRINK_WRAP)*/\\n[ADD] if (TRBuildFlags.host_POWER) {\\n \\t\\t\\t\\t\\tmapCursor += lowestRegister.intValue(); /* access gpr15 in the vm register state */\\n \\t\\t\\t\\t\\tU8 i = new U8(lowestRegister.add(1));\\n \\t\\t\\t\\t\\tdo ',\n",
              " '                 *heap = nullptr;\\n             }\\n \\n[DEL] Var* localModuleFunctions = moduleMemoryPtr + wasmModule->funcOffset;\\n[DEL] \\n[DEL] FrameDisplay * frameDisplay = RecyclerNewPlus(GetRecycler(), sizeof(void*), FrameDisplay, 1);\\n[DEL] frameDisplay->SetItem(0, moduleMemoryPtr);\\n[DEL] \\n[DEL] // TODO, refactor this function into smaller functions\\n[DEL] for (uint i = 0; i < wasmModule->functions->Count(); ++i)\\n[DEL] {\\n[DEL] AsmJsScriptFunction * funcObj = javascriptLibrary->CreateAsmJsScriptFunction(functionArray[i]->body);\\n[DEL] funcObj->GetDynamicType()->SetEntryPoint(AsmJsExternalEntryPoint);\\n[DEL] funcObj->SetModuleMemory(moduleMemoryPtr);\\n[DEL] FunctionEntryPointInfo * entypointInfo = (FunctionEntryPointInfo*)funcObj->GetEntryPointInfo();\\n[DEL] entypointInfo->SetIsAsmJSFunction(true);\\n[DEL] entypointInfo->address = AsmJsDefaultEntryThunk;\\n[DEL] entypointInfo->SetModuleAddress((uintptr_t)moduleMemoryPtr);\\n[DEL] funcObj->SetEnvironment(frameDisplay);\\n[DEL] localModuleFunctions[i] = funcObj;\\n[DEL] }\\n[DEL] \\n             for (uint32 iExport = 0; iExport < wasmModule->info->GetExportCount(); ++iExport)\\n             {\\n                 Wasm::WasmExport* funcExport = wasmModule->info->GetFunctionExport(iExport);\\n[DEL] if (funcExport)\\n[ADD] if (funcExport && funcExport->nameLength > 0)\\n                 {\\n                     PropertyRecord const * propertyRecord = nullptr;\\n                     GetOrAddPropertyRecord(funcExport->name, funcExport->nameLength, &propertyRecord);\\n                     Var funcObj;\\n                     // todo:: This should not happen, we need to add validation that the `function_bodies` section is present\\n[DEL] if (funcExport->funcIndex < wasmModule->functions->Count())\\n[ADD] if (funcExport->funcIndex < wasmModule->funcCount)\\n                     {\\n                         funcObj = localModuleFunctions[funcExport->funcIndex];\\n                     }',\n",
              " '                 SearchCriteria<HostVO> sc = _hostDao.createSearchCriteria();\\n                 sc.addAnd(\"status\", SearchCriteria.Op.EQ, Status.Up.toString());\\n                 sc.addAnd(\"resourceState\", SearchCriteria.Op.NIN, ResourceState.Maintenance, ResourceState.PrepareForMaintenance, ResourceState.ErrorInMaintenance);\\n[DEL] sc.addAnd(\"type\", SearchCriteria.Op.NEQ, Host.Type.Storage.toString());\\n[DEL] sc.addAnd(\"type\", SearchCriteria.Op.NEQ, Host.Type.ConsoleProxy.toString());\\n[DEL] sc.addAnd(\"type\", SearchCriteria.Op.NEQ, Host.Type.SecondaryStorage.toString());\\n[DEL] sc.addAnd(\"type\", SearchCriteria.Op.NEQ, Host.Type.LocalSecondaryStorage.toString());\\n[DEL] sc.addAnd(\"type\", SearchCriteria.Op.NEQ, Host.Type.TrafficMonitor.toString());\\n[DEL] sc.addAnd(\"type\", SearchCriteria.Op.NEQ, Host.Type.SecondaryStorageVM.toString());\\n[DEL] sc.addAnd(\"type\", SearchCriteria.Op.NEQ, Host.Type.ExternalFirewall.toString());\\n[DEL] sc.addAnd(\"type\", SearchCriteria.Op.NEQ, Host.Type.ExternalLoadBalancer.toString());\\n[DEL] sc.addAnd(\"type\", SearchCriteria.Op.NEQ, Host.Type.NetScalerControlCenter.toString());\\n[DEL] sc.addAnd(\"type\", SearchCriteria.Op.NEQ, Host.Type.L2Networking.toString());\\n[DEL] sc.addAnd(\"type\", SearchCriteria.Op.NEQ, Host.Type.BaremetalDhcp.toString());\\n[DEL] sc.addAnd(\"type\", SearchCriteria.Op.NEQ, Host.Type.BaremetalPxe.toString());\\n[ADD] sc.addAnd(\"type\", SearchCriteria.Op.EQ, Host.Type.Routing.toString());\\n[ADD] \\n                 ConcurrentHashMap<Long, HostStats> hostStats = new ConcurrentHashMap<Long, HostStats>();\\n[ADD] Map<Object, Object> metrics = new HashMap<>();\\n                 List<HostVO> hosts = _hostDao.search(sc, null);\\n[ADD] \\n                 for (HostVO host : hosts) {\\n[DEL] HostStatsEntry stats = (HostStatsEntry)_resourceMgr.getHostStatistics(host.getId());\\n[DEL] if (stats != null) {\\n[DEL] hostStats.put(host.getId(), stats);\\n[ADD] HostStatsEntry hostStatsEntry = (HostStatsEntry)_resourceMgr.getHostStatistics(host.getId());\\n[ADD] if (hostStatsEntry != null) {\\n[ADD] hostStatsEntry.setHostVo(host);\\n[ADD] metrics.put(hostStatsEntry.getHostId(), hostStatsEntry);\\n[ADD] _hostStats.put(host.getId(), hostStatsEntry);\\n                     } else {\\n                         s_logger.warn(\"Received invalid host stats for host: \" + host.getId());\\n                     }\\n                 }\\n[DEL] _hostStats = hostStats;\\n[DEL] // Get a subset of hosts with GPU support from the list of \"hosts\"\\n[DEL] List<HostVO> gpuEnabledHosts = new ArrayList<HostVO>();\\n[DEL] if (hostIds != null) {\\n[DEL] for (HostVO host : hosts) {\\n[DEL] if (hostIds.contains(host.getId())) {\\n[DEL] gpuEnabledHosts.add(host);\\n[DEL] }\\n[DEL] }\\n[DEL] } else {\\n[DEL] // Check for all the hosts managed by CloudStack.\\n[DEL] gpuEnabledHosts = hosts;\\n[DEL] }\\n[DEL] for (HostVO host : gpuEnabledHosts) {\\n[DEL] HashMap<String, HashMap<String, VgpuTypesInfo>> groupDetails = _resourceMgr.getGPUStatistics(host);\\n[DEL] if (groupDetails != null) {\\n[DEL] _resourceMgr.updateGPUDetails(host.getId(), groupDetails);\\n[DEL] }\\n[ADD] \\n[ADD] if (externalStatsType == ExternalStatsProtocol.INFLUXDB) {\\n[ADD] sendMetricsToInfluxdb(metrics);\\n                 }\\n[DEL] hostIds = _hostGpuGroupsDao.listHostIds();\\n[ADD] \\n[ADD] updateGpuEnabledHostsDetails(hosts);\\n             } catch (Throwable t) {\\n                 s_logger.error(\"Error trying to retrieve host stats\", t);\\n             }\\n         }\\n[ADD] \\n[ADD] /**\\n[ADD] * Updates GPU details on hosts supporting GPU.\\n[ADD] */\\n[ADD] private void updateGpuEnabledHostsDetails(List<HostVO> hosts) {\\n[ADD] // Get a subset of hosts with GPU support from the list of \"hosts\"\\n[ADD] List<HostVO> gpuEnabledHosts = new ArrayList<HostVO>();\\n[ADD] if (hostIds != null) {\\n[ADD] for (HostVO host : hosts) {\\n[ADD] if (hostIds.contains(host.getId())) {\\n[ADD] gpuEnabledHosts.add(host);\\n[ADD] }\\n[ADD] }\\n[ADD] } else {\\n[ADD] // Check for all the hosts managed by CloudStack.\\n[ADD] gpuEnabledHosts = hosts;\\n[ADD] }\\n[ADD] for (HostVO host : gpuEnabledHosts) {\\n[ADD] HashMap<String, HashMap<String, VgpuTypesInfo>> groupDetails = _resourceMgr.getGPUStatistics(host);\\n[ADD] if (groupDetails != null) {\\n[ADD] _resourceMgr.updateGPUDetails(host.getId(), groupDetails);\\n[ADD] }\\n[ADD] }\\n[ADD] hostIds = _hostGpuGroupsDao.listHostIds();\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] protected Point creteInfluxDbPoint(Object metricsObject) {\\n[ADD] return createInfluxDbPointForHostMetrics(metricsObject);\\n[ADD] }\\n     }\\n \\n[DEL] class VmStatsCollector extends ManagedContextRunnable {\\n[ADD] class VmStatsCollector extends AbstractStatsCollector {\\n         @Override\\n         protected void runInContext() {\\n             try {',\n",
              " ' import org.mule.runtime.core.api.construct.Pipeline;\\n import org.mule.runtime.core.api.context.notification.FlowCallStack;\\n import org.mule.runtime.core.api.message.GroupCorrelation;\\n[DEL] import org.mule.runtime.core.api.security.SecurityContext;\\n import org.mule.runtime.core.api.session.DefaultMuleSession;\\n import org.mule.runtime.core.api.store.DeserializationPostInitialisable;\\n import org.mule.runtime.core.api.transformer.TransformerException;',\n",
              " ' \\n     @Override\\n     public ProfilingDataConsumerDiscoveryStrategy getDiscoveryStrategy() {\\n[DEL] return (new ProfilingDataConsumerDiscoveryStrategy() {\\n[DEL] \\n[DEL] @Override\\n[DEL] public Set<ProfilingDataConsumer<?>> discover() {\\n[DEL] return singleton(profilingDataConsumer);\\n[DEL] }\\n[DEL] });\\n[ADD] return (() -> singleton(profilingDataConsumer));\\n     }\\n   };\\n ',\n",
              " '         }\\n \\n         ctx.addVariable(IS_XA, isXa);\\n[DEL] ctx.bindConnection(connection);\\n[ADD] txHandle = ctx.bindConnection(connection);\\n         sourceCallback.handle(Result.<TestTransactionalConnection, Object>builder().output(connection).build(), ctx);\\n       } catch (ConnectionException e) {\\n         sourceCallback.onConnectionException(e);\\n       } catch (Exception e) {\\n         throw new RuntimeException(e);\\n[ADD] } finally {\\n[ADD] try {\\n[ADD] if (txHandle != null) {\\n[ADD] txHandle.commit();\\n[ADD] }\\n[ADD] } catch (TransactionException e) {\\n[ADD] throw new RuntimeException(e);\\n[ADD] }\\n       }\\n     });\\n   }',\n",
              " '  */\\n package org.mule.runtime.core;\\n \\n[ADD] import static java.util.Collections.singletonList;\\n import static org.junit.Assert.assertEquals;\\n import static org.junit.Assert.assertNotNull;\\n import static org.junit.Assert.assertTrue;\\n[ADD] import static org.mockito.Mockito.mock;\\n[ADD] \\n import org.mule.runtime.core.api.processor.Processor;\\n import org.mule.runtime.core.internal.exception.AbstractExceptionListener;\\n import org.mule.runtime.core.internal.exception.OnErrorPropagateHandler;',\n",
              " '     if (_taskAttempt != null) {\\n       try {\\n         _logger.info(\"Task cancelled: Shutdown starting for tasks with jobId: {}\", _jobId);\\n[DEL] _taskAttempt.shutdownTasks();\\n[ADD] _taskAttempt.cancel();\\n         _logger.info(\"Task cancelled: Shutdown complete for tasks with jobId: {}\", _jobId);\\n       } catch (InterruptedException e) {\\n         throw new RuntimeException(\"Interrupted while shutting down task with jobId: \" + _jobId, e);',\n",
              " ' import com.google.common.collect.Lists;\\n import org.objectweb.asm.Opcodes;\\n import org.objectweb.asm.Type;\\n[ADD] import org.objectweb.asm.util.Printer;\\n import org.sonar.java.bytecode.cfg.BytecodeCFGBuilder;\\n[ADD] import org.sonar.java.bytecode.cfg.Instruction;\\n import org.sonar.java.bytecode.loader.SquidClassLoader;\\n import org.sonar.java.se.ExplodedGraph;\\n import org.sonar.java.se.Pair;',\n",
              " ' \\t\\tif (this.handler instanceof MessageProducer) {\\n \\t\\t\\treturn ((MessageProducer) this.handler).getOutputChannel();\\n \\t\\t}\\n[ADD] else if (this.handler instanceof AbstractMessageRouter) {\\n[ADD] return ((AbstractMessageRouter) this.handler).getDefaultOutputChannel();\\n[ADD] }\\n \\t\\telse {\\n \\t\\t\\treturn null;\\n \\t\\t}',\n",
              " ' \\t\\t\\treturn true;\\n \\t\\t}\\n \\n[DEL] private boolean containsSequenceNumber(Collection<Message<?>> messages, Integer messageSequenceNumber) {\\n[ADD] protected boolean containsSequenceNumber(Collection<Message<?>> messages, Integer messageSequenceNumber) {\\n \\t\\t\\tfor (Message<?> member : messages) {\\n \\t\\t\\t\\tInteger memberSequenceNumber = new IntegrationMessageHeaderAccessor(member).getSequenceNumber();\\n \\t\\t\\t\\tif (messageSequenceNumber.equals(memberSequenceNumber)) {',\n",
              " '                 }\\n \\n                 final VirtualMachineProfileImpl vmProfile = new VirtualMachineProfileImpl(vm, template, offering, owner, params);\\n[ADD] s_logger.info(\" Uefi params \" + \"UefiFlag: \" + params.get(VirtualMachineProfile.Param.UefiFlag)\\n[ADD] + \" Boot Type: \" + params.get(VirtualMachineProfile.Param.BootType)\\n[ADD] + \" Boot Mode: \" + params.get(VirtualMachineProfile.Param.BootMode)\\n[ADD] );\\n                 DeployDestination dest = null;\\n                 try {\\n                     dest = _dpMgr.planDeployment(vmProfile, plan, avoids, planner);',\n",
              " \"   this.$pending = undefined; // keep pending keys here\\n   this.$name = $interpolate($attr.name || '', false)($scope);\\n   this.$$parentForm = nullFormCtrl;\\n[DEL] this.$options = $modelOptions;\\n[ADD] this.$options = $defaultModelOptions;\\n \\n   this.$$parsedNgModel = $parse($attr.ngModel);\\n   this.$$parsedNgModelAssign = this.$$parsedNgModel.assign;\",\n",
              " ' \\t\\t}\\n \\t\\tif (metadata.getFailures().get() >= this.threshold &&\\n \\t\\t\\t\\tSystem.currentTimeMillis() - metadata.getLastFailure() < this.halfOpenAfter) {\\n[DEL] throw new CircuitBreakerOpenException(\"Circuit Breaker is Open for \" + target);\\n[ADD] throw new CircuitBreakerOpenException(message, \"Circuit Breaker is Open for \" + target);\\n \\t\\t}\\n \\t\\ttry {\\n \\t\\t\\tObject result = callback.execute();',\n",
              " '       ClassLoader agentClassLoader, ClassLoader userClassLoader, boolean assertPass)\\n       throws Exception {\\n     // muzzle validate all instrumenters\\n[DEL] for (Object instrumenter : loadAllInstrumenters(agentClassLoader)) {\\n[DEL] if (!(instrumenter instanceof Instrumenter.Default\\n[DEL] || instrumenter instanceof InstrumentationModule)) {\\n[DEL] // only default Instrumenters and modules use muzzle. Skip custom instrumenters.\\n[DEL] continue;\\n[DEL] }\\n[ADD] for (InstrumentationModule instrumentationModule :\\n[ADD] ServiceLoader.load(InstrumentationModule.class, agentClassLoader)) {\\n       Method getMuzzleReferenceMatcher = null;\\n       try {\\n         getMuzzleReferenceMatcher =\\n[DEL] instrumenter.getClass().getDeclaredMethod(\"getMuzzleReferenceMatcher\");\\n[ADD] InstrumentationModule.class.getDeclaredMethod(\"getMuzzleReferenceMatcher\");\\n         getMuzzleReferenceMatcher.setAccessible(true);\\n[DEL] ReferenceMatcher muzzle = (ReferenceMatcher) getMuzzleReferenceMatcher.invoke(instrumenter);\\n[ADD] ReferenceMatcher muzzle =\\n[ADD] (ReferenceMatcher) getMuzzleReferenceMatcher.invoke(instrumentationModule);\\n         List<Mismatch> mismatches = muzzle.getMismatchedReferenceSources(userClassLoader);\\n \\n[DEL] Method getClassLoaderMatcher = instrumenter.getClass().getMethod(\"classLoaderMatcher\");\\n[DEL] getClassLoaderMatcher.setAccessible(true);\\n         boolean classLoaderMatch =\\n[DEL] ((ElementMatcher<ClassLoader>) getClassLoaderMatcher.invoke(instrumenter))\\n[DEL] .matches(userClassLoader);\\n[ADD] instrumentationModule.classLoaderMatcher().matches(userClassLoader);\\n         boolean passed = mismatches.isEmpty() && classLoaderMatch;\\n \\n         if (passed && !assertPass) {\\n           System.err.println(\\n               \"MUZZLE PASSED \"\\n[DEL] + instrumenter.getClass().getSimpleName()\\n[ADD] + instrumentationModule.getClass().getSimpleName()\\n                   + \" BUT FAILURE WAS EXPECTED\");\\n           throw new RuntimeException(\"Instrumentation unexpectedly passed Muzzle validation\");\\n         } else if (!passed && assertPass) {\\n           System.err.println(\\n[DEL] \"FAILED MUZZLE VALIDATION: \" + instrumenter.getClass().getName() + \" mismatches:\");\\n[ADD] \"FAILED MUZZLE VALIDATION: \"\\n[ADD] + instrumentationModule.getClass().getName()\\n[ADD] + \" mismatches:\");\\n \\n           if (!classLoaderMatch) {\\n             System.err.println(\"-- classloader mismatch\");',\n",
              " '       //Add tracking config to JobSpec.\\n       addTrackingEventConfig(jobSpec, sysConfig);\\n \\n[DEL] addAdditionalConfig(jobSpec, sysConfig);\\n[ADD] addAdditionalConfig(jobSpec, sysConfig, specExecutorConfig);\\n \\n       // Add dynamic config to jobSpec if a dynamic config generator is specified in sysConfig\\n       DynamicConfigGenerator dynamicConfigGenerator = DynamicConfigGeneratorFactory.createDynamicConfigGenerator(sysConfig);',\n",
              " ' \\n package org.springframework.integration.file.dsl;\\n \\n[DEL] import java.nio.charset.Charset;\\n[DEL] import java.util.Collections;\\n[DEL] import java.util.Map;\\n[DEL] import java.util.function.Function;\\n[DEL] \\n import org.springframework.expression.common.LiteralExpression;\\n import org.springframework.integration.dsl.ComponentsRegistration;\\n import org.springframework.integration.dsl.MessageHandlerSpec;',\n",
              " '                                            e);\\n       }\\n \\n[DEL] mulePluginModel.getClassLoaderModelLoaderDescriptor().ifPresent(\\n[DEL] classLoaderModelDescriptor -> {\\n[DEL] exportPackages\\n[DEL] .addAll((List<String>) classLoaderModelDescriptor\\n[DEL] .getAttributes().getOrDefault(EXPORTED_PACKAGES,\\n[DEL] new ArrayList<>()));\\n[DEL] exportResources\\n[DEL] .addAll((List<String>) classLoaderModelDescriptor\\n[DEL] .getAttributes().getOrDefault(EXPORTED_RESOURCES,\\n[DEL] new ArrayList<>()));\\n[ADD] MuleArtifactLoaderDescriptor classLoaderModelDescriptor = mulePluginModel.getClassLoaderModelLoaderDescriptor();\\n[ADD] exportPackages\\n[ADD] .addAll((List<String>) classLoaderModelDescriptor\\n[ADD] .getAttributes().getOrDefault(EXPORTED_PACKAGES,\\n[ADD] new ArrayList<>()));\\n[ADD] exportResources\\n[ADD] .addAll((List<String>) classLoaderModelDescriptor\\n[ADD] .getAttributes().getOrDefault(EXPORTED_RESOURCES,\\n[ADD] new ArrayList<>()));\\n \\n[DEL] privilegedExportedPackages\\n[DEL] .addAll((List<String>) classLoaderModelDescriptor\\n[DEL] .getAttributes()\\n[DEL] .getOrDefault(PRIVILEGED_EXPORTED_PACKAGES,\\n[DEL] new ArrayList<>()));\\n[ADD] privilegedExportedPackages\\n[ADD] .addAll((List<String>) classLoaderModelDescriptor\\n[ADD] .getAttributes()\\n[ADD] .getOrDefault(PRIVILEGED_EXPORTED_PACKAGES,\\n[ADD] new ArrayList<>()));\\n \\n[DEL] privilegedArtifacts\\n[DEL] .addAll((List<String>) classLoaderModelDescriptor\\n[DEL] .getAttributes()\\n[DEL] .getOrDefault(PRIVILEGED_ARTIFACTS_IDS,\\n[DEL] new ArrayList<>()));\\n[DEL] });\\n[ADD] privilegedArtifacts\\n[ADD] .addAll((List<String>) classLoaderModelDescriptor\\n[ADD] .getAttributes()\\n[ADD] .getOrDefault(PRIVILEGED_ARTIFACTS_IDS,\\n[ADD] new ArrayList<>()));\\n \\n       return new PluginUrlClassification(pluginUrlClassification.getName(), pluginUrlClassification.getUrls(),\\n                                          pluginUrlClassification.getExportClasses(),',\n",
              " \"     return artifactClassLoader;\\n   }\\n \\n[ADD] /**\\n[ADD] * Goes over the elements in the {@code pluginDescriptors} collection looking if it hasn't been resolved yet. If it hasn't\\n[ADD] * then it looks it up through the {@link DependenciesProvider} and then creates an {@link ArtifactPluginDescriptor}.\\n[ADD] *\\n[ADD] * @param pluginDescriptors plugins to validate.\\n[ADD] * @param visited plugins that are already resolved (by either the container or application initially, or by the resolver).\\n[ADD] * @return the plugins that were obtained initially plus all the ones that were found.\\n[ADD] * @throws DeploymentException if any dependency wasn't found properly\\n[ADD] */\\n[ADD] private List<ArtifactPluginDescriptor> getArtifactsWithDependencies(List<ArtifactPluginDescriptor> pluginDescriptors,\\n[ADD] Set<String> visited) {\\n[ADD] if (!pluginDescriptors.isEmpty()) {\\n[ADD] List<ArtifactPluginDescriptor> foundDependencies = new ArrayList<>();\\n[ADD] pluginDescriptors.stream()\\n[ADD] .filter(pluginDescriptor -> !pluginDescriptor.getClassLoaderModel().getDependencies().isEmpty())\\n[ADD] .forEach(pluginDescriptor -> pluginDescriptor.getClassLoaderModel().getDependencies()\\n[ADD] .forEach(dependency -> {\\n[ADD] if (!visited.contains(dependency)) {\\n[ADD] File mulePluginLocation = dependenciesProvider.resolve(dependency);\\n[ADD] foundDependencies.add(artifactDescriptorFactory.create(new File(mulePluginLocation.toURI())));\\n[ADD] visited.add(dependency);\\n[ADD] }\\n[ADD] }));\\n[ADD] \\n[ADD] pluginDescriptors.addAll(getArtifactsWithDependencies(foundDependencies, visited));\\n[ADD] }\\n[ADD] return pluginDescriptors;\\n[ADD] }\\n[ADD] \\n   private ArtifactClassLoaderFilter createClassLoaderFilter(ClassLoaderModel classLoaderModel) {\\n     return new DefaultArtifactClassLoaderFilter(classLoaderModel.getExportedPackages(), classLoaderModel.getExportedResources());\\n   }\",\n",
              " '     public void processElement(\\n         @Element Row row, @Timestamp Instant t, BoundedWindow w, OutputReceiver<Row> r)\\n         throws InterruptedException {\\n[DEL] final Future<Value> valueFuture;\\n[DEL] \\n[DEL] if (row.equals(previousRow)) {\\n[DEL] valueFuture = previousFuture;\\n[DEL] } else {\\n[DEL] Map<String, Value> columns = new HashMap<>();\\n[DEL] for (int i : referencedColumns) {\\n[DEL] columns.put(\\n[DEL] columnName(i),\\n[DEL] ZetaSqlBeamTranslationUtils.toZetaSqlValue(\\n[DEL] row.getBaseValue(i, Object.class), inputSchema.getField(i).getType()));\\n[DEL] }\\n[DEL] \\n[DEL] valueFuture = stream.execute(columns, nullParams);\\n[ADD] Map<String, Value> columns = new HashMap<>();\\n[ADD] for (int i : checkArgumentNotNull(referencedColumns)) {\\n[ADD] columns.put(\\n[ADD] columnName(i),\\n[ADD] ZetaSqlBeamTranslationUtils.toZetaSqlValue(\\n[ADD] row.getBaseValue(i, Object.class),\\n[ADD] inputSchema.getField(i).getType()));\\n       }\\n[DEL] previousRow = row;\\n \\n[DEL] @Nullable Queue<TimestampedFuture> pendingWindow = pending.get(w);\\n[ADD] @NonNull\\n[ADD] Future<Value> valueFuture = checkArgumentNotNull(stream).execute(columns, nullParams);\\n[ADD] \\n[ADD] @Nullable Queue<TimestampedFuture> pendingWindow = checkArgumentNotNull(pending).get(w);\\n       if (pendingWindow == null) {\\n         pendingWindow = new ArrayDeque<>();\\n         pending.put(w, pendingWindow);',\n",
              " '   public String getExpression() {\\n     return expression;\\n   }\\n[ADD] \\n[ADD] public boolean isMelAvailable() {\\n[ADD] return melAvailable;\\n[ADD] }\\n }',\n",
              " '       throws Exception {\\n     if (AGENT_CLASSLOADER == null) {\\n       ClassLoader agentClassLoader = createAgentClassLoader(\"inst\", bootstrapUrl);\\n[ADD] AGENT_CLASSLOADER = agentClassLoader;\\n[ADD] \\n       Class<?> agentInstallerClass =\\n           agentClassLoader.loadClass(\"io.opentelemetry.javaagent.tooling.AgentInstaller\");\\n       Method agentInstallerMethod =\\n           agentInstallerClass.getMethod(\"installBytebuddyAgent\", Instrumentation.class);\\n       ClassLoader savedContextClassLoader = Thread.currentThread().getContextClassLoader();\\n       try {\\n[DEL] Thread.currentThread().setContextClassLoader(AGENT_CLASSLOADER);\\n[ADD] Thread.currentThread().setContextClassLoader(agentClassLoader);\\n         agentInstallerMethod.invoke(null, inst);\\n       } finally {\\n         Thread.currentThread().setContextClassLoader(savedContextClassLoader);\\n       }\\n[DEL] AGENT_CLASSLOADER = agentClassLoader;\\n     }\\n   }\\n \\n[ADD] // TODO misleading name\\n[ADD] public static synchronized ClassLoader getAgentClassloader() {\\n[ADD] return AGENT_CLASSLOADER;\\n[ADD] }\\n[ADD] \\n   /**\\n    * Create the agent classloader. This must be called after the bootstrap jar has been appended to\\n    * the bootstrap classpath.',\n",
              " '     @Parameter(name = ApiConstants.HYPERVISOR, type = BaseCmd.CommandType.STRING, required = true, description = \"Hypervisor type\")\\n     private String hypervisor;\\n \\n[ADD] @Parameter(name = ApiConstants.ZONE_ID, type = CommandType.UUID, entityType = ZoneResponse.class,\\n[ADD] description = \"Zone to upload certificate\", required = true)\\n[ADD] private Long zoneId;\\n[ADD] \\n     @Override\\n[DEL] public void execute() throws ResourceUnavailableException, InsufficientCapacityException, ServerApiException, ConcurrentOperationException, ResourceAllocationException, NetworkRuleConflictException {\\n[ADD] public void execute() {\\n         if (!hypervisor.equalsIgnoreCase(\"kvm\")) {\\n             throw new ServerApiException(ApiErrorCode.PARAM_ERROR, \"Currently supporting KVM hosts only\");\\n         }\\n \\n[DEL] SuccessResponse response = new SuccessResponse(getCommandName());\\n[ADD] if (name.equalsIgnoreCase(\"cloud\")) {\\n[ADD] throw new ServerApiException(ApiErrorCode.PARAM_ERROR, \"Please provide a different alias name for the certificate\");\\n[ADD] }\\n[ADD] \\n         try {\\n             LOG.debug(\"Uploading certificate \" + name + \" to agents for Direct Download\");\\n[DEL] boolean result = directDownloadManager.uploadCertificateToHosts(certificate, name, hypervisor);\\n[ADD] boolean result = directDownloadManager.uploadCertificateToHosts(certificate, name, hypervisor, zoneId);\\n[ADD] SuccessResponse response = new SuccessResponse(getCommandName());\\n             response.setSuccess(result);\\n             setResponseObject(response);\\n         } catch (Exception e) {',\n",
              " ' \\t@Override\\n \\tprotected void onInit() throws Exception {\\n \\t\\tsuper.onInit();\\n[ADD] boolean resultFactorySpecified = StringUtils.hasText(this.getResultFactoryName()) || StringUtils.hasText(this.getResultType());\\n[ADD] if(resultFactorySpecified){\\n[ADD] alwaysUseResultFactory=true;\\n[ADD] }\\n \\t\\tthis.evaluationContext = ExpressionUtils.createStandardEvaluationContext(this.getBeanFactory());\\n \\t\\tif (this.templates == null) {\\n \\t\\t\\tTransformerFactory transformerFactory;',\n",
              " ' \\t\\t\\tthrow new IllegalStateException(\\n \\t\\t\\t\\t\\t\"Failure during the destruction of Mail receiver: \" + mailReceiver, e);\\n \\t\\t}\\n[ADD] /*\\n[ADD] * If we\\'re running with the default executor, shut it down.\\n[ADD] */\\n[ADD] if (!sendingTaskExecutorSet) {\\n[ADD] ((ExecutorService) sendingTaskExecutor).shutdown();\\n[ADD] }\\n \\t}\\n \\n ',\n",
              " ' import org.mule.extensions.jms.api.connection.JmsConnection;\\n import org.mule.extensions.jms.api.connection.JmsSession;\\n import org.mule.extensions.jms.api.destination.ConsumerType;\\n[ADD] import org.mule.extensions.jms.api.destination.TopicConsumer;\\n import org.mule.extensions.jms.api.exception.JmsExtensionException;\\n import org.mule.extensions.jms.api.message.JmsAttributes;\\n import org.mule.extensions.jms.api.message.MessageBuilder;\\n[ADD] import org.mule.extensions.jms.api.publish.JmsPublishParameters;\\n import org.mule.extensions.jms.internal.consume.JmsMessageConsumer;\\n[DEL] import org.mule.extensions.jms.internal.message.JmsResultFactory;\\n import org.mule.extensions.jms.internal.metadata.JmsOutputResolver;\\n[DEL] import org.mule.extensions.jms.internal.publish.JmsPublishParameters;\\n import org.mule.extensions.jms.internal.support.Jms102bSupport;\\n import org.mule.extensions.jms.internal.support.JmsSupport;\\n import org.mule.runtime.api.exception.MuleException;\\n import org.mule.runtime.api.message.Error;\\n[ADD] import org.mule.runtime.core.api.util.Pair;\\n import org.mule.runtime.core.util.StringMessageUtils;\\n import org.mule.runtime.extension.api.annotation.Alias;\\n import org.mule.runtime.extension.api.annotation.dsl.xml.XmlHints;',\n",
              " '         return previousRole.getId();\\n     }\\n \\n[ADD] /**\\n[ADD] * This method was deprecated in Graylog 4.1.\\n[ADD] * Prefer use of the {@link #ensureUser(String, String, String, String, String, Set)} method instead, which sets\\n[ADD] * both the firstName and lastName.\\n[ADD] */\\n     @Nullable\\n[DEL] public String ensureUser(String userName, String password, String fullName, String email, Set<String> expectedRoles) {\\n[ADD] @Deprecated\\n[ADD] public String ensureUser(String userName, String password, String fullName, String email,\\n[ADD] Set<String> expectedRoles) {\\n[ADD] throw new UnsupportedOperationException(\"This method is no longer supported. Please use the alternate method \" +\\n[ADD] \"providing both a firstName and lastName instead of fullName.\");\\n[ADD] }\\n[ADD] \\n[ADD] @Nullable\\n[ADD] public String ensureUser(String userName, String password, String firstName, String lastName, String email,\\n[ADD] Set<String> expectedRoles) {\\n         User previousUser = null;\\n         try {\\n             previousUser = userService.load(userName);',\n",
              " ' \\t\\t\\t\\t\\tIPC.logMessage(\"deleted \", cd.getAbsolutePath()); //$NON-NLS-1$\\n \\t\\t\\t\\t}\\n \\t\\t\\t\\tIPC.mkdirWithPermissions(cd.getAbsolutePath(), COMMON_DIRECTORY_PERMISSIONS);\\n[ADD] IPC.logMessage(\"After deletion of an existing file, created the common directory at \", cd.getAbsolutePath()); //$NON-NLS-1$\\n[ADD] } else {\\n[ADD] if (LOGGING_DISABLED != loggingStatus) {\\n[ADD] IPC.logMessage(\"The common directory already exists at \", cd.getAbsolutePath()); //$NON-NLS-1$\\n[ADD] }\\n \\t\\t\\t}\\n \\t\\t} else {\\n \\t\\t\\tIPC.mkdirWithPermissions(cd.getAbsolutePath(), COMMON_DIRECTORY_PERMISSIONS);\\n[ADD] if (LOGGING_DISABLED != loggingStatus) {\\n[ADD] IPC.logMessage(\"Created the common directory at \", cd.getAbsolutePath()); //$NON-NLS-1$\\n[ADD] }\\n \\t\\t}\\n \\t\\t/*[PR Jazz 30075] setupSemaphore was redundant. */\\n \\t}',\n",
              " '                             result = new DecodeableRpcResult(channel, res, is,\\n                                     (Invocation) getRequestData(id), proto);\\n                             result.decode();\\n[ADD] System.out.println(Thread.currentThread().getName());\\n                         } else {\\n                             result = new DecodeableRpcResult(channel, res,\\n                                     new UnsafeByteArrayInputStream(readMessageData(is)),',\n",
              " '  */\\n public abstract class AbstractClusterInvoker<T> implements Invoker<T> {\\n \\n[DEL] private static final Logger logger = LoggerFactory\\n[DEL] .getLogger(AbstractClusterInvoker.class);\\n[DEL] protected final Directory<T> directory;\\n[ADD] private static final Logger logger = LoggerFactory.getLogger(AbstractClusterInvoker.class);\\n \\n[DEL] protected final boolean availablecheck;\\n[ADD] protected Directory<T> directory;\\n[ADD] \\n[ADD] protected boolean availablecheck;\\n \\n     private AtomicBoolean destroyed = new AtomicBoolean(false);\\n \\n     private volatile Invoker<T> stickyInvoker = null;\\n \\n[ADD] public AbstractClusterInvoker() {\\n[ADD] }\\n[ADD] \\n     public AbstractClusterInvoker(Directory<T> directory) {\\n         this(directory, directory.getUrl());\\n     }\\n \\n     public AbstractClusterInvoker(Directory<T> directory, URL url) {\\n[DEL] if (directory == null)\\n[ADD] if (directory == null) {\\n             throw new IllegalArgumentException(\"service directory == null\");\\n[ADD] }\\n \\n         this.directory = directory;\\n         //sticky: invoker.isAvailable() should always be checked before using when availablecheck is true.',\n",
              " ' \\n         public abstract Builder creationDate(ZonedDateTime creationDate);\\n \\n[ADD] public abstract Builder indexAnalyzer(String analyzer);\\n[ADD] \\n[ADD] public abstract Builder indexTemplateName(String templateName);\\n[ADD] \\n[ADD] public abstract Builder indexOptimizationMaxNumSegments(Integer indexOptimizationMaxNumSegments);\\n[ADD] \\n[ADD] public abstract Builder indexOptimizationDisabled(Boolean indexOptimizationDisabled);\\n[ADD] \\n         public abstract IndexSetConfig build();\\n     }\\n }',\n",
              " ' import org.apache.hudi.common.model.HoodieRecord;\\n import org.apache.hudi.common.model.HoodieRecordLocation;\\n import org.apache.hudi.common.model.HoodieRecordPayload;\\n[DEL] import org.apache.hudi.common.model.HoodieWriteStat;\\n import org.apache.hudi.common.model.HoodieWriteStat.RuntimeStats;\\n import org.apache.hudi.common.model.IOType;\\n import org.apache.hudi.common.table.log.HoodieLogFormat;',\n",
              " '                       + \"of %s. Please verify credentials are valid and that you have \"\\n                       + \"write access to %s. Stale credentials can be resolved by executing \"\\n                       + \"\\'gcloud auth application-default login\\'.\",\\n[DEL] source, target);\\n[ADD] sourceDescription, target);\\n           LOG.error(errorMessage);\\n           throw new IOException(errorMessage, ioException);\\n         }\\n \\n         long sleep = backoff.nextBackOffMillis();\\n         if (sleep == BackOff.STOP) {\\n[DEL] LOG.error(\"Upload failed, will NOT retry staging of package: {}\", source, ioException);\\n[ADD] LOG.error(\\n[ADD] \"Upload failed, will NOT retry staging of package: {}\",\\n[ADD] sourceDescription,\\n[ADD] ioException);\\n           throw new RuntimeException(\"Could not stage %s to %s\", ioException);\\n         } else {\\n           LOG.warn(\\n               \"Upload attempt failed, sleeping before retrying staging of package: {}\",\\n[DEL] source,\\n[ADD] sourceDescription,\\n               ioException);\\n           retrySleeper.sleep(sleep);\\n         }',\n",
              " ' import org.mule.runtime.module.extension.internal.capability.xml.schema.model.TopLevelSimpleType;\\n import org.mule.runtime.module.extension.internal.capability.xml.schema.model.Union;\\n import org.mule.runtime.module.extension.internal.exception.IllegalParameterModelDefinitionException;\\n[DEL] import org.mule.runtime.module.extension.internal.introspection.SubTypesMappingContainer;\\n import org.mule.runtime.module.extension.internal.model.property.InfrastructureParameterModelProperty;\\n import org.mule.runtime.module.extension.internal.model.property.TypeRestrictionModelProperty;\\n[DEL] import org.mule.runtime.module.extension.internal.util.IntrospectionUtils;\\n import org.mule.runtime.module.extension.internal.xml.SchemaConstants;\\n \\n import com.google.common.collect.ImmutableMap;',\n",
              " '                 // creates a new tab item\\n                 tab = new TabItem();\\n                 tab.Header = viewExtension.Name;\\n[DEL] tab.Tag = viewExtension.GetType();\\n[ADD] tab.Tag = viewExtension;\\n                 tab.Uid = viewExtension.UniqueId;\\n                 tab.HeaderTemplate = tabDynamic.FindResource(\"TabHeader\") as DataTemplate;\\n ',\n",
              " '  */\\n package org.mule.runtime.module.launcher.log4j2;\\n \\n[ADD] import static java.lang.System.getProperty;\\n import static org.mule.runtime.api.i18n.I18nMessageFactory.createStaticMessage;\\n import static org.mule.runtime.module.launcher.log4j2.ArtifactAwareContextSelector.LOGGER;\\n import static org.mule.runtime.module.reboot.api.MuleContainerBootstrapUtils.getMuleBase;\\n[ADD] \\n import org.mule.runtime.api.exception.MuleRuntimeException;\\n[ADD] import org.mule.runtime.deployment.model.api.application.ApplicationDescriptor;\\n import org.mule.runtime.module.artifact.api.classloader.ArtifactClassLoader;\\n import org.mule.runtime.module.artifact.api.classloader.DirectoryResourceLocator;\\n import org.mule.runtime.module.artifact.api.classloader.LocalResourceLocator;\\n[DEL] import org.mule.runtime.deployment.model.api.application.ApplicationDescriptor;\\n import org.mule.runtime.module.reboot.api.MuleContainerBootstrapUtils;\\n \\n[ADD] import org.apache.logging.log4j.core.LoggerContext;\\n[ADD] \\n import java.io.File;\\n[DEL] import java.net.MalformedURLException;\\n import java.net.URI;\\n import java.net.URISyntaxException;\\n import java.net.URL;\\n \\n[DEL] import org.apache.logging.log4j.core.LoggerContext;\\n[DEL] \\n /**\\n  * Encapsulates the logic to get the proper log configuration.\\n[DEL] *\\n[ADD] *\\n  * @since 3.8.0\\n  */\\n public class MuleLoggerContextFactory {\\n \\n   /**\\n    * Builds a new {@link LoggerContext} for the given {@code classLoader} and {@code selector}\\n[DEL] *\\n[ADD] *\\n    * @param classLoader the classloader of the artifact this logger context is for.\\n    * @param selector the selector to bew used when building the loggers for the new context.\\n    * @return',\n",
              " '    */\\n   public void overrideWith(Properties properties) {\\n     for (String key : properties.stringPropertyNames()) {\\n[DEL] if (this.properties.containsKey(key)) {\\n[DEL] this.properties.setProperty(key, properties.getProperty(key));\\n[ADD] if (this.specProperties.containsKey(key)) {\\n[ADD] this.specProperties.setProperty(key, properties.getProperty(key));\\n[ADD] }\\n[ADD] if (this.commonProperties.containsKey(key)) {\\n[ADD] this.commonProperties.setProperty(key, properties.getProperty(key));\\n       }\\n     }\\n   }',\n",
              " '         referencePath.getModificationTime() < replacementFile.getModificationTime();\\n   }\\n \\n[DEL] private void checkTableCompatibility(Table desiredTargetTable, Table existingTargetTable) throws IOException {\\n[DEL] \\n[ADD] private void checkPartitionedTableCompatibility(Table desiredTargetTable, Table existingTargetTable) throws IOException {\\n     if (!desiredTargetTable.getDataLocation().equals(existingTargetTable.getDataLocation())) {\\n[DEL] throw new IOException(\\n[DEL] String.format(\"Desired target location %s and already registered target location %s do not agree.\",\\n[DEL] desiredTargetTable.getDataLocation(), existingTargetTable.getDataLocation()));\\n[ADD] throw new HiveTableLocationNotMatchException(desiredTargetTable.getDataLocation(),\\n[ADD] existingTargetTable.getDataLocation());\\n     }\\n \\n     if (HiveUtils.isPartitioned(desiredTargetTable) != HiveUtils.isPartitioned(existingTargetTable)) {',\n",
              " '         if (uriParamNames.contains(uriParamName)) {\\n           // TODO: MULE-8946 This should throw a MuleException\\n           throw new MuleRuntimeException(CoreMessages\\n[DEL] .createStaticMessage(String.format(\"Http Listener with path %s contains duplicated uri param names\", this.path)));\\n[ADD] .createStaticMessage(String.format(\\n[ADD] \"Http Listener with path %s contains duplicated uri param names\", this.path)));\\n         }\\n         uriParamNames.add(uriParamName);\\n       } else {',\n",
              " ' import com.google.gwt.animation.client.Animation;\\n import com.google.gwt.aria.client.DialogRole;\\n import com.google.gwt.aria.client.Id;\\n[ADD] import com.google.gwt.core.client.Scheduler;\\n import com.google.gwt.dom.client.Document;\\n import com.google.gwt.dom.client.Element;\\n import com.google.gwt.dom.client.NativeEvent;',\n",
              " '         .add(create(DECORATORS_READ, \"\"))\\n         .add(create(DEFLECTOR_CYCLE, \"\"))\\n         .add(create(DEFLECTOR_READ, \"\"))\\n[ADD] .add(create(EVENT_DEFINITIONS_CREATE, \"\"))\\n[ADD] .add(create(EVENT_DEFINITIONS_DELETE, \"\"))\\n[ADD] .add(create(EVENT_DEFINITIONS_EDIT, \"\"))\\n[ADD] .add(create(EVENT_DEFINITIONS_EXECUTE, \"\"))\\n[ADD] .add(create(EVENT_DEFINITIONS_READ, \"\"))\\n[ADD] .add(create(EVENT_NOTIFICATIONS_CREATE, \"\"))\\n[ADD] .add(create(EVENT_NOTIFICATIONS_DELETE, \"\"))\\n[ADD] .add(create(EVENT_NOTIFICATIONS_EDIT, \"\"))\\n[ADD] .add(create(EVENT_NOTIFICATIONS_READ, \"\"))\\n         .add(create(FIELDNAMES_READ, \"\"))\\n         .add(create(INDEXERCLUSTER_READ, \"\"))\\n         .add(create(INDEXRANGES_READ, \"\"))',\n",
              " '         m_localSimdSlots[localRegisterID] = bValue;\\n     }\\n \\n[ADD] int InterpreterStackFrame::OP_GetMemorySize()\\n[ADD] {\\n[ADD] JavascriptArrayBuffer* arr = *(JavascriptArrayBuffer**)GetNonVarReg(AsmJsFunctionMemory::ArrayBufferRegister);\\n[ADD] return arr->GetByteLength() >> 16;\\n[ADD] }\\n[ADD] \\n     template <class T>\\n     void InterpreterStackFrame::OP_SimdLdArrGeneric(const unaligned T* playout)\\n     {',\n",
              " '  */\\n package org.mule.runtime.module.extension.internal.capability.xml.schema.builder;\\n \\n[ADD] import static com.google.common.hash.Hashing.sha256;\\n import static java.lang.String.format;\\n import static java.math.BigInteger.ONE;\\n import static java.math.BigInteger.ZERO;\\n[ADD] import static java.nio.charset.StandardCharsets.UTF_8;\\n import static java.util.Arrays.asList;\\n import static java.util.Collections.singleton;\\n import static java.util.Comparator.comparing;\\n[ADD] import static java.util.stream.Collectors.toList;\\n[ADD] import static jdk.nashorn.internal.objects.NativeArray.join;\\n import static org.apache.commons.lang3.StringUtils.EMPTY;\\n import static org.apache.commons.lang3.StringUtils.isNotBlank;\\n import static org.mule.runtime.api.meta.ExpressionSupport.NOT_SUPPORTED;',\n",
              " '       asList(\"ordered\", \"insert\", \"count\", \"find\", \"create\");\\n \\n   private JsonWriterSettings createJsonWriterSettings(int maxNormalizedQueryLength) {\\n[DEL] JsonWriterSettings settings = new JsonWriterSettings(false);\\n[ADD] JsonWriterSettings settings = null;\\n     try {\\n       // The static JsonWriterSettings.builder() method was introduced in the 3.5 release\\n       Optional<Method> buildMethod =',\n",
              " '         {\\n             PGPCryptInfo pgpCryptInfo = this.safeGetCryptInfo(cryptInfo);\\n             PGPPublicKey publicKey = pgpCryptInfo.getPublicKey();\\n[DEL] StreamTransformer transformer = new EncryptStreamTransformer(data, publicKey, provider);\\n[ADD] StreamTransformer transformer = new EncryptStreamTransformer(data, publicKey, provider, encryptionAlgorithm);\\n             return new LazyTransformedInputStream(new TransformContinuouslyPolicy(), transformer);\\n         }\\n         catch (Exception e)',\n",
              " ' import org.mule.runtime.core.internal.execution.SourceResultAdapter;\\n import org.mule.runtime.core.internal.policy.PolicyManager;\\n import org.mule.runtime.core.internal.policy.SourcePolicy;\\n[ADD] import org.mule.runtime.core.internal.streaming.bytes.ManagedCursorStreamProvider;\\n import org.mule.sdk.api.runtime.operation.Result;\\n import org.mule.sdk.api.runtime.streaming.PagingProvider;\\n import org.mule.tck.junit4.rule.SystemProperty;',\n",
              " ' \\n   protected HiveDatasetFinder(FileSystem fs, Properties properties, HiveMetastoreClientPool clientPool,\\n       EventSubmitter eventSubmitter) throws IOException {\\n[ADD] this(fs, properties, clientPool, eventSubmitter, ConfigClientCache.getClient(VersionStabilityPolicy.STRONG_LOCAL_STABILITY));\\n[ADD] }\\n[ADD] \\n[ADD] protected HiveDatasetFinder(FileSystem fs, Properties properties, HiveMetastoreClientPool clientPool,\\n[ADD] EventSubmitter eventSubmitter, ConfigClient configClient) throws IOException {\\n[ADD] \\n     this.properties = properties;\\n     this.clientPool = clientPool;\\n     this.fs = fs;',\n",
              " '             return 0;\\n         }\\n \\n[DEL] Class clazz1 = (Class) o1;\\n[DEL] Class clazz2 = (Class) o2;\\n[DEL] \\n[DEL] Class<?> inf = findSpi(clazz1);\\n[ADD] Class clazz1 = o1;\\n[ADD] Class clazz2 = o2;\\n \\n         OrderInfo a1 = parseOrder(clazz1);\\n         OrderInfo a2 = parseOrder(clazz2);',\n",
              " '     }\\n   }\\n \\n[ADD] @VisibleForTesting\\n[ADD] public Set<String> getNonHoodiePathCache() {\\n[ADD] return nonHoodiePathCache;\\n[ADD] }\\n[ADD] \\n[ADD] \\n   @Override\\n   public void setConf(Configuration conf) {\\n     this.conf = new SerializableConfiguration(conf);',\n",
              " ' import java.util.LinkedHashSet;\\n import java.util.List;\\n import java.util.Map;\\n[DEL] import javax.annotation.Nonnull;\\n[DEL] import javax.annotation.Nullable;\\n[DEL] \\n \\n /**\\n[DEL] * Utility implementing the necessary reflection for working with {@link DoFn}s.\\n[ADD] * Parses a {@link DoFn} and computes its {@link DoFnSignature}. See {@link #getOrParseSignature}.\\n  */\\n public abstract class DoFnReflector {\\n[ADD] private DoFnReflector() {}\\n \\n[DEL] private static final String FN_DELEGATE_FIELD_NAME = \"delegate\";\\n[DEL] \\n[DEL] private enum Availability {\\n[DEL] /** Indicates parameters only available in {@code @ProcessElement} methods. */\\n[DEL] PROCESS_ELEMENT_ONLY,\\n[DEL] /** Indicates parameters available in all methods. */\\n[DEL] EVERYWHERE\\n[DEL] }\\n[DEL] \\n[DEL] /**\\n[DEL] * Enumeration of the parameters available from the {@link ExtraContextFactory} to use as\\n[DEL] * additional parameters for {@link DoFn} methods.\\n[DEL] * <p>\\n[DEL] * We don\\'t rely on looking for properly annotated methods within {@link ExtraContextFactory}\\n[DEL] * because erasure would make it impossible to completely fill in the type token for context\\n[DEL] * parameters that depend on the input/output type.\\n[DEL] */\\n[DEL] private enum AdditionalParameter {\\n[DEL] \\n[DEL] /** Any {@link BoundedWindow} parameter is populated by the window of the current element. */\\n[DEL] WINDOW_OF_ELEMENT(Availability.PROCESS_ELEMENT_ONLY, BoundedWindow.class, \"window\") {\\n[DEL] @Override\\n[DEL] public <InputT, OutputT> TypeToken<?> tokenFor(TypeToken<InputT> in, TypeToken<OutputT> out) {\\n[DEL] return TypeToken.of(BoundedWindow.class);\\n[DEL] }\\n[DEL] },\\n[DEL] \\n[DEL] INPUT_PROVIDER(Availability.PROCESS_ELEMENT_ONLY, DoFn.InputProvider.class, \"inputProvider\") {\\n[DEL] @Override\\n[DEL] public <InputT, OutputT> TypeToken<?> tokenFor(TypeToken<InputT> in, TypeToken<OutputT> out) {\\n[DEL] return new TypeToken<DoFn.InputProvider<InputT>>() {}.where(\\n[DEL] new TypeParameter<InputT>() {}, in);\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public boolean isHidden() {\\n[DEL] return true;\\n[DEL] }\\n[DEL] },\\n[DEL] \\n[DEL] OUTPUT_RECEIVER(\\n[DEL] Availability.PROCESS_ELEMENT_ONLY, DoFn.OutputReceiver.class, \"outputReceiver\") {\\n[DEL] @Override\\n[DEL] public <InputT, OutputT> TypeToken<?> tokenFor(TypeToken<InputT> in, TypeToken<OutputT> out) {\\n[DEL] return new TypeToken<DoFn.OutputReceiver<OutputT>>() {}.where(\\n[DEL] new TypeParameter<OutputT>() {}, out);\\n[DEL] }\\n[ADD] private static final Map<Class<?>, DoFnSignature> SIGNATURE_CACHE = new LinkedHashMap<>();\\n \\n[DEL] @Override\\n[DEL] public boolean isHidden() {\\n[DEL] return true;\\n[DEL] }\\n[DEL] };\\n[DEL] \\n[DEL] /**\\n[DEL] * Create a type token representing the given parameter. May use the type token associated\\n[DEL] * with the input and output types of the {@link DoFn}, depending on the extra\\n[DEL] * context.\\n[DEL] */\\n[DEL] abstract <InputT, OutputT> TypeToken<?> tokenFor(\\n[DEL] TypeToken<InputT> in, TypeToken<OutputT> out);\\n[DEL] \\n[DEL] /**\\n[DEL] * Indicates whether this enum is for testing only, hence should not appear in error messages,\\n[DEL] * etc. Defaults to {@code false}.\\n[DEL] */\\n[DEL] boolean isHidden() {\\n[DEL] return false;\\n[ADD] /** @return the {@link DoFnSignature} for the given {@link DoFn}. */\\n[ADD] static synchronized DoFnSignature getOrParseSignature(\\n[ADD] @SuppressWarnings(\"rawtypes\") Class<? extends DoFn> fn) {\\n[ADD] DoFnSignature signature = SIGNATURE_CACHE.get(fn);\\n[ADD] if (signature != null) {\\n[ADD] return signature;\\n     }\\n \\n[DEL] private final Class<?> rawType;\\n[DEL] private final Availability availability;\\n[DEL] private final transient MethodDescription method;\\n[DEL] \\n[DEL] private AdditionalParameter(Availability availability, Class<?> rawType, String method) {\\n[DEL] this.availability = availability;\\n[DEL] this.rawType = rawType;\\n[DEL] try {\\n[DEL] this.method = new MethodDescription.ForLoadedMethod(\\n[DEL] ExtraContextFactory.class.getMethod(method));\\n[DEL] } catch (NoSuchMethodException | SecurityException e) {\\n[DEL] throw new RuntimeException(\\n[DEL] \"Unable to access method \" + method + \" on \" + ExtraContextFactory.class, e);\\n[DEL] }\\n[DEL] }\\n[ADD] signature = parseSignature(fn);\\n[ADD] SIGNATURE_CACHE.put(fn, signature);\\n[ADD] return signature;\\n   }\\n \\n[DEL] private static final Map<Class<?>, AdditionalParameter> EXTRA_CONTEXTS;\\n[DEL] private static final Map<Class<?>, AdditionalParameter> EXTRA_PROCESS_CONTEXTS;\\n[DEL] \\n[DEL] static {\\n[DEL] ImmutableMap.Builder<Class<?>, AdditionalParameter> everywhereBuilder =\\n[DEL] ImmutableMap.<Class<?>, AdditionalParameter>builder();\\n[DEL] ImmutableMap.Builder<Class<?>, AdditionalParameter> processElementBuilder =\\n[DEL] ImmutableMap.<Class<?>, AdditionalParameter>builder();\\n[ADD] /** @return the {@link DoFnInvoker} for the given {@link DoFn}. */\\n[ADD] public static <InputT, OutputT> DoFnInvoker<InputT, OutputT> newByteBuddyInvoker(\\n[ADD] DoFn<InputT, OutputT> fn) {\\n[ADD] return DoFnInvokers.newByteBuddyInvoker(getOrParseSignature(fn.getClass()), fn);\\n[ADD] }\\n \\n[DEL] for (AdditionalParameter value : AdditionalParameter.values()) {\\n[DEL] switch (value.availability) {\\n[DEL] case EVERYWHERE:\\n[DEL] everywhereBuilder.put(value.rawType, value);\\n[DEL] break;\\n[DEL] case PROCESS_ELEMENT_ONLY:\\n[DEL] processElementBuilder.put(value.rawType, value);\\n[DEL] break;\\n[ADD] /** Analyzes a given {@link DoFn} class and extracts its {@link DoFnSignature}. */\\n[ADD] private static DoFnSignature parseSignature(Class<? extends DoFn> fnClass) {\\n[ADD] TypeToken<?> inputT = null;\\n[ADD] TypeToken<?> outputT = null;\\n[ADD] \\n[ADD] // Extract the input and output type.\\n[ADD] checkArgument(\\n[ADD] DoFn.class.isAssignableFrom(fnClass),\\n[ADD] \"%s must be subtype of DoFn\",\\n[ADD] fnClass.getSimpleName());\\n[ADD] TypeToken<? extends DoFn> fnToken = TypeToken.of(fnClass);\\n[ADD] for (TypeToken<?> supertype : fnToken.getTypes()) {\\n[ADD] if (!supertype.getRawType().equals(DoFn.class)) {\\n[ADD] continue;\\n       }\\n[DEL] }\\n[DEL] \\n[DEL] EXTRA_CONTEXTS = everywhereBuilder.build();\\n[DEL] EXTRA_PROCESS_CONTEXTS = processElementBuilder\\n[DEL] // Process Element contexts include everything available everywhere\\n[DEL] .putAll(EXTRA_CONTEXTS)\\n[DEL] .build();\\n[ADD] Type[] args = ((ParameterizedType) supertype.getType()).getActualTypeArguments();\\n[ADD] inputT = TypeToken.of(args[0]);\\n[ADD] outputT = TypeToken.of(args[1]);\\n[ADD] }\\n[ADD] Preconditions.checkNotNull(inputT, \"Unable to determine input type from %s\", fnClass);\\n[ADD] \\n[ADD] Method processElementMethod = findAnnotatedMethod(DoFn.ProcessElement.class, fnClass, true);\\n[ADD] Method startBundleMethod = findAnnotatedMethod(DoFn.StartBundle.class, fnClass, false);\\n[ADD] Method finishBundleMethod = findAnnotatedMethod(DoFn.FinishBundle.class, fnClass, false);\\n[ADD] Method setupMethod = findAnnotatedMethod(DoFn.Setup.class, fnClass, false);\\n[ADD] Method teardownMethod = findAnnotatedMethod(DoFn.Teardown.class, fnClass, false);\\n[ADD] \\n[ADD] return new DoFnSignature(\\n[ADD] fnClass,\\n[ADD] inputT,\\n[ADD] outputT,\\n[ADD] analyzeProcessElementMethod(fnToken, processElementMethod, inputT, outputT),\\n[ADD] (startBundleMethod == null)\\n[ADD] ? null\\n[ADD] : analyzeBundleMethod(fnToken, startBundleMethod, inputT, outputT),\\n[ADD] (finishBundleMethod == null)\\n[ADD] ? null\\n[ADD] : analyzeBundleMethod(fnToken, finishBundleMethod, inputT, outputT),\\n[ADD] (setupMethod == null) ? null : analyzeLifecycleMethod(setupMethod),\\n[ADD] (teardownMethod == null) ? null : analyzeLifecycleMethod(teardownMethod));\\n   }\\n \\n   /**\\n[DEL] * @return true if the reflected {@link DoFn} uses a Single Window.\\n[DEL] */\\n[DEL] public abstract boolean usesSingleWindow();\\n[DEL] \\n[DEL] /** Create an {@link DoFnInvoker} bound to the given {@link OldDoFn}. */\\n[DEL] public abstract <InputT, OutputT> DoFnInvoker<InputT, OutputT> bindInvoker(\\n[DEL] DoFn<InputT, OutputT> fn);\\n[DEL] \\n[DEL] private static final Map<Class<?>, DoFnReflector> REFLECTOR_CACHE =\\n[DEL] new LinkedHashMap<Class<?>, DoFnReflector>();\\n[DEL] \\n[DEL] /**\\n[DEL] * @return the {@link DoFnReflector} for the given {@link DoFn}.\\n[ADD] * Generates a type token for {@code DoFn<InputT, OutputT>.ProcessContext} given {@code InputT}\\n[ADD] * and {@code OutputT}.\\n    */\\n[DEL] public static DoFnReflector of(\\n[DEL] @SuppressWarnings(\"rawtypes\") Class<? extends DoFn> fn) {\\n[DEL] DoFnReflector reflector = REFLECTOR_CACHE.get(fn);\\n[DEL] if (reflector != null) {\\n[DEL] return reflector;\\n[DEL] }\\n[DEL] \\n[DEL] reflector = new GenericDoFnReflector(fn);\\n[DEL] REFLECTOR_CACHE.put(fn, reflector);\\n[DEL] return reflector;\\n[ADD] private static <InputT, OutputT>\\n[ADD] TypeToken<DoFn<InputT, OutputT>.ProcessContext> doFnProcessContextTypeOf(\\n[ADD] TypeToken<InputT> inputT, TypeToken<OutputT> outputT) {\\n[ADD] return new TypeToken<DoFn<InputT, OutputT>.ProcessContext>() {}.where(\\n[ADD] new TypeParameter<InputT>() {}, inputT)\\n[ADD] .where(new TypeParameter<OutputT>() {}, outputT);\\n   }\\n \\n   /**\\n[DEL] * Create a {@link OldDoFn} that the {@link DoFn}.\\n[ADD] * Generates a type token for {@code DoFn<InputT, OutputT>.Context} given {@code InputT} and\\n[ADD] * {@code OutputT}.\\n    */\\n[DEL] public <InputT, OutputT> OldDoFn<InputT, OutputT> toDoFn(DoFn<InputT, OutputT> fn) {\\n[DEL] if (usesSingleWindow()) {\\n[DEL] return new WindowDoFnAdapter<InputT, OutputT>(this, fn);\\n[DEL] } else {\\n[DEL] return new SimpleDoFnAdapter<InputT, OutputT>(this, fn);\\n[DEL] }\\n[ADD] private static <InputT, OutputT> TypeToken<DoFn<InputT, OutputT>.Context> doFnContextTypeOf(\\n[ADD] TypeToken<InputT> inputT, TypeToken<OutputT> outputT) {\\n[ADD] return new TypeToken<DoFn<InputT, OutputT>.Context>() {}.where(\\n[ADD] new TypeParameter<InputT>() {}, inputT)\\n[ADD] .where(new TypeParameter<OutputT>() {}, outputT);\\n   }\\n \\n[DEL] private static String formatType(TypeToken<?> t) {\\n[DEL] return ReflectHelpers.TYPE_SIMPLE_DESCRIPTION.apply(t.getType());\\n[ADD] /** Generates a type token for {@code DoFn.InputProvider<InputT>} given {@code InputT}. */\\n[ADD] private static <InputT> TypeToken<DoFn.InputProvider<InputT>> inputProviderTypeOf(\\n[ADD] TypeToken<InputT> inputT) {\\n[ADD] return new TypeToken<DoFn.InputProvider<InputT>>() {}.where(\\n[ADD] new TypeParameter<InputT>() {}, inputT);\\n   }\\n \\n[DEL] private static String format(Method m) {\\n[DEL] return ReflectHelpers.CLASS_AND_METHOD_FORMATTER.apply(m);\\n[ADD] /** Generates a type token for {@code DoFn.OutputReceiver<OutputT>} given {@code OutputT}. */\\n[ADD] private static <OutputT> TypeToken<DoFn.OutputReceiver<OutputT>> outputReceiverTypeOf(\\n[ADD] TypeToken<OutputT> inputT) {\\n[ADD] return new TypeToken<DoFn.OutputReceiver<OutputT>>() {}.where(\\n[ADD] new TypeParameter<OutputT>() {}, inputT);\\n   }\\n \\n[DEL] private static Collection<String> describeSupportedTypes(\\n[DEL] Map<Class<?>, AdditionalParameter> extraProcessContexts,\\n[DEL] final TypeToken<?> in, final TypeToken<?> out) {\\n[DEL] return FluentIterable\\n[DEL] .from(extraProcessContexts.values())\\n[DEL] .filter(new Predicate<AdditionalParameter>() {\\n[DEL] @Override\\n[DEL] public boolean apply(@Nonnull AdditionalParameter additionalParameter) {\\n[DEL] return !additionalParameter.isHidden();\\n[DEL] }\\n[DEL] })\\n[DEL] .transform(new Function<AdditionalParameter, String>() {\\n[DEL] @Override\\n[DEL] @Nonnull\\n[DEL] public String apply(@Nonnull AdditionalParameter input) {\\n[DEL] return formatType(input.tokenFor(in, out));\\n[DEL] }\\n[DEL] })\\n[DEL] .toSortedSet(String.CASE_INSENSITIVE_ORDER);\\n[DEL] }\\n[ADD] static DoFnSignature.ProcessElementMethod analyzeProcessElementMethod(\\n[ADD] TypeToken<? extends DoFn> fnClass, Method m, TypeToken<?> inputT, TypeToken<?> outputT) {\\n[ADD] checkArgument(\\n[ADD] void.class.equals(m.getReturnType()), \"%s must have a void return type\", format(m));\\n[ADD] checkArgument(!m.isVarArgs(), \"%s must not have var args\", format(m));\\n \\n[DEL] @VisibleForTesting\\n[DEL] static <InputT, OutputT> List<AdditionalParameter> verifyProcessMethodArguments(Method m) {\\n[DEL] return verifyMethodArguments(m,\\n[DEL] EXTRA_PROCESS_CONTEXTS,\\n[DEL] new TypeToken<DoFn<InputT, OutputT>.ProcessContext>() {},\\n[DEL] new TypeParameter<InputT>() {},\\n[DEL] new TypeParameter<OutputT>() {});\\n[DEL] }\\n[ADD] TypeToken<?> processContextToken = doFnProcessContextTypeOf(inputT, outputT);\\n \\n[DEL] @VisibleForTesting\\n[DEL] static <InputT, OutputT> List<AdditionalParameter> verifyBundleMethodArguments(Method m) {\\n[DEL] if (m == null) {\\n[DEL] return null;\\n[DEL] }\\n[DEL] return verifyMethodArguments(m,\\n[DEL] EXTRA_CONTEXTS,\\n[DEL] new TypeToken<DoFn<InputT, OutputT>.Context>() {},\\n[DEL] new TypeParameter<InputT>() {},\\n[DEL] new TypeParameter<OutputT>() {});\\n[DEL] }\\n[DEL] \\n[DEL] @VisibleForTesting\\n[DEL] static void verifyLifecycleMethodArguments(Method m) {\\n[DEL] if (m == null) {\\n[DEL] return;\\n[DEL] }\\n[DEL] checkState(void.class.equals(m.getReturnType()), \"%s must have void return type\", format(m));\\n[DEL] checkState(m.getGenericParameterTypes().length == 0, \"%s must take zero arguments\", format(m));\\n[DEL] }\\n[DEL] \\n[DEL] /**\\n[DEL] * Verify the method arguments for a given {@link DoFn} method.\\n[DEL] *\\n[DEL] * <p>The requirements for a method to be valid, are:\\n[DEL] * <ol>\\n[DEL] * <li>The method has at least one argument.\\n[DEL] * <li>The first argument is of type firstContextArg.\\n[DEL] * <li>The remaining arguments have raw types that appear in {@code contexts}\\n[DEL] * <li>Any generics on the extra context arguments match what is expected. Currently, this\\n[DEL] * is exercised only by placeholders. For example, {@code InputReceiver<InputT> must either match\\n[DEL] * the {@code InputT} {@code OldDoFn<InputT, OutputT>.ProcessContext} or use a wildcard, etc.\\n[DEL] * </ol>\\n[DEL] *\\n[DEL] * @param m the method to verify\\n[DEL] * @param contexts mapping from raw classes to the {@link AdditionalParameter} used\\n[DEL] *     to create new instances.\\n[DEL] * @param firstContextArg the expected type of the first context argument\\n[DEL] * @param iParam TypeParameter representing the input type\\n[DEL] * @param oParam TypeParameter representing the output type\\n[DEL] */\\n[DEL] @VisibleForTesting\\n[DEL] static <InputT, OutputT> List<AdditionalParameter> verifyMethodArguments(\\n[DEL] Method m,\\n[DEL] Map<Class<?>, AdditionalParameter> contexts,\\n[DEL] TypeToken<?> firstContextArg,\\n[DEL] TypeParameter<InputT> iParam,\\n[DEL] TypeParameter<OutputT> oParam) {\\n[DEL] \\n[DEL] if (!void.class.equals(m.getReturnType())) {\\n[DEL] throw new IllegalStateException(String.format(\\n[DEL] \"%s must have a void return type\", format(m)));\\n[DEL] }\\n[DEL] if (m.isVarArgs()) {\\n[DEL] throw new IllegalStateException(String.format(\\n[DEL] \"%s must not have var args\", format(m)));\\n[DEL] }\\n[DEL] \\n[DEL] // The first parameter must be present, and must be the specified type\\n     Type[] params = m.getGenericParameterTypes();\\n     TypeToken<?> contextToken = null;\\n     if (params.length > 0) {\\n[DEL] contextToken = TypeToken.of(params[0]);\\n[DEL] }\\n[DEL] if (contextToken == null\\n[DEL] || !contextToken.getRawType().equals(firstContextArg.getRawType())) {\\n[DEL] throw new IllegalStateException(String.format(\\n[DEL] \"%s must take a %s as its first argument\",\\n[DEL] format(m), firstContextArg.getRawType().getSimpleName()));\\n[DEL] }\\n[DEL] AdditionalParameter[] contextInfos = new AdditionalParameter[params.length - 1];\\n[DEL] \\n[DEL] // Fill in the generics in the allExtraContextArgs interface from the types in the\\n[DEL] // Context or ProcessContext OldDoFn.\\n[DEL] ParameterizedType pt = (ParameterizedType) contextToken.getType();\\n[DEL] // We actually want the owner, since ProcessContext and Context are owned by DoFn.\\n[DEL] pt = (ParameterizedType) pt.getOwnerType();\\n[DEL] @SuppressWarnings(\"unchecked\")\\n[DEL] TypeToken<InputT> iActual = (TypeToken<InputT>) TypeToken.of(pt.getActualTypeArguments()[0]);\\n[DEL] @SuppressWarnings(\"unchecked\")\\n[DEL] TypeToken<OutputT> oActual = (TypeToken<OutputT>) TypeToken.of(pt.getActualTypeArguments()[1]);\\n[DEL] \\n[DEL] // All of the remaining parameters must be a super-interface of allExtraContextArgs\\n[DEL] // that is not listed in the EXCLUDED_INTERFACES set.\\n[DEL] for (int i = 1; i < params.length; i++) {\\n[DEL] TypeToken<?> param = TypeToken.of(params[i]);\\n[DEL] \\n[DEL] AdditionalParameter info = contexts.get(param.getRawType());\\n[DEL] if (info == null) {\\n[DEL] throw new IllegalStateException(String.format(\\n[ADD] contextToken = fnClass.resolveType(params[0]);\\n[ADD] }\\n[ADD] checkArgument(\\n[ADD] contextToken != null && contextToken.equals(processContextToken),\\n[ADD] \"%s must take a %s as its first argument\",\\n[ADD] format(m),\\n[ADD] formatType(processContextToken));\\n[ADD] \\n[ADD] List<DoFnSignature.ProcessElementMethod.Parameter> extraParameters = new ArrayList<>();\\n[ADD] TypeToken<?> expectedInputProviderT = inputProviderTypeOf(inputT);\\n[ADD] TypeToken<?> expectedOutputReceiverT = outputReceiverTypeOf(outputT);\\n[ADD] for (int i = 1; i < params.length; ++i) {\\n[ADD] TypeToken<?> param = fnClass.resolveType(params[i]);\\n[ADD] Class<?> rawType = param.getRawType();\\n[ADD] if (rawType.equals(BoundedWindow.class)) {\\n[ADD] checkArgument(\\n[ADD] !extraParameters.contains(DoFnSignature.ProcessElementMethod.Parameter.BOUNDED_WINDOW),\\n[ADD] \"Multiple BoundedWindow parameters in %s\",\\n[ADD] format(m));\\n[ADD] extraParameters.add(DoFnSignature.ProcessElementMethod.Parameter.BOUNDED_WINDOW);\\n[ADD] } else if (rawType.equals(DoFn.InputProvider.class)) {\\n[ADD] checkArgument(\\n[ADD] !extraParameters.contains(DoFnSignature.ProcessElementMethod.Parameter.INPUT_PROVIDER),\\n[ADD] \"Multiple InputProvider parameters in %s\",\\n[ADD] format(m));\\n[ADD] checkArgument(\\n[ADD] param.equals(expectedInputProviderT),\\n[ADD] \"Wrong type of InputProvider parameter for method %s: %s, should be %s\",\\n[ADD] format(m),\\n[ADD] formatType(param),\\n[ADD] formatType(expectedInputProviderT));\\n[ADD] extraParameters.add(DoFnSignature.ProcessElementMethod.Parameter.INPUT_PROVIDER);\\n[ADD] } else if (rawType.equals(DoFn.OutputReceiver.class)) {\\n[ADD] checkArgument(\\n[ADD] !extraParameters.contains(DoFnSignature.ProcessElementMethod.Parameter.OUTPUT_RECEIVER),\\n[ADD] \"Multiple OutputReceiver parameters in %s\",\\n[ADD] format(m));\\n[ADD] checkArgument(\\n[ADD] param.equals(expectedOutputReceiverT),\\n[ADD] \"Wrong type of OutputReceiver parameter for method %s: %s, should be %s\",\\n[ADD] format(m),\\n[ADD] formatType(param),\\n[ADD] formatType(expectedOutputReceiverT));\\n[ADD] extraParameters.add(DoFnSignature.ProcessElementMethod.Parameter.OUTPUT_RECEIVER);\\n[ADD] } else {\\n[ADD] List<String> allowedParamTypes =\\n[ADD] Arrays.asList(formatType(new TypeToken<BoundedWindow>() {}));\\n[ADD] checkArgument(\\n[ADD] false,\\n             \"%s is not a valid context parameter for method %s. Should be one of %s\",\\n[DEL] formatType(param), format(m),\\n[DEL] describeSupportedTypes(contexts, iActual, oActual)));\\n[DEL] }\\n[DEL] \\n[DEL] // If we get here, the class matches, but maybe the generics don\\'t:\\n[DEL] TypeToken<?> expected = info.tokenFor(iActual, oActual);\\n[DEL] if (!expected.isSubtypeOf(param)) {\\n[DEL] throw new IllegalStateException(String.format(\\n[DEL] \"Incompatible generics in context parameter %s for method %s. Should be %s\",\\n[DEL] formatType(param), format(m), formatType(info.tokenFor(iActual, oActual))));\\n[ADD] formatType(param),\\n[ADD] format(m),\\n[ADD] allowedParamTypes);\\n       }\\n[DEL] \\n[DEL] // Register the (now validated) context info\\n[DEL] contextInfos[i - 1] = info;\\n     }\\n[DEL] return ImmutableList.copyOf(contextInfos);\\n[DEL] }\\n[DEL] \\n[DEL] /** Interface for invoking the {@code OldDoFn} processing methods. */\\n[DEL] public interface DoFnInvoker<InputT, OutputT>  {\\n[DEL] /** Invoke {@link OldDoFn#setup} on the bound {@code OldDoFn}. */\\n[DEL] void invokeSetup();\\n[DEL] /** Invoke {@link OldDoFn#startBundle} on the bound {@code OldDoFn}. */\\n[DEL] void invokeStartBundle(\\n[DEL] DoFn<InputT, OutputT>.Context c,\\n[DEL] ExtraContextFactory<InputT, OutputT> extra);\\n[DEL] /** Invoke {@link OldDoFn#finishBundle} on the bound {@code OldDoFn}. */\\n[DEL] void invokeFinishBundle(\\n[DEL] DoFn<InputT, OutputT>.Context c,\\n[DEL] ExtraContextFactory<InputT, OutputT> extra);\\n[DEL] \\n[DEL] /** Invoke {@link OldDoFn#teardown()} on the bound {@code DoFn}. */\\n[DEL] void invokeTeardown();\\n \\n[DEL] /** Invoke {@link OldDoFn#processElement} on the bound {@code OldDoFn}. */\\n[DEL] public void invokeProcessElement(\\n[DEL] DoFn<InputT, OutputT>.ProcessContext c,\\n[DEL] ExtraContextFactory<InputT, OutputT> extra);\\n[ADD] return new DoFnSignature.ProcessElementMethod(m, extraParameters);\\n   }\\n \\n[DEL] /**\\n[DEL] * Implementation of {@link DoFnReflector} for the arbitrary {@link DoFn}.\\n[DEL] */\\n[DEL] private static class GenericDoFnReflector extends DoFnReflector {\\n[DEL] \\n[DEL] private final Method setup;\\n[DEL] private final Method startBundle;\\n[DEL] private final Method processElement;\\n[DEL] private final Method finishBundle;\\n[DEL] private final Method teardown;\\n[DEL] private final List<AdditionalParameter> processElementArgs;\\n[DEL] private final List<AdditionalParameter> startBundleArgs;\\n[DEL] private final List<AdditionalParameter> finishBundleArgs;\\n[DEL] private final Constructor<?> constructor;\\n[DEL] \\n[DEL] private GenericDoFnReflector(\\n[DEL] @SuppressWarnings(\"rawtypes\") Class<? extends DoFn> fn) {\\n[DEL] // Locate the annotated methods\\n[DEL] this.processElement = findAnnotatedMethod(ProcessElement.class, fn, true);\\n[DEL] this.setup = findAnnotatedMethod(Setup.class, fn, false);\\n[DEL] this.startBundle = findAnnotatedMethod(StartBundle.class, fn, false);\\n[DEL] this.finishBundle = findAnnotatedMethod(FinishBundle.class, fn, false);\\n[DEL] this.teardown = findAnnotatedMethod(Teardown.class, fn, false);\\n[DEL] \\n[DEL] // Verify that their method arguments satisfy our conditions.\\n[DEL] this.processElementArgs = verifyProcessMethodArguments(processElement);\\n[DEL] this.startBundleArgs = verifyBundleMethodArguments(startBundle);\\n[DEL] this.finishBundleArgs = verifyBundleMethodArguments(finishBundle);\\n[DEL] verifyLifecycleMethodArguments(setup);\\n[DEL] verifyLifecycleMethodArguments(teardown);\\n[DEL] \\n[DEL] this.constructor = createInvokerConstructor(fn);\\n[DEL] }\\n[ADD] static DoFnSignature.BundleMethod analyzeBundleMethod(\\n[ADD] TypeToken<? extends DoFn> fnToken, Method m, TypeToken<?> inputT, TypeToken<?> outputT) {\\n[ADD] checkArgument(\\n[ADD] void.class.equals(m.getReturnType()), \"%s must have a void return type\", format(m));\\n[ADD] checkArgument(!m.isVarArgs(), \"%s must not have var args\", format(m));\\n \\n[DEL] private static Collection<Method> declaredMethodsWithAnnotation(\\n[DEL] Class<? extends Annotation> anno,\\n[DEL] Class<?> startClass, Class<?> stopClass) {\\n[DEL] Collection<Method> matches = new ArrayList<>();\\n[ADD] TypeToken<?> expectedContextToken = doFnContextTypeOf(inputT, outputT);\\n \\n[DEL] Class<?> clazz = startClass;\\n[DEL] LinkedHashSet<Class<?>> interfaces = new LinkedHashSet<>();\\n[ADD] Type[] params = m.getGenericParameterTypes();\\n[ADD] checkArgument(\\n[ADD] params.length == 1,\\n[ADD] \"%s must have a single argument of type %s\",\\n[ADD] format(m),\\n[ADD] formatType(expectedContextToken));\\n[ADD] TypeToken<?> contextToken = fnToken.resolveType(params[0]);\\n[ADD] checkArgument(\\n[ADD] contextToken.equals(expectedContextToken),\\n[ADD] \"Wrong type of context argument to %s: %s, must be %s\",\\n[ADD] format(m),\\n[ADD] formatType(contextToken),\\n[ADD] formatType(expectedContextToken));\\n[ADD] \\n[ADD] return new DoFnSignature.BundleMethod(m);\\n[ADD] }\\n \\n[DEL] // First, find all declared methods on the startClass and parents (up to stopClass)\\n[DEL] while (clazz != null && !clazz.equals(stopClass)) {\\n[DEL] for (Method method : clazz.getDeclaredMethods()) {\\n[DEL] if (method.isAnnotationPresent(anno)) {\\n[DEL] matches.add(method);\\n[DEL] }\\n[DEL] }\\n[ADD] static DoFnSignature.LifecycleMethod analyzeLifecycleMethod(Method m) {\\n[ADD] checkArgument(\\n[ADD] void.class.equals(m.getReturnType()), \"%s must have a void return type\", format(m));\\n[ADD] checkArgument(\\n[ADD] m.getGenericParameterTypes().length == 0, \"%s must take zero arguments\", format(m));\\n[ADD] return new DoFnSignature.LifecycleMethod(m);\\n[ADD] }\\n \\n[DEL] Collections.addAll(interfaces, clazz.getInterfaces());\\n[ADD] private static Collection<Method> declaredMethodsWithAnnotation(\\n[ADD] Class<? extends Annotation> anno, Class<?> startClass, Class<?> stopClass) {\\n[ADD] Collection<Method> matches = new ArrayList<>();\\n \\n[DEL] clazz = clazz.getSuperclass();\\n[DEL] }\\n[ADD] Class<?> clazz = startClass;\\n[ADD] LinkedHashSet<Class<?>> interfaces = new LinkedHashSet<>();\\n \\n[DEL] // Now, iterate over all the discovered interfaces\\n[DEL] for (Method method : ReflectHelpers.getClosureOfMethodsOnInterfaces(interfaces)) {\\n[ADD] // First, find all declared methods on the startClass and parents (up to stopClass)\\n[ADD] while (clazz != null && !clazz.equals(stopClass)) {\\n[ADD] for (Method method : clazz.getDeclaredMethods()) {\\n         if (method.isAnnotationPresent(anno)) {\\n           matches.add(method);\\n         }\\n       }\\n[DEL] return matches;\\n[DEL] }\\n[DEL] \\n[DEL] private static Method findAnnotatedMethod(\\n[DEL] Class<? extends Annotation> anno, Class<?> fnClazz, boolean required) {\\n[DEL] Collection<Method> matches = declaredMethodsWithAnnotation(\\n[DEL] anno, fnClazz, DoFn.class);\\n[DEL] \\n[DEL] if (matches.size() == 0) {\\n[DEL] if (required) {\\n[DEL] throw new IllegalStateException(String.format(\\n[DEL] \"No method annotated with @%s found in %s\",\\n[DEL] anno.getSimpleName(), fnClazz.getName()));\\n[DEL] } else {\\n[DEL] return null;\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] // If we have at least one match, then either it should be the only match\\n[DEL] // or it should be an extension of the other matches (which came from parent\\n[DEL] // classes).\\n[DEL] Method first = matches.iterator().next();\\n[DEL] for (Method other : matches) {\\n[DEL] if (!first.getName().equals(other.getName())\\n[DEL] || !Arrays.equals(first.getParameterTypes(), other.getParameterTypes())) {\\n[DEL] throw new IllegalStateException(String.format(\\n[DEL] \"Found multiple methods annotated with @%s. [%s] and [%s]\",\\n[DEL] anno.getSimpleName(), format(first), format(other)));\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] // We need to be able to call it. We require it is public.\\n[DEL] if ((first.getModifiers() & Modifier.PUBLIC) == 0) {\\n[DEL] throw new IllegalStateException(format(first) + \" must be public\");\\n[DEL] }\\n[DEL] \\n[DEL] // And make sure its not static.\\n[DEL] if ((first.getModifiers() & Modifier.STATIC) != 0) {\\n[DEL] throw new IllegalStateException(format(first) + \" must not be static\");\\n[DEL] }\\n[DEL] \\n[DEL] return first;\\n[DEL] }\\n \\n[DEL] @Override\\n[DEL] public boolean usesSingleWindow() {\\n[DEL] return usesContext(AdditionalParameter.WINDOW_OF_ELEMENT);\\n[DEL] }\\n[ADD] Collections.addAll(interfaces, clazz.getInterfaces());\\n \\n[DEL] private boolean usesContext(AdditionalParameter param) {\\n[DEL] return processElementArgs.contains(param)\\n[DEL] || (startBundleArgs != null && startBundleArgs.contains(param))\\n[DEL] || (finishBundleArgs != null && finishBundleArgs.contains(param));\\n[ADD] clazz = clazz.getSuperclass();\\n     }\\n \\n[DEL] /**\\n[DEL] * Use ByteBuddy to generate the code for a {@link DoFnInvoker} that invokes the given\\n[DEL] * {@link DoFn}.\\n[DEL] * @param clazz\\n[DEL] * @return\\n[DEL] */\\n[DEL] private Constructor<? extends DoFnInvoker<?, ?>> createInvokerConstructor(\\n[DEL] @SuppressWarnings(\"rawtypes\") Class<? extends DoFn> clazz) {\\n[DEL] \\n[DEL] final TypeDescription clazzDescription = new TypeDescription.ForLoadedType(clazz);\\n[DEL] \\n[DEL] DynamicType.Builder<?> builder = new ByteBuddy()\\n[DEL] // Create subclasses inside the target class, to have access to\\n[DEL] // private and package-private bits\\n[DEL] .with(new SuffixingRandom(\"auxiliary\") {\\n[DEL] @Override\\n[DEL] public String subclass(Generic superClass) {\\n[DEL] return super.name(clazzDescription);\\n[DEL] }\\n[DEL] })\\n[DEL] // Create a subclass of DoFnInvoker\\n[DEL] .subclass(DoFnInvoker.class, Default.NO_CONSTRUCTORS)\\n[DEL] .defineField(FN_DELEGATE_FIELD_NAME, clazz, Visibility.PRIVATE, FieldManifestation.FINAL)\\n[DEL] // Define a constructor to populate fields appropriately.\\n[DEL] .defineConstructor(Visibility.PUBLIC)\\n[DEL] .withParameter(clazz)\\n[DEL] .intercept(new InvokerConstructor())\\n[DEL] // Implement the three methods by calling into the appropriate functions on the fn.\\n[DEL] .method(ElementMatchers.named(\"invokeProcessElement\"))\\n[DEL] .intercept(InvokerDelegation.create(\\n[DEL] processElement, BeforeDelegation.NOOP, processElementArgs))\\n[DEL] .method(ElementMatchers.named(\"invokeStartBundle\"))\\n[DEL] .intercept(InvokerDelegation.create(\\n[DEL] startBundle, BeforeDelegation.INVOKE_PREPARE_FOR_PROCESSING, startBundleArgs))\\n[DEL] .method(ElementMatchers.named(\"invokeFinishBundle\"))\\n[DEL] .intercept(InvokerDelegation.create(finishBundle,\\n[DEL] BeforeDelegation.NOOP,\\n[DEL] finishBundleArgs))\\n[DEL] .method(ElementMatchers.named(\"invokeSetup\"))\\n[DEL] .intercept(InvokerDelegation.create(setup,\\n[DEL] BeforeDelegation.NOOP,\\n[DEL] Collections.<AdditionalParameter>emptyList()))\\n[DEL] .method(ElementMatchers.named(\"invokeTeardown\"))\\n[DEL] .intercept(InvokerDelegation.create(teardown,\\n[DEL] BeforeDelegation.NOOP,\\n[DEL] Collections.<AdditionalParameter>emptyList()));\\n[DEL] \\n[DEL] @SuppressWarnings(\"unchecked\")\\n[DEL] Class<? extends DoFnInvoker<?, ?>> dynamicClass = (Class<? extends DoFnInvoker<?, ?>>) builder\\n[DEL] .make()\\n[DEL] .load(getClass().getClassLoader(), ClassLoadingStrategy.Default.INJECTION)\\n[DEL] .getLoaded();\\n[DEL] try {\\n[DEL] return dynamicClass.getConstructor(clazz);\\n[DEL] } catch (IllegalArgumentException\\n[DEL] | NoSuchMethodException\\n[DEL] | SecurityException e) {\\n[DEL] throw new RuntimeException(e);\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public <InputT, OutputT> DoFnInvoker<InputT, OutputT> bindInvoker(\\n[DEL] DoFn<InputT, OutputT> fn) {\\n[DEL] try {\\n[DEL] @SuppressWarnings(\"unchecked\")\\n[DEL] DoFnInvoker<InputT, OutputT> invoker =\\n[DEL] (DoFnInvoker<InputT, OutputT>) constructor.newInstance(fn);\\n[DEL] return invoker;\\n[DEL] } catch (InstantiationException\\n[DEL] | IllegalAccessException\\n[DEL] | IllegalArgumentException\\n[DEL] | InvocationTargetException\\n[DEL] | SecurityException e) {\\n[DEL] throw new RuntimeException(\"Unable to bind invoker for \" + fn.getClass(), e);\\n[ADD] // Now, iterate over all the discovered interfaces\\n[ADD] for (Method method : ReflectHelpers.getClosureOfMethodsOnInterfaces(interfaces)) {\\n[ADD] if (method.isAnnotationPresent(anno)) {\\n[ADD] matches.add(method);\\n       }\\n     }\\n[ADD] return matches;\\n   }\\n \\n[DEL] private static class ContextAdapter<InputT, OutputT>\\n[DEL] extends DoFn<InputT, OutputT>.Context\\n[DEL] implements DoFn.ExtraContextFactory<InputT, OutputT> {\\n[DEL] \\n[DEL] private OldDoFn<InputT, OutputT>.Context context;\\n[DEL] \\n[DEL] private ContextAdapter(\\n[DEL] DoFn<InputT, OutputT> fn, OldDoFn<InputT, OutputT>.Context context) {\\n[DEL] fn.super();\\n[DEL] this.context = context;\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public PipelineOptions getPipelineOptions() {\\n[DEL] return context.getPipelineOptions();\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public void output(OutputT output) {\\n[DEL] context.output(output);\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public void outputWithTimestamp(OutputT output, Instant timestamp) {\\n[DEL] context.outputWithTimestamp(output, timestamp);\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public <T> void sideOutput(TupleTag<T> tag, T output) {\\n[DEL] context.sideOutput(tag, output);\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\\n[DEL] context.sideOutputWithTimestamp(tag, output, timestamp);\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public BoundedWindow window() {\\n[DEL] // The DoFn doesn\\'t allow us to ask for these outside ProcessElements, so this\\n[DEL] // should be unreachable.\\n[DEL] throw new UnsupportedOperationException(\"Can only get the window in ProcessElements\");\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public DoFn.InputProvider<InputT> inputProvider() {\\n[DEL] throw new UnsupportedOperationException(\"inputProvider() exists only for testing\");\\n[DEL] }\\n[ADD] private static Method findAnnotatedMethod(\\n[ADD] Class<? extends Annotation> anno, Class<?> fnClazz, boolean required) {\\n[ADD] Collection<Method> matches = declaredMethodsWithAnnotation(anno, fnClazz, DoFn.class);\\n \\n[DEL] @Override\\n[DEL] public DoFn.OutputReceiver<OutputT> outputReceiver() {\\n[DEL] throw new UnsupportedOperationException(\"outputReceiver() exists only for testing\");\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] private static class ProcessContextAdapter<InputT, OutputT>\\n[DEL] extends DoFn<InputT, OutputT>.ProcessContext\\n[DEL] implements DoFn.ExtraContextFactory<InputT, OutputT> {\\n[DEL] \\n[DEL] private OldDoFn<InputT, OutputT>.ProcessContext context;\\n[DEL] \\n[DEL] private ProcessContextAdapter(\\n[DEL] DoFn<InputT, OutputT> fn,\\n[DEL] OldDoFn<InputT, OutputT>.ProcessContext context) {\\n[DEL] fn.super();\\n[DEL] this.context = context;\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public PipelineOptions getPipelineOptions() {\\n[DEL] return context.getPipelineOptions();\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public <T> T sideInput(PCollectionView<T> view) {\\n[DEL] return context.sideInput(view);\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public void output(OutputT output) {\\n[DEL] context.output(output);\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public void outputWithTimestamp(OutputT output, Instant timestamp) {\\n[DEL] context.outputWithTimestamp(output, timestamp);\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public <T> void sideOutput(TupleTag<T> tag, T output) {\\n[DEL] context.sideOutput(tag, output);\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\\n[DEL] context.sideOutputWithTimestamp(tag, output, timestamp);\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public InputT element() {\\n[DEL] return context.element();\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public Instant timestamp() {\\n[DEL] return context.timestamp();\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public PaneInfo pane() {\\n[DEL] return context.pane();\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public BoundedWindow window() {\\n[DEL] return context.window();\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public DoFn.InputProvider<InputT> inputProvider() {\\n[DEL] throw new UnsupportedOperationException(\"inputProvider() exists only for testing\");\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public DoFn.OutputReceiver<OutputT> outputReceiver() {\\n[DEL] throw new UnsupportedOperationException(\"outputReceiver() exists only for testing\");\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] public static Class<?> getDoFnClass(OldDoFn<?, ?> fn) {\\n[DEL] if (fn instanceof SimpleDoFnAdapter) {\\n[DEL] return ((SimpleDoFnAdapter<?, ?>) fn).fn.getClass();\\n[DEL] } else {\\n[DEL] return fn.getClass();\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] private static class SimpleDoFnAdapter<InputT, OutputT> extends OldDoFn<InputT, OutputT> {\\n[DEL] \\n[DEL] private final DoFn<InputT, OutputT> fn;\\n[DEL] private transient DoFnInvoker<InputT, OutputT> invoker;\\n[DEL] \\n[DEL] private SimpleDoFnAdapter(DoFnReflector reflector, DoFn<InputT, OutputT> fn) {\\n[DEL] super(fn.aggregators);\\n[DEL] this.fn = fn;\\n[DEL] this.invoker = reflector.bindInvoker(fn);\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public void setup() throws Exception {\\n[DEL] invoker.invokeSetup();\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public void startBundle(OldDoFn<InputT, OutputT>.Context c) throws Exception {\\n[DEL] ContextAdapter<InputT, OutputT> adapter = new ContextAdapter<>(fn, c);\\n[DEL] invoker.invokeStartBundle(adapter, adapter);\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public void finishBundle(OldDoFn<InputT, OutputT>.Context c) throws Exception {\\n[DEL] ContextAdapter<InputT, OutputT> adapter = new ContextAdapter<>(fn, c);\\n[DEL] invoker.invokeFinishBundle(adapter, adapter);\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public void teardown() {\\n[DEL] invoker.invokeTeardown();\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public void processElement(OldDoFn<InputT, OutputT>.ProcessContext c) throws Exception {\\n[DEL] ProcessContextAdapter<InputT, OutputT> adapter = new ProcessContextAdapter<>(fn, c);\\n[DEL] invoker.invokeProcessElement(adapter, adapter);\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] protected TypeDescriptor<InputT> getInputTypeDescriptor() {\\n[DEL] return fn.getInputTypeDescriptor();\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] protected TypeDescriptor<OutputT> getOutputTypeDescriptor() {\\n[DEL] return fn.getOutputTypeDescriptor();\\n[ADD] if (matches.size() == 0) {\\n[ADD] checkArgument(\\n[ADD] !required,\\n[ADD] \"No method annotated with @%s found in %s\",\\n[ADD] anno.getSimpleName(),\\n[ADD] fnClazz.getName());\\n[ADD] return null;\\n     }\\n \\n[DEL] @Override\\n[DEL] public Duration getAllowedTimestampSkew() {\\n[DEL] return fn.getAllowedTimestampSkew();\\n[ADD] // If we have at least one match, then either it should be the only match\\n[ADD] // or it should be an extension of the other matches (which came from parent\\n[ADD] // classes).\\n[ADD] Method first = matches.iterator().next();\\n[ADD] for (Method other : matches) {\\n[ADD] checkArgument(\\n[ADD] first.getName().equals(other.getName())\\n[ADD] && Arrays.equals(first.getParameterTypes(), other.getParameterTypes()),\\n[ADD] \"Found multiple methods annotated with @%s. [%s] and [%s]\",\\n[ADD] anno.getSimpleName(),\\n[ADD] format(first),\\n[ADD] format(other));\\n     }\\n \\n[DEL] @Override\\n[DEL] public void populateDisplayData(DisplayData.Builder builder) {\\n[DEL] builder.include(fn);\\n[DEL] }\\n[ADD] // We need to be able to call it. We require it is public.\\n[ADD] checkArgument(\\n[ADD] (first.getModifiers() & Modifier.PUBLIC) != 0, \"%s must be public\", format(first));\\n \\n[DEL] private void readObject(java.io.ObjectInputStream in)\\n[DEL] throws IOException, ClassNotFoundException {\\n[DEL] in.defaultReadObject();\\n[DEL] invoker = DoFnReflector.of(fn.getClass()).bindInvoker(fn);\\n[DEL] }\\n[DEL] }\\n[ADD] // And make sure its not static.\\n[ADD] checkArgument(\\n[ADD] (first.getModifiers() & Modifier.STATIC) == 0, \"%s must not be static\", format(first));\\n \\n[DEL] private static class WindowDoFnAdapter<InputT, OutputT>\\n[DEL] extends SimpleDoFnAdapter<InputT, OutputT> implements OldDoFn.RequiresWindowAccess {\\n[DEL] \\n[DEL] private WindowDoFnAdapter(DoFnReflector reflector, DoFn<InputT, OutputT> fn) {\\n[DEL] super(reflector, fn);\\n[DEL] }\\n[ADD] return first;\\n   }\\n \\n[DEL] private static enum BeforeDelegation {\\n[DEL] NOOP {\\n[DEL] @Override\\n[DEL] StackManipulation manipulation(\\n[DEL] TypeDescription delegateType, MethodDescription instrumentedMethod, boolean finalStep) {\\n[DEL] Preconditions.checkArgument(!finalStep,\\n[DEL] \"Shouldn\\'t use NOOP delegation if there is nothing to do afterwards.\");\\n[DEL] return StackManipulation.Trivial.INSTANCE;\\n[DEL] }\\n[DEL] },\\n[DEL] INVOKE_PREPARE_FOR_PROCESSING {\\n[DEL] private final Assigner assigner = Assigner.DEFAULT;\\n[DEL] \\n[DEL] @Override\\n[DEL] StackManipulation manipulation(\\n[DEL] TypeDescription delegateType, MethodDescription instrumentedMethod, boolean finalStep) {\\n[DEL] MethodDescription prepareMethod;\\n[DEL] try {\\n[DEL] prepareMethod = new MethodLocator.ForExplicitMethod(\\n[DEL] new MethodDescription.ForLoadedMethod(\\n[DEL] DoFn.class.getDeclaredMethod(\"prepareForProcessing\")))\\n[DEL] .resolve(instrumentedMethod);\\n[DEL] } catch (NoSuchMethodException | SecurityException e) {\\n[DEL] throw new RuntimeException(\"Unable to locate prepareForProcessing method\", e);\\n[DEL] }\\n[DEL] \\n[DEL] if (finalStep) {\\n[DEL] return new StackManipulation.Compound(\\n[DEL] // Invoke the prepare method\\n[DEL] MethodInvoker.Simple.INSTANCE.invoke(prepareMethod),\\n[DEL] // Return from the invokeStartBundle when we\\'re done.\\n[DEL] TerminationHandler.Returning.INSTANCE.resolve(\\n[DEL] assigner, instrumentedMethod, prepareMethod));\\n[DEL] } else {\\n[DEL] return new StackManipulation.Compound(\\n[DEL] // Duplicate the delegation target so that it remains after we invoke prepare\\n[DEL] Duplication.duplicate(delegateType),\\n[DEL] // Invoke the prepare method\\n[DEL] MethodInvoker.Simple.INSTANCE.invoke(prepareMethod),\\n[DEL] // Drop the return value from prepareForProcessing\\n[DEL] TerminationHandler.Dropping.INSTANCE.resolve(\\n[DEL] assigner, instrumentedMethod, prepareMethod));\\n[DEL] }\\n[DEL] }\\n[DEL] };\\n[DEL] \\n[DEL] /**\\n[DEL] * Stack manipulation to perform prior to the delegate call.\\n[DEL] *\\n[DEL] * <ul>\\n[DEL] * <li>Precondition: Stack has the delegate target on top of the stack\\n[DEL] * <li>Postcondition: If finalStep is true, then we\\'ve returned from the method. Otherwise, the\\n[DEL] * stack still has the delegate target on top of the stack.\\n[DEL] * </ul>\\n[DEL] *\\n[DEL] * @param delegateType The type of the delegate target, in case it needs to be duplicated.\\n[DEL] * @param instrumentedMethod The method bing instrumented. Necessary for resolving types and\\n[DEL] *     other information.\\n[DEL] * @param finalStep If true, return from the {@code invokeStartBundle} method after invoking\\n[DEL] * {@code prepareForProcessing} on the delegate.\\n[DEL] */\\n[DEL] abstract StackManipulation manipulation(\\n[DEL] TypeDescription delegateType, MethodDescription instrumentedMethod, boolean finalStep);\\n[DEL] }\\n[DEL] \\n[DEL] /**\\n[DEL] * A byte-buddy {@link Implementation} that delegates a call that receives\\n[DEL] * {@link AdditionalParameter} to the given {@link DoFn} method.\\n[DEL] */\\n[DEL] private static final class InvokerDelegation implements Implementation {\\n[DEL] @Nullable\\n[DEL] private final Method target;\\n[DEL] private final BeforeDelegation before;\\n[DEL] private final List<AdditionalParameter> args;\\n[DEL] private final Assigner assigner = Assigner.DEFAULT;\\n[DEL] private FieldDescription field;\\n[DEL] \\n[DEL] /**\\n[DEL] * Create the {@link InvokerDelegation} for the specified method.\\n[DEL] *\\n[DEL] * @param target the method to delegate to\\n[DEL] * @param isStartBundle whether or not this is the {@code startBundle} call\\n[DEL] * @param args the {@link AdditionalParameter} to be passed to the {@code target}\\n[DEL] */\\n[DEL] private InvokerDelegation(\\n[DEL] @Nullable Method target,\\n[DEL] BeforeDelegation before,\\n[DEL] List<AdditionalParameter> args) {\\n[DEL] this.target = target;\\n[DEL] this.before = before;\\n[DEL] this.args = args;\\n[DEL] }\\n[DEL] \\n[DEL] /**\\n[DEL] * Generate the {@link Implementation} of one of the life-cycle methods of a\\n[DEL] * {@link DoFn}.\\n[DEL] */\\n[DEL] private static Implementation create(\\n[DEL] @Nullable final Method target, BeforeDelegation before, List<AdditionalParameter> args) {\\n[DEL] if (target == null && before == BeforeDelegation.NOOP) {\\n[DEL] // There is no target to call and nothing needs to happen before. Just produce a stub.\\n[DEL] return StubMethod.INSTANCE;\\n[DEL] } else {\\n[DEL] // We need to generate a non-empty method implementation.\\n[DEL] return new InvokerDelegation(target, before, args);\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public InstrumentedType prepare(InstrumentedType instrumentedType) {\\n[DEL] // Remember the field description of the instrumented type.\\n[DEL] field = instrumentedType.getDeclaredFields()\\n[DEL] .filter(ElementMatchers.named(FN_DELEGATE_FIELD_NAME)).getOnly();\\n[DEL] \\n[DEL] // Delegating the method call doesn\\'t require any changes to the instrumented type.\\n[DEL] return instrumentedType;\\n[DEL] }\\n[DEL] \\n[DEL] /**\\n[DEL] * Stack manipulation to push the {@link DoFn} reference stored in the\\n[DEL] * delegate field of the invoker on to the top of the stack.\\n[DEL] *\\n[DEL] * <p>This implementation is derived from the code for\\n[DEL] * {@code MethodCall.invoke(m).onInstanceField(clazz, delegateField)} with two key differences.\\n[DEL] * First, it doesn\\'t add a synthetic field each time, which is critical to avoid duplicate field\\n[DEL] * definitions. Second, it uses the {@link AdditionalParameter} to populate the arguments to the\\n[DEL] * method.\\n[DEL] */\\n[DEL] private StackManipulation pushDelegateField() {\\n[DEL] return new StackManipulation.Compound(\\n[DEL] // Push \"this\" reference to the stack\\n[DEL] MethodVariableAccess.REFERENCE.loadOffset(0),\\n[DEL] // Access the delegate field of the the invoker\\n[DEL] FieldAccess.forField(field).getter());\\n[DEL] }\\n[DEL] \\n[DEL] private StackManipulation pushArgument(\\n[DEL] AdditionalParameter arg, MethodDescription instrumentedMethod) {\\n[DEL] MethodDescription transform = arg.method;\\n[DEL] \\n[DEL] return new StackManipulation.Compound(\\n[DEL] // Push the ExtraContextFactory which must have been argument 2 of the instrumented method\\n[DEL] MethodVariableAccess.REFERENCE.loadOffset(2),\\n[DEL] // Invoke the appropriate method to produce the context argument\\n[DEL] MethodInvocation.invoke(transform));\\n[DEL] }\\n[DEL] \\n[DEL] private StackManipulation invokeTargetMethod(MethodDescription instrumentedMethod) {\\n[DEL] MethodDescription targetMethod = new MethodLocator.ForExplicitMethod(\\n[DEL] new MethodDescription.ForLoadedMethod(target)).resolve(instrumentedMethod);\\n[DEL] ParameterList<?> params = targetMethod.getParameters();\\n[DEL] \\n[DEL] List<StackManipulation> parameters;\\n[DEL] if (!params.isEmpty()) {\\n[DEL] // Instructions to setup the parameters for the call\\n[DEL] parameters = new ArrayList<>(args.size() + 1);\\n[DEL] // 1. The first argument in the delegate method must be the context. This corresponds to\\n[DEL] //    the first argument in the instrumented method, so copy that.\\n[DEL] parameters.add(MethodVariableAccess.of(params.get(0).getType().getSuperClass())\\n[DEL] .loadOffset(1));\\n[DEL] // 2. For each of the extra arguments push the appropriate value.\\n[DEL] for (AdditionalParameter arg : args) {\\n[DEL] parameters.add(pushArgument(arg, instrumentedMethod));\\n[DEL] }\\n[DEL] } else {\\n[DEL] parameters = Collections.emptyList();\\n[DEL] }\\n[DEL] \\n[DEL] return new StackManipulation.Compound(\\n[DEL] // Push the parameters\\n[DEL] new StackManipulation.Compound(parameters),\\n[DEL] // Invoke the target method\\n[DEL] wrapWithUserCodeException(MethodInvoker.Simple.INSTANCE.invoke(targetMethod)),\\n[DEL] // Return from the instrumented method\\n[DEL] TerminationHandler.Returning.INSTANCE.resolve(\\n[DEL] assigner, instrumentedMethod, targetMethod));\\n[DEL] }\\n[DEL] \\n[DEL] /**\\n[DEL] * Wrap a given stack manipulation in a try catch block. Any exceptions thrown within the\\n[DEL] * try are wrapped with a {@link UserCodeException}.\\n[DEL] */\\n[DEL] private StackManipulation wrapWithUserCodeException(\\n[DEL] final StackManipulation tryBody) {\\n[DEL] final MethodDescription createUserCodeException;\\n[DEL] try {\\n[DEL] createUserCodeException = new MethodDescription.ForLoadedMethod(\\n[DEL] UserCodeException.class.getDeclaredMethod(\"wrap\", Throwable.class));\\n[DEL] } catch (NoSuchMethodException | SecurityException e) {\\n[DEL] throw new RuntimeException(\"Unable to find UserCodeException.wrap\", e);\\n[DEL] }\\n[DEL] \\n[DEL] return new StackManipulation() {\\n[DEL] @Override\\n[DEL] public boolean isValid() {\\n[DEL] return tryBody.isValid();\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public Size apply(MethodVisitor mv, Context implementationContext) {\\n[DEL] Label tryBlockStart = new Label();\\n[DEL] Label tryBlockEnd = new Label();\\n[DEL] Label catchBlockStart = new Label();\\n[DEL] Label catchBlockEnd = new Label();\\n[DEL] \\n[DEL] String throwableName =\\n[DEL] new TypeDescription.ForLoadedType(Throwable.class).getInternalName();\\n[DEL] mv.visitTryCatchBlock(tryBlockStart, tryBlockEnd, catchBlockStart, throwableName);\\n[DEL] \\n[DEL] // The try block attempts to perform the expected operations, then jumps to success\\n[DEL] mv.visitLabel(tryBlockStart);\\n[DEL] Size trySize = tryBody.apply(mv, implementationContext);\\n[DEL] mv.visitJumpInsn(Opcodes.GOTO, catchBlockEnd);\\n[DEL] mv.visitLabel(tryBlockEnd);\\n[DEL] \\n[DEL] // The handler wraps the exception, and then throws.\\n[DEL] mv.visitLabel(catchBlockStart);\\n[DEL] // Add the exception to the frame\\n[DEL] mv.visitFrame(Opcodes.F_SAME1,\\n[DEL] // No local variables\\n[DEL] 0, new Object[] {},\\n[DEL] // 1 stack element (the throwable)\\n[DEL] 1, new Object[] { throwableName });\\n[DEL] \\n[DEL] Size catchSize = new StackManipulation.Compound(\\n[DEL] MethodInvocation.invoke(createUserCodeException),\\n[DEL] Throw.INSTANCE)\\n[DEL] .apply(mv, implementationContext);\\n[DEL] \\n[DEL] mv.visitLabel(catchBlockEnd);\\n[DEL] // The frame contents after the try/catch block is the same\\n[DEL] // as it was before.\\n[DEL] mv.visitFrame(Opcodes.F_SAME,\\n[DEL] // No local variables\\n[DEL] 0, new Object[] {},\\n[DEL] // No new stack variables\\n[DEL] 0, new Object[] {});\\n[DEL] \\n[DEL] return new Size(\\n[DEL] trySize.getSizeImpact(),\\n[DEL] Math.max(trySize.getMaximalSize(), catchSize.getMaximalSize()));\\n[DEL] }\\n[DEL] };\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public ByteCodeAppender appender(final Target implementationTarget) {\\n[DEL] return new ByteCodeAppender() {\\n[DEL] @Override\\n[DEL] public Size apply(\\n[DEL] MethodVisitor methodVisitor,\\n[DEL] Context implementationContext,\\n[DEL] MethodDescription instrumentedMethod) {\\n[DEL] StackManipulation.Size size = new StackManipulation.Compound(\\n[DEL] // Put the target on top of the stack\\n[DEL] pushDelegateField(),\\n[DEL] // Do any necessary pre-delegation work\\n[DEL] before.manipulation(field.getType().asErasure(), instrumentedMethod, target == null),\\n[DEL] // Invoke the target method, if there is one. If there wasn\\'t, then isStartBundle was\\n[DEL] // true, and we\\'ve already emitted the appropriate return instructions.\\n[DEL] target != null\\n[DEL] ? invokeTargetMethod(instrumentedMethod)\\n[DEL] : StackManipulation.Trivial.INSTANCE)\\n[DEL] .apply(methodVisitor, implementationContext);\\n[DEL] return new Size(size.getMaximalSize(), instrumentedMethod.getStackSize());\\n[DEL] }\\n[DEL] };\\n[DEL] }\\n[ADD] private static String format(Method m) {\\n[ADD] return ReflectHelpers.CLASS_AND_METHOD_FORMATTER.apply(m);\\n   }\\n \\n[DEL] /**\\n[DEL] * A constructor {@link Implementation} for a {@link DoFnInvoker class}. Produces the byte code\\n[DEL] * for a constructor that takes a single argument and assigns it to the delegate field.\\n[DEL] * {@link AdditionalParameter} to the given {@link DoFn} method.\\n[DEL] */\\n[DEL] private static final class InvokerConstructor implements Implementation {\\n[DEL] @Override\\n[DEL] public InstrumentedType prepare(InstrumentedType instrumentedType) {\\n[DEL] return instrumentedType;\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] public ByteCodeAppender appender(final Target implementationTarget) {\\n[DEL] return new ByteCodeAppender() {\\n[DEL] @Override\\n[DEL] public Size apply(\\n[DEL] MethodVisitor methodVisitor,\\n[DEL] Context implementationContext,\\n[DEL] MethodDescription instrumentedMethod) {\\n[DEL] StackManipulation.Size size = new StackManipulation.Compound(\\n[DEL] // Load the this reference\\n[DEL] MethodVariableAccess.REFERENCE.loadOffset(0),\\n[DEL] // Invoke the super constructor (default constructor of Object)\\n[DEL] MethodInvocation\\n[DEL] .invoke(new TypeDescription.ForLoadedType(Object.class)\\n[DEL] .getDeclaredMethods()\\n[DEL] .filter(ElementMatchers.isConstructor()\\n[DEL] .and(ElementMatchers.takesArguments(0)))\\n[DEL] .getOnly()),\\n[DEL] // Load the this reference\\n[DEL] MethodVariableAccess.REFERENCE.loadOffset(0),\\n[DEL] // Load the delegate argument\\n[DEL] MethodVariableAccess.REFERENCE.loadOffset(1),\\n[DEL] // Assign the delegate argument to the delegate field\\n[DEL] FieldAccess.forField(implementationTarget.getInstrumentedType()\\n[DEL] .getDeclaredFields()\\n[DEL] .filter(ElementMatchers.named(FN_DELEGATE_FIELD_NAME))\\n[DEL] .getOnly()).putter(),\\n[DEL] // Return void.\\n[DEL] MethodReturn.VOID\\n[DEL] ).apply(methodVisitor, implementationContext);\\n[DEL] return new Size(size.getMaximalSize(), instrumentedMethod.getStackSize());\\n[DEL] }\\n[DEL] };\\n[DEL] }\\n[ADD] private static String formatType(TypeToken<?> t) {\\n[ADD] return ReflectHelpers.TYPE_SIMPLE_DESCRIPTION.apply(t.getType());\\n   }\\n }',\n",
              " '     sourceResponseSendErrorType = errorTypeRepository.getErrorType(SOURCE_RESPONSE_SEND).get();\\n     sourceErrorResponseGenerateErrorType = errorTypeRepository.getErrorType(SOURCE_ERROR_RESPONSE_GENERATE).get();\\n     sourceErrorResponseSendErrorType = errorTypeRepository.getErrorType(SOURCE_ERROR_RESPONSE_SEND).get();\\n[ADD] flowBackPressureErrorType = errorTypeRepository.getErrorType(FLOW_BACK_PRESSURE).get();\\n \\n     if (processorInterceptorManager != null) {\\n       processorInterceptorManager.getSourceInterceptorFactories().stream().forEach(interceptorFactory -> {',\n",
              " '     assertEquals(numDocs, currentNumDocs);\\n     assertEquals(\\n         numDocs / NUM_SCIENTISTS,\\n[DEL] countByScientistName(connectionConfiguration, restClient, \"Einstein\"));\\n[ADD] countByScientistName(connectionConfiguration, restClient, \"Einstein\", null));\\n \\n     // Partial update assertions\\n[DEL] assertEquals(numDocs / 2, countByMatch(connectionConfiguration, restClient, \"group\", \"0\"));\\n[DEL] assertEquals(numDocs / 2, countByMatch(connectionConfiguration, restClient, \"group\", \"1\"));\\n[ADD] assertEquals(\\n[ADD] numDocs / 2, countByMatch(connectionConfiguration, restClient, \"group\", \"0\", null, null));\\n[ADD] assertEquals(\\n[ADD] numDocs / 2, countByMatch(connectionConfiguration, restClient, \"group\", \"1\", null, null));\\n[ADD] }\\n[ADD] \\n[ADD] void testWriteWithDocVersion() throws Exception {\\n[ADD] List<ObjectNode> jsonData =\\n[ADD] ElasticsearchIOTestUtils.createJsonDocuments(\\n[ADD] numDocs, ElasticsearchIOTestUtils.InjectionMode.DO_NOT_INJECT_INVALID_DOCS);\\n[ADD] \\n[ADD] List<String> data = new ArrayList<>();\\n[ADD] for (ObjectNode doc : jsonData) {\\n[ADD] doc.put(\"my_version\", \"1\");\\n[ADD] data.add(doc.toString());\\n[ADD] }\\n[ADD] \\n[ADD] insertTestDocuments(connectionConfiguration, data, restClient);\\n[ADD] long currentNumDocs = refreshIndexAndGetCurrentNumDocs(connectionConfiguration, restClient);\\n[ADD] assertEquals(numDocs, currentNumDocs);\\n[ADD] // Check that all docs have the same \"my_version\"\\n[ADD] assertEquals(\\n[ADD] numDocs,\\n[ADD] countByMatch(\\n[ADD] connectionConfiguration, restClient, \"my_version\", \"1\", null, KV.of(1, numDocs)));\\n[ADD] \\n[ADD] Write write =\\n[ADD] ElasticsearchIO.write()\\n[ADD] .withConnectionConfiguration(connectionConfiguration)\\n[ADD] .withIdFn(new ExtractValueFn(\"id\"))\\n[ADD] .withDocVersionFn(new ExtractValueFn(\"my_version\"))\\n[ADD] .withDocVersionType(\"external\");\\n[ADD] \\n[ADD] data = new ArrayList<>();\\n[ADD] for (ObjectNode doc : jsonData) {\\n[ADD] // Set version to larger number than originally set, and larger than next logical version\\n[ADD] // number set by default by ES.\\n[ADD] doc.put(\"my_version\", \"3\");\\n[ADD] data.add(doc.toString());\\n[ADD] }\\n[ADD] \\n[ADD] // Test that documents with lower version are rejected, but rejections ignored when specified\\n[ADD] pipeline.apply(Create.of(data)).apply(write);\\n[ADD] pipeline.run();\\n[ADD] \\n[ADD] currentNumDocs = refreshIndexAndGetCurrentNumDocs(connectionConfiguration, restClient);\\n[ADD] assertEquals(numDocs, currentNumDocs);\\n[ADD] \\n[ADD] // my_version and doc version should have changed\\n[ADD] assertEquals(\\n[ADD] 0,\\n[ADD] countByMatch(\\n[ADD] connectionConfiguration, restClient, \"my_version\", \"1\", null, KV.of(1, numDocs)));\\n[ADD] assertEquals(\\n[ADD] numDocs,\\n[ADD] countByMatch(\\n[ADD] connectionConfiguration, restClient, \"my_version\", \"3\", null, KV.of(3, numDocs)));\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Tests upsert script by adding a group field to each document in the standard test set. The\\n[ADD] * group field is populated as the modulo 2 of the document id allowing for a test to ensure the\\n[ADD] * documents are split into 2 groups.\\n[ADD] */\\n[ADD] void testWriteScriptedUpsert() throws Exception {\\n[ADD] List<String> data =\\n[ADD] ElasticsearchIOTestUtils.createDocuments(\\n[ADD] numDocs, ElasticsearchIOTestUtils.InjectionMode.DO_NOT_INJECT_INVALID_DOCS);\\n[ADD] \\n[ADD] Write write =\\n[ADD] ElasticsearchIO.write()\\n[ADD] .withConnectionConfiguration(connectionConfiguration)\\n[ADD] .withIdFn(new ExtractValueFn(\"id\"))\\n[ADD] .withUpsertScript(SCRIPT_SOURCE);\\n[ADD] \\n[ADD] // Test that documents can be inserted/created by using withUpsertScript\\n[ADD] pipeline.apply(Create.of(data)).apply(write);\\n[ADD] pipeline.run();\\n[ADD] \\n[ADD] // defensive coding to ensure our initial state is as expected\\n[ADD] long currentNumDocs = refreshIndexAndGetCurrentNumDocs(connectionConfiguration, restClient);\\n[ADD] // check we have not unwittingly modified existing behaviour\\n[ADD] assertEquals(numDocs, currentNumDocs);\\n[ADD] assertEquals(\\n[ADD] numDocs / NUM_SCIENTISTS,\\n[ADD] countByScientistName(connectionConfiguration, restClient, \"Einstein\", null));\\n[ADD] \\n[ADD] // All docs should have have group = 0 added by the script upon creation\\n[ADD] assertEquals(\\n[ADD] numDocs, countByMatch(connectionConfiguration, restClient, \"group\", \"0\", null, null));\\n[ADD] \\n[ADD] // Run the same data again. This time, because all docs exist in the index already, scripted\\n[ADD] // updates should happen rather than scripted inserts.\\n[ADD] pipeline.apply(Create.of(data)).apply(write);\\n[ADD] pipeline.run();\\n[ADD] \\n[ADD] currentNumDocs = refreshIndexAndGetCurrentNumDocs(connectionConfiguration, restClient);\\n[ADD] \\n[ADD] // check we have not unwittingly modified existing behaviour\\n[ADD] assertEquals(numDocs, currentNumDocs);\\n[ADD] assertEquals(\\n[ADD] numDocs / NUM_SCIENTISTS,\\n[ADD] countByScientistName(connectionConfiguration, restClient, \"Einstein\", null));\\n[ADD] \\n[ADD] // The script will set either 0 or 1 for the group value on update operations\\n[ADD] assertEquals(\\n[ADD] numDocs / 2, countByMatch(connectionConfiguration, restClient, \"group\", \"0\", null, null));\\n[ADD] assertEquals(\\n[ADD] numDocs / 2, countByMatch(connectionConfiguration, restClient, \"group\", \"1\", null, null));\\n[ADD] }\\n[ADD] \\n[ADD] void testMaxParallelRequestsPerWindow() throws Exception {\\n[ADD] List<String> data =\\n[ADD] ElasticsearchIOTestUtils.createDocuments(\\n[ADD] numDocs, ElasticsearchIOTestUtils.InjectionMode.DO_NOT_INJECT_INVALID_DOCS);\\n[ADD] \\n[ADD] Write write =\\n[ADD] ElasticsearchIO.write()\\n[ADD] .withConnectionConfiguration(connectionConfiguration)\\n[ADD] .withMaxParallelRquestsPerWindow(1);\\n[ADD] \\n[ADD] PCollection<KV<Integer, Long>> batches =\\n[ADD] pipeline\\n[ADD] .apply(Create.of(data))\\n[ADD] .apply(Window.into(new GlobalWindows()))\\n[ADD] .apply(StatefulBatching.fromSpec(write.getBulkIO()))\\n[ADD] .apply(Count.perKey());\\n[ADD] \\n[ADD] // Number of unique keys produced should be number of maxParallelRequestsPerWindow * numWindows\\n[ADD] // There is only 1 request (key) per window, and 1 (global) window ie. one key total\\n[ADD] PAssert.that(batches).containsInAnyOrder(Collections.singletonList(KV.of(0, 1L)));\\n[ADD] \\n[ADD] pipeline.run();\\n[ADD] }\\n[ADD] \\n[ADD] void testMaxBufferingDurationAndMaxParallelRequestsPerWindow() throws Exception {\\n[ADD] List<String> data =\\n[ADD] ElasticsearchIOTestUtils.createDocuments(\\n[ADD] numDocs, ElasticsearchIOTestUtils.InjectionMode.DO_NOT_INJECT_INVALID_DOCS);\\n[ADD] \\n[ADD] Write write =\\n[ADD] ElasticsearchIO.write()\\n[ADD] .withConnectionConfiguration(connectionConfiguration)\\n[ADD] .withMaxBufferingDuration(Duration.standardSeconds(1))\\n[ADD] .withMaxParallelRquestsPerWindow(1);\\n[ADD] \\n[ADD] PCollection<KV<Integer, Long>> batches =\\n[ADD] pipeline\\n[ADD] .apply(Create.of(data))\\n[ADD] .apply(Window.into(new GlobalWindows()))\\n[ADD] .apply(StatefulBatching.fromSpec(write.getBulkIO()))\\n[ADD] .apply(Count.perKey());\\n[ADD] \\n[ADD] // Number of unique keys produced should be number of maxParallelRequestsPerWindow * numWindows\\n[ADD] // There is only 1 request (key) per window, and 1 (global) window ie. one key total\\n[ADD] PAssert.that(batches).containsInAnyOrder(Collections.singletonList(KV.of(0, 1L)));\\n[ADD] \\n[ADD] pipeline.run();\\n   }\\n \\n   /**',\n",
              " ' import org.apache.gobblin.util.ConfigUtils;\\n import org.apache.gobblin.util.reflection.GobblinConstructorUtils;\\n \\n[ADD] import static org.apache.gobblin.service.ExecutionStatus.CANCELLED;\\n import static org.apache.gobblin.service.ExecutionStatus.COMPLETE;\\n import static org.apache.gobblin.service.ExecutionStatus.FAILED;\\n import static org.apache.gobblin.service.ExecutionStatus.RUNNING;',\n",
              " '         metadata.getSourceInfo().\\n             setSourceId(params.getUuid()).\\n             setOwner(ownerId).\\n[DEL] setGroupOwner(Integer.valueOf(params.getOwnerIdGroup()));\\n[ADD] setGroupOwner(getGroupOwner());\\n         metadata.getHarvestInfo().\\n             setHarvested(true).\\n             setUuid(params.getUuid());\\n \\n[DEL] try {\\n[DEL] metadata.getSourceInfo().setGroupOwner(Integer.valueOf(params.getOwnerIdGroup()));\\n[DEL] } catch (NumberFormatException e) {\\n[DEL] }\\n[DEL] \\n[ADD] metadata.getSourceInfo().setGroupOwner(getGroupOwner());\\n         addCategories(metadata, params.getCategories(), localCateg, context, log, null, false);\\n \\n[DEL] metadata = dataMan.insertMetadata(context, metadata, md, true, false, false, UpdateDatestamp.NO, false, false);\\n[ADD] metadata = metadataManager.insertMetadata(context, metadata, md, true, false, false, UpdateDatestamp.NO, false, false);\\n \\n         String id = String.valueOf(metadata.getId());\\n \\n         addPrivileges(id, params.getPrivileges(), localGroups, dataMan, context, log);\\n \\n[DEL] dataMan.indexMetadata(id, Math.random() < 0.01, null);\\n[ADD] metadataIndexer.indexMetadata(id, true, null);\\n         result.addedMetadata++;\\n     }\\n ',\n",
              " '     super( stepMeta, stepDataInterface, copyNr, transMeta, trans );\\n   }\\n \\n[ADD] @Override\\n   public boolean processRow( StepMetaInterface smi, StepDataInterface sdi ) throws KettleException {\\n     meta = (ExecProcessMeta) smi;\\n     data = (ExecProcessData) sdi;',\n",
              " '   public abstract Iterator<JobStatus> getJobStatusesForFlowExecution(String flowName, String flowGroup,\\n       long flowExecutionId);\\n \\n[ADD] public Iterator<JobStatus> getJobStatusesForFlowExecution(String flowName, String flowGroup,\\n[ADD] long flowExecutionId, String jobGroup, String jobName) {\\n[ADD] throw new NotImplementedException(\"getJobStatusesForFlowExecution is not implemented by this JobStatusRetriever.\");\\n[ADD] }\\n[ADD] \\n   /**\\n    * Get the latest {@link JobStatus}es that belongs to the same latest flow execution. Currently, latest flow execution\\n    * is decided by comparing {@link JobStatus#getFlowExecutionId()}.',\n",
              " ' import java.util.concurrent.TimeUnit;\\n import java.util.concurrent.TimeoutException;\\n \\n[DEL] import gobblin.admin.AdminWebServer;\\n import org.apache.commons.configuration.ConfigurationConverter;\\n import org.apache.commons.configuration.PropertiesConfiguration;\\n import org.slf4j.Logger;',\n",
              " ' \\tfinal static String\\t\\t\\t\\tLAUNCH_NAME\\t\\t\\t\\t\\t= \"launch.name\";\\n \\tfinal static String\\t\\t\\t\\tLAUNCH_NOREFERENCES\\t\\t\\t= \"launch.noreferences\";\\n \\tfinal static String\\t\\t\\t\\tLAUNCH_NOTIFICATION_PORT\\t= \"launch.notificationPort\";\\n[ADD] final static String\\t\\t\\t\\tLAUNCH_ACTIVATION_EAGER\\t\\t= \"launch.activation.eager\";\\n \\n \\tpublic final static String[]\\tLAUNCHER_PROPERTY_KEYS\\t\\t= {\\n \\t\\tLAUNCH_SERVICES, LAUNCH_STORAGE_DIR, LAUNCH_KEEP, LAUNCH_NOREFERENCES, LAUNCH_RUNBUNDLES, LAUNCH_SYSTEMPACKAGES,\\n \\t\\tLAUNCH_SYSTEMCAPABILITIES, LAUNCH_SYSTEMPACKAGES, LAUNCH_TRACE, LAUNCH_TIMEOUT, LAUNCH_ACTIVATORS,\\n[DEL] LAUNCH_EMBEDDED, LAUNCH_NAME, LAUNCH_NOREFERENCES, LAUNCH_NOTIFICATION_PORT\\n[ADD] LAUNCH_EMBEDDED, LAUNCH_NAME, LAUNCH_NOREFERENCES, LAUNCH_NOTIFICATION_PORT, LAUNCH_ACTIVATION_EAGER\\n \\t};\\n \\t/**\\n \\t * The command line arguments of the launcher. Launcher are not supposed to',\n",
              " ' \\n   @Parameter\\n   @Optional\\n[DEL] private String ref;\\n[ADD] @XmlHints(allowInlineDefinition = false)\\n[ADD] private T ref;\\n \\n   public ObjectSource() {}\\n \\n[DEL] public ObjectSource(String type, String ref) {\\n[ADD] public ObjectSource(String type, T ref) {\\n     this.type = type;\\n     this.ref = ref;\\n   }\\n \\n[DEL] public final T getObject(MuleContext muleContext) {\\n[DEL] boolean hasType = !StringUtils.isBlank(type);\\n[DEL] boolean hasRef = !StringUtils.isBlank(ref);\\n[DEL] \\n[DEL] checkArgument(!(hasType && hasRef), \"type and ref attributes are mutually exclusive. Please provide only one of them\");\\n[DEL] checkArgument(hasType ^ hasRef, \"One of class or ref attributes are required. Please provide one of them\");\\n[DEL] \\n[DEL] if (hasRef) {\\n[DEL] return doGetByRef(muleContext);\\n[DEL] } else {\\n[DEL] return doGetByClassName();\\n[DEL] }\\n[ADD] public final T getObject() {\\n[ADD] return !isBlank(type) ? doGetByClassName() : ref;\\n   }\\n \\n   protected T doGetByClassName() {',\n",
              " '     private static final int DIALOG_STATISTIC_TYPE = 0;\\n     private static final int DIALOG_CUSTOM_STUDY = 1;\\n     private static final int DIALOG_CUSTOM_STUDY_DETAILS = 2;\\n[ADD] private static final int DIALOG_CUSTOM_STUDY_TAGS = 3;\\n \\n     private int mCustomDialogChoice;\\n     ',\n",
              " '   public static final String OPTIMIZED_CHECK_ENABLED = \"hiveRegister.cacheDbTableExistence\";\\n \\n   private final HiveMetastoreClientPool clientPool;\\n[DEL] private final HiveLock locks = new HiveLock();\\n[ADD] private final HiveLock locks;\\n   private final EventSubmitter eventSubmitter;\\n   private final MetricContext metricContext;\\n ',\n",
              " '             return config.serverLifecycle().equals(container.getLifecycle())\\n                     && config.mavenProjectDirProvider().equals(container.getMavenProjectDirProvider())\\n                     && config.pluginJarsProvider().equals(container.getPluginJarsProvider())\\n[DEL] && Sets.newHashSet(config.esVersions()).contains(container.getEsVersion())\\n[DEL] && Sets.newHashSet(config.mongoVersions()).contains(container.getMongoVersion());\\n[ADD] && getSearchSevers(config).contains(container.getEsVersion())\\n[ADD] && getMongodbServers(config).contains(container.getMongoVersion());\\n         } else {\\n             // Annotation should be present!\\n             return false;\\n         }\\n     }\\n \\n[ADD] private Set<MongodbServer> getMongodbServers(ContainerMatrixTestsConfiguration config) {\\n[ADD] if (config.mongoVersions().length == 0) {\\n[ADD] return Sets.newHashSet(MongodbServer.DEFAULT_VERSION);\\n[ADD] } else {\\n[ADD] return Sets.newHashSet(config.mongoVersions());\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private Set<SearchVersion> getSearchSevers(ContainerMatrixTestsConfiguration config) {\\n[ADD] if (config.searchVersions().length == 0) {\\n[ADD] return Sets.newHashSet(SearchServer.DEFAULT_VERSION.getSearchVersion());\\n[ADD] } else {\\n[ADD] return Stream.of(config.searchVersions()).map(SearchServer::getSearchVersion).collect(Collectors.toSet());\\n[ADD] }\\n[ADD] }\\n[ADD] \\n     @Override\\n     public boolean test(Class<?> candidate) {\\n         if (AnnotationSupport.isAnnotated(candidate, ContainerMatrixTestsConfiguration.class)) {',\n",
              " '   }\\n \\n   private void initPolicyContext() {\\n[DEL] if (policyContext == null) {\\n[DEL] ArtifactContextBuilder artifactBuilder =\\n[DEL] newBuilder().setArtifactType(APP)\\n[DEL] .setArtifactProperties(new HashMap<>(parametrization.getParameters()))\\n[DEL] .setArtifactName(parametrization.getId())\\n[DEL] .setConfigurationFiles(getResourcePaths(template.getDescriptor().getConfigResourceFiles()))\\n[DEL] .setExecutionClassloader(template.getArtifactClassLoader().getClassLoader())\\n[DEL] .setServiceRepository(serviceRepository)\\n[DEL] .setClassLoaderRepository(classLoaderRepository)\\n[DEL] .setArtifactPlugins(artifactPlugins)\\n[DEL] .setExtensionManagerFactory(new PolicyTemplateExtensionManagerFactory(application, extensionModelLoaderRepository,\\n[DEL] artifactPlugins,\\n[DEL] new DefaultExtensionManagerFactory()));\\n[DEL] \\n[DEL] artifactBuilder.withServiceConfigurator(customizationService -> customizationService\\n[DEL] .overrideDefaultServiceImpl(MuleProperties.OBJECT_POLICY_MANAGER_STATE_HANDLER,\\n[DEL] application.getMuleContext().getRegistry()\\n[DEL] .lookupObject(MuleProperties.OBJECT_POLICY_MANAGER_STATE_HANDLER)));\\n[DEL] \\n[DEL] try {\\n[DEL] policyContext = artifactBuilder.build();\\n[DEL] policyContext.getMuleContext().start();\\n[DEL] } catch (MuleException e) {\\n[DEL] throw new IllegalStateException(\"Cannot create artifact context for the policy instance\", e);\\n[DEL] }\\n[ADD] ArtifactContextBuilder artifactBuilder =\\n[ADD] newBuilder().setArtifactType(APP)\\n[ADD] .setArtifactProperties(new HashMap<>(parametrization.getParameters()))\\n[ADD] .setArtifactName(parametrization.getId())\\n[ADD] .setConfigurationFiles(getResourcePaths(template.getDescriptor().getConfigResourceFiles()))\\n[ADD] .setExecutionClassloader(template.getArtifactClassLoader().getClassLoader())\\n[ADD] .setServiceRepository(serviceRepository)\\n[ADD] .setClassLoaderRepository(classLoaderRepository)\\n[ADD] .setArtifactPlugins(artifactPlugins)\\n[ADD] .setExtensionManagerFactory(new PolicyTemplateExtensionManagerFactory(application, extensionModelLoaderRepository,\\n[ADD] artifactPlugins,\\n[ADD] new DefaultExtensionManagerFactory()));\\n[ADD] \\n[ADD] artifactBuilder.withServiceConfigurator(customizationService -> customizationService\\n[ADD] .overrideDefaultServiceImpl(MuleProperties.OBJECT_POLICY_MANAGER_STATE_HANDLER,\\n[ADD] application.getMuleContext().getRegistry()\\n[ADD] .lookupObject(MuleProperties.OBJECT_POLICY_MANAGER_STATE_HANDLER)));\\n[ADD] \\n[ADD] try {\\n[ADD] policyContext = artifactBuilder.build();\\n[ADD] policyContext.getMuleContext().start();\\n[ADD] } catch (MuleException e) {\\n[ADD] throw new IllegalStateException(\"Cannot create artifact context for the policy instance\", e);\\n     }\\n   }\\n ',\n",
              " '   }\\n \\n   public ObjectRevision getObjectRevision() {\\n[DEL] return objectRevision;\\n[ADD] lock.readLock().lock();\\n[ADD] try {\\n[ADD] return objectRevision;\\n[ADD] } finally {\\n[ADD] lock.readLock().unlock();\\n[ADD] }\\n   }\\n \\n   public void setObjectRevision( ObjectRevision objectRevision ) {\\n[ADD] lock.writeLock().lock();\\n     this.objectRevision = objectRevision;\\n[ADD] lock.writeLock().unlock();\\n   }\\n \\n   public String getDescription() {',\n",
              " '             }\\n             return virtualSize;\\n         } catch (Exception e) {\\n[DEL] String msg = \"Unable to parse OVF XML document to get the virtual disk size due to\" + e;\\n[ADD] String msg = \"getTemplateVirtualSize: Unable to parse OVF XML document \" + templatePath + \" to get the virtual disk \" + templateName + \" size due to \" + e;\\n[ADD] s_logger.error(msg);\\n[ADD] throw new InternalErrorException(msg);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] public Pair<Long, Long> getDiskDetails(String ovfFilePath, String diskName) throws InternalErrorException {\\n[ADD] long virtualSize = 0;\\n[ADD] long fileSize = 0;\\n[ADD] String fileId = null;\\n[ADD] try {\\n[ADD] Document ovfDoc = null;\\n[ADD] ovfDoc = DocumentBuilderFactory.newInstance().newDocumentBuilder().parse(new File(ovfFilePath));\\n[ADD] NodeList disks = ovfDoc.getElementsByTagName(\"Disk\");\\n[ADD] NodeList files = ovfDoc.getElementsByTagName(\"File\");\\n[ADD] for (int j = 0; j < files.getLength(); j++) {\\n[ADD] Element file = (Element)files.item(j);\\n[ADD] if (file.getAttribute(\"ovf:href\").equals(diskName)) {\\n[ADD] fileSize = Long.parseLong(file.getAttribute(\"ovf:size\"));\\n[ADD] fileId = file.getAttribute(\"ovf:id\");\\n[ADD] break;\\n[ADD] }\\n[ADD] }\\n[ADD] for (int i = 0; i < disks.getLength(); i++) {\\n[ADD] Element disk = (Element)disks.item(i);\\n[ADD] if (disk.getAttribute(\"ovf:fileRef\").equals(fileId)) {\\n[ADD] virtualSize = Long.parseLong(disk.getAttribute(\"ovf:capacity\"));\\n[ADD] String allocationUnits = disk.getAttribute(\"ovf:capacityAllocationUnits\");\\n[ADD] if ((virtualSize != 0) && (allocationUnits != null)) {\\n[ADD] long units = 1;\\n[ADD] if (allocationUnits.equalsIgnoreCase(\"KB\") || allocationUnits.equalsIgnoreCase(\"KiloBytes\") || allocationUnits.equalsIgnoreCase(\"byte * 2^10\")) {\\n[ADD] units = 1024;\\n[ADD] } else if (allocationUnits.equalsIgnoreCase(\"MB\") || allocationUnits.equalsIgnoreCase(\"MegaBytes\") || allocationUnits.equalsIgnoreCase(\"byte * 2^20\")) {\\n[ADD] units = 1024 * 1024;\\n[ADD] } else if (allocationUnits.equalsIgnoreCase(\"GB\") || allocationUnits.equalsIgnoreCase(\"GigaBytes\") || allocationUnits.equalsIgnoreCase(\"byte * 2^30\")) {\\n[ADD] units = 1024 * 1024 * 1024;\\n[ADD] }\\n[ADD] virtualSize = virtualSize * units;\\n[ADD] } else {\\n[ADD] throw new InternalErrorException(\"Failed to read capacity and capacityAllocationUnits from the OVF file: \" + ovfFilePath);\\n[ADD] }\\n[ADD] break;\\n[ADD] }\\n[ADD] }\\n[ADD] return new Pair<Long, Long>(virtualSize, fileSize);\\n[ADD] } catch (Exception e) {\\n[ADD] String msg = \"getDiskDetails: Unable to parse OVF XML document \" + ovfFilePath + \" to get the virtual disk \" + diskName + \" size due to \" + e;\\n             s_logger.error(msg);\\n             throw new InternalErrorException(msg);\\n         }',\n",
              " ' \\n     protected static final int MENU_DISABLED = 3;\\n \\n[ADD] // Deckname\\n[ADD] private String title;\\n[ADD] \\n[ADD] // Card counts\\n[ADD] private SpannableString newCount;\\n[ADD] private SpannableString lrnCount;\\n[ADD] private SpannableString revCount;\\n[ADD] \\n[ADD] // ETA\\n[ADD] private int eta;\\n[ADD] \\n[ADD] // Card Mark\\n[ADD] private boolean isCardMarked;\\n[ADD] \\n[ADD] // card flag status\\n[ADD] private int currentCardFlagStatus;\\n \\n     /**\\n      * Broadcast that informs us when the sd card is about to be unmounted',\n",
              " ' import com.alibaba.dubbo.rpc.Invoker;\\n import com.alibaba.dubbo.rpc.RpcContext;\\n import com.alibaba.dubbo.rpc.RpcException;\\n[DEL] import com.alibaba.dubbo.rpc.cluster.Router;\\n[ADD] import com.alibaba.dubbo.rpc.cluster.router.AbstractRouter;\\n \\n import javax.script.Bindings;\\n import javax.script.Compilable;',\n",
              " '       + \"{\\\\\"name\\\\\":\\\\\"driver\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"},{\\\\\"name\\\\\":\\\\\"fare\\\\\",\\\\\"type\\\\\":\\\\\"double\\\\\"},{\\\\\"name\\\\\": \\\\\"_hoodie_is_deleted\\\\\", \\\\\"type\\\\\": \\\\\"boolean\\\\\", \\\\\"default\\\\\": false}]}\";\\n \\n   public static final String NULL_SCHEMA = Schema.create(Schema.Type.NULL).toString();\\n[DEL] public static final String TRIP_HIVE_COLUMN_TYPES = \"bigint,string,string,string,double,double,double,double,int,bigint,float,binary,int,bigint,decimal(10,6),\"\\n[ADD] public static final String TRIP_HIVE_COLUMN_TYPES = \"bigint,string,string,string,string,double,double,double,double,int,bigint,float,binary,int,bigint,decimal(10,6),\"\\n       + \"map<string,string>,struct<amount:double,currency:string>,array<struct<amount:double,currency:string>>,boolean\";\\n \\n ',\n",
              " '                 }\\n \\n                 if (CollectionUtils.isNotEmpty(wrapperClassesList)) {\\n[DEL] for (Class<?> wrapperClass : wrapperClassesList) {\\n[DEL] Wrapper wrapper = wrapperClass.getAnnotation(Wrapper.class);\\n[DEL] if (wrapper == null\\n[DEL] || (ArrayUtils.contains(wrapper.matches(), name) && !ArrayUtils.contains(wrapper.mismatches(), name))) {\\n[DEL] instance = injectExtension((T) wrapperClass.getConstructor(type).newInstance(instance));\\n[DEL] instance = postProcessAfterInitialization(instance, name);\\n[DEL] }\\n[ADD] wrapperClassesList = wrapperClassesList.stream().filter(wrapperClz -> {\\n[ADD] Wrapper wrapper = wrapperClz.getAnnotation(Wrapper.class);\\n[ADD] return (wrapper == null) ||\\n[ADD] ((ArrayUtils.isEmpty(wrapper.matches()) || ArrayUtils.contains(wrapper.matches(), name)) &&\\n[ADD] !ArrayUtils.contains(wrapper.mismatches(), name));\\n[ADD] }).collect(Collectors.toList());\\n[ADD] for (Class<?> wrapperClz : wrapperClassesList) {\\n[ADD] instance = injectExtension((T) wrapperClz.getConstructor(type).newInstance(instance));\\n[ADD] instance = postProcessAfterInitialization(instance, name);\\n                     }\\n                 }\\n             }',\n",
              " ' \\n     assertStackElements(stackToAssert,\\n                         isFlowStackElement(\"subFlow\",\\n[DEL] \"/subFlowDynamicWithScatterGatherChain/processors/0/1/0/subFlow/subprocessors/0\"),\\n[ADD] \"subFlow/processors/0\"),\\n                         isFlowStackElement(\"subFlowDynamicWithScatterGatherChain\",\\n[DEL] \"/subFlowDynamicWithScatterGatherChain/processors/0/1/0\"));\\n[ADD] \"subFlowDynamicWithScatterGatherChain/processors/0/route/1/processors/0/processors/0\"));\\n   }\\n \\n   @Test',\n",
              " '     FunctionParameter second = dataType.getParameters().get(1);\\n     assertThat(second.getName(), is(\"snd\"));\\n     assertThat(second.getType(), equalTo(OBJECT));\\n[DEL] //Default\\n[ADD] // Default\\n     assertThat(second.getDefaultValueResolver().getDefaultValue(builder().build()), is(\"wow\"));\\n   }\\n ',\n",
              " '       String outputFormatClassName = HoodieInputFormatUtils.getOutputFormatClassName(baseFileFormat);\\n       String serDeFormatClassName = HoodieInputFormatUtils.getSerDeClassName(baseFileFormat);\\n \\n[ADD] Map<String, String> serdeProperties = ConfigUtils.toMap(cfg.serdeProperties);\\n[ADD] \\n[ADD] // The serdeProperties is non-empty only for spark sync meta data currently.\\n[ADD] if (!serdeProperties.isEmpty()) {\\n[ADD] String queryTypeKey = serdeProperties.remove(ConfigUtils.SPARK_QUERY_TYPE_KEY);\\n[ADD] String queryAsROKey = serdeProperties.remove(ConfigUtils.SPARK_QUERY_AS_RO_KEY);\\n[ADD] String queryAsRTKey = serdeProperties.remove(ConfigUtils.SPARK_QUERY_AS_RT_KEY);\\n[ADD] \\n[ADD] if (queryTypeKey != null && queryAsROKey != null && queryAsRTKey != null) {\\n[ADD] if (readAsOptimized) { // read optimized\\n[ADD] serdeProperties.put(queryTypeKey, queryAsROKey);\\n[ADD] } else { // read snapshot\\n[ADD] serdeProperties.put(queryTypeKey, queryAsRTKey);\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n       // Custom serde will not work with ALTER TABLE REPLACE COLUMNS\\n       // https://github.com/apache/hive/blob/release-1.1.0/ql/src/java/org/apache/hadoop/hive\\n       // /ql/exec/DDLTask.java#L3488\\n       hoodieHiveClient.createTable(tableName, schema, inputFormatClassName,\\n[DEL] outputFormatClassName, serDeFormatClassName, ConfigUtils.toMap(cfg.serdeProperties), ConfigUtils.toMap(cfg.tableProperties));\\n[ADD] outputFormatClassName, serDeFormatClassName, serdeProperties, ConfigUtils.toMap(cfg.tableProperties));\\n     } else {\\n       // Check if the table schema has evolved\\n       Map<String, String> tableSchema = hoodieHiveClient.getTableSchema(tableName);',\n",
              " \"     }\\n   });\\n   if (appElement) {\\n[ADD] if (!isAutoBootstrapAllowed) {\\n[ADD] window.console.error('Angular: disabling automatic bootstrap. <script> protocol indicates ' +\\n[ADD] 'an extension, document.location.href does not match.');\\n[ADD] return;\\n[ADD] }\\n     config.strictDi = getNgAttribute(appElement, 'strict-di') !== null;\\n     bootstrap(appElement, module ? [module] : [], config);\\n   }\",\n",
              " '       PrintFormalsWIndent(pnode->sxFnc.pnodeParams, indentAmt + INDENT_SIZE);\\n       PrintPnodeWIndent(pnode->sxFnc.pnodeRest, indentAmt + INDENT_SIZE);\\n       PrintPnodeWIndent(pnode->sxFnc.pnodeBody, indentAmt + INDENT_SIZE);\\n[ADD] if (pnode->sxFnc.pnodeBody == nullptr)\\n[ADD] {\\n[ADD] Output::Print(_u(\"[%4d, %4d): \"), pnode->ichMin, pnode->ichLim);\\n[ADD] Indent(indentAmt + INDENT_SIZE);\\n[ADD] Output::Print(_u(\"<parse deferred body>\\\\n\"));\\n[ADD] }\\n       break;\\n       //PTNODE(knopProg       , \"program\"    ,None    ,Fnc  ,fnopNone)\\n   case knopProg:',\n",
              " '     this.nonBlocking = rewriteEvent.isAllowNonBlocking() || isFlowConstructNonBlockingProcessingStrategy();\\n     this.flowCallStack =\\n         rewriteEvent.getFlowCallStack() == null ? new DefaultFlowCallStack() : rewriteEvent.getFlowCallStack().clone();\\n[DEL] this.error = rewriteEvent.getError();\\n[ADD] this.error = rewriteEvent.getError().orElseGet(() -> null);\\n   }\\n \\n   // Use this constructor from the builder',\n",
              " '         return result;\\n     }\\n \\n[ADD] @Override\\n[ADD] public boolean addNewDisk(NicProfile nicProfile, Network network, VirtualMachineProfile vm, DeployDestination dest) throws ResourceUnavailableException {\\n[ADD] if (_networkModel.getUserDataUpdateProvider(network).getProvider().equals(Provider.VirtualRouter) && vm.getVirtualMachine().getType() == VirtualMachine.Type.User) {\\n[ADD] VirtualMachine uvm = vm.getVirtualMachine();\\n[ADD] UserVmVO destVm = _userVmDao.findById(uvm.getId());\\n[ADD] VirtualMachineProfile profile = null;\\n[ADD] \\n[ADD] if (destVm != null) {\\n[ADD] destVm.setHostId(dest.getHost().getId());\\n[ADD] _userVmDao.update(uvm.getId(), destVm);\\n[ADD] profile = new VirtualMachineProfileImpl(destVm);\\n[ADD] profile.setDisks(vm.getDisks());\\n[ADD] profile.setNics(vm.getNics());\\n[ADD] profile.setVmData(vm.getVmData());\\n[ADD] } else {\\n[ADD] profile = vm;\\n[ADD] }\\n[ADD] \\n[ADD] updateUserVmData(nicProfile, network, profile);\\n[ADD] if (destVm != null) {\\n[ADD] destVm.setHostId(uvm.getHostId());\\n[ADD] _userVmDao.update(uvm.getId(), destVm);\\n[ADD] }\\n[ADD] }\\n[ADD] return true;\\n[ADD] }\\n[ADD] \\n     @Override\\n     public boolean saveUserData(final Network network, final NicProfile nic, final VirtualMachineProfile vm) throws ResourceUnavailableException {\\n         if (!canHandle(network, null)) {',\n",
              " '                     ZetaSqlCalciteTranslationUtils.toSimpleRelDataType(kind, rexBuilder()));\\n         break;\\n       case TYPE_DOUBLE:\\n[ADD] // Cannot simply call makeApproxLiteral() for ZetaSQL DOUBLE type because positive infinity,\\n[ADD] // negative infinity and Nan cannot be directly converted to BigDecimal. So we create three\\n[ADD] // wrapper functions here for these three cases such that we can later recognize it and\\n[ADD] // customize its unparsing in BeamBigQuerySqlDialect.\\n         double val = value.getDoubleValue();\\n[DEL] if (Double.isInfinite(val) || Double.isNaN(val)) {\\n[DEL] throw new UnsupportedOperationException(\"Does not support Infinite or NaN literals.\");\\n[ADD] String wrapperFun = null;\\n[ADD] if (val == Double.POSITIVE_INFINITY) {\\n[ADD] wrapperFun = BeamBigQuerySqlDialect.DOUBLE_POSITIVE_INF_FUNCTION;\\n[ADD] } else if (val == Double.NEGATIVE_INFINITY) {\\n[ADD] wrapperFun = BeamBigQuerySqlDialect.DOUBLE_NEGATIVE_INF_FUNCTION;\\n[ADD] } else if (Double.isNaN(val)) {\\n[ADD] wrapperFun = BeamBigQuerySqlDialect.DOUBLE_NAN_FUNCTION;\\n[ADD] }\\n[ADD] \\n[ADD] RelDataType returnType =\\n[ADD] ZetaSqlCalciteTranslationUtils.toSimpleRelDataType(kind, rexBuilder());\\n[ADD] if (wrapperFun == null) {\\n[ADD] ret = rexBuilder().makeApproxLiteral(new BigDecimal(val), returnType);\\n[ADD] } else if (BeamBigQuerySqlDialect.DOUBLE_NAN_FUNCTION.equals(wrapperFun)) {\\n[ADD] // TODO[BEAM-10550]: Update the temporary workaround below after vendored Calcite version\\n[ADD] \\n[ADD] // Adding an additional random parameter for the wrapper function of NaN, to avoid\\n[ADD] // triggering Calcite operation simplification. (e.g. \\'NaN == NaN\\' would be simplify to\\n[ADD] // \\'null or NaN is not null\\' in Calcite. This would miscalculate the expression to be\\n[ADD] // true, which should be false.)\\n[ADD] ret =\\n[ADD] rexBuilder()\\n[ADD] .makeCall(\\n[ADD] SqlOperators.createZetaSqlFunction(wrapperFun, returnType.getSqlTypeName()),\\n[ADD] ImmutableList.of(\\n[ADD] rexBuilder()\\n[ADD] .makeApproxLiteral(new BigDecimal(Math.random()), returnType)));\\n[ADD] ;\\n[ADD] } else {\\n[ADD] ret =\\n[ADD] rexBuilder()\\n[ADD] .makeCall(\\n[ADD] SqlOperators.createZetaSqlFunction(wrapperFun, returnType.getSqlTypeName()));\\n         }\\n[DEL] ret =\\n[DEL] rexBuilder()\\n[DEL] .makeApproxLiteral(\\n[DEL] new BigDecimal(val),\\n[DEL] ZetaSqlCalciteTranslationUtils.toSimpleRelDataType(kind, rexBuilder()));\\n         break;\\n       case TYPE_STRING:\\n         // has to allow CAST because Calcite create CHAR type first and does a CAST to VARCHAR.',\n",
              " '       callbacks.output(value);\\n     }\\n \\n[ADD] @Override\\n[ADD] public void outputRetraction(OutputT value) {\\n[ADD] callbacks.outputRetraction(value);\\n[ADD] }\\n[ADD] \\n     @Override\\n     public Timers timers() {\\n       return timers;',\n",
              " ' \\tString name = clz.getName();\\n \\tint index = name.lastIndexOf(\\'.\\');\\n \\tif (index >= 0) {\\n[DEL] return name.substring(0, index);\\n[ADD] return name.substring(0, index).intern();\\n \\t}\\n \\treturn \"\"; //$NON-NLS-1$\\n }',\n",
              " '   private static void recreateMarkerFiles(final String commitInstantTime,\\n                                           HoodieSparkTable table,\\n                                           HoodieEngineContext context,\\n[ADD] MarkerType ioMode,\\n                                           int parallelism) throws HoodieRollbackException {\\n     try {\\n       // fetch hoodie instant',\n",
              " ' \\n \\tfinal void initializeClassIfRequired() {\\n \\t\\tif (Modifier.isStatic(this.rawModifiers)) {\\n[DEL] long defcClass = getJ9ClassFromClass(defc);\\n[DEL] Unsafe unsafe = getUnsafe();\\n[DEL] int initStatus = 0;\\n[DEL] if (4 == VM.ADDRESS_SIZE) {\\n[DEL] initStatus = unsafe.getInt(defcClass + com.ibm.oti.vm.VM.J9CLASS_INITIALIZE_STATUS_OFFSET);\\n[DEL] } else {\\n[DEL] initStatus = (int)unsafe.getLong(defcClass + com.ibm.oti.vm.VM.J9CLASS_INITIALIZE_STATUS_OFFSET);\\n[DEL] }\\n[DEL] if (initStatus != 1) {\\n[DEL] unsafe.ensureClassInitialized(defc);\\n[ADD] if (JITHELPERS.getClassInitializeStatus(defc) != VM.J9CLASS_INIT_SUCCEEDED) {\\n[ADD] UNSAFE.ensureClassInitialized(defc);\\n \\t\\t\\t}\\n \\t\\t}\\n \\t}',\n",
              " '         return metadataErrors;\\n     }\\n \\n[DEL] public synchronized void addMetadataError(int metadataId, Exception error) {\\n[ADD] public synchronized void addMetadataError(int metadataId, String metadataUUID, boolean draft, boolean approved,\\n[ADD] Exception error) {\\n[ADD] Report errorReport = new ErrorReport(error);\\n[ADD] errorReport.setUuid(metadataUUID);\\n[ADD] errorReport.setDraft(draft);\\n[ADD] errorReport.setApproved(approved);\\n         if (this.metadataErrors.get(metadataId) == null) {\\n             List<Report> errors = new ArrayList<>();\\n[DEL] errors.add(new ErrorReport(error));\\n[ADD] errors.add(errorReport);\\n             this.metadataErrors.put(metadataId, errors);\\n         } else {\\n[DEL] this.metadataErrors.get(metadataId).add(new ErrorReport(error));\\n[ADD] this.metadataErrors.get(metadataId).add(errorReport);\\n         }\\n     }\\n \\n[DEL] public synchronized void addMetadataError(int metadataId, String error) {\\n[ADD] public void addMetadataError(AbstractMetadata metadata, Exception error) {\\n[ADD] addMetadataError(metadata.getId(), metadata.getUuid(), isMetadataDraft(metadata.getId()),\\n[ADD] isMetadataApproved(metadata.getId()), error);\\n[ADD] }\\n[ADD] \\n[ADD] public synchronized void addMetadataError(int metadataId, String metadataUUID, boolean draft, boolean approved,\\n[ADD] String error) {\\n[ADD] Report errorReport = new ErrorReport(error);\\n[ADD] errorReport.setUuid(metadataUUID);\\n[ADD] errorReport.setDraft(draft);\\n[ADD] errorReport.setApproved(approved);\\n         if (this.metadataErrors.get(metadataId) == null) {\\n             List<Report> errors = new ArrayList<>();\\n[DEL] errors.add(new ErrorReport(error));\\n[ADD] errors.add(errorReport);\\n             this.metadataErrors.put(metadataId, errors);\\n         } else {\\n[DEL] this.metadataErrors.get(metadataId).add(new ErrorReport(error));\\n[ADD] this.metadataErrors.get(metadataId).add(errorReport);\\n         }\\n     }\\n \\n[ADD] public void addMetadataError(AbstractMetadata metadata, String error) {\\n[ADD] addMetadataError(metadata.getId(), metadata.getUuid(), isMetadataDraft(metadata.getId()),\\n[ADD] isMetadataApproved(metadata.getId()), error);\\n[ADD] }\\n[ADD] \\n     @XmlElement(name = \"infos\")\\n     public Map<Integer, List<InfoReport>> getMetadataInfos() {\\n         return metadataInfos;\\n     }\\n \\n[DEL] public void addMetadataInfos(int metadataId, String message) {\\n[ADD] public void addMetadataInfos(int metadataId, String metadataUUID, boolean draft, boolean approved, String message) {\\n[ADD] InfoReport infoReport = new InfoReport(message);\\n[ADD] infoReport.setUuid(metadataUUID);\\n[ADD] infoReport.setDraft(draft);\\n[ADD] infoReport.setApproved(approved);\\n         if (this.metadataInfos.get(metadataId) == null) {\\n             List<InfoReport> infos = new ArrayList<>();\\n[DEL] infos.add(new InfoReport(message));\\n[ADD] infos.add(infoReport);\\n             this.metadataInfos.put(metadataId, infos);\\n         } else {\\n[DEL] this.metadataInfos.get(metadataId).add(new InfoReport(message));\\n[ADD] this.metadataInfos.get(metadataId).add(infoReport);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] public void addMetadataInfos(AbstractMetadata metadata, String message) {\\n[ADD] addMetadataInfos(metadata.getId(), metadata.getUuid(), isMetadataDraft(metadata.getId()),\\n[ADD] isMetadataApproved(metadata.getId()), message);\\n[ADD] }\\n[ADD] \\n[ADD] private boolean isMetadataDraft(int metadataId) {\\n[ADD] boolean metadataDraft = false;\\n[ADD] try {\\n[ADD] metadataDraft = ApplicationContextHolder.get().getBean(IMetadataUtils.class).isMetadataDraft(metadataId);\\n[ADD] } catch (Exception e) {\\n[ADD] throw new RuntimeException(\"Error detecting if metadata is draft\");\\n[ADD] }\\n[ADD] return metadataDraft;\\n[ADD] }\\n[ADD] \\n[ADD] private boolean isMetadataApproved(int metadataId) {\\n[ADD] boolean metadataApproved = false;\\n[ADD] try {\\n[ADD] metadataApproved = ApplicationContextHolder.get().getBean(IMetadataUtils.class).isMetadataApproved(metadataId);\\n[ADD] } catch (Exception e) {\\n[ADD] throw new RuntimeException(\"Error detecting if metadata is draft\");\\n         }\\n[ADD] return metadataApproved;\\n     }\\n \\n     @XmlTransient',\n",
              " ' \\n         try {\\n             response = client.execute(request);\\n[DEL] } catch(Exception e) {\\n[ADD] } catch (Exception e) {\\n             Log.warning(Log.SERVICE, \"Error calling INSPIRE service: \" + endPoint, e);\\n             return false;\\n         } finally {\\n[DEL] if(close) {\\n[ADD] if (close) {\\n                 try {\\n                     client.close();\\n                 } catch (IOException e) {',\n",
              " ' \\n package org.fao.geonet.kernel;\\n \\n[DEL] import static org.fao.geonet.repository.specification.OperationAllowedSpecs.hasMetadataId;\\n[DEL] import static org.fao.geonet.repository.specification.OperationAllowedSpecs.hasOperation;\\n[DEL] import static org.springframework.data.jpa.domain.Specifications.where;\\n[DEL] \\n[DEL] import java.sql.SQLException;\\n[DEL] import java.util.ArrayList;\\n[DEL] import java.util.Collection;\\n[DEL] import java.util.HashSet;\\n[DEL] import java.util.List;\\n[DEL] import java.util.Set;\\n[DEL] import java.util.StringTokenizer;\\n[DEL] \\n[ADD] import jeeves.server.UserSession;\\n[ADD] import jeeves.server.context.ServiceContext;\\n import org.apache.commons.lang.StringUtils;\\n import org.fao.geonet.ApplicationContextHolder;\\n import org.fao.geonet.constants.Geonet;',\n",
              " '   private void parseNullSafe(ExtensionParameter extensionParameter, ParameterDeclarer parameter) {\\n     if (extensionParameter.isAnnotatedWith(NullSafe.class)) {\\n       if (extensionParameter.isRequired()) {\\n[DEL] throw new IllegalParameterModelDefinitionException(format(\"Parameter \\'%s\\' is required but annotated with \\'@%s\\', which is redundant\",\\n[ADD] throw new IllegalParameterModelDefinitionException(\\n[ADD] format(\"Parameter \\'%s\\' is required but annotated with \\'@%s\\', which is redundant\",\\n                                                                   extensionParameter.getName(), NullSafe.class.getSimpleName()));\\n       }\\n ',\n",
              " '     @Test\\n     public void defaultMulePollingController() throws Exception\\n     {\\n[DEL] MuleContext context = muleContextFactory.createMuleContext();\\n[ADD] context = muleContextFactory.createMuleContext();\\n         context.start();\\n         assertThat(context.isPrimaryPollingInstance(), is(true));\\n     }',\n",
              " '       tutorialTab_ = tutorialTab;\\n       \\n       binder.bind(commands, this);\\n[ADD] source_.loadFullSource();\\n       \\n       PaneConfig config = validateConfig(userPrefs.panes().getValue().cast());\\n       initPanes(config);',\n",
              " '         try {\\n             JSONObject json = new JSONObject();\\n             JSONArray tests = new JSONArray();\\n[DEL] JSONObject argumets = new JSONObject();\\n[ADD] JSONObject arguments = new JSONObject();\\n             JSONObject testObject = new JSONObject();\\n \\n             json.put(\"label\", \"TEST \" + testTitle + \" - \" + System.currentTimeMillis());\\n             json.put(\"executableTestSuiteIds\", tests);\\n[DEL] json.put(\"argumets\", argumets);\\n[ADD] json.put(\"arguments\", arguments);\\n             json.put(\"testObject\", testObject);\\n \\n             for (String test : testList) {\\n                 tests.put(test);\\n             }\\n \\n[DEL] argumets.put(\"files_to_test\", \".*\");\\n[DEL] argumets.put(\"tests_to_execute\", \".*\");\\n[ADD] arguments.put(\"files_to_test\", \".*\");\\n[ADD] arguments.put(\"tests_to_execute\", \".*\");\\n \\n[DEL] testObject.put(\"id\", fileId);\\n[ADD] if (fileId.startsWith(\"http\")) {\\n[ADD] JSONObject resourceObject = new JSONObject();\\n[ADD] resourceObject.put(\"data\", fileId);\\n[ADD] testObject.put(\"resources\", resourceObject);\\n[ADD] } else {\\n[ADD] testObject.put(\"id\", fileId);\\n[ADD] }\\n \\n             StringEntity entity = new StringEntity(json.toString());\\n             request.setEntity(entity);',\n",
              " '     }\\n \\n     HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\\n[DEL] metaClient.getActiveTimeline().getCommitsTimeline()\\n[DEL] .filterCompletedInstants(), fileStatuses);\\n[ADD] // file-slice after pending compaction-requested instant-time is also considered valid\\n[ADD] metaClient.getCommitsAndCompactionTimeline().filterCompletedAndCompactionInstants(), fileStatuses);\\n     String latestCommit = fsView.getLastInstant().get().getTimestamp();\\n     final String mergeType = this.conf.getString(FlinkOptions.MERGE_TYPE);\\n     final AtomicInteger cnt = new AtomicInteger(0);',\n",
              " ' import org.mule.runtime.extension.api.declaration.type.ExtensionsTypeLoaderFactory;\\n import org.mule.runtime.extension.api.dsl.syntax.DslElementSyntax;\\n import org.mule.runtime.extension.api.dsl.syntax.resolver.DslSyntaxResolver;\\n[ADD] import org.mule.runtime.extension.api.runtime.parameter.Literal;\\n import org.mule.runtime.extension.api.util.ExtensionMetadataTypeUtils;\\n import org.mule.runtime.extension.internal.property.InfrastructureParameterModelProperty;\\n import org.mule.runtime.module.extension.internal.config.dsl.construct.RouteComponentParser;',\n",
              " ' \\tprotected boolean isExpectReply() {\\n \\t\\treturn expectReply;\\n \\t}\\n[ADD] \\n[ADD] public void setPath(String path) {\\n[ADD] this.path = path;\\n[ADD] }\\n[ADD] \\n[ADD] String getPath() {\\n[ADD] return path;\\n[ADD] }\\n \\n[ADD] public void setPayloadExpression(Expression payloadExpression) {\\n[ADD] this.payloadExpression = payloadExpression;\\n[ADD] }\\n[ADD] \\n[ADD] public void setHeaderExpressions(Map<String, Expression> headerExpressions) {\\n[ADD] this.headerExpressions = headerExpressions;\\n[ADD] }\\n \\t/**\\n \\t * Set the message body converters to use. These converters are used to convert from and to HTTP requests and\\n \\t * responses.',\n",
              " ' /**\\n  * Validates names clashes in the model by comparing:\\n  * <ul>\\n[DEL] * <li>The {@link Described#getName()} value of all the {@link ConfigurationModel}, {@link OperationModel} and {@link ConnectionProviderModel}</li>\\n[ADD] * <li>The {@link Named#getName()} value of all the {@link ConfigurationModel}, {@link OperationModel} and {@link ConnectionProviderModel}</li>\\n  * <li>Makes sure that there no two {@link ParameterModel}s with the same name but different types, for those which represent an object</li>\\n  * <li>Makes sure that no {@link ConfigurationModel}, {@link OperationModel} or {@link ConnectionProviderModel} have parameters with repeated name</li>\\n  * </ul>',\n",
              " ' \\n   @Override\\n   public BeamIOType getSourceType() {\\n[DEL] return BeamIOType.UNBOUNDED;\\n[ADD] return BeamIOType.BOUNDED;\\n   }\\n \\n   @Override\\n[ADD] \\n   public PCollection<BeamSqlRow> buildIOReader(Pipeline pipeline) {\\n     return PBegin.in(pipeline).apply(\\n         \"MockedBeamSQLTable_Reader_\" + COUNTER.incrementAndGet(), Create.of(inputRecords));',\n",
              " ' import org.eclipse.aether.resolution.VersionRangeRequest;\\n import org.eclipse.aether.resolution.VersionRangeResolutionException;\\n import org.eclipse.aether.resolution.VersionRangeResult;\\n[ADD] import org.eclipse.aether.version.Version;\\n import org.slf4j.Logger;\\n import org.slf4j.LoggerFactory;\\n \\n[DEL] import aQute.bnd.differ.Baseline;\\n[DEL] import aQute.bnd.differ.Baseline.BundleInfo;\\n[DEL] import aQute.bnd.differ.Baseline.Info;\\n[DEL] import aQute.bnd.differ.DiffPluginImpl;\\n[DEL] import aQute.bnd.osgi.Jar;\\n[DEL] import aQute.libg.reporter.ReporterAdapter;\\n[DEL] import aQute.service.reporter.Reporter;\\n[ADD] import java.io.IOException;\\n[ADD] import java.util.ArrayList;\\n[ADD] import java.util.List;\\n[ADD] \\n[ADD] import static org.apache.maven.plugins.annotations.LifecyclePhase.VERIFY;\\n \\n /**\\n  * Exports project dependencies to OSGi R5 index format.',\n",
              " '     private Write() {}\\n   }\\n \\n[ADD] private static String jobToPrettyString(@Nullable Job job) throws IOException {\\n[ADD] return job == null ? \"null\" : job.toPrettyString();\\n[ADD] }\\n[ADD] \\n[ADD] private static String statusToPrettyString(@Nullable JobStatus status) throws IOException {\\n[ADD] return status == null ? \"Unknown status: null.\" : status.toPrettyString();\\n[ADD] }\\n[ADD] \\n   private static void verifyDatasetPresence(DatasetService datasetService, TableReference table) {\\n     try {\\n       datasetService.getDataset(table.getProjectId(), table.getDatasetId());',\n",
              " ' \\n     Var JavascriptOperators::OP_ResumeYield(ResumeYieldData* yieldData, RecyclableObject* iterator)\\n     {\\n[DEL] // CONSIDER: Fast path this early out return path in JITed code before helper call to avoid the helper call overhead in the common case e.g. next() calls.\\n[DEL] if (yieldData->exceptionObj == nullptr)\\n[DEL] {\\n[DEL] return yieldData->data;\\n[DEL] }\\n[DEL] \\n[DEL] ScriptContext* scriptContext = yieldData->exceptionObj->GetScriptContext();\\n[DEL] bool isReturn = yieldData->exceptionObj->IsGeneratorReturnException();\\n[ADD] bool isNext = yieldData->exceptionObj == nullptr;\\n[ADD] bool isThrow = !isNext && !yieldData->exceptionObj->IsGeneratorReturnException();\\n \\n[DEL] if (iterator != nullptr)\\n[ADD] if (iterator != nullptr) // yield*\\n         {\\n[DEL] PropertyId propertyId = isReturn ? PropertyIds::return_ : PropertyIds::throw_;\\n[DEL] Var prop = nullptr;\\n[DEL] Var args[] = { iterator, yieldData->data };\\n[DEL] CallInfo callInfo(CallFlags_Value, _countof(args));\\n[ADD] ScriptContext* scriptContext = iterator->GetScriptContext();\\n[ADD] PropertyId propertyId = isNext ? PropertyIds::next : isThrow ? PropertyIds::throw_ : PropertyIds::return_;\\n[ADD] Var prop = JavascriptOperators::GetProperty(iterator, propertyId, scriptContext);\\n \\n[DEL] if (JavascriptOperators::GetProperty(iterator, iterator, propertyId, &prop, iterator->GetScriptContext())\\n[DEL] && prop != iterator->GetLibrary()->GetUndefined())\\n[ADD] if (!isNext && JavascriptOperators::IsUndefinedOrNull(prop))\\n             {\\n[DEL] RecyclableObject* method = RecyclableObject::FromVar(prop);\\n[DEL] \\n[DEL] Var result = JavascriptFunction::CallFunction<true>(method, method->GetEntryPoint(), Arguments(callInfo, args));\\n[DEL] \\n[DEL] if (isReturn)\\n[ADD] if (isThrow)\\n                 {\\n[DEL] if (!JavascriptOperators::IsObject(result))\\n[DEL] {\\n[DEL] JavascriptError::ThrowTypeError(scriptContext, JSERR_NeedObject);\\n[DEL] }\\n[ADD] // 5.b.iii.2\\n[ADD] // NOTE: If iterator does not have a throw method, this throw is going to terminate the yield* loop.\\n[ADD] // But first we need to give iterator a chance to clean up.\\n \\n[DEL] Var value = JavascriptOperators::GetProperty(RecyclableObject::FromVar(result), PropertyIds::value, scriptContext);\\n[DEL] // CONSIDER: Using an exception to carry the return value and force finally code to execute is a bit of a janky\\n[DEL] // solution since we have to override the value here in the case of yield* expressions.  It works but is there\\n[DEL] // a more elegant way?\\n[DEL] //\\n[DEL] // Instead what if ResumeYield was a \"set Dst then optionally branch\" opcode, that could also throw? Then we could\\n[DEL] // avoid using a special exception entirely with byte code something like this:\\n[DEL] //\\n[DEL] // ;; Ry is the yieldData\\n[DEL] //\\n[DEL] // ResumeYield Rx Ry $returnPathLabel\\n[DEL] // ... code like normal\\n[DEL] // $returnPathLabel:\\n[DEL] // Ld_A R0 Rx\\n[DEL] // Br $exitFinallyAndReturn\\n[DEL] //\\n[DEL] // This would probably give better performance for the common case of calling next() on generators since we wouldn\\'t\\n[DEL] // have to wrap the call to the generator code in a try catch.\\n[DEL] yieldData->exceptionObj->SetThrownObject(value);\\n[DEL] }\\n[DEL] }\\n[DEL] else if (!isReturn)\\n[DEL] {\\n[DEL] // Throw is called on yield* but the iterator does not have a throw method. This is a protocol violation.\\n[DEL] // So we have to call IteratorClose().\\n[DEL] if (JavascriptOperators::GetProperty(iterator, iterator, PropertyIds::return_, &prop, iterator->GetScriptContext())\\n[DEL] && prop != iterator->GetLibrary()->GetUndefined())\\n[DEL] {\\n[DEL] // As per the spec we ignore the inner result after checking whether it is a valid object\\n[DEL] RecyclableObject* method = RecyclableObject::FromVar(prop);\\n[DEL] Var result = JavascriptFunction::CallFunction<true>(method, method->GetEntryPoint(), Arguments(callInfo, args));\\n[DEL] if (!JavascriptOperators::IsObject(result))\\n[ADD] prop = JavascriptOperators::GetProperty(iterator, PropertyIds::return_, scriptContext);\\n[ADD] if (!JavascriptOperators::IsUndefinedOrNull(prop))\\n                     {\\n[DEL] JavascriptError::ThrowTypeError(scriptContext, JSERR_NeedObject);\\n[ADD] if (!JavascriptConversion::IsCallable(prop))\\n[ADD] {\\n[ADD] JavascriptError::ThrowTypeError(scriptContext, JSERR_Property_NeedFunction, _u(\"return\"));\\n[ADD] }\\n[ADD] \\n[ADD] RecyclableObject* method = RecyclableObject::FromVar(prop);\\n[ADD] Var args[] = { iterator, yieldData->data };\\n[ADD] CallInfo callInfo(CallFlags_Value, _countof(args));\\n[ADD] Var result = JavascriptFunction::CallFunction<true>(method, method->GetEntryPoint(), Arguments(callInfo, args));\\n[ADD] \\n[ADD] if (!JavascriptOperators::IsObject(result))\\n[ADD] {\\n[ADD] JavascriptError::ThrowTypeError(scriptContext, JSERR_NeedObject);\\n[ADD] }\\n                     }\\n[ADD] \\n[ADD] // 5.b.iii.3\\n[ADD] // NOTE: The next step throws a TypeError to indicate that there was a yield* protocol violation:\\n[ADD] // iterator does not have a throw method.\\n[ADD] JavascriptError::ThrowTypeError(scriptContext, JSERR_Property_NeedFunction, _u(\"throw\"));\\n                 }\\n[ADD] \\n[ADD] // Do not use ThrowExceptionObject for return() API exceptions since these exceptions are not real exceptions\\n[ADD] throw yieldData->exceptionObj;\\n[ADD] }\\n[ADD] \\n[ADD] if (!JavascriptConversion::IsCallable(prop))\\n[ADD] {\\n[ADD] JavascriptError::ThrowTypeError(scriptContext, JSERR_Property_NeedFunction, isNext ? _u(\"next\") : isThrow ? _u(\"throw\") : _u(\"return\"));\\n             }\\n[ADD] \\n[ADD] RecyclableObject* method = RecyclableObject::FromVar(prop);\\n[ADD] Var args[] = { iterator, yieldData->data };\\n[ADD] CallInfo callInfo(CallFlags_Value, _countof(args));\\n[ADD] Var result = JavascriptFunction::CallFunction<true>(method, method->GetEntryPoint(), Arguments(callInfo, args));\\n[ADD] \\n[ADD] if (!JavascriptOperators::IsObject(result))\\n[ADD] {\\n[ADD] JavascriptError::ThrowTypeError(scriptContext, JSERR_NeedObject);\\n[ADD] }\\n[ADD] \\n[ADD] if (isThrow || isNext)\\n[ADD] {\\n[ADD] // 5.b.ii.2\\n[ADD] // NOTE: Exceptions from the inner iterator throw method are propagated.\\n[ADD] // Normal completions from an inner throw method are processed similarly to an inner next.\\n[ADD] return result;\\n[ADD] }\\n[ADD] \\n[ADD] RecyclableObject* obj = RecyclableObject::FromVar(result);\\n[ADD] Var done = JavascriptOperators::GetProperty(obj, PropertyIds::done, scriptContext);\\n[ADD] if (done == iterator->GetLibrary()->GetTrue())\\n[ADD] {\\n[ADD] Var value = JavascriptOperators::GetProperty(obj, PropertyIds::value, scriptContext);\\n[ADD] yieldData->exceptionObj->SetThrownObject(value);\\n[ADD] // Do not use ThrowExceptionObject for return() API exceptions since these exceptions are not real exceptions\\n[ADD] throw yieldData->exceptionObj;\\n[ADD] }\\n[ADD] return result;\\n[ADD] }\\n[ADD] \\n[ADD] // CONSIDER: Fast path this early out return path in JITed code before helper call to avoid the helper call overhead in the common case e.g. next() calls.\\n[ADD] if (isNext)\\n[ADD] {\\n[ADD] return yieldData->data;\\n         }\\n \\n[DEL] if (!isReturn)\\n[ADD] if (isThrow)\\n         {\\n             // Use ThrowExceptionObject() to get debugger support for breaking on throw\\n[DEL] JavascriptExceptionOperators::ThrowExceptionObject(yieldData->exceptionObj, scriptContext, true);\\n[ADD] JavascriptExceptionOperators::ThrowExceptionObject(yieldData->exceptionObj, yieldData->exceptionObj->GetScriptContext(), true);\\n         }\\n \\n[ADD] // CONSIDER: Using an exception to carry the return value and force finally code to execute is a bit of a janky\\n[ADD] // solution since we have to override the value here in the case of yield* expressions.  It works but is there\\n[ADD] // a more elegant way?\\n[ADD] //\\n[ADD] // Instead what if ResumeYield was a \"set Dst then optionally branch\" opcode, that could also throw? Then we could\\n[ADD] // avoid using a special exception entirely with byte code something like this:\\n[ADD] //\\n[ADD] // ;; Ry is the yieldData\\n[ADD] //\\n[ADD] // ResumeYield Rx Ry $returnPathLabel\\n[ADD] // ... code like normal\\n[ADD] // $returnPathLabel:\\n[ADD] // Ld_A R0 Rx\\n[ADD] // Br $exitFinallyAndReturn\\n[ADD] //\\n[ADD] // This would probably give better performance for the common case of calling next() on generators since we wouldn\\'t\\n[ADD] // have to wrap the call to the generator code in a try catch.\\n[ADD] \\n         // Do not use ThrowExceptionObject for return() API exceptions since these exceptions are not real exceptions\\n         throw yieldData->exceptionObj;\\n     }',\n",
              " \" import org.apache.gobblin.source.workunit.WorkUnit;\\n import org.apache.gobblin.util.Id;\\n \\n[ADD] import static org.apache.gobblin.util.retry.RetryerFactory.RETRY_INTERVAL_MS;\\n[ADD] import static org.apache.gobblin.util.retry.RetryerFactory.RETRY_TIME_OUT_MS;\\n[ADD] \\n \\n /**\\n  * An implementation of Helix's {@link org.apache.helix.task.Task} that wraps and runs one or more Gobblin\",\n",
              " '    {\\n       return getSession().getTextRange(Range.fromPoints(start, end));\\n    }\\n[DEL] \\n[DEL] \\n[ADD] \\n[ADD] \\n    @Override\\n    public InputEditorSelection search(String needle,\\n                                       boolean backwards,',\n",
              " '                                    short replication, long blockSize, Progressable progress, Options.ChecksumOpt checksumOpt) throws IOException {\\n     return executeFuncWithTimeMetrics(MetricName.create.name(), f, () -> {\\n       return wrapOutputStream(f, fileSystem.create(convertToDefaultPath(f), permission, flags, bufferSize, replication,\\n[DEL] blockSize, progress, checksumOpt));\\n[ADD] blockSize, progress, checksumOpt), bufferSize);\\n     });\\n   }\\n ',\n",
              " '         }\\n \\n         CardBrowserContextMenu.ensureConsistentStateWithSharedPreferences(this);\\n[DEL] setLanguage(preferences.getString(Preferences.LANGUAGE, \"\"));\\n         NotificationChannels.setup(getApplicationContext());\\n \\n         // Configure WebView to allow file scheme pages to access cookies.',\n",
              " '    */\\n   @Override\\n   public void onStop() {\\n[ADD] scheduledListen.cancel(false);\\n     stopRequested.set(true);\\n     workManager.stop(muleContext.getConfiguration().getShutdownTimeout(), MILLISECONDS);\\n[DEL] shutdownExecutor();\\n   }\\n \\n   /**',\n",
              " '     public ValueResolvingContext build() {\\n       if (event == null) {\\n         return new ValueResolvingContext(null, null, null, true);\\n[ADD] } else if (manager == null) {\\n[ADD] return new ValueResolvingContext(event, null, config.orElse(null), resolveCursors);\\n[ADD] } else {\\n[ADD] return new ValueResolvingContext(event, manager.openSession(event.asBindingContext()), config.orElse(null),\\n[ADD] resolveCursors);\\n       }\\n[DEL] LazyValue<ExpressionManagerSession> session = new LazyValue<>();\\n[DEL] if (manager != null) {\\n[DEL] session = new LazyValue<>(() -> manager.openSession(event.asBindingContext()));\\n[DEL] }\\n[DEL] return new ValueResolvingContext(event, session, config.orElse(null), resolveCursors);\\n     }\\n   }\\n }',\n",
              " '         break;\\n       // Java 8 constructions : ignored for now.\\n       case METHOD_REFERENCE:\\n[DEL] // assert can be ignored by VM so skip them for now.\\n[ADD] // assert can be ignored by VM so skip them for now.\\n       case ASSERT_STATEMENT:\\n[ADD] //Ignore assert statement as they are disabled by default in JVM\\n         break;\\n       // store declarations as complete blocks.\\n       case EMPTY_STATEMENT:',\n",
              " '         try {\\n             storageWriter.close();\\n \\n[ADD] String relativePath = path.toString().replace(new Path(config.getBasePath()) + \"/\", \"\");\\n[ADD] \\n             HoodieWriteStat stat = new HoodieWriteStat();\\n             stat.setNumWrites(recordsWritten);\\n             stat.setNumDeletes(recordsDeleted);\\n             stat.setPrevCommit(HoodieWriteStat.NULL_COMMIT);\\n             stat.setFileId(status.getFileId());\\n[DEL] stat.setFullPath(path.toString());\\n[ADD] stat.setPath(relativePath);\\n             stat.setTotalWriteBytes(FSUtils.getFileSize(fs, path));\\n             stat.setTotalWriteErrors(status.getFailedRecords().size());\\n             status.setStat(stat);',\n",
              " '             if (iStores == null || iStores.size() == 0) {\\n                 // Mark template as Inactive.\\n                 template.setState(VirtualMachineTemplate.State.Inactive);\\n[ADD] templateDao.remove(template.getId());\\n                 _tmpltDao.update(template.getId(), template);\\n \\n                     // Decrement the number of templates and total secondary storage',\n",
              " '   private static final byte[] oldMagicBuffer = new byte[4];\\n   private static final byte[] magicBuffer = new byte[6];\\n   private final Schema readerSchema;\\n[DEL] private HoodieLogBlock nextBlock = null;\\n   private LogFormatVersion nextBlockVersion;\\n   private boolean readBlockLazily;\\n   private long reverseLogFilePosition;',\n",
              " '         if (UNTAGGED_VLAN_NAME.equalsIgnoreCase(vlanId)) {\\n             return \"cloud.public.untagged\";\\n         } else {\\n[DEL] return \"cloud.public.\" + vlanId;\\n[ADD] return \"cloud.public.\" + vlanId + \".\";\\n         }\\n     }\\n ',\n",
              " '     assertEquals(1, metaClient.getActiveTimeline().getCompletedReplaceTimeline().getInstants().toArray().length);\\n   }\\n \\n[ADD] @Test\\n[ADD] public void testHoodieAsyncClusteringJobWithScheduleAndExecute() throws Exception {\\n[ADD] String tableBasePath = dfsBasePath + \"/asyncClustering2\";\\n[ADD] // Keep it higher than batch-size to test continuous mode\\n[ADD] int totalRecords = 3000;\\n[ADD] \\n[ADD] // Initial bulk insert\\n[ADD] HoodieDeltaStreamer.Config cfg = TestHelpers.makeConfig(tableBasePath, WriteOperationType.INSERT);\\n[ADD] cfg.continuousMode = true;\\n[ADD] cfg.tableType = HoodieTableType.COPY_ON_WRITE.name();\\n[ADD] cfg.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\\n[ADD] cfg.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\\n[ADD] cfg.configs.add(String.format(\"%s=true\", HoodieClusteringConfig.ASYNC_CLUSTERING_ENABLE_OPT_KEY.key()));\\n[ADD] HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);\\n[ADD] deltaStreamerTestRunner(ds, cfg, (r) -> {\\n[ADD] TestHelpers.assertAtLeastNCommits(2, tableBasePath, dfs);\\n[ADD] HoodieClusteringJob.Config scheduleClusteringConfig = buildHoodieClusteringUtilConfig(tableBasePath,\\n[ADD] null, true);\\n[ADD] scheduleClusteringConfig.runningMode = \"scheduleAndExecute\";\\n[ADD] HoodieClusteringJob scheduleClusteringJob = new HoodieClusteringJob(jsc, scheduleClusteringConfig);\\n[ADD] \\n[ADD] try {\\n[ADD] int result = scheduleClusteringJob.doScheduleAndCluster();\\n[ADD] if (result == 0) {\\n[ADD] LOG.info(\"Cluster success\");\\n[ADD] } else {\\n[ADD] LOG.warn(\"Import failed\");\\n[ADD] return false;\\n[ADD] }\\n[ADD] } catch (Exception e) {\\n[ADD] LOG.warn(\"ScheduleAndExecute clustering failed\", e);\\n[ADD] return false;\\n[ADD] }\\n[ADD] \\n[ADD] HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(this.dfs.getConf()).setBasePath(tableBasePath).setLoadActiveTimelineOnLoad(true).build();\\n[ADD] int pendingReplaceSize = metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants().toArray().length;\\n[ADD] int completeReplaceSize = metaClient.getActiveTimeline().getCompletedReplaceTimeline().getInstants().toArray().length;\\n[ADD] LOG.info(\"PendingReplaceSize=\" + pendingReplaceSize + \",completeReplaceSize = \" + completeReplaceSize);\\n[ADD] return completeReplaceSize > 0;\\n[ADD] });\\n[ADD] HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(this.dfs.getConf()).setBasePath(tableBasePath).setLoadActiveTimelineOnLoad(true).build();\\n[ADD] assertEquals(1, metaClient.getActiveTimeline().getCompletedReplaceTimeline().getInstants().toArray().length);\\n[ADD] }\\n[ADD] \\n   /**\\n    * Test Bulk Insert and upserts with hive syncing. Tests Hudi incremental processing using a 2 step pipeline The first\\n    * step involves using a SQL template to transform a source TEST-DATA-SOURCE ============================> HUDI TABLE',\n",
              " ' \\n         return views.stream().anyMatch(viewReadPermission);\\n     }\\n[DEL] \\n[DEL] private boolean isOwned(Search search, User user) {\\n[DEL] return search.owner().map(o -> o.equals(user.getName())).orElse(false);\\n[DEL] }\\n }',\n",
              " ' @Alpha\\n public class HiveMetaStoreBasedRegister extends HiveRegister {\\n \\n[ADD] public static final String HIVE_REGISTER_METRICS_PREFIX = \"hiveRegister.\";\\n[ADD] public static final String ADD_PARTITION_TIMER = HIVE_REGISTER_METRICS_PREFIX + \"addPartitionTimerTimer\";\\n[ADD] public static final String GET_HIVE_PARTITION = HIVE_REGISTER_METRICS_PREFIX + \"getPartitionTimer\";\\n[ADD] public static final String ALTER_PARTITION = HIVE_REGISTER_METRICS_PREFIX + \"alterPartitionTimer\";\\n[ADD] public static final String TABLE_EXISTS = HIVE_REGISTER_METRICS_PREFIX + \"tableExistsTimer\";\\n[ADD] public static final String ALTER_TABLE = HIVE_REGISTER_METRICS_PREFIX + \"alterTableTimer\";\\n[ADD] public static final String GET_HIVE_DATABASE = HIVE_REGISTER_METRICS_PREFIX + \"getDatabaseTimer\";\\n[ADD] public static final String CREATE_HIVE_DATABASE = HIVE_REGISTER_METRICS_PREFIX + \"createDatabaseTimer\";\\n[ADD] public static final String CREATE_HIVE_TABLE = HIVE_REGISTER_METRICS_PREFIX + \"createTableTimer\";\\n[ADD] public static final String GET_HIVE_TABLE = HIVE_REGISTER_METRICS_PREFIX + \"getTableTimer\";\\n[ADD] public static final String DROP_TABLE = HIVE_REGISTER_METRICS_PREFIX + \"dropTableTimer\";\\n[ADD] public static final String PATH_REGISTER_TIMER = HIVE_REGISTER_METRICS_PREFIX + \"pathRegisterTimer\";\\n[ADD] \\n   private final HiveMetastoreClientPool clientPool;\\n   private final HiveLock locks = new HiveLock();\\n   private final EventSubmitter eventSubmitter;\\n[ADD] private final MetricContext metricContext;\\n \\n   public HiveMetaStoreBasedRegister(State state, Optional<String> metastoreURI) throws IOException {\\n     super(state);',\n",
              " '                 // Update deck\\n                 if (!getCol().getConf().optBoolean(\"addToCur\", true)) {\\n                     mCurrentDid = model.getLong(\"did\");\\n[DEL] updateDeckPosition();\\n[ADD] mDeckSpinnerSelection.updateDeckPosition();\\n                 }\\n \\n                 refreshNoteData(FieldChangeType.changeFieldCount(shouldReplaceNewlines()));',\n",
              " '                 ConnectInput(data, outData, startPort.Owner);\\n                 startPort.Owner.ConnectOutput(outData, data, this);\\n                 OnConnectorAdded(connector);\\n[DEL] ForceReExecuteOfNode = true;\\n[DEL] OnAstUpdated();\\n[ADD] \\n[ADD] // Mark node for update\\n[ADD] //MarkAsDirty();\\n[ADD] //OnAstUpdated();\\n             }\\n         }\\n ',\n",
              " ' \\t\\tif (conversionService != null && this.messageProcessor instanceof AbstractMessageProcessor) {\\n \\t\\t\\t((AbstractMessageProcessor<?>) this.messageProcessor).setConversionService(conversionService);\\n \\t\\t}\\n[ADD] this.messageBuilderFactory = IntegrationContextUtils.getMessageBuilderFactory(beanFactory);\\n \\t}\\n \\n[ADD] @Override\\n \\tpublic final Message<?> transform(Message<?> message) {\\n \\t\\tObject result = this.messageProcessor.processMessage(message);\\n \\t\\tif (result == null) {',\n",
              " ' \\t\\tboolean doubleAlignment = (totalDoubleCount > 0) || (totalFlatFieldDoubleBytes > 0);\\n \\n \\t\\tif (\\n[DEL] ((getSuperclassObjectSize() % ObjectModel.getObjectAlignmentInBytes()) != 0) && /* superclass is not end-aligned */\\n[ADD] ((getSuperclassObjectSize() % OBJECT_SIZE_INCREMENT_IN_BYTES) != 0) && /* superclass is not end-aligned */\\n \\t\\t\\t(doubleAlignment || (!objectCanUseBackfill && (totalObjectCount > 0)))\\n \\t\\t) { /* our fields start on a 8-byte boundary */\\n \\t\\t\\tfieldDataStart += BACKFILL_SIZE;',\n",
              " '         mailProps.setContent(content);\\n         mailProps.setContentType(\"text/plain\");\\n \\n[ADD] if (recipients == null) {\\n[ADD] return;\\n[ADD] }\\n[ADD] \\n         Set<MailAddress> addresses = new HashSet<>();\\n         for (String recipient : recipients) {\\n             addresses.add(new MailAddress(recipient));',\n",
              " '    */\\n   @Override\\n   public void run() {\\n[ADD] if (this.needToFail) throw new RuntimeException(this.dataset.datasetURN() + \" is failed\");\\n     List<CompactionVerifier> verifiers = this.suite.getMapReduceVerifiers();\\n     for (CompactionVerifier verifier : verifiers) {\\n       if (!verifier.verify(dataset)) {',\n",
              " '     return DEFAULT_TABLE_TYPE;\\n   }\\n \\n[ADD] public TimelineLayoutVersion getTimelineLayoutVersion() {\\n[ADD] return new TimelineLayoutVersion(Integer.valueOf(props.getProperty(HOODIE_TIMELINE_LAYOUT_VERSION,\\n[ADD] String.valueOf(DEFAULT_TIMELINE_LAYOUT_VERSION))));\\n[ADD] \\n[ADD] }\\n[ADD] \\n   /**\\n    * Read the payload class for HoodieRecords from the table properties.\\n    */',\n",
              " \" \\tpublic static function configure() {\\n \\t\\t$manager = new self();\\n \\n[ADD] add_filter(\\n[ADD] 'jetpack_constant_default_value',\\n[ADD] __NAMESPACE__ . '\\\\Utils::jetpack_api_constant_filter',\\n[ADD] 10,\\n[ADD] 2\\n[ADD] );\\n[ADD] \\n \\t\\t$manager->setup_xmlrpc_handlers(\\n \\t\\t\\t$_GET, // phpcs:ignore WordPress.Security.NonceVerification.Recommended\\n \\t\\t\\t$manager->is_active(),\",\n",
              " '   public void dispose() {\\n     disposeIfNeeded(delegate, LOGGER);\\n   }\\n[ADD] \\n[ADD] @Override\\n[ADD] public boolean isAsync() {\\n[ADD] return false;\\n[ADD] }\\n }',\n",
              " ' import javax.annotation.processing.ProcessingEnvironment;\\n import javax.annotation.processing.Processor;\\n import javax.annotation.processing.RoundEnvironment;\\n[DEL] import javax.lang.model.element.ExecutableElement;\\n[ADD] import javax.lang.model.element.Element;\\n import javax.lang.model.element.TypeElement;\\n import javax.lang.model.element.VariableElement;\\n ',\n",
              " '         {\\n             if (logger.isDebugEnabled())\\n             {\\n[DEL] logger.debug(\"Comparing transformers for best match: input = \" + input + \" output = \" + output + \" Possible transformers = \" + trans);\\n[ADD] logger.debug(\"Comparing transformers for best match: source = \" + input + \" target = \" + output + \" Possible transformers = \" + trans);\\n             }\\n \\n             List<TransformerWeighting> weightings = calculateTransformerWeightings(trans, input, output);',\n",
              " ' \\t\\tMetadata:              request.Metadata,\\n \\t\\tReprocessSpec:         request.ReprocessSpec,\\n \\t}\\n[ADD] \\n \\tif err := setPipelineDefaults(pipelineInfo); err != nil {\\n[DEL] return err\\n[ADD] return nil, err\\n \\t}\\n \\t// Validate final PipelineInfo (now that defaults have been populated)\\n[DEL] if err := a.validatePipelineInTransaction(txnCtx, pipelineInfo); err != nil {\\n[ADD] if err := a.validatePipeline(pipelineInfo); err != nil {\\n[ADD] return nil, err\\n[ADD] }\\n[ADD] \\n[ADD] pps.SortInput(pipelineInfo.Input) // Makes datum hashes comparable\\n[ADD] \\n[ADD] if oldPipelineInfo != nil {\\n[ADD] // Modify pipelineInfo (increment Version, and *preserve Stopped* so\\n[ADD] // that updating a pipeline doesn\\'t restart it)\\n[ADD] pipelineInfo.Version = oldPipelineInfo.Version + 1\\n[ADD] if oldPipelineInfo.Stopped {\\n[ADD] pipelineInfo.Stopped = true\\n[ADD] }\\n[ADD] if !request.Reprocess {\\n[ADD] pipelineInfo.Salt = oldPipelineInfo.Salt\\n[ADD] }\\n[ADD] // Cannot disable stats after it has been enabled.\\n[ADD] if oldPipelineInfo.EnableStats && !pipelineInfo.EnableStats {\\n[ADD] return nil, newErrPipelineUpdate(pipelineInfo.Pipeline.Name, \"cannot disable stats\")\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] return pipelineInfo, nil\\n[ADD] }\\n[ADD] \\n[ADD] // latestPipelineInfo doesn\\'t actually work transactionally because\\n[ADD] // ppsutil.GetPipelineInfo needs to use pfs.GetFile to read the spec commit,\\n[ADD] // which does not support transactions.\\n[ADD] func (a *apiServer) latestPipelineInfo(txnCtx *txnenv.TransactionContext, pipelineName string) (*pps.PipelineInfo, error) {\\n[ADD] pipelinePtr := pps.StoredPipelineInfo{}\\n[ADD] if err := a.pipelines.ReadWrite(txnCtx.SqlTx).Get(pipelineName, &pipelinePtr); err != nil {\\n[ADD] if col.IsErrNotFound(err) {\\n[ADD] return nil, nil\\n[ADD] }\\n[ADD] return nil, err\\n[ADD] }\\n[ADD] // the spec commit must already exist outside of the transaction, so we can retrieve it normally\\n[ADD] return ppsutil.GetPipelineInfo(txnCtx.Client, &pipelinePtr)\\n[ADD] }\\n[ADD] \\n[ADD] func (a *apiServer) pipelineInfosForUpdate(txnCtx *txnenv.TransactionContext, request *pps.CreatePipelineRequest) (*pps.PipelineInfo, *pps.PipelineInfo, error) {\\n[ADD] if request.Pipeline == nil {\\n[ADD] return nil, nil, errors.New(\"invalid pipeline spec: request.Pipeline cannot be nil\")\\n[ADD] }\\n[ADD] \\n[ADD] // We can\\'t recover from an incomplete pipeline info here because\\n[ADD] // modifying the spec repo depends on being able to access the previous\\n[ADD] // commit. We therefore use `GetPipelineInfo` which will error if the\\n[ADD] // spec commit isn\\'t working.\\n[ADD] oldPipelineInfo, err := a.latestPipelineInfo(txnCtx, request.Pipeline.Name)\\n[ADD] if err != nil {\\n[ADD] return nil, nil, err\\n[ADD] }\\n[ADD] \\n[ADD] newPipelineInfo, err := a.initializePipelineInfo(request, oldPipelineInfo)\\n[ADD] if err != nil {\\n[ADD] return nil, nil, err\\n[ADD] }\\n[ADD] \\n[ADD] return oldPipelineInfo, newPipelineInfo, nil\\n[ADD] }\\n[ADD] \\n[ADD] func (a *apiServer) CreatePipelineInTransaction(\\n[ADD] txnCtx *txnenv.TransactionContext,\\n[ADD] request *pps.CreatePipelineRequest,\\n[ADD] specFilesetID *string,\\n[ADD] prevSpecCommit **pfs.Commit,\\n[ADD] ) error {\\n[ADD] oldPipelineInfo, newPipelineInfo, err := a.pipelineInfosForUpdate(txnCtx, request)\\n[ADD] if err != nil {\\n \\t\\treturn err\\n \\t}\\n[ADD] pipelineName := request.Pipeline.Name\\n[ADD] \\n[ADD] if *specFilesetID != \"\" {\\n[ADD] // If we already have a fileset, try to renew it - if that fails, invalidate it\\n[ADD] if err := txnCtx.Client.RenewFileSet(*specFilesetID, 600*time.Second); err != nil {\\n[ADD] *specFilesetID = \"\"\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] // If the expected spec commit doesn\\'t match up with oldPipelineInfo, we need\\n[ADD] // to recreate the fileset\\n[ADD] staleFileset := false\\n[ADD] if oldPipelineInfo == nil {\\n[ADD] staleFileset = (*prevSpecCommit != nil)\\n[ADD] } else {\\n[ADD] staleFileset = !proto.Equal(oldPipelineInfo.SpecCommit, *prevSpecCommit)\\n[ADD] }\\n[ADD] \\n[ADD] if staleFileset || *specFilesetID == \"\" {\\n[ADD] // No existing fileset or the old one expired, create a new fileset - the\\n[ADD] // pipeline spec to be written into a fileset outside of the transaction.\\n[ADD] *specFilesetID, err = a.writePipelineInfoToFileset(txnCtx.Client, newPipelineInfo)\\n[ADD] if err != nil {\\n[ADD] return err\\n[ADD] }\\n[ADD] if oldPipelineInfo != nil {\\n[ADD] *prevSpecCommit = oldPipelineInfo.SpecCommit\\n[ADD] }\\n \\n[ADD] // The transaction cannot continue because it cannot see the fileset - abort and retry\\n[ADD] return &col.ErrTransactionConflict{}\\n[ADD] }\\n[ADD] \\n[ADD] // Verify that all input repos exist (create cron and git repos if necessary)\\n \\tvar visitErr error\\n[DEL] pps.VisitInput(pipelineInfo.Input, func(input *pps.Input) {\\n[ADD] pps.VisitInput(newPipelineInfo.Input, func(input *pps.Input) {\\n[ADD] if input.Pfs != nil {\\n[ADD] if _, err := txnCtx.Pfs().InspectRepoInTransaction(txnCtx,\\n[ADD] &pfs.InspectRepoRequest{\\n[ADD] Repo: client.NewRepo(input.Pfs.Repo),\\n[ADD] },\\n[ADD] ); err != nil {\\n[ADD] visitErr = err\\n[ADD] }\\n[ADD] }\\n \\t\\tif input.Cron != nil {\\n \\t\\t\\tif err := txnCtx.Pfs().CreateRepoInTransaction(txnCtx,\\n \\t\\t\\t\\t&pfs.CreateRepoRequest{\\n \\t\\t\\t\\t\\tRepo:        client.NewRepo(input.Cron.Repo),\\n \\t\\t\\t\\t\\tDescription: fmt.Sprintf(\"Cron tick repo for pipeline %s.\", request.Pipeline.Name),\\n[DEL] }); err != nil && !isAlreadyExistsErr(err) {\\n[ADD] },\\n[ADD] ); err != nil && !isAlreadyExistsErr(err) {\\n \\t\\t\\t\\tvisitErr = err\\n \\t\\t\\t}\\n \\t\\t}',\n",
              " ' import android.content.res.Resources;\\n import android.os.AsyncTask;\\n \\n[ADD] import com.ichi2.libanki.Collection;\\n[ADD] import com.ichi2.utils.ThreadUtil;\\n[ADD] \\n import java.util.ArrayList;\\n import java.util.Collections;\\n import java.util.LinkedList;',\n",
              " ' package org.mule.runtime.core.internal.el;\\n \\n import static com.github.benmanes.caffeine.cache.Caffeine.newBuilder;\\n[DEL] import static java.lang.Boolean.valueOf;\\n import static java.lang.String.format;\\n[DEL] import static java.lang.System.getProperty;\\n import static java.util.regex.Pattern.compile;\\n import static org.apache.commons.lang3.StringUtils.isEmpty;\\n import static org.apache.commons.lang3.StringUtils.join;\\n import static org.mule.runtime.api.i18n.I18nMessageFactory.createStaticMessage;\\n[DEL] import static org.mule.runtime.core.api.config.MuleProperties.MULE_MEL_AS_DEFAULT;\\n import static org.mule.runtime.core.internal.el.DefaultExpressionManager.DW_PREFIX;\\n import static org.mule.runtime.core.internal.el.DefaultExpressionManager.MEL_PREFIX;\\n[DEL] \\n import org.mule.runtime.api.component.location.ComponentLocation;\\n import org.mule.runtime.api.el.BindingContext;\\n import org.mule.runtime.api.el.ExpressionExecutionException;',\n",
              " '         HoodieTimeline.CLEAN_ACTION), expectedActiveInstants, commitsAfterArchival);\\n   }\\n \\n[DEL] @Test\\n[DEL] public void testArchiveCompletedRollbackAndClean() throws Exception {\\n[ADD] @ParameterizedTest\\n[ADD] @ValueSource(booleans = {true, false})\\n[ADD] public void testArchiveCompletedRollbackAndClean(boolean isEmpty) throws Exception {\\n     init();\\n     int minInstantsToKeep = 2;\\n     int maxInstantsToKeep = 10;',\n",
              " '             \"Unsupported type \" + logicalType.getTypeRoot() + \" for \" + StringToRowDataConverter.class.getName());\\n     }\\n   }\\n[ADD] \\n[ADD] private static TimestampData convertToTimestamp(long timestamp, TimestampType logicalType) {\\n[ADD] final int precision = logicalType.getPrecision();\\n[ADD] final ChronoUnit chronoUnit;\\n[ADD] if (precision <= 3) {\\n[ADD] chronoUnit = ChronoUnit.MILLIS;\\n[ADD] } else if (precision <= 6) {\\n[ADD] chronoUnit = ChronoUnit.MICROS;\\n[ADD] } else {\\n[ADD] throw new IllegalArgumentException(\\n[ADD] \"Avro does not support TIMESTAMP type with precision: \"\\n[ADD] + precision\\n[ADD] + \", it only supports precision less than 6.\");\\n[ADD] }\\n[ADD] return TimestampData.fromLocalDateTime(\\n[ADD] LocalDateTime.ofInstant(Instant.ofEpochSecond(0).plus(timestamp, chronoUnit), ZoneId.systemDefault()));\\n[ADD] }\\n }',\n",
              " ' import java.util.function.Supplier;\\n import java.util.stream.Collectors;\\n \\n[ADD] import static org.apache.hudi.common.config.LockConfiguration.HIVE_DATABASE_NAME_PROP;\\n[ADD] import static org.apache.hudi.common.config.LockConfiguration.HIVE_TABLE_NAME_PROP;\\n \\n /**\\n  * Class storing configs for the HoodieWriteClient.',\n",
              " \"         }\\n \\n         // If exoticToPrim is not undefined, then\\n[DEL] if (nullptr != exoticToPrim)\\n[ADD] Assert(nullptr != exoticToPrim);\\n[ADD] ThreadContext * threadContext = requestContext->GetThreadContext();\\n[ADD] result = threadContext->ExecuteImplicitCall(exoticToPrim, ImplicitCall_ToPrimitive, [=]()->Js::Var\\n         {\\n[DEL] ThreadContext * threadContext = requestContext->GetThreadContext();\\n[DEL] result = threadContext->ExecuteImplicitCall(exoticToPrim, ImplicitCall_ToPrimitive, [=]()->Js::Var\\n[DEL] {\\n[DEL] // Stack object should have a pre-op bail on implicit call.  We shouldn't see them here.\\n[DEL] Assert(!ThreadContext::IsOnStack(recyclableObject));\\n[DEL] \\n[DEL] // Let result be the result of calling the[[Call]] internal method of exoticToPrim, with input as thisArgument and(hint) as argumentsList.\\n[DEL] return CALL_FUNCTION(threadContext, exoticToPrim, CallInfo(CallFlags_Value, 2), recyclableObject, hintString);\\n[DEL] });\\n[ADD] // Stack object should have a pre-op bail on implicit call.  We shouldn't see them here.\\n[ADD] Assert(!ThreadContext::IsOnStack(recyclableObject));\\n \\n[DEL] if (!result)\\n[DEL] {\\n[DEL] // There was an implicit call and implicit calls are disabled. This would typically cause a bailout.\\n[DEL] Assert(threadContext->IsDisableImplicitCall());\\n[DEL] return requestContext->GetLibrary()->GetNull();\\n[DEL] }\\n[ADD] // Let result be the result of calling the[[Call]] internal method of exoticToPrim, with input as thisArgument and(hint) as argumentsList.\\n[ADD] return CALL_FUNCTION(threadContext, exoticToPrim, CallInfo(CallFlags_Value, 2), recyclableObject, hintString);\\n[ADD] });\\n \\n[DEL] Assert(!CrossSite::NeedMarshalVar(result, requestContext));\\n[ADD] if (!result)\\n[ADD] {\\n[ADD] // There was an implicit call and implicit calls are disabled. This would typically cause a bailout.\\n[ADD] Assert(threadContext->IsDisableImplicitCall());\\n[ADD] return requestContext->GetLibrary()->GetNull();\\n         }\\n[ADD] \\n[ADD] Assert(!CrossSite::NeedMarshalVar(result, requestContext));\\n         // If result is an ECMAScript language value and Type(result) is not Object, then return result.\\n         if (TaggedInt::Is(result) || !JavascriptOperators::IsObjectType(JavascriptOperators::GetTypeId(result)))\\n         {\",\n",
              " '     }\\n     return out.toByteString();\\n   }\\n[DEL] \\n[DEL] @Test\\n[DEL] public void testRegistration() {\\n[DEL] for (Registrar registrar :\\n[DEL] ServiceLoader.load(Registrar.class)) {\\n[DEL] if (registrar instanceof FnApiDoFnRunner.Registrar) {\\n[DEL] assertThat(registrar.getPTransformRunnerFactories(),\\n[DEL] IsMapContaining.hasKey(ParDoTranslation.CUSTOM_JAVA_DO_FN_URN));\\n[DEL] return;\\n[DEL] }\\n[DEL] }\\n[DEL] fail(\"Expected registrar not found.\");\\n[DEL] }\\n }',\n",
              " '                     \"SELECT c.id, n.id, n.mid, c.did, c.ord, \"\\n                             + \"n.tags, n.flds FROM cards c, notes n WHERE c.nid == n.id \" + where, null);\\n             while (cur.moveToNext()) {\\n[DEL] data.add(new Object[] { cur.getLong(0), cur.getLong(1), cur.getLong(2), cur.getLong(3), cur.getInt(4),\\n[ADD] data.add(new Object[] { cur.getLong(0), cur.getLong(1), getModels().get(cur.getLong(2)), cur.getLong(3), cur.getInt(4),\\n                         cur.getString(5), cur.getString(6) });\\n             }\\n         } finally {',\n",
              " '         .toNanos();\\n \\n     if (expired) {\\n[DEL] log.info(\"Session expired. Generating a new session.\");\\n[ADD] log.debug(\"Session expired. Generating a new session.\");\\n     } else if (forceRefresh) {\\n       log.info(\"Force to refresh session. Generating a new session.\");\\n     }',\n",
              " ' // GetRequiredPlugins computes the complete set of anticipated plugins required by a program.\\n func (host *pythonLanguageHost) GetRequiredPlugins(ctx context.Context,\\n \\treq *pulumirpc.GetRequiredPluginsRequest) (*pulumirpc.GetRequiredPluginsResponse, error) {\\n[DEL] // TODO: implement this.\\n[DEL] return &pulumirpc.GetRequiredPluginsResponse{}, nil\\n[ADD] \\n[ADD] cwd, err := os.Getwd()\\n[ADD] if err != nil {\\n[ADD] return nil, errors.Wrap(err, \"getting the working directory\")\\n[ADD] }\\n[ADD] \\n[ADD] // Prepare the virtual environment (if needed).\\n[ADD] virtualenv, err := host.prepareVirtualEnvironment(ctx, cwd)\\n[ADD] if err != nil {\\n[ADD] return nil, err\\n[ADD] }\\n[ADD] \\n[ADD] // Now, determine which Pulumi packages are installed.\\n[ADD] pulumiPackages, err := determinePulumiPackages(virtualenv, cwd)\\n[ADD] if err != nil {\\n[ADD] return nil, err\\n[ADD] }\\n[ADD] \\n[ADD] plugins := []*pulumirpc.PluginDependency{}\\n[ADD] for _, pkg := range pulumiPackages {\\n[ADD] \\n[ADD] plugin, err := determinePluginDependency(virtualenv, cwd, pkg.Name, pkg.Version)\\n[ADD] if err != nil {\\n[ADD] return nil, err\\n[ADD] }\\n[ADD] \\n[ADD] if plugin != nil {\\n[ADD] plugins = append(plugins, plugin)\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] return &pulumirpc.GetRequiredPluginsResponse{Plugins: plugins}, nil\\n[ADD] }\\n[ADD] \\n[ADD] // prepareVirtualEnvironment will create and install dependencies in the virtual environment if host.virtualenv is set.\\n[ADD] // The full path to the virtual environment is returned.\\n[ADD] func (host *pythonLanguageHost) prepareVirtualEnvironment(ctx context.Context, cwd string) (string, error) {\\n[ADD] virtualenv := host.virtualenv\\n[ADD] if virtualenv == \"\" {\\n[ADD] return \"\", nil\\n[ADD] }\\n[ADD] \\n[ADD] // Make sure it\\'s an absolute path.\\n[ADD] if !path.IsAbs(virtualenv) {\\n[ADD] virtualenv = filepath.Join(cwd, virtualenv)\\n[ADD] }\\n[ADD] \\n[ADD] // If the virtual environment directory doesn\\'t exist, create it.\\n[ADD] var createVirtualEnv bool\\n[ADD] info, err := os.Stat(virtualenv)\\n[ADD] if err != nil {\\n[ADD] if os.IsNotExist(err) {\\n[ADD] createVirtualEnv = true\\n[ADD] } else {\\n[ADD] return \"\", err\\n[ADD] }\\n[ADD] } else if !info.IsDir() {\\n[ADD] return \"\",\\n[ADD] errors.Errorf(\"the \\'virtualenv\\' option in Pulumi.yaml is set to %q but it is not a directory\", virtualenv)\\n[ADD] }\\n[ADD] \\n[ADD] // If the virtual environment directory exists, but is empty, it needs to be created.\\n[ADD] if !createVirtualEnv {\\n[ADD] empty, err := fsutil.IsDirEmpty(virtualenv)\\n[ADD] if err != nil {\\n[ADD] return \"\", err\\n[ADD] }\\n[ADD] createVirtualEnv = empty\\n[ADD] }\\n[ADD] \\n[ADD] // Create the virtual environment and install dependencies into it, if needed.\\n[ADD] if createVirtualEnv {\\n[ADD] // Make a connection to the real engine that we will log messages to.\\n[ADD] conn, err := grpc.Dial(\\n[ADD] host.engineAddress,\\n[ADD] grpc.WithInsecure(),\\n[ADD] rpcutil.GrpcChannelOptions(),\\n[ADD] )\\n[ADD] if err != nil {\\n[ADD] return \"\", errors.Wrapf(err, \"language host could not make connection to engine\")\\n[ADD] }\\n[ADD] \\n[ADD] // Make a client around that connection.\\n[ADD] engineClient := pulumirpc.NewEngineClient(conn)\\n[ADD] \\n[ADD] // Create writers that log the output of the install operation as ephemeral messages.\\n[ADD] streamID := rand.Int31() //nolint:gosec\\n[ADD] \\n[ADD] infoWriter := &logWriter{\\n[ADD] ctx:          ctx,\\n[ADD] engineClient: engineClient,\\n[ADD] streamID:     streamID,\\n[ADD] severity:     pulumirpc.LogSeverity_INFO,\\n[ADD] }\\n[ADD] \\n[ADD] errorWriter := &logWriter{\\n[ADD] ctx:          ctx,\\n[ADD] engineClient: engineClient,\\n[ADD] streamID:     streamID,\\n[ADD] severity:     pulumirpc.LogSeverity_ERROR,\\n[ADD] }\\n[ADD] \\n[ADD] if err := python.InstallDependenciesWithWriters(\\n[ADD] cwd, virtualenv, true /*showOutput*/, infoWriter, errorWriter); err != nil {\\n[ADD] return \"\", err\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] // Ensure the specified virtual directory is a valid virtual environment.\\n[ADD] if !python.IsVirtualEnv(virtualenv) {\\n[ADD] return \"\", python.NewVirtualEnvError(host.virtualenv, virtualenv)\\n[ADD] }\\n[ADD] \\n[ADD] // Return the full path to the virtual environment.\\n[ADD] return virtualenv, nil\\n[ADD] }\\n[ADD] \\n[ADD] type logWriter struct {\\n[ADD] ctx          context.Context\\n[ADD] engineClient pulumirpc.EngineClient\\n[ADD] streamID     int32\\n[ADD] severity     pulumirpc.LogSeverity\\n[ADD] }\\n[ADD] \\n[ADD] func (w *logWriter) Write(p []byte) (n int, err error) {\\n[ADD] val := string(p)\\n[ADD] if _, err := w.engineClient.Log(w.ctx, &pulumirpc.LogRequest{\\n[ADD] Message:   strings.ToValidUTF8(val, \"�\"),\\n[ADD] Urn:       \"\",\\n[ADD] Ephemeral: true,\\n[ADD] StreamId:  w.streamID,\\n[ADD] Severity:  w.severity,\\n[ADD] }); err != nil {\\n[ADD] return 0, err\\n[ADD] }\\n[ADD] return len(val), nil\\n[ADD] }\\n[ADD] \\n[ADD] // These packages are known not to have any plugins.\\n[ADD] var packagesWithoutPlugins = map[string]struct{}{\\n[ADD] \"pulumi-policy\": {},\\n[ADD] }\\n[ADD] \\n[ADD] type pythonPackage struct {\\n[ADD] Name    string `json:\"name\"`\\n[ADD] Version string `json:\"version\"`\\n[ADD] }\\n[ADD] \\n[ADD] func determinePulumiPackages(virtualenv, cwd string) ([]pythonPackage, error) {\\n[ADD] logging.V(5).Infof(\"GetRequiredPlugins: Determining pulumi packages\")\\n[ADD] \\n[ADD] // Run the `python -m pip list --format json` command.\\n[ADD] args := []string{\"-m\", \"pip\", \"list\", \"--format\", \"json\"}\\n[ADD] output, err := runPythonCommand(virtualenv, cwd, args...)\\n[ADD] if err != nil {\\n[ADD] return nil, err\\n[ADD] }\\n[ADD] \\n[ADD] // Parse the JSON output.\\n[ADD] var packages []pythonPackage\\n[ADD] if err := json.Unmarshal(output, &packages); err != nil {\\n[ADD] return nil, errors.Wrapf(err, \"parsing `python %s` output\", strings.Join(args, \" \"))\\n[ADD] }\\n[ADD] \\n[ADD] // Only return Pulumi packages.\\n[ADD] var pulumiPackages []pythonPackage\\n[ADD] for _, pkg := range packages {\\n[ADD] // We\\'re only interested in packages that start with \"pulumi-\".\\n[ADD] if !strings.HasPrefix(pkg.Name, \"pulumi-\") {\\n[ADD] continue\\n[ADD] }\\n[ADD] \\n[ADD] // Skip packages that are known not have an associated plugin.\\n[ADD] if _, ok := packagesWithoutPlugins[pkg.Name]; ok {\\n[ADD] continue\\n[ADD] }\\n[ADD] \\n[ADD] pulumiPackages = append(pulumiPackages, pkg)\\n[ADD] }\\n[ADD] \\n[ADD] logging.V(5).Infof(\"GetRequiredPlugins: Pulumi packages: %#v\", pulumiPackages)\\n[ADD] \\n[ADD] return pulumiPackages, nil\\n[ADD] }\\n[ADD] \\n[ADD] //nolint:lll\\n[ADD] type pulumiPlugin struct {\\n[ADD] Resource bool   `json:\"resource\"` // Indicates whether the package has an associated resource plugin. Set to false to indicate no plugin.\\n[ADD] Name     string `json:\"name\"`     // Optional plugin name. If not set, the plugin name is derived from the package name.\\n[ADD] Version  string `json:\"version\"`  // Optional plugin version. If not set, the version is derived from the package version (if possible).\\n[ADD] Server   string `json:\"server\"`   // Optional plugin server. If not set, the default server is used when installing the plugin.\\n[ADD] }\\n[ADD] \\n[ADD] // determinePluginDependency attempts to determine a plugin associated with a package. It checks to see if the package\\n[ADD] // contains a pulumiplugin.json file and uses the information in that file to determine the plugin. If `resource` in\\n[ADD] // pulumiplugin.json is set to false, nil is returned. If the name or version aren\\'t specified in the file, these values\\n[ADD] // are derived from the package name and version. If the plugin version cannot be determined from the package version,\\n[ADD] // nil is returned.\\n[ADD] func determinePluginDependency(\\n[ADD] virtualenv, cwd, packageName, packageVersion string) (*pulumirpc.PluginDependency, error) {\\n[ADD] \\n[ADD] logging.V(5).Infof(\"GetRequiredPlugins: Determining plugin dependency: %v, %v\", packageName, packageVersion)\\n[ADD] \\n[ADD] // Determine the location of the installed package.\\n[ADD] packageLocation, err := determinePackageLocation(virtualenv, cwd, packageName)\\n[ADD] if err != nil {\\n[ADD] return nil, err\\n[ADD] }\\n[ADD] \\n[ADD] // The name of the module inside the package can be different from the package name.\\n[ADD] // However, our convention is to always use the same name, e.g. a package name of\\n[ADD] // \"pulumi-aws\" will have a module named \"pulumi_aws\", so we can determine the module\\n[ADD] // by replacing hyphens with underscores.\\n[ADD] packageModuleName := strings.ReplaceAll(packageName, \"-\", \"_\")\\n[ADD] \\n[ADD] pulumiPluginFilePath := filepath.Join(packageLocation, packageModuleName, \"pulumiplugin.json\")\\n[ADD] logging.V(5).Infof(\"GetRequiredPlugins: pulumiplugin.json file path: %s\", pulumiPluginFilePath)\\n[ADD] \\n[ADD] var name, version, server string\\n[ADD] b, err := ioutil.ReadFile(pulumiPluginFilePath)\\n[ADD] if err == nil {\\n[ADD] var plugin pulumiPlugin\\n[ADD] if err := json.Unmarshal(b, &plugin); err != nil {\\n[ADD] return nil, err\\n[ADD] }\\n[ADD] \\n[ADD] // If `resource` is set to false, the Pulumi package has indicated that there is no associated plugin.\\n[ADD] // Ignore it.\\n[ADD] if !plugin.Resource {\\n[ADD] logging.V(5).Infof(\"GetRequiredPlugins: Ignoring package %s with resource set to false\", packageName)\\n[ADD] return nil, nil\\n[ADD] }\\n[ADD] \\n[ADD] name, version, server = plugin.Name, plugin.Version, plugin.Server\\n[ADD] } else if !os.IsNotExist(err) {\\n[ADD] // If the file doesn\\'t exist, the name and version of the plugin will attempt to be determined from the\\n[ADD] // packageName and packageVersion. If it\\'s some other error, report it.\\n[ADD] logging.V(5).Infof(\"GetRequiredPlugins: err: %v\", err)\\n[ADD] return nil, err\\n[ADD] }\\n[ADD] \\n[ADD] if name == \"\" {\\n[ADD] name = strings.TrimPrefix(packageName, \"pulumi-\")\\n[ADD] }\\n[ADD] \\n[ADD] if version == \"\" {\\n[ADD] // The packageVersion may include additional pre-release tags (e.g. \"2.14.0a1605583329\" for an alpha\\n[ADD] // release, \"2.14.0b1605583329\" for a beta release, \"2.14.0rc1605583329\" for an rc release, etc.).\\n[ADD] // Unfortunately, this is not enough information to determine the plugin version. A package version of\\n[ADD] // \"3.31.0a1605189729\" will have an associated plugin with a version of \"3.31.0-alpha.1605189729+42435656\".\\n[ADD] // The \"+42435656\" suffix cannot be determined so the plugin version cannot be determined. In such cases,\\n[ADD] // log the issue and skip the package.\\n[ADD] version, err = determinePluginVersion(packageVersion)\\n[ADD] if err != nil {\\n[ADD] logging.V(5).Infof(\\n[ADD] \"GetRequiredPlugins: Could not determine plugin version for package %s with version %s\",\\n[ADD] packageName, packageVersion)\\n[ADD] return nil, nil\\n[ADD] }\\n[ADD] }\\n[ADD] if !strings.HasPrefix(version, \"v\") {\\n[ADD] // Add \"v\" prefix if not already present.\\n[ADD] version = fmt.Sprintf(\"v%s\", version)\\n[ADD] }\\n[ADD] \\n[ADD] result := pulumirpc.PluginDependency{\\n[ADD] Name:    name,\\n[ADD] Version: version,\\n[ADD] Kind:    \"resource\",\\n[ADD] Server:  server,\\n[ADD] }\\n[ADD] \\n[ADD] logging.V(5).Infof(\"GetRequiredPlugins: Determining plugin dependency: %#v\", result)\\n[ADD] return &result, nil\\n[ADD] }\\n[ADD] \\n[ADD] // determinePackageLocation determines the location on disk of the package by running `python -m pip show <package>`\\n[ADD] // and parsing the output.\\n[ADD] func determinePackageLocation(virtualenv, cwd, packageName string) (string, error) {\\n[ADD] b, err := runPythonCommand(virtualenv, cwd, \"-m\", \"pip\", \"show\", packageName)\\n[ADD] if err != nil {\\n[ADD] return \"\", err\\n[ADD] }\\n[ADD] return parseLocation(packageName, string(b))\\n[ADD] }\\n[ADD] \\n[ADD] func parseLocation(packageName, pipShowOutput string) (string, error) {\\n[ADD] // We want the value of Location from the following output of `python -m pip show <packageName>`:\\n[ADD] // $ python -m pip show pulumi-aws\\n[ADD] // Name: pulumi-aws\\n[ADD] // Version: 3.12.2\\n[ADD] // Summary: A Pulumi package for creating and managing Amazon Web Services (AWS) cloud resources.\\n[ADD] // Home-page: https://pulumi.io\\n[ADD] // Author: None\\n[ADD] // Author-email: None\\n[ADD] // License: Apache-2.0\\n[ADD] // Location: /Users/user/proj/venv/lib/python3.8/site-packages\\n[ADD] // Requires: parver, pulumi, semver\\n[ADD] // Required-by:\\n[ADD] lines := strings.Split(pipShowOutput, \"\\\\n\")\\n[ADD] for _, line := range lines {\\n[ADD] line = strings.TrimSpace(line)\\n[ADD] if strings.HasPrefix(line, \"Location:\") {\\n[ADD] return strings.TrimSpace(strings.TrimPrefix(line, \"Location:\")), nil\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] return \"\", errors.Errorf(\"determining location of package %s\", packageName)\\n[ADD] }\\n[ADD] \\n[ADD] // determinePluginVersion attempts to convert a PEP440 package version into a plugin version.\\n[ADD] // The package version must have only major.minor.patch components and each must be integers only.\\n[ADD] // If there are any other characters in the component (e.g. pre-release tags), an error is returned\\n[ADD] // because there isn\\'t enough information to determine the plugin version from a pre-release tag.\\n[ADD] func determinePluginVersion(packageVersion string) (string, error) {\\n[ADD] components := strings.Split(packageVersion, \".\")\\n[ADD] if len(components) < 2 || len(components) > 3 {\\n[ADD] return \"\", errors.Errorf(\"unexpected number of components in version %q\", packageVersion)\\n[ADD] }\\n[ADD] \\n[ADD] // Ensure each component is an integer.\\n[ADD] for i := range components {\\n[ADD] if _, err := strconv.ParseInt(components[i], 10, 64); err != nil {\\n[ADD] names := []string{\"major\", \"minor\", \"patch\"}\\n[ADD] return \"\", errors.Errorf(\"parsing %s: %q\", names[i], components[i])\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] return packageVersion, nil\\n[ADD] }\\n[ADD] \\n[ADD] func runPythonCommand(virtualenv, cwd string, arg ...string) ([]byte, error) {\\n[ADD] var err error\\n[ADD] var cmd *exec.Cmd\\n[ADD] if virtualenv != \"\" {\\n[ADD] cmd = python.VirtualEnvCommand(virtualenv, \"python\", arg...)\\n[ADD] } else {\\n[ADD] cmd, err = python.Command(arg...)\\n[ADD] if err != nil {\\n[ADD] return nil, err\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] if logging.V(5) {\\n[ADD] commandStr := strings.Join(arg, \" \")\\n[ADD] logging.V(5).Infof(\"Language host launching process: %s %s\", cmd.Path, commandStr)\\n[ADD] }\\n[ADD] \\n[ADD] cmd.Dir = cwd\\n[ADD] output, err := cmd.Output()\\n[ADD] if err != nil {\\n[ADD] return nil, err\\n[ADD] }\\n[ADD] \\n[ADD] if logging.V(9) {\\n[ADD] logging.V(9).Infof(\"Process output: %s\", string(output))\\n[ADD] }\\n[ADD] \\n[ADD] return output, err\\n }\\n \\n // RPC endpoint for LanguageRuntimeServer::Run',\n",
              " '       Instant windowExpiry = window.maxTimestamp().plus(allowedLateness);\\n       if (TimeDomain.EVENT_TIME.equals(spec.getTimeDomain())) {\\n         checkArgument(\\n[DEL] !outputTimestamp.isAfter(target),\\n[ADD] !outputTimestamp.isAfter(windowExpiry),\\n             \"Attempted to set an event-time timer with an output timestamp of %s that is\"\\n[DEL] + \" after the timer firing timestamp %s\",\\n[ADD] + \" after the expiration of window %s\",\\n             outputTimestamp,\\n[DEL] target);\\n[ADD] windowExpiry);\\n         checkArgument(\\n             !target.isAfter(windowExpiry),\\n             \"Attempted to set an event-time timer with a firing timestamp of %s that is\"',\n",
              " ' \\t\\t}\\n \\t}\\n \\n[ADD] @Test\\n[ADD] public void testServiceProviderProvidesService() throws Exception {\\n[ADD] try (Builder b = new Builder();) {\\n[ADD] b.addClasspath(IO.getFile(\"bin_test\"));\\n[ADD] b.setPrivatePackage(\"test.annotationheaders.spi.provider\");\\n[ADD] b.build();\\n[ADD] b.getJar()\\n[ADD] .getManifest()\\n[ADD] .write(System.out);\\n[ADD] assertTrue(b.check());\\n[ADD] \\n[ADD] Attributes mainAttributes = b.getJar()\\n[ADD] .getManifest()\\n[ADD] .getMainAttributes();\\n[ADD] \\n[ADD] Header req = Header.parseHeader(mainAttributes.getValue(Constants.REQUIRE_CAPABILITY));\\n[ADD] assertEquals(2, req.size());\\n[ADD] \\n[ADD] assertExtender(req, \"osgi.serviceloader.registrar\");\\n[ADD] assertEE(req);\\n[ADD] \\n[ADD] Parameters cap = OSGiHeader.parseHeader(mainAttributes.getValue(Constants.PROVIDE_CAPABILITY));\\n[ADD] assertEquals(3, cap.size());\\n[ADD] \\n[ADD] Attrs p = cap.get(\"osgi.service\");\\n[ADD] assertNotNull(p);\\n[ADD] assertNotNull(p.getTyped(\"objectClass\"));\\n[ADD] assertThat(p.getTyped(Attrs.LIST_STRING, \"objectClass\")).contains(\"test.annotationheaders.spi.SPIService\");\\n[ADD] assertEquals(\"active\", p.get(\"effective:\"));\\n[ADD] }\\n[ADD] }\\n[ADD] \\n \\t@Test\\n \\tpublic void testServiceProvider() throws Exception {\\n \\t\\ttry (Builder b = new Builder();) {',\n",
              " '         }\\n     }\\n \\n[ADD] @Override\\n[ADD] protected void onActivityResult(int requestCode, int resultCode, Intent data) {\\n[ADD] super.onActivityResult(requestCode, resultCode, data);\\n[ADD] try {\\n[ADD] if (requestCode == RESULT_LOAD_IMG && resultCode == RESULT_OK && null != data) {\\n[ADD] Cursor cursor = null;\\n[ADD] try {\\n[ADD] Uri selectedImage = data.getData();\\n[ADD] String[] filePathColumn = { MediaStore.Images.Media.DATA };\\n[ADD] cursor = getContentResolver().query(selectedImage, filePathColumn, null, null, null);\\n[ADD] cursor.moveToFirst();\\n[ADD] int columnIndex = cursor.getColumnIndex(filePathColumn[0]);\\n[ADD] String imgPathString = cursor.getString(columnIndex);\\n[ADD] File sourceFile = new File(imgPathString);\\n[ADD] \\n[ADD] String currentAnkiDroidDirectory = CollectionHelper.getCurrentAnkiDroidDirectory(this);\\n[ADD] String imageName = \"DeckPickerBackground.png\";\\n[ADD] File destFile = new File(currentAnkiDroidDirectory, imageName);\\n[ADD] \\n[ADD] try (FileChannel sourceChannel = new FileInputStream(sourceFile).getChannel();\\n[ADD] FileChannel destChannel = new FileOutputStream(destFile).getChannel()) {\\n[ADD] destChannel.transferFrom(sourceChannel, 0, sourceChannel.size());\\n[ADD] UIUtils.showThemedToast(this, getString(R.string.background_image_applied), false);\\n[ADD] }\\n[ADD] } catch (Exception ex1) {\\n[ADD] Timber.e(\"%s\",ex1.getLocalizedMessage());\\n[ADD] } finally {\\n[ADD] if (cursor != null) {\\n[ADD] cursor.close();\\n[ADD] }\\n[ADD] }\\n[ADD] } else {\\n[ADD] UIUtils.showThemedToast(this, getString(R.string.no_image_selected), false);\\n[ADD] }\\n[ADD] } catch (OutOfMemoryError | Exception e) {\\n[ADD] UIUtils.showThemedToast(this, getString(R.string.error_selecting_image), false);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private void addThirdPartyAppsListener(PreferenceScreen screen) {\\n[ADD] //#5864 - some people don\\'t have a browser so we can\\'t use <intent>\\n[ADD] //and need to handle the keypress ourself.\\n[ADD] Preference showThirdParty = screen.findPreference(\"thirdpartyapps_link\");\\n[ADD] final String githubThirdPartyAppsUrl = \"https://github.com/ankidroid/Anki-Android/wiki/Third-Party-Apps\";\\n[ADD] showThirdParty.setOnPreferenceClickListener((preference) -> {\\n[ADD] try {\\n[ADD] Intent openThirdPartyAppsIntent = new Intent(Intent.ACTION_VIEW, Uri.parse(githubThirdPartyAppsUrl));\\n[ADD] super.startActivity(openThirdPartyAppsIntent);\\n[ADD] } catch (ActivityNotFoundException e) {\\n[ADD] //We use a different message here. We have limited space in the snackbar\\n[ADD] String error = getString(R.string.activity_start_failed_load_url, githubThirdPartyAppsUrl);\\n[ADD] UIUtils.showSimpleSnackbar(this, error, false);\\n[ADD] }\\n[ADD] return true;\\n[ADD] });\\n[ADD] }\\n[ADD] \\n \\n     /**\\n      * Loop over every preference in the list and set the summary text',\n",
              " ' \\n     protected static final String JERSEY_RESPONSE = \"jersey_response\";\\n \\n[ADD] public static final String EXTERNAL_ENTITIES_PROPERTY =\\n[ADD] SYSTEM_PROPERTY_PREFIX + \"xml.expandExternalEntities\";\\n[ADD] public static final String EXPAND_ENTITIES_PROPERTY =\\n[ADD] SYSTEM_PROPERTY_PREFIX + \"xml.expandInternalEntities\";\\n[ADD] \\n[ADD] public static Boolean isExternalEntitiesEnabled()\\n[ADD] {\\n[ADD] String externalEntitiesValue = System.getProperty(EXTERNAL_ENTITIES_PROPERTY, \"false\");\\n[ADD] return Boolean.parseBoolean(externalEntitiesValue);\\n[ADD] }\\n[ADD] \\n[ADD] public static Boolean isExpandEntitiesEnabled()\\n[ADD] {\\n[ADD] String expandInternalEntitiesValue = System.getProperty(EXPAND_ENTITIES_PROPERTY, \"false\");\\n[ADD] return Boolean.parseBoolean(expandInternalEntitiesValue);\\n[ADD] }\\n[ADD] \\n     /**\\n      * Default dummy security context.\\n      */',\n",
              " '                 return rv;\\n             }\\n             case MODELS_ID_TEMPLATES_ID: {\\n[DEL] /* Direct access model template with specific ID\\n[DEL] */\\n[ADD] /* Direct access model template with specific ID */\\n                 int ord = Integer.parseInt(uri.getLastPathSegment());\\n                 JSONObject currentModel = col.getModels().get(getModelIdFromUri(uri, col));\\n                 String[] columns = ((projection != null) ? projection : CardTemplate.DEFAULT_PROJECTION);',\n",
              " ' \\t\\tmh = mh.bindTo(g);\\n \\t\\tmh = mh.asVarargsCollector(String[].class);\\n \\t\\t\\n[DEL] AssertJUnit.assertEquals(MethodType.methodType(String.class,String[].class),mh.type());\\n[DEL] AssertJUnit.assertEquals(\"[A,star,I am looking at,may already have,died]\",mh.invoke(\"A\",\"star\",\"I am looking at\",\"may already have\",\"died\"));\\n[ADD] Assert.assertEquals(mh.type(), MethodType.methodType(String.class,String[].class));\\n[ADD] Assert.assertEquals(mh.invoke(\"A\",\"star\",\"I am looking at\",\"may already have\",\"died\"), \"[A,star,I am looking at,may already have,died]\");\\n \\t}\\n \\t\\n \\t/**',\n",
              " '           ConfigurationKeys.LOCAL_FS_URI));\\n       this.writerFileSystemByBranches.add(FileSystem.get(writerUri, conf));\\n \\n[DEL] URI publisherUri = URI.create(this.getState().getProp(ForkOperatorUtils.getPropertyNameForBranch(\\n[DEL] ConfigurationKeys.DATA_PUBLISHER_FILE_SYSTEM_URI, this.numBranches, i), writerUri.toString()));\\n[ADD] URI publisherUri = URI.create(this.getState().getProp(ForkOperatorUtils\\n[ADD] .getPropertyNameForBranch(ConfigurationKeys.DATA_PUBLISHER_FILE_SYSTEM_URI, this.numBranches, i),\\n[ADD] writerUri.toString()));\\n       this.publisherFileSystemByBranches.add(FileSystem.get(publisherUri, conf));\\n       this.metaDataWriterFileSystemByBranches.add(FileSystem.get(publisherUri, conf));\\n ',\n",
              " '   }\\n \\n   @Override\\n[DEL] protected String responseHeader(HttpResponse httpResponse, String name) {\\n[ADD] protected String responseHeader(HttpResponse<?> httpResponse, String name) {\\n     return httpResponse.headers().firstValue(name).orElse(null);\\n   }\\n \\n[ADD] @Override\\n[ADD] protected Span onResponse(Span span, HttpResponse<?> httpResponse) {\\n[ADD] span = super.onResponse(span, httpResponse);\\n[ADD] \\n[ADD] if (httpResponse != null) {\\n[ADD] SemanticAttributes.HTTP_FLAVOR.set(\\n[ADD] span, httpResponse.version() == Version.HTTP_1_1 ? \"1.1\" : \"2.0\");\\n[ADD] }\\n[ADD] \\n[ADD] return span;\\n[ADD] }\\n[ADD] \\n   @Override\\n   protected Setter<HttpRequest> getSetter() {\\n     return HttpHeadersInjectAdapter.SETTER;',\n",
              " ' \\n   public static class Builder {\\n \\n[ADD] protected final EngineType engineType;\\n     protected final Properties props = new Properties();\\n     private boolean isIndexConfigSet = false;\\n     private boolean isStorageConfigSet = false;',\n",
              " '   }\\n \\n   static boolean useUnifiedWorker(DataflowPipelineOptions options) {\\n[DEL] return hasExperiment(options, \"use_runner_v2\") || hasExperiment(options, \"use_unified_worker\");\\n[ADD] return hasExperiment(options, \"beam_fn_api\")\\n[ADD] || hasExperiment(options, \"use_runner_v2\")\\n[ADD] || hasExperiment(options, \"use_unified_worker\");\\n   }\\n \\n   static void verifyDoFnSupportedBatch(DoFn<?, ?> fn) {',\n",
              " '         PROTOCOL_PORT = 0;\\n         serviceConfig = null;\\n         referenceConfig = null;\\n[ADD] // The exported service has been unexported\\n[ADD] Assertions.assertTrue(exportedServiceListener.getExportedServices().isEmpty());\\n[ADD] exportedServiceListener = null;\\n         logger.info(getClass().getSimpleName() + \" testcase is ending...\");\\n         // destroy zookeeper only once\\n         logger.info(SingleZooKeeperServer.getZookeeperServerName() + \" is beginning to shutdown...\");',\n",
              " ' \\n import java.util.List;\\n \\n[ADD] import javax.inject.Inject;\\n[ADD] \\n /**\\n  * An object that owns Mule objects and delegates startup/shutdown events to them.\\n  */\\n public abstract class AbstractMuleObjectOwner<T>\\n     implements Lifecycle, MuleContextAware, FlowConstructAware, MessagingExceptionHandlerAware {\\n \\n[ADD] @Inject\\n   protected MuleContext muleContext;\\n   protected FlowConstruct flowConstruct;\\n   protected MessagingExceptionHandler messagingExceptionHandler;\\n \\n[ADD] @Override\\n   public void setMuleContext(MuleContext context) {\\n     this.muleContext = context;\\n   }\\n \\n[ADD] @Override\\n   public void setFlowConstruct(FlowConstruct flowConstruct) {\\n     this.flowConstruct = flowConstruct;\\n   }',\n",
              " '     if (windmillBinary != null) {\\n       filesToStage.add(\"windmill_main=\" + windmillBinary);\\n     }\\n[ADD] int uploadSizeBytes = options.getGcsUploadBufferSizeBytes() == null\\n[ADD] ? 1024 * 1024\\n[ADD] : Math.min(options.getGcsUploadBufferSizeBytes(), 1024 * 1024);\\n[ADD] GcsUtil util = GcsUtilFactory.create(\\n[ADD] Transport.newStorageClient(options).build(),\\n[ADD] options.getExecutorService(),\\n[ADD] uploadSizeBytes);\\n     return PackageUtil.stageClasspathElements(\\n[DEL] options.getFilesToStage(), options.getStagingLocation());\\n[ADD] options.getFilesToStage(), options.getStagingLocation(), util);\\n   }\\n }',\n",
              " '   private boolean isPathDedupeEnabled;\\n \\n   /**\\n[DEL] * Make the deduplication of path to be registered in the Publisher level,\\n[DEL] * So that each invocation of {@link #publishData(Collection)} contribute paths registered to this set.\\n[ADD] * This map serves two purpose:\\n[ADD] * 1. Make the deduplication of path to be registered in the publisher level,\\n[ADD] * so that each invocation of {@link #publishData(Collection)} contribute paths registered to this set.\\n[ADD] *\\n[ADD] * 2. Other than registering a path, there are certain metadata that will be included in hiveObject like numRecords\\n[ADD] * in a partition.\\n[ADD] *\\n[ADD] * Key: The path to be registered.\\n[ADD] * Value: Number of records contained in the partition whose underlying path is the key.\\n    */\\n[DEL] private static Set<String> pathsToRegisterFromSingleState = Sets.newHashSet();\\n[ADD] Map<String, Long> pathToRecordCount = Maps.newHashMap();\\n[ADD] \\n[ADD] public static final String PARTITION_RECORD_COUNT = \"recordCount\";\\n \\n   /**\\n    * @param state This is a Job State',\n",
              " '             // pre-route and build cache, notice that route cache should build on original Invoker list.\\n             // toMergeMethodInvokerMap() will wrap some invokers having different groups, those wrapped invokers not should be routed.\\n             routerChain.setInvokers(newInvokers);\\n[DEL] //            this.methodInvokerMap = multiGroup ? toMergeMethodInvokerMap(newMethodInvokerMap) : newMethodInvokerMap;\\n             this.invokers = multiGroup ? toMergeInvokerList(newInvokers) : newInvokers;\\n             this.urlInvokerMap = newUrlInvokerMap;\\n ',\n",
              " '    private Toolbar toolbar_;\\n    \\n    private RSConnectPublishButton publishButton_;\\n[DEL] \\n[ADD] \\n[ADD] private ToolbarButton zoomButton_;\\n[ADD] private ToolbarButton clearAllButton_;\\n    private ToolbarMenuButton exportButton_;\\n    private Widget exportButtonSeparator_;\\n ',\n",
              " ' \\n package org.fao.geonet.kernel.metadata;\\n \\n[DEL] import com.google.common.base.Function;\\n[DEL] import com.google.common.collect.Lists;\\n import jeeves.server.UserSession;\\n import jeeves.server.context.ServiceContext;\\n[ADD] import org.apache.commons.lang.StringUtils;\\n import org.fao.geonet.ApplicationContextHolder;\\n[DEL] import org.fao.geonet.constants.Params;\\n import org.fao.geonet.domain.*;\\n import org.fao.geonet.kernel.DataManager;\\n[ADD] import org.fao.geonet.kernel.datamanager.IMetadataStatus;\\n import org.fao.geonet.kernel.setting.SettingManager;\\n import org.fao.geonet.kernel.setting.Settings;\\n import org.fao.geonet.repository.MetadataRepository;',\n",
              " '             closeCardBrowser(RESULT_OK, data);\\n         }\\n     }\\n[DEL] \\n[ADD] \\n     @Override\\n     protected void onResume() {\\n         Timber.d(\"onResume()\");',\n",
              " ' import com.ichi2.utils.AdaptionUtil;\\n import com.ichi2.utils.AndroidUiUtils;\\n \\n[ADD] import java.lang.reflect.Method;\\n[ADD] \\n import timber.log.Timber;\\n \\n[ADD] import static androidx.browser.customtabs.CustomTabsIntent.COLOR_SCHEME_DARK;\\n[ADD] import static androidx.browser.customtabs.CustomTabsIntent.COLOR_SCHEME_LIGHT;\\n[ADD] import static androidx.browser.customtabs.CustomTabsIntent.COLOR_SCHEME_SYSTEM;\\n import static com.ichi2.anim.ActivityTransitionAnimation.Direction.*;\\n import static com.ichi2.anim.ActivityTransitionAnimation.Direction;\\n ',\n",
              " '           DEFAULT_PARALLELISM);\\n       setDefaultOnCondition(props, !props.containsKey(UPSERT_PARALLELISM), UPSERT_PARALLELISM, DEFAULT_PARALLELISM);\\n       setDefaultOnCondition(props, !props.containsKey(DELETE_PARALLELISM), DELETE_PARALLELISM, DEFAULT_PARALLELISM);\\n[ADD] setDefaultOnCondition(props, !props.containsKey(UPDATE_PARTIAL_FIELDS), UPDATE_PARTIAL_FIELDS, DEFAULT_UPDATE_PARTIAL_FIELDS);\\n \\n       setDefaultOnCondition(props, !props.containsKey(ROLLBACK_PARALLELISM), ROLLBACK_PARALLELISM,\\n           DEFAULT_ROLLBACK_PARALLELISM);',\n",
              " ' package gobblin.data.management.copy.hive;\\n \\n import java.io.IOException;\\n[ADD] import java.util.Map;\\n import java.util.Properties;\\n[DEL] import java.util.Set;\\n \\n import lombok.Data;\\n import lombok.extern.slf4j.Slf4j;\\n \\n[ADD] import org.apache.hadoop.fs.FileStatus;\\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;\\n import org.apache.hadoop.hive.ql.metadata.Partition;\\n import org.apache.hadoop.hive.ql.metadata.Table;\\n import org.apache.hadoop.mapred.InputFormat;\\n \\n[ADD] import com.google.common.collect.Maps;\\n[ADD] \\n import gobblin.data.management.copy.RecursivePathFinder;\\n import gobblin.util.PathUtils;\\n ',\n",
              " '   @Test\\n   public void testCleanWithReplaceCommits() throws Exception {\\n     HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)\\n[DEL] .withMetadataConfig(HoodieMetadataConfig.newBuilder().withAssumeDatePartitioning(true).build())\\n[ADD] .withMetadataConfig(HoodieMetadataConfig.newBuilder().withAssumeDatePartitioning(true).enable(false).build())\\n         .withCompactionConfig(HoodieCompactionConfig.newBuilder()\\n             .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(2).build())\\n         .build();',\n",
              " ' import java.io.IOException;\\n import java.util.List;\\n import java.util.Properties;\\n[ADD] import java.util.concurrent.TimeUnit;\\n[ADD] import java.util.concurrent.locks.Condition;\\n[ADD] import java.util.concurrent.locks.Lock;\\n[ADD] import java.util.concurrent.locks.ReentrantLock;\\n \\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;',\n",
              " '                     x.stop();\\n \\n                     final String msg = \"SystemJob <\" + job.getId() + \"> [\" + jobClass + \"] finished in \" + x.elapsed(\\n[DEL] TimeUnit.MILLISECONDS) + \"ms.\";\\n[ADD] TimeUnit.MILLISECONDS) + \"ms.\";\\n                     LOG.info(msg);\\n                     activityWriter.write(new Activity(msg, SystemJobManager.class));\\n                 } catch (SystemJobConcurrencyException ignored) {',\n",
              " '     }\\n \\n     // make next commit, with 2 updates to existing files, and 1 insert\\n[DEL] String file3P0C2 = testTable.addCommit(\"00000000000003\")\\n[DEL] .withBaseFilesInPartition(p0, file1P0C0, file2P0C1)\\n[DEL] .getFileIdsWithBaseFilesInPartitions(p0).get(p0);\\n[ADD] final String file3P0C2 = UUID.randomUUID().toString();\\n[ADD] Map<String, List<Pair<String, Integer>>> c3PartitionToFilesNameLengthMap = new HashMap<>();\\n[ADD] c3PartitionToFilesNameLengthMap.put(p0, Arrays.asList(Pair.of(file1P0C0, 102), Pair.of(file2P0C1, 101),\\n[ADD] Pair.of(file3P0C2, 100)));\\n[ADD] testTable.doWriteOperation(\"00000000000003\", WriteOperationType.UPSERT, Collections.emptyList(),\\n[ADD] c3PartitionToFilesNameLengthMap, false, false);\\n[ADD] \\n     List<HoodieCleanStat> hoodieCleanStatsThree = runCleaner(config, 3);\\n     assertEquals(2,\\n         getCleanStat(hoodieCleanStatsThree, p0)',\n",
              " '          actually sent to the server. */\\n       final ChangeTracker thisChangeTracker = changeTracker_.fork();\\n \\n[ADD] // TODO: we should have a more structured way of handling various\\n[ADD] // document properties that we check + save here\\n       final String newContents = docDisplay_.getCode();\\n       String oldContents = sourceDoc_.getContents();\\n       final String hash = sourceDoc_.getHash();\\n \\n[DEL] final String foldSpec = Fold.encode(Fold.flatten(docDisplay_.getFolds()));\\n[DEL] String oldFoldSpec = sourceDoc_.getFoldSpec();\\n[ADD] final String newFoldSpec = Fold.encode(Fold.flatten(docDisplay_.getFolds()));\\n[ADD] final String oldFoldSpec = sourceDoc_.getFoldSpec();\\n[ADD] \\n[ADD] final String newSelectionSpec = Range.encode(docDisplay_.getSelectionRange());\\n[ADD] final String oldSelectionSpec = sourceDoc_.getSelectionSpec();\\n[ADD] \\n[ADD] final String newMarksSpec = docDisplay_.isVimModeOn()\\n[ADD] ? VimMarks.encode(docDisplay_.getMarks())\\n[ADD] : \"\";\\n[ADD] final String oldMarksSpec = sourceDoc_.getMarksSpec();\\n       \\n       final JsArray<ChunkDefinition> newChunkDefs = docDisplay_.getChunkDefs();\\n       JsArray<ChunkDefinition> oldChunkDefs = ',\n",
              " ' \\t\\t\\t} else {\\n \\t\\t\\t\\tm = BINDDESCRIPTORDS13.matcher(methodDescriptor);\\n \\t\\t\\t\\tif (m.matches()) {\\n[DEL] inferredService = m.group(6);\\n[ADD] inferredService = m.group(7);\\n \\t\\t\\t\\t\\tif (inferredService != null)\\n \\t\\t\\t\\t\\t\\tinferredService = Descriptors.binaryToFQN(inferredService);\\n \\t\\t\\t\\t\\tdef.updateVersion(V1_3);\\n[DEL] if (!ReferenceScope.PROTOTYPE.equals(scope) && m.group(3) != null) {\\n[ADD] if (!ReferenceScope.PROTOTYPE.equals(def.scope) && m.group(3) != null) {\\n \\t\\t\\t\\t\\t\\tanalyzer.error(\\n[DEL] \"In component %s, to use ServiceObjects the scope must be \\'prototype\\'\",\\n[ADD] \"In component %s, to use ComponentServiceObjects the scope must be \\'prototype\\'\",\\n \\t\\t\\t\\t\\t\\t\\t\\tcomponent.implementation, \"\");\\t\\t\\t\\t\\n \\t\\t\\t\\t\\t}\\n[DEL] hasMapReturnType = m.group(8) != null;\\n[ADD] if (annoService == null)\\n[ADD] if (m.group(2) != null)\\n[ADD] plainType = \"Lorg/osgi/framework/ServiceReference<\";\\n[ADD] else if (m.group(3) != null)\\n[ADD] plainType = \"Lorg/osgi/service/component/ComponentServiceObjects<\";\\n[ADD] else if (m.group(5) != null)\\n[ADD] plainType = \"Ljava/util/Map$Entry<Ljava/util/Map<Ljava/lang/String;Ljava/lang/Object;>;\";\\n[ADD] \\n[ADD] hasMapReturnType = m.group(9) != null;\\n \\t\\t\\t\\t} else { \\n \\t\\t\\t\\t\\treturn null;\\n \\t\\t\\t\\t}',\n",
              " ' \\t\\t\\t\\t\\ttry {\\n \\t\\t\\t\\t\\t\\tcreateNode(key, value);\\n \\t\\t\\t\\t\\t}\\n[DEL] catch (KeeperException.NodeExistsException e) {\\n[ADD] catch (@SuppressWarnings(UNUSED) KeeperException.NodeExistsException e) {\\n \\t\\t\\t\\t\\t\\tupdateNode(key, value, -1);\\n \\t\\t\\t\\t\\t}\\n \\t\\t\\t\\t}',\n",
              " '     ProgramState.Pop pop = programState.unstackValue(arity);\\n     Preconditions.checkState(pop.values.size() == arity, \"Arguments mismatch for INVOKE\");\\n     // TODO use constraintManager.createMethodSymbolicValue to create relational SV for equals\\n[ADD] programState = pop.state;\\n     SymbolicValue returnSV = instruction.hasReturnValue() ? constraintManager.createSymbolicValue(instruction) : null;\\n     String signature = instruction.fieldOrMethod.completeSignature();\\n     MethodBehavior methodInvokedBehavior = behaviorCache.get(signature);\\n[ADD] enqueueUncheckedExceptions();\\n     if (methodInvokedBehavior != null && methodInvokedBehavior.isComplete()) {\\n       List<SymbolicValue> stack = Lists.reverse(pop.values);\\n       if (!isStatic) {',\n",
              " '     private static final String ZOOKEEPER_BINARY_URL_FORMAT = \"https://archive.apache.org/dist/zookeeper/zookeeper-%s/\" + ZOOKEEPER_FILE_NAME_FORMAT;\\n \\n     /**\\n[DEL] * The temporary directory name\\n[ADD] * The temporary directory.\\n      */\\n[DEL] private static final String TEMPORARY_DIRECTORY_NAME = \"dubbo-mocked-zookeeper\";\\n[ADD] private static final String TEMPORARY_DIRECTORY = \"zookeeper\";\\n[ADD] \\n[ADD] /**\\n[ADD] * Returns {@code true} if the file exists with the given file path, otherwise {@code false}.\\n[ADD] *\\n[ADD] * @param filePath the file path to check.\\n[ADD] */\\n[ADD] private boolean checkFile(Path filePath) {\\n[ADD] return Files.exists(filePath) && filePath.toFile().isFile();\\n[ADD] }\\n \\n     @Override\\n     protected void doInitialize(ZookeeperContext context) throws DubboTestException {\\n[ADD] // checks the zookeeper binary file exists or not\\n[ADD] if (checkFile(context.getSourceFile())) {\\n[ADD] return;\\n[ADD] }\\n         String zookeeperFileName = String.format(ZOOKEEPER_FILE_NAME_FORMAT, context.getVersion());\\n[ADD] Path temporaryFilePath;\\n         try {\\n[DEL] context.setSourceFile(Paths.get(Files.createTempDirectory(\"\").getParent().toString(),\\n[DEL] TEMPORARY_DIRECTORY_NAME,\\n[DEL] zookeeperFileName));\\n[ADD] temporaryFilePath = Paths.get(Files.createTempDirectory(\"\").getParent().toString(),\\n[ADD] TEMPORARY_DIRECTORY,\\n[ADD] zookeeperFileName);\\n         } catch (IOException e) {\\n[DEL] throw new RuntimeException(String.format(\"Cannot create the temporary directory, related directory:%s/%s\",\\n[DEL] TEMPORARY_DIRECTORY_NAME, zookeeperFileName), e);\\n[DEL] }\\n[DEL] // check if the zookeeper binary file exists\\n[DEL] if (context.getSourceFile() != null && context.getSourceFile().toFile().isFile()) {\\n[DEL] return;\\n[ADD] throw new RuntimeException(String.format(\"Cannot create the temporary directory, file path: %s\", TEMPORARY_DIRECTORY), e);\\n         }\\n[ADD] \\n         // create the temporary directory path.\\n[DEL] if (!Files.exists(context.getSourceFile())) {\\n[DEL] try {\\n[DEL] Files.createDirectories(context.getSourceFile());\\n[DEL] } catch (IOException e) {\\n[DEL] throw new RuntimeException(String.format(\"Failed to create the temporary directory to save zookeeper binary file, file path:%s\", context.getSourceFile()), e);\\n[DEL] }\\n[ADD] try {\\n[ADD] Files.createDirectories(temporaryFilePath.getParent());\\n[ADD] } catch (IOException e) {\\n[ADD] throw new RuntimeException(String.format(\"Failed to create the temporary directory to save zookeeper binary file, file path:%s\", temporaryFilePath.getParent()), e);\\n         }\\n[DEL] // download zookeeper binary file\\n[ADD] \\n[ADD] // download zookeeper binary file in temporary directory.\\n         String zookeeperBinaryUrl = String.format(ZOOKEEPER_BINARY_URL_FORMAT, context.getVersion(), context.getVersion());\\n         try {\\n             logger.info(\"It is beginning to download the zookeeper binary archive, it will take several minutes...\");\\n             URL zookeeperBinaryURL = new URL(zookeeperBinaryUrl);\\n             InputStream inputStream = zookeeperBinaryURL.openStream();\\n[DEL] Files.copy(inputStream, context.getSourceFile(), StandardCopyOption.REPLACE_EXISTING);\\n[ADD] Files.copy(inputStream, temporaryFilePath, StandardCopyOption.REPLACE_EXISTING);\\n         } catch (Exception e) {\\n             throw new RuntimeException(String.format(\"Download zookeeper binary archive failed, download url:%s, file path:%s\",\\n[DEL] zookeeperBinaryUrl, context.getSourceFile()), e);\\n[ADD] zookeeperBinaryUrl, temporaryFilePath), e);\\n[ADD] }\\n[ADD] \\n[ADD] // check downloaded zookeeper binary file in temporary directory.\\n[ADD] if (!checkFile(temporaryFilePath)) {\\n[ADD] throw new IllegalArgumentException(String.format(\"There are some unknown problem occurred when downloaded the zookeeper binary archive file, file path:%s\", temporaryFilePath));\\n[ADD] }\\n[ADD] \\n[ADD] // create target directory if necessary\\n[ADD] if (!Files.exists(context.getSourceFile())) {\\n[ADD] try {\\n[ADD] Files.createDirectories(context.getSourceFile().getParent());\\n[ADD] } catch (IOException e) {\\n[ADD] throw new IllegalArgumentException(String.format(\"Failed to create target directory, the directory path: %s\", context.getSourceFile().getParent()), e);\\n[ADD] }\\n         }\\n[DEL] // check if the zookeeper binary file exists again.\\n[DEL] if (context.getSourceFile() == null || !context.getSourceFile().toFile().isFile()) {\\n[ADD] \\n[ADD] // copy the downloaded zookeeper binary file into the target file path\\n[ADD] try {\\n[ADD] Files.copy(temporaryFilePath, context.getSourceFile(), StandardCopyOption.REPLACE_EXISTING);\\n[ADD] } catch (IOException e) {\\n[ADD] throw new IllegalArgumentException(String.format(\"Failed to copy file, the source file path: %s, the target file path: %s\", temporaryFilePath, context.getSourceFile()), e);\\n[ADD] }\\n[ADD] \\n[ADD] // checks the zookeeper binary file exists or not again\\n[ADD] if (!checkFile(context.getSourceFile())) {\\n             throw new IllegalArgumentException(String.format(\"The zookeeper binary archive file doesn\\'t exist, file path:%s\", context.getSourceFile()));\\n         }\\n     }',\n",
              " '   /**\\n    * This predicate checks if a work unit should be skipped. If yes, then it will removed\\n    * from the list of workUnits and it\\'s state will be saved.\\n[ADD] *\\n[ADD] * If complement is set to true, then the filter will be reversed\\n[ADD] *\\n[ADD] * If a MultiWorkUnit is marked as skipped all WorkUnits contained in it will be skipped.\\n[ADD] * If a MultiWorkUnit contains some skipped WorkUnits, only those will be skipped.\\n    */\\n   @RequiredArgsConstructor\\n[ADD] @AllArgsConstructor\\n   private static class SkippedWorkUnitsFilter implements Predicate<WorkUnit> {\\n     private final JobState jobState;\\n[ADD] private boolean complement = false;\\n \\n     @Override\\n     public boolean apply(WorkUnit workUnit) {\\n       if (workUnit instanceof MultiWorkUnit) {\\n[DEL] Preconditions.checkArgument(!workUnit.contains(ConfigurationKeys.WORK_UNIT_SKIP_KEY),\\n[DEL] \"Error: MultiWorkUnit cannot be skipped\");\\n[DEL] for (WorkUnit wu : ((MultiWorkUnit) workUnit).getWorkUnits()) {\\n[DEL] Preconditions.checkArgument(!wu.contains(ConfigurationKeys.WORK_UNIT_SKIP_KEY),\\n[DEL] \"Error: MultiWorkUnit cannot contain skipped WorkUnit\");\\n[ADD] WorkUnitStream wus = ((MultiWorkUnit) workUnit).getWorkUnitsStream();\\n[ADD] ((MultiWorkUnit) workUnit).removeWorkUnits(wus.filter(new SkippedWorkUnitsFilter(this.jobState, true)));\\n[ADD] if (workUnit.getPropAsBoolean(ConfigurationKeys.WORK_UNIT_SKIP_KEY, false)) {\\n[ADD] return complement;\\n         }\\n[ADD] } else if (workUnit.getPropAsBoolean(ConfigurationKeys.WORK_UNIT_SKIP_KEY, false)) {\\n[ADD] setSkipTaskState(this.jobState, workUnit);\\n[ADD] return complement;\\n       }\\n[DEL] if (workUnit.getPropAsBoolean(ConfigurationKeys.WORK_UNIT_SKIP_KEY, false)) {\\n[DEL] WorkUnitState workUnitState = new WorkUnitState(workUnit, this.jobState);\\n[DEL] workUnitState.setWorkingState(WorkUnitState.WorkingState.SKIPPED);\\n[DEL] this.jobState.addSkippedTaskState(new TaskState(workUnitState));\\n[DEL] return false;\\n[DEL] }\\n[DEL] return true;\\n[ADD] return !complement;\\n     }\\n   }\\n \\n[ADD] /**\\n[ADD] * Skip the {@link TaskState} corresponding to the WorkUnit\\n[ADD] */\\n[ADD] private static void setSkipTaskState(JobState jobState, WorkUnit workUnit) {\\n[ADD] WorkUnitState workUnitState = new WorkUnitState(workUnit, jobState);\\n[ADD] workUnitState.setWorkingState(WorkUnitState.WorkingState.SKIPPED);\\n[ADD] TaskState taskState = new TaskState(workUnitState);\\n[ADD] jobState.addSkippedTaskState(taskState);\\n[ADD] }\\n[ADD] \\n   @Override\\n   public void launchJob(JobListener jobListener)\\n       throws JobException {',\n",
              " '   public boolean matches(T target) {\\n     String name = target.getActualName();\\n \\n[ADD] if (name.startsWith(\"jdk.internal.net.http.\")) {\\n[ADD] return false;\\n[ADD] }\\n[ADD] \\n     if (name.startsWith(\"net.bytebuddy.\")\\n         || name.startsWith(\"jdk.\")\\n         || name.startsWith(\"org.aspectj.\")',\n",
              " ' import org.rstudio.studio.client.workbench.views.output.find.FindOutputTab;\\n import org.rstudio.studio.client.workbench.views.output.find.model.FindInFilesServerOperations;\\n import org.rstudio.studio.client.workbench.views.output.renderrmd.RenderRmdOutputTab;\\n[ADD] import org.rstudio.studio.client.workbench.views.output.shinyappsdeploy.ShinyAppsDeployOutputTab;\\n import org.rstudio.studio.client.workbench.views.output.sourcecpp.SourceCppOutputPane;\\n import org.rstudio.studio.client.workbench.views.output.sourcecpp.SourceCppOutputPresenter;\\n import org.rstudio.studio.client.workbench.views.output.sourcecpp.SourceCppOutputTab;',\n",
              " \"    * For each incoming record, produce N output records, 1 each for each file against which the record's key needs to be\\n    * checked. For datasets, where the keys have a definite insert order (e.g: timestamp as prefix), the number of files\\n    * to be compared gets cut down a lot from range pruning.\\n[DEL] *\\n[ADD] * <p>\\n    * Sub-partition to ensure the records can be looked up against files & also prune file<=>record comparisons based on\\n    * recordKey ranges in the index info. the partition path of the incoming record (partitionRecordKeyPairRDD._2()) will\\n    * be ignored since the search scope should be bigger than that\",\n",
              " '    * @return Outer file Path from the InLineFS Path\\n    */\\n   public static Path getOuterFilePathFromInlinePath(Path inlineFSPath) {\\n[DEL] final String scheme = inlineFSPath.getParent().getName();\\n[ADD] assertInlineFSPath(inlineFSPath);\\n[ADD] \\n[ADD] final String baseFileScheme = inlineFSPath.getParent().getName();\\n     final Path basePath = inlineFSPath.getParent().getParent();\\n[DEL] ValidationUtils.checkArgument(basePath.toString().contains(SCHEME_SEPARATOR),\\n[DEL] \"Invalid InLineFSPath: \" + inlineFSPath);\\n[ADD] checkArgument(\\n[ADD] basePath.toString().contains(SCHEME_SEPARATOR),\\n[ADD] \"Invalid InLineFS path: \" + inlineFSPath);\\n \\n     final String pathExceptScheme = basePath.toString().substring(basePath.toString().indexOf(SCHEME_SEPARATOR) + 1);\\n[DEL] final String fullPath = scheme + SCHEME_SEPARATOR\\n[DEL] + (scheme.equals(LOCAL_FILESYSTEM_SCHEME) ? PATH_SEPARATOR : \"\")\\n[ADD] final String fullPath = baseFileScheme + SCHEME_SEPARATOR\\n[ADD] + (baseFileScheme.equals(LOCAL_FILESYSTEM_SCHEME) ? PATH_SEPARATOR : \"\")\\n         + pathExceptScheme;\\n     return new Path(fullPath);\\n   }\\n \\n   /**\\n[DEL] * Eg input : \"inlinefs://file1/s3a/?start_offset=20&length=40\".\\n[DEL] * output: 20\\n[ADD] * Returns start offset w/in the base for the block identified by the given InlineFS path\\n    *\\n[DEL] * @param inlinePath\\n[DEL] * @return\\n[ADD] * input: \"inlinefs://file1/s3a/?start_offset=20&length=40\".\\n[ADD] * output: 20\\n    */\\n[DEL] public static int startOffset(Path inlinePath) {\\n[DEL] String[] slices = inlinePath.toString().split(\"[?&=]\");\\n[ADD] public static int startOffset(Path inlineFSPath) {\\n[ADD] assertInlineFSPath(inlineFSPath);\\n[ADD] \\n[ADD] String[] slices = inlineFSPath.toString().split(\"[?&=]\");\\n     return Integer.parseInt(slices[slices.length - 3]);\\n   }\\n \\n   /**\\n[DEL] * Eg input : \"inlinefs:/file1/s3a/?start_offset=20&length=40\".\\n[DEL] * Output: 40\\n[ADD] * Returns length of the block (embedded w/in the base file) identified by the given InlineFS path\\n    *\\n[DEL] * @param inlinePath\\n[DEL] * @return\\n[ADD] * input: \"inlinefs:/file1/s3a/?start_offset=20&length=40\".\\n[ADD] * output: 40\\n    */\\n   public static int length(Path inlinePath) {\\n[ADD] assertInlineFSPath(inlinePath);\\n[ADD] \\n     String[] slices = inlinePath.toString().split(\"[?&=]\");\\n     return Integer.parseInt(slices[slices.length - 1]);\\n   }\\n \\n[ADD] private static void assertInlineFSPath(Path inlinePath) {\\n[ADD] String scheme = inlinePath.toUri().getScheme();\\n[ADD] checkArgument(InLineFileSystem.SCHEME.equals(scheme));\\n[ADD] }\\n }',\n",
              " ' import org.junit.Rule;\\n import org.junit.Test;\\n \\n[DEL] public class SecureHttpPollingFunctionalTestCase extends FunctionalTestCase {\\n[ADD] public class SecureHttpPollingFunctionalTestCase extends ExtensionFunctionalTestCase {\\n \\n   @Rule\\n   public DynamicPort port1 = new DynamicPort(\"port1\");\\n \\n[ADD] @Override\\n[ADD] protected Class<?>[] getAnnotatedExtensionClasses() {\\n[ADD] return new Class[] {SocketsExtension.class, HttpConnector.class};\\n[ADD] }\\n \\n   @Override\\n   protected String[] getConfigFiles() {',\n",
              " '   public void verifyConfig() throws Exception\\n   {\\n     DefaultHttpRequesterConfig requestConfig = muleContext.getRegistry().lookupObject(\"requestConfig\");\\n[DEL] assertThat(requestConfig.getResponseBufferSize(), equalTo(RESPONSE_BUFFER));\\n     assertThat(requestConfig.getMaxConnections(), equalTo(MAX_CONNECTIONS));\\n     assertThat(requestConfig.getConnectionIdleTimeout(), equalTo(IDLE_TIMEOUT));\\n   }',\n",
              " ' \\n     BeamSqlRecordType beamSQLRowType = CalciteUtils.toBeamRowType(this.getRowType());\\n     for (ImmutableList<RexLiteral> tuple : tuples) {\\n[DEL] BeamRecord row = new BeamRecord(beamSQLRowType);\\n[ADD] List<Object> fieldsValue = new ArrayList<>();\\n       for (int i = 0; i < tuple.size(); i++) {\\n[DEL] BeamTableUtils.addFieldWithAutoTypeCasting(row, i, tuple.get(i).getValue());\\n[ADD] fieldsValue.add(BeamTableUtils.autoCastField(\\n[ADD] beamSQLRowType.getFieldsType().get(i), tuple.get(i).getValue()));\\n       }\\n[DEL] rows.add(row);\\n[ADD] rows.add(new BeamRecord(beamSQLRowType, fieldsValue));\\n     }\\n \\n     return inputPCollections.getPipeline().apply(stageName, Create.of(rows))',\n",
              " '     }\\n \\n     @Test\\n[DEL] public void testVariableAlias() throws RegistrationException, InitialisationException\\n[ADD] public void testVariableAlias() throws Exception\\n     {\\n         MVELExpressionLanguage mvel = new MVELExpressionLanguage(muleContext);\\n         mvel.initialise();\\n \\n[DEL] MuleMessage message = new DefaultMuleMessage(\"foo\", muleContext);\\n[ADD] MuleEvent event = getTestEvent(\"foo\");\\n \\n[DEL] Assert.assertEquals(\"foo\", mvel.evaluate(\"p\", message));\\n[ADD] Assert.assertEquals(\"foo\", mvel.evaluate(\"p\", event));\\n     }\\n \\n     @Test\\n[DEL] public void testAssignValueToVariableAlias() throws RegistrationException, InitialisationException\\n[ADD] public void testAssignValueToVariableAlias() throws Exception\\n     {\\n         MVELExpressionLanguage mvel = new MVELExpressionLanguage(muleContext);\\n         mvel.initialise();\\n \\n[DEL] MuleMessage message = new DefaultMuleMessage(\"foo\", muleContext);\\n[ADD] MuleEvent event = getTestEvent(\"\");\\n \\n[DEL] mvel.evaluate(\"p=\\'bar\\'\", message);\\n[DEL] Assert.assertEquals(\"bar\", message.getPayload());\\n[ADD] mvel.evaluate(\"p=\\'bar\\'\", event);\\n[ADD] Assert.assertEquals(\"bar\", event.getMessage().getPayload());\\n     }\\n \\n     @Test',\n",
              " '     if ($IsAzureStack) {\\n         Add-Member -InputObject $configJson.plugins[0].ipam -MemberType NoteProperty -Name \"environment\" -Value \"mas\"\\n     }\\n[DEL] \\n[ADD] \\n[ADD] if ($global:KubeproxyFeatureGates.Contains(\"WinDSR=true\")) {\\n[ADD] Write-Log \"Setting enableLoopbackDSR in Azure CNI conflist for WinDSR\"\\n[ADD] $jsonContent = [PSCustomObject]@{\\n[ADD] \\'enableLoopbackDSR\\' = $True\\n[ADD] }\\n[ADD] $configJson.plugins[0]|Add-Member -Name \"windowsSettings\" -Value $jsonContent -MemberType NoteProperty\\n[ADD] }\\n[ADD] \\n     $aclRule1 = [PSCustomObject]@{\\n         Type = \\'ACL\\'\\n         Protocols = \\'6\\'',\n",
              " '     return EVENT_NAME + \"{\" + \"action=\" + getActionName(action) + \", transactionStringId=\" + transactionStringId + \", timestamp=\"\\n         + timestamp + \"}\";\\n   }\\n[ADD] \\n[ADD] private static EnrichedNotificationInfo emptyInfo() {\\n[ADD] return new EnrichedNotificationInfo(null, null, null, null, null, null, null, new HashMap<>(), null, null);\\n[ADD] }\\n }',\n",
              " '   }\\n \\n   private HoodieLogBlock readBlock() throws IOException {\\n[DEL] // 2. Read the block type\\n[DEL] int ordinal = inputStream.readInt();\\n[DEL] Preconditions.checkArgument(ordinal < HoodieLogBlockType.values().length,\\n[DEL] \"Invalid block byte ordinal found \" + ordinal);\\n[DEL] HoodieLogBlockType blockType = HoodieLogBlockType.values()[ordinal];\\n \\n[DEL] // 3. Read the size of the block\\n[DEL] int blocksize = inputStream.readInt();\\n[ADD] int blocksize = -1; int ordinal = -1;\\n[ADD] HoodieLogBlockType blockType = null;\\n[ADD] Map<HeaderMetadataType, String> header = null;\\n[ADD] \\n[ADD] try {\\n[ADD] \\n[ADD] if (isOldMagic()) {\\n[ADD] // 1 Read the block type for a log block\\n[ADD] ordinal = inputStream.readInt();\\n[ADD] \\n[ADD] Preconditions.checkArgument(ordinal < HoodieLogBlockType.values().length,\\n[ADD] \"Invalid block byte ordinal found \" + ordinal);\\n[ADD] blockType = HoodieLogBlockType.values()[ordinal];\\n[ADD] \\n[ADD] // 2 Read the total size of the block\\n[ADD] blocksize = inputStream.readInt();\\n[ADD] } else {\\n[ADD] // 1 Read the total size of the block\\n[ADD] blocksize = inputStream.readInt();\\n[ADD] }\\n[ADD] \\n[ADD] } catch(Exception e) {\\n[ADD] // An exception reading any of the above indicates a corrupt block\\n[ADD] // Create a corrupt block by finding the next MAGIC marker or EOF\\n[ADD] return createCorruptBlock();\\n[ADD] }\\n \\n     // We may have had a crash which could have written this block partially\\n     // Skip blocksize in the stream and we should either find a sync marker (start of the next block) or EOF',\n",
              " '    * Creates a {@link WriteFiles} transform that writes to the given {@link FileBasedSink}, letting\\n    * the runner control how many different shards are produced.\\n    */\\n[DEL] public static <T> WriteFiles<T> to(FileBasedSink<T> sink) {\\n[ADD] public static <InT, DestT, OutT> WriteFiles<InT, DestT, OutT> to(\\n[ADD] FileBasedSink<OutT, DestT> sink, SerializableFunction<InT, OutT> formatFunction) {\\n     checkNotNull(sink, \"sink\");\\n[DEL] return new WriteFiles<>(sink, null /* runner-determined sharding */, null,\\n[DEL] false, DEFAULT_MAX_NUM_WRITERS_PER_BUNDLE);\\n[ADD] return new WriteFiles<>(\\n[ADD] sink,\\n[ADD] formatFunction,\\n[ADD] null /* runner-determined sharding */,\\n[ADD] null,\\n[ADD] false,\\n[ADD] DEFAULT_MAX_NUM_WRITERS_PER_BUNDLE);\\n   }\\n \\n   private WriteFiles(\\n[DEL] FileBasedSink<T> sink,\\n[DEL] @Nullable PTransform<PCollection<T>, PCollectionView<Integer>> computeNumShards,\\n[ADD] FileBasedSink<OutputT, DestinationT> sink,\\n[ADD] SerializableFunction<UserT, OutputT> formatFunction,\\n[ADD] @Nullable PTransform<PCollection<UserT>, PCollectionView<Integer>> computeNumShards,\\n       @Nullable ValueProvider<Integer> numShardsProvider,\\n       boolean windowedWrites,\\n       int maxNumWritersPerBundle) {\\n     this.sink = sink;\\n[ADD] this.formatFunction = checkNotNull(formatFunction);\\n     this.computeNumShards = computeNumShards;\\n     this.numShardsProvider = numShardsProvider;\\n     this.windowedWrites = windowedWrites;',\n",
              " '          EventBus events,\\n          FileTypeRegistry registry,\\n          GlobalDisplay display, \\n[DEL] SourceShim sourceShim,\\n[ADD] Source source,\\n          Session session,\\n          UserPrefs uiPrefs)\\n    {',\n",
              " ' import static org.hamcrest.Matchers.notNullValue;\\n import static org.hamcrest.Matchers.nullValue;\\n import static org.hamcrest.Matchers.sameInstance;\\n[ADD] import static org.hamcrest.core.IsNot.not;\\n import static org.junit.rules.ExpectedException.none;\\n import static org.mockito.Matchers.any;\\n import static org.mockito.Mockito.RETURNS_DEEP_STUBS;',\n",
              " '  */\\n public class ConsumeOperation {\\n \\n[ADD] @Inject\\n[ADD] private MuleExpressionLanguage expressionExecutor;\\n[ADD] \\n   /**\\n    * Consumes an operation from a SOAP Web Service.\\n    *\\n    * @param connection the connection resolved to execute the operation.\\n[DEL] * @param operation the name of the web service operation that aims to invoke.\\n[DEL] * @param message the constructed SOAP message to perform the request.\\n[ADD] * @param operation  the name of the web service operation that aims to invoke.\\n[ADD] * @param message    the constructed SOAP message to perform the request.\\n    */\\n   @OnException(WscExceptionEnricher.class)\\n   @Throws(ConsumeErrorTypeProvider.class)\\n   @OutputResolver(output = ConsumeOutputResolver.class, attributes = WscAttributesResolver.class)\\n[DEL] public Result<Object, WscAttributes> consume(@Connection SoapClient connection,\\n[DEL] @MetadataKeyId(OperationKeysResolver.class) String operation,\\n[DEL] // TODO MULE-11235 MULE-11584\\n[DEL] @NullSafe @Optional @TypeResolver(MessageBuilderResolver.class) SoapMessageBuilder message)\\n[ADD] public Result<?, SoapAttributes> consume(@Connection SoapClient connection,\\n[ADD] @MetadataKeyId(OperationKeysResolver.class) String operation,\\n[ADD] // TODO MULE-11235 MULE-11584\\n[ADD] @NullSafe @Optional @TypeResolver(MessageBuilderResolver.class) SoapMessageBuilder message)\\n       throws SoapFaultException {\\n     SoapRequestBuilder requestBuilder = getSoapRequest(operation, message);\\n     SoapResponse response = connection.consume(requestBuilder.build());\\n[DEL] Object result = getResponsePayload(response);\\n[DEL] \\n[DEL] return Result.<Object, WscAttributes>builder().output(result)\\n[DEL] .attributes(new WscAttributes(response.getSoapHeaders(), response.getTransportHeaders())).build();\\n[DEL] }\\n[DEL] \\n[DEL] private Object getResponsePayload(SoapResponse response) {\\n[DEL] Object result;\\n[DEL] if (!response.getAttachments().isEmpty()) {\\n[DEL] ImmutableList<Message> parts = ImmutableList.<Message>builder()\\n[DEL] .add(Message.builder().payload(response.getContent()).attributes(BODY_ATTRIBUTES).build())\\n[DEL] .addAll(response.getAttachments().stream()\\n[DEL] .map(a -> Message.builder().payload(a.getContent()).attributes(new PartAttributes(a.getId())).build())\\n[DEL] .collect(Collectors.toList()))\\n[DEL] .build();\\n[DEL] result = new WscMultipartPayload(parts);\\n[DEL] } else {\\n[DEL] result = response.getContent();\\n[DEL] }\\n[DEL] return result;\\n[ADD] return response.getAsResult();\\n   }\\n \\n   private SoapRequestBuilder getSoapRequest(String operation, SoapMessageBuilder message) {\\n     SoapRequestBuilder requestBuilder = SoapRequest.builder();\\n[DEL] message.getAttachments().forEach((id, attachment) -> requestBuilder\\n[DEL] .withAttachment(new SoapAttachment(id, attachment.getContentType(), attachment.getContent())));\\n[ADD] requestBuilder.withAttachments(message.getAttachments());\\n     requestBuilder.withOperation(operation);\\n[DEL] requestBuilder.withSoapHeaders(buildHeaders(message.getHeaders()));\\n[ADD] \\n[ADD] if (!isBlank(message.getHeaders())) {\\n[ADD] requestBuilder.withSoapHeaders((Map<String, String>) evaluateHeaders(message.getHeaders()));\\n[ADD] }\\n \\n     if (!isBlank(message.getBody())) {\\n       requestBuilder.withContent(message.getBody());',\n",
              " ' \\n     TokenUtils.getAllFSTokens(new Configuration(), credentials, renewerName,\\n         Optional.absent(), ConfigUtils.getStringList(this.config, TokenUtils.OTHER_NAMENODES));\\n[DEL] \\n[ADD] // Only pass token here and no secrets. (since there is no simple way to remove single token/ get secrets)\\n[ADD] // For RM token, only pass the RM token for the current RM, or the RM will fail to update the token\\n[ADD] Credentials finalCredentials = new Credentials();\\n[ADD] for ( Token<? extends TokenIdentifier> token: credentials.getAllTokens()) {\\n[ADD] if (token.getKind().equals(new Text(\"RM_DELEGATION_TOKEN\")) && !token.getService().equals(new Text(this.originalYarnRMAddress))) {\\n[ADD] continue;\\n[ADD] }\\n[ADD] finalCredentials.addToken(token.getService(), token);\\n[ADD] }\\n     Closer closer = Closer.create();\\n     try {\\n       DataOutputBuffer dataOutputBuffer = closer.register(new DataOutputBuffer());\\n[DEL] credentials.writeTokenStorageToStream(dataOutputBuffer);\\n[ADD] finalCredentials.writeTokenStorageToStream(dataOutputBuffer);\\n       ByteBuffer fsTokens = ByteBuffer.wrap(dataOutputBuffer.getData(), 0, dataOutputBuffer.getLength());\\n       containerLaunchContext.setTokens(fsTokens);\\n[DEL] LOGGER.info(\"Setting containerLaunchContext with All credential tokens: \" + credentials.getAllTokens());\\n[ADD] LOGGER.info(\"Setting containerLaunchContext with All credential tokens: \" + finalCredentials.getAllTokens());\\n     } catch (Throwable t) {\\n       throw closer.rethrow(t);\\n     } finally {',\n",
              " '   @Parameter\\n   protected Integer itemsPerPoll;\\n \\n[DEL] @Override\\n[DEL] protected void doStart() throws MuleException {\\n[DEL] resetCounters();\\n[DEL] polls = (pets.size() / itemsPerPoll) + 1;\\n[DEL] }\\n[DEL] \\n[DEL] @Override\\n[DEL] protected void doStop() {\\n[DEL] resetCounters();\\n[DEL] }\\n[DEL] \\n   @OnSuccess\\n   public synchronized void onSuccess() {}\\n ',\n",
              " '     super.afterStart(span);\\n     if (spanName.startsWith(\"queue/\")) {\\n       SemanticAttributes.MESSAGING_DESTINATION_KIND.set(span, \"queue\");\\n[DEL] SemanticAttributes.MESSAGING_DESTINATION.set(span, spanName.replaceFirst(\"queue/\", \"\"));\\n[ADD] SemanticAttributes.MESSAGING_DESTINATION.set(\\n[ADD] span, spanName.replaceFirst(\"^queue/\", \"\").replaceFirst(\" (send|receive)$\", \"\"));\\n     } else if (spanName.startsWith(\"topic/\")) {\\n       SemanticAttributes.MESSAGING_DESTINATION_KIND.set(span, \"topic\");\\n[DEL] SemanticAttributes.MESSAGING_DESTINATION.set(span, spanName.replaceFirst(\"topic/\", \"\"));\\n[ADD] SemanticAttributes.MESSAGING_DESTINATION.set(\\n[ADD] span, spanName.replaceFirst(\"^topic/\", \"\").replaceFirst(\" (send|receive)$\", \"\"));\\n     }\\n[DEL] if (spanName.equals(\"queue/<temporary>\") || spanName.equals(\"topic/<temporary>\")) {\\n[ADD] if (spanName.startsWith(\"queue/<temporary>\") || spanName.startsWith(\"topic/<temporary>\")) {\\n       SemanticAttributes.MESSAGING_TEMP_DESTINATION.set(span, true);\\n     }\\n ',\n",
              " '         mActionBarSpinner.setSelection(position);\\n         if (position == 0) {\\n             mRestrictOnDeck = \"\";\\n[ADD] saveLastDeckId(ALL_DECKS_ID);\\n         } else {\\n             JSONObject deck = mDropDownDecks.get(position - 1);\\n[DEL] String deckName;\\n             try {\\n[DEL] deckName = deck.getString(\"name\");\\n[ADD] mRestrictOnDeck = \"deck:\\\\\"\" + deck.getString(\"name\") + \"\\\\\" \";\\n[ADD] saveLastDeckId(deck.getLong(\"id\"));\\n             } catch (JSONException e) {\\n                 throw new RuntimeException();\\n             }\\n[DEL] try {\\n[DEL] getCol().getDecks().select(deck.getLong(\"id\"));\\n[DEL] } catch (JSONException e) {\\n[DEL] Timber.e(e, \"Could not get ID from deck\");\\n[DEL] }\\n[DEL] mRestrictOnDeck = \"deck:\\\\\"\" + deckName + \"\\\\\" \";\\n         }\\n         searchCards();\\n     }',\n",
              " ' import static org.hamcrest.CoreMatchers.nullValue;\\n import static org.junit.Assert.assertThat;\\n import static org.mule.api.config.MuleProperties.MULE_FLOW_TRACE;\\n[ADD] import static org.mule.tck.util.FlowTraceUtils.FlowStackAsserter.stackToAssert;\\n import static org.mule.tck.util.FlowTraceUtils.assertStackElements;\\n import static org.mule.tck.util.FlowTraceUtils.isFlowStackElement;\\n ',\n",
              " ' \\n     @ProcessElement\\n     public void processElement(ProcessContext c) {\\n[DEL] try {\\n[DEL] ActualT actualContents = c.sideInput(actual);\\n[DEL] doChecks(actualContents, checkerFn, success, failure);\\n[DEL] } catch (Throwable t) {\\n[DEL] // Suppress exception in streaming\\n[DEL] if (!c.getPipelineOptions().as(StreamingOptions.class).isStreaming()) {\\n[DEL] throw t;\\n[DEL] }\\n[DEL] }\\n[ADD] ActualT actualContents = c.sideInput(actual);\\n[ADD] doChecks(actualContents, checkerFn, success, failure);\\n     }\\n   }\\n ',\n",
              " ' @Table(name = \"vm_instance\")\\n @Inheritance(strategy = InheritanceType.JOINED)\\n @DiscriminatorColumn(name = \"type\", discriminatorType = DiscriminatorType.STRING, length = 32)\\n[DEL] public class VMInstanceVO implements VirtualMachine, FiniteStateObject<State, VirtualMachine.Event> {\\n[ADD] public class\\n[ADD] VMInstanceVO implements VirtualMachine, FiniteStateObject<State, VirtualMachine.Event> {\\n     private static final Logger s_logger = Logger.getLogger(VMInstanceVO.class);\\n     @Id\\n     @TableGenerator(name = \"vm_instance_sq\", table = \"sequence\", pkColumnName = \"name\", valueColumnName = \"value\", pkColumnValue = \"vm_instance_seq\", allocationSize = 1)',\n",
              " '         HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)\\n             .withParallelism(2, 2).forTable(\"test-trip-table\")\\n             .withCompactionConfig(HoodieCompactionConfig.newBuilder().retainCommits(1).archiveCommitsWith(minInstantsToKeep, maxInstantsToKeep).build())\\n[ADD] .withFileSystemViewConfig(FileSystemViewStorageConfig.newBuilder()\\n[ADD] .withRemoteServerPort(timelineServicePort).build())\\n             .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build())\\n             .build();\\n     metaClient = HoodieTableMetaClient.reload(metaClient);',\n",
              " ' \\n /**\\n  * Main. (API, Static, ThreadSafe)\\n[ADD] *\\n[ADD] * This class is entry point loading containers.\\n  */\\n public class Main {\\n ',\n",
              " ' \\n   void setFailOnCheckpointingErrors(Boolean failOnCheckpointingErrors);\\n \\n[ADD] @Description(\\n[ADD] \"Shuts down sources which have been idle for the configured time of milliseconds. Once a source has been \"\\n[ADD] + \"shut down, checkpointing is not possible anymore. Shutting down the sources eventually leads to pipeline \"\\n[ADD] + \"shutdown (=Flink job finishes) once all input has been processed. Unless explicitly set, this will \"\\n[ADD] + \"default to Long.MAX_VALUE when checkpointing is enabled and to 0 when checkpointing is disabled. \"\\n[ADD] + \"See https://issues.apache.org/jira/browse/FLINK-2491 for progress on this issue.\")\\n[ADD] @Default.Long(-1L)\\n[ADD] Long getShutdownSourcesAfterIdleMs();\\n[ADD] \\n[ADD] void setShutdownSourcesAfterIdleMs(Long timeoutMs);\\n[ADD] \\n   @Description(\\n       \"Sets the number of times that failed tasks are re-executed. \"\\n           + \"A value of zero effectively disables fault tolerance. A value of -1 indicates \"',\n",
              " \"         startWorker = ComponentContext.inject(startWorker);\\n         if (onCreate) {\\n             // Start for Kubernetes cluster in 'Created' state\\n[ADD] Account owner = accountService.getActiveAccountById(kubernetesCluster.getAccountId());\\n[ADD] String[] keys = getServiceUserKeys(owner);\\n[ADD] startWorker.setKeys(keys);\\n             return startWorker.startKubernetesClusterOnCreate();\\n         } else {\\n             // Start for Kubernetes cluster in 'Stopped' state. Resources are already provisioned, just need to be started\",\n",
              " ' \\n package org.rstudio.studio.client.application.ui.impl;\\n \\n[ADD] import com.google.gwt.aria.client.Roles;\\n import com.google.gwt.core.client.GWT;\\n import com.google.gwt.core.client.Scheduler;\\n import com.google.gwt.dom.client.ImageElement;',\n",
              " '         return cumulativeValues;\\n     }\\n \\n[ADD] public static Cursor getCardTypesStats(@NonNull SQLiteDatabase db) {\\n[ADD] return getCardTypesStats(db, null);\\n[ADD] }\\n[ADD] \\n[ADD] /*\\n[ADD] Returns Cursor that points to number of mature cards, number of young cards, number of new cards, number of suspended cards\\n[ADD] \\n[ADD] Param limit as specified in Stats._limit(). Leave null for no limit.\\n[ADD] */\\n[ADD] public static Cursor getCardTypesStats(@NonNull SQLiteDatabase db, @Nullable String limit) {\\n[ADD] String query = \"select \" +\\n[ADD] \"sum(case when queue=2 and ivl >= 21 then 1 else 0 end), -- mtr\\\\n\" +\\n[ADD] \"sum(case when queue in (1,3) or (queue=2 and ivl < 21) then 1 else 0 end), -- yng/lrn\\\\n\" +\\n[ADD] \"sum(case when queue=0 then 1 else 0 end), -- new\\\\n\" +\\n[ADD] \"sum(case when queue<0 then 1 else 0 end) -- susp\\\\n\";\\n[ADD] \\n[ADD] if(limit == null) {\\n[ADD] query += \"from cards where did in \" + limit;\\n[ADD] }\\n[ADD] \\n[ADD] Timber.d(\"CardsTypes query: %s\", query);\\n[ADD] return db.rawQuery(query, null);\\n[ADD] }\\n[ADD] \\n     private int _periodDays() {\\n         switch (mType) {\\n             case TYPE_MONTH:',\n",
              " '     private static final String TARGET_SCHEMA_FILE_PROP = \"hoodie.deltastreamer.schemaprovider.target.schema.file\";\\n   }\\n \\n[DEL] private final FileSystem fs;\\n[DEL] \\n   private final Schema sourceSchema;\\n \\n   private Schema targetSchema;\\n \\n   public FilebasedSchemaProvider(TypedProperties props) {\\n[DEL] super(props);\\n     StreamerUtil.checkRequiredProperties(props, Collections.singletonList(Config.SOURCE_SCHEMA_FILE_PROP));\\n[DEL] this.fs = FSUtils.getFs(props.getString(Config.SOURCE_SCHEMA_FILE_PROP), StreamerUtil.getHadoopConf());\\n[ADD] FileSystem fs = FSUtils.getFs(props.getString(Config.SOURCE_SCHEMA_FILE_PROP), StreamerUtil.getHadoopConf());\\n     try {\\n       this.sourceSchema = new Schema.Parser().parse(fs.open(new Path(props.getString(Config.SOURCE_SCHEMA_FILE_PROP))));\\n       if (props.containsKey(Config.TARGET_SCHEMA_FILE_PROP)) {',\n",
              " ' \\n     protected VariableResolverFactory createVariableVariableResolverFactory(MuleEvent event)\\n     {\\n[DEL] return new VariableVariableResolverFactory(parserConfiguration, muleContext, event);\\n[ADD] if (autoResolveVariables)\\n[ADD] {\\n[ADD] return new VariableVariableResolverFactory(parserConfiguration, muleContext, event);\\n[ADD] }\\n[ADD] else\\n[ADD] {\\n[ADD] return null;\\n[ADD] }\\n     }\\n \\n     @Deprecated',\n",
              " ' \\t\\t\\t\\t.append(attr);\\n \\t\\t}\\n \\n[DEL] add(Constants.REQUIRE_CAPABILITY, req.toString());\\n[ADD] add(a, Constants.REQUIRE_CAPABILITY, req.toString());\\n \\t}\\n \\n \\tprivate String getFilter(Annotation a, Requirement annotation) {',\n",
              " '          return JsUtil.toStringArray(getNamesNative());\\n       }\\n \\n[ADD] public final native String getActiveColumn() /*-{\\n[ADD] if (this.activeColumn)\\n[ADD] return this.activeColumn;\\n[ADD] else\\n[ADD] return \"\";\\n[ADD] }-*/;\\n[ADD] \\n       private native JsArrayString getNamesNative() /*-{\\n           return this.names;\\n       }-*/;',\n",
              " '                       PipelineOptions pipelineOptions,\\n                       BeamFnDataClient beamFnDataClient,\\n                       BeamFnStateClient beamFnStateClient,\\n[ADD] BeamFnTimerClient beamFnTimerClient,\\n                       String pTransformId,\\n                       PTransform pTransform,\\n                       Supplier<String> processBundleInstructionId,',\n",
              " ' \\t\\t\\t\\t}\\n \\t\\t\\t\\tps.setString(1, groupKey);\\n \\t\\t\\t\\tps.setString(2, messageId);\\n[ADD] ps.setString(3, region);\\n \\t\\t\\t}\\n \\t\\t});\\n \\t\\treturn getMessageGroup(groupId);',\n",
              " ' \\n import java.util.List;\\n \\n[ADD] import org.springframework.core.convert.support.DefaultConversionService;\\n import org.springframework.data.mongodb.MongoDbFactory;\\n import org.springframework.data.mongodb.core.MongoOperations;\\n import org.springframework.data.mongodb.core.MongoTemplate;',\n",
              " ' {\\n \\n     // @GuardedBy(this)\\n[DEL] private Context jndiContext;\\n[ADD] protected Context jndiContext;\\n \\n     public synchronized Object lookup(String name) throws NamingException\\n     {\\n[DEL] return jndiContext.lookup(name);\\n[ADD] try\\n[ADD] {\\n[ADD] return doLookUp(name);\\n[ADD] }\\n[ADD] catch (CommunicationException e)\\n[ADD] {\\n[ADD] jndiContext = this.createInitialContext();\\n[ADD] return doLookUp(name);\\n[ADD] }\\n     }\\n \\n     public void initialise() throws InitialisationException',\n",
              " '     jobEntry.setCreateDestinationFolder( wCreateDestinationFolder.getSelection() );\\n \\n     int nritems = wFields.nrNonEmpty();\\n[DEL] int nr = 0;\\n[DEL] for ( int i = 0; i < nritems; i++ ) {\\n[DEL] String arg = wFields.getNonEmpty( i ).getText( 1 );\\n[DEL] if ( arg != null && arg.length() != 0 ) {\\n[DEL] nr++;\\n[DEL] }\\n[DEL] }\\n[ADD] \\n[ADD] \\n     Map<String, String> sourceDestinationMappings = new HashMap<String, String>();\\n[DEL] jobEntry.source_filefolder = new String[nr];\\n[DEL] jobEntry.destination_filefolder = new String[nr];\\n[DEL] jobEntry.wildcard = new String[nr];\\n[DEL] nr = 0;\\n[ADD] jobEntry.source_filefolder = new String[nritems];\\n[ADD] jobEntry.destination_filefolder = new String[nritems];\\n[ADD] jobEntry.wildcard = new String[nritems];\\n[ADD] \\n     for ( int i = 0; i < nritems; i++ ) {\\n       String sourceNc = wFields.getNonEmpty( i ).getText( 1 );\\n       sourceNc = sourceNc.equals( LOCAL_ENVIRONMENT ) ? JobEntryCopyFiles.LOCAL_SOURCE_FILE + i : sourceNc;',\n",
              " '             deploymentListener.onUndeploymentFailure(artifact.getArtifactName(), e);\\n             throw e;\\n         }\\n[DEL] catch (InterruptedException e)\\n[DEL] {\\n[DEL] Thread.currentThread().interrupt();\\n[DEL] }\\n[DEL] finally\\n[DEL] {\\n[DEL] if (deploymentLock.isHeldByCurrentThread())\\n[DEL] {\\n[DEL] deploymentLock.unlock();\\n[DEL] }\\n[DEL] }\\n     }\\n \\n     ArtifactDeployer getDeployer()',\n",
              " ' import java.net.InetAddress;\\n import java.net.URI;\\n import java.net.UnknownHostException;\\n[ADD] import java.util.Map;\\n[ADD] import java.util.Properties;\\n \\n import org.apache.hadoop.conf.Configuration;\\n import org.apache.hadoop.fs.FileSystem;\\n import org.apache.hadoop.fs.Path;\\n[ADD] import org.apache.hadoop.yarn.api.ApplicationConstants;\\n \\n import com.typesafe.config.Config;\\n[ADD] import com.typesafe.config.ConfigFactory;\\n \\n import lombok.extern.slf4j.Slf4j;\\n ',\n",
              " '         }\\n \\n         /// <summary>\\n[DEL] /// Stores the path and function definition without initializing a node.  Overwrites\\n[DEL] /// the existing NodeInfo if necessary\\n[ADD] /// Stores the path and function definition without initializing a node.\\n[ADD] /// Overwrites the existing NodeInfo if necessary!\\n         /// </summary>\\n[DEL] private void SetNodeInfo(CustomNodeInfo newInfo, bool isTestMode)\\n[ADD] private void SetNodeInfo(CustomNodeInfo newInfo)\\n         {\\n[ADD] //TODO what is this doing?\\n             var guids = NodeInfos.Where(x =>\\n                         {\\n                             return !string.IsNullOrEmpty(x.Value.Path) &&',\n",
              " '          FileTypeRegistry.OBJECT_EXPLORER.getTypeId(),\\n          null,\\n          (JsObject) handle.cast(),\\n[DEL] new SimpleRequestCallback<SourceDocument>(\"Show Object Explorer\")\\n[ADD] new SimpleRequestCallback<SourceDocument>(constants_.showObjectExplorer())\\n          {\\n             @Override\\n             public void onResponseReceived(SourceDocument response)',\n",
              " '     return toAvroSchema(beamSchema, null, null);\\n   }\\n \\n[ADD] /** Convert a {@link GenericRecord} to an corresponding array of bytes. */\\n[ADD] public static byte[] toBytes(GenericRecord record) {\\n[ADD] org.apache.avro.Schema schema = record.getSchema();\\n[ADD] DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(schema);\\n[ADD] try (ByteArrayOutputStream outputStream = new ByteArrayOutputStream()) {\\n[ADD] Encoder encoder = EncoderFactory.get().binaryEncoder(outputStream, null);\\n[ADD] datumWriter.write(record, encoder);\\n[ADD] encoder.flush();\\n[ADD] return outputStream.toByteArray();\\n[ADD] } catch (IOException exception) {\\n[ADD] throw new RuntimeException(\"Fail to parse generic record\", exception);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] /** Convert an array to bytes to a {@link GenericRecord} with the target schema. */\\n[ADD] public static GenericRecord toGenericRecord(byte[] bytes, org.apache.avro.Schema schema) {\\n[ADD] DatumReader<GenericRecord> datumReader = new GenericDatumReader<>(schema);\\n[ADD] try (InputStream inputStream = new SeekableByteArrayInput(bytes)) {\\n[ADD] Decoder decoder = DecoderFactory.get().binaryDecoder(inputStream, null);\\n[ADD] GenericRecord record = datumReader.read(null, decoder);\\n[ADD] if (record != null) {\\n[ADD] return record;\\n[ADD] }\\n[ADD] } catch (IOException exception) {\\n[ADD] throw new RuntimeException(\"Failed to extract the record from the payload.\", exception);\\n[ADD] }\\n[ADD] throw new RuntimeException(\"No record is extracted from the payload\");\\n[ADD] }\\n[ADD] \\n   /**\\n    * Strict conversion from AVRO to Beam, strict because it doesn\\'t do widening or narrowing during\\n    * conversion. If Schema is not provided, one is inferred from the AVRO schema.',\n",
              " ' \\n   private void setAllDatasetStatesToVerified() {\\n     for (Dataset dataset : this.datasets) {\\n[DEL] dataset.setState(VERIFIED);\\n[ADD] dataset.compareAndSetState(UNVERIFIED, VERIFIED);\\n     }\\n   }\\n ',\n",
              " '         }\\n     }\\n \\n[ADD] // for Windows Globalization, this function takes a number (date) and a state object and returns a string\\n[ADD] // for ICU, this function takes a state object, number, and boolean for whether or not to format to parts.\\n[ADD] //   if args.Values[3] ~= true, an array of objects is returned; else, a string is returned\\n     Var IntlEngineInterfaceExtensionObject::EntryIntl_FormatDateTime(RecyclableObject* function, CallInfo callInfo, ...)\\n     {\\n         EngineInterfaceObject_CommonFunctionProlog(function, callInfo);\\n \\n[ADD] #ifdef INTL_WINGLOB\\n         if (args.Info.Count < 3 || !(TaggedInt::Is(args.Values[1]) || JavascriptNumber::Is(args.Values[1])) || !DynamicObject::Is(args.Values[2]))\\n         {\\n             return scriptContext->GetLibrary()->GetUndefined();\\n         }\\n \\n[DEL] #ifdef INTL_WINGLOB\\n         Windows::Foundation::DateTime winDate;\\n         HRESULT hr;\\n         if (TaggedInt::Is(args.Values[1]))',\n",
              " '       public void widgetSelected( SelectionEvent e ) {\\n         input.setChanged();\\n         setTableFieldCombo();\\n[ADD] validateSelection();\\n       }\\n     };\\n     backupChanged = input.hasChanged();',\n",
              " '     assertStreamIsWrapped(Result.builder().output(new ByteArrayInputStream(HELLO_WORLD_MSG.getBytes(UTF_8))).build());\\n   }\\n \\n[ADD] @Test\\n[ADD] public void operationWithResultInputStreamCollectionOutput() throws Exception {\\n[ADD] MetadataType metadataType = BaseTypeBuilder.create(JAVA).arrayType().of(new MessageMetadataTypeBuilder().build()).build();\\n[ADD] when(outputModel.getType()).thenReturn(metadataType);\\n[ADD] \\n[ADD] delegate = createReturnDelegate();\\n[ADD] \\n[ADD] List<Result> resultList = new ArrayList<>();\\n[ADD] resultList.add(Result.builder().output(new ByteArrayInputStream(HELLO_WORLD_MSG.getBytes(UTF_8))).build());\\n[ADD] \\n[ADD] CoreEvent result = delegate.asReturnValue(resultList, operationContext);\\n[ADD] \\n[ADD] Message message = getOutputMessage(result);\\n[ADD] Message message1 = ((ResultsToMessageList) message.getPayload().getValue()).get(0);\\n[ADD] \\n[ADD] ManagedCursorStreamProvider actual = (ManagedCursorStreamProvider) message1.getPayload().getValue();\\n[ADD] InputStream resultingStream = actual.openCursor();\\n[ADD] assertThat(IOUtils.toString(resultingStream), is(HELLO_WORLD_MSG));\\n[ADD] resultingStream.close();\\n[ADD] actual.releaseResources();\\n[ADD] }\\n[ADD] \\n   private void assertStreamIsWrapped(Object value) throws InitialisationException, IOException {\\n     delegate = createReturnDelegate();\\n     CoreEvent result = delegate.asReturnValue(value, operationContext);',\n",
              " ' \\t\\t}\\n \\t\\tthrow newStringIndexOutOfBoundsException(begin, end, length);\\n \\t}\\n[ADD] \\n[ADD] @Override\\n[ADD] public IntStream chars() {\\n[ADD] /* Following generic CharSequence method invoking need to be updated with optimized implementation specifically for this class */\\n[ADD] return CharSequence.super.chars();\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public IntStream codePoints() {\\n[ADD] /* Following generic CharSequence method invoking need to be updated with optimized implementation specifically for this class */\\n[ADD] return CharSequence.super.codePoints();\\n[ADD] }\\n /*[ENDIF]*/\\t\\n }',\n",
              " '                builder.append(\"library(dplyr)\\\\n\");\\n             }\\n             \\n[ADD] String[] masterComponents = master_.getSelection().split(\" - \");\\n[ADD] \\n             builder.append(\"sc <- spark_connect(master = \\\\\"\");\\n[DEL] builder.append(master_.getSelection());\\n[ADD] builder.append(masterComponents[0]);\\n             builder.append(\"\\\\\"\");\\n[ADD] if (masterComponents.length > 0)\\n[ADD] {\\n[ADD] builder.append(\", app_name = \\\\\"\");\\n[ADD] builder.append(masterComponents[1]);\\n[ADD] builder.append(\"\\\\\"\");\\n[ADD] }\\n            \\n             // spark version\\n             if (master_.isLocalMasterSelected())',\n",
              " '   // TODO: (BEAM-723) Create a shared ExecutorService for maintenance tasks in the DirectRunner.\\n   private static final ExecutorService COUNTER_COMMITTER = Executors.newCachedThreadPool();\\n \\n[DEL] private interface MetricAggregation<UpdateT, ResultT> {\\n[DEL] UpdateT zero();\\n[DEL] UpdateT combine(Iterable<UpdateT> updates);\\n[DEL] ResultT extract(UpdateT data);\\n[ADD] private static class MetricAggregation<UpdateT extends MetricData<ResultT>, ResultT> {\\n[ADD] UpdateT initialValue;\\n[ADD] \\n[ADD] public MetricAggregation(UpdateT zero) {\\n[ADD] this.initialValue = zero;\\n[ADD] }\\n[ADD] \\n[ADD] @SuppressWarnings(\"unchecked\")\\n[ADD] UpdateT combine(Iterable<UpdateT> updates) {\\n[ADD] UpdateT value = initialValue;\\n[ADD] for (UpdateT update : updates) {\\n[ADD] value = (UpdateT) value.combine(update);\\n[ADD] }\\n[ADD] return value;\\n[ADD] }\\n[ADD] \\n[ADD] ResultT extract(UpdateT data) {\\n[ADD] return data.extractResult();\\n[ADD] }\\n   }\\n \\n   /**',\n",
              " '                 CSVFileDataAdapter.Config.class);\\n \\n         installLookupDataAdapter2(DnsLookupDataAdapter.NAME,\\n[DEL] DnsLookupDataAdapter.class,\\n[DEL] DnsLookupDataAdapter.Factory.class,\\n[DEL] DnsLookupDataAdapter.Config.class);\\n[ADD] DnsLookupDataAdapter.class,\\n[ADD] DnsLookupDataAdapter.Factory.class,\\n[ADD] DnsLookupDataAdapter.Config.class);\\n \\n         installLookupDataAdapter2(HTTPJSONPathDataAdapter.NAME,\\n                 HTTPJSONPathDataAdapter.class,',\n",
              " '    * @param numReturns The number of returnIds.\\n    * @return The Return Ids of this task.\\n    */\\n[DEL] public static UniqueId[] genReturnIds(UniqueId taskId, int numReturns) {\\n[DEL] UniqueId[] ret = new UniqueId[numReturns];\\n[ADD] public static ObjectId[] genReturnIds(TaskId taskId, int numReturns) {\\n[ADD] ObjectId[] ret = new ObjectId[numReturns];\\n     for (int i = 0; i < numReturns; i++) {\\n       ret[i] = UniqueIdUtil.computeReturnId(taskId, i + 1);\\n     }\\n     return ret;\\n   }\\n \\n[DEL] public static byte[][] getIdBytes(List<UniqueId> objectIds) {\\n[ADD] public static <T extends BaseId> byte[][] getIdBytes(List<T> objectIds) {\\n     int size = objectIds.size();\\n     byte[][] ids = new byte[size][];\\n     for (int i = 0; i < size; i++) {',\n",
              " ' import com.ichi2.anki.reviewer.ActionButtons;\\n import com.ichi2.async.CollectionTask;\\n import com.ichi2.async.TaskManager;\\n[ADD] import com.ichi2.compat.CompatHelper;\\n import com.ichi2.libanki.Card;\\n import com.ichi2.libanki.Collection;\\n import com.ichi2.libanki.Consts;',\n",
              " ' import org.apache.hadoop.hive.ql.metadata.Table;\\n \\n import com.google.common.base.Optional;\\n[DEL] import com.google.common.base.Splitter;\\n[ADD] import com.google.common.base.Preconditions;\\n[ADD] import com.google.common.collect.Maps;\\n import com.google.common.collect.Sets;\\n import com.typesafe.config.Config;\\n[DEL] import com.typesafe.config.ConfigException;\\n import com.typesafe.config.ConfigFactory;\\n \\n import gobblin.data.management.copy.hive.HiveDataset;\\n[ADD] import gobblin.data.management.copy.hive.HiveDatasetFinder;\\n import gobblin.hive.HiveMetastoreClientPool;\\n import gobblin.util.ConfigUtils;\\n \\n[ADD] \\n /**\\n[DEL] * A {@link HiveDataset} that can be converted to another {@link HiveDataset}\\n[ADD] * <p>\\n[ADD] * A {@link HiveDataset} that can be converted from one source format to several destination formats.\\n[ADD] * This class holds the {@link ConversionConfig}s required for conversion into each\\n[ADD] * destination format. The {@link ConversionConfig} for a destination format can be accessed by calling {@link #getConversionConfigForFormat(String)}.\\n[ADD] * </p>\\n[ADD] *\\n[ADD] * <p>\\n[ADD] * <b>Instantiation</b>\\n[ADD] * <ul>\\n[ADD] *  <li> The constructor takes in a dataset {@link Config} which MUST have a comma separated list of destination formats at key,\\n[ADD] *  {@value #DESTINATION_CONVERSION_FORMATS_KEY}\\n[ADD] *  <li> Conversion configuration for a format can be set by using this destination format as prefix.\\n[ADD] *  <li> E.g. If {@value #DESTINATION_CONVERSION_FORMATS_KEY}=flattenedOrc,nestedOrc.<br>\\n[ADD] *  The destination table name for flattened ORC is set at flattenedOrc.tableName<br>\\n[ADD] *  And the destination table name for nested ORC is set at nestedOrc.tableName\\n[ADD] * </ul>\\n[ADD] * </p>\\n  */\\n[DEL] @Getter\\n @ToString\\n public class ConvertibleHiveDataset extends HiveDataset {\\n \\n[DEL] public static final String DESTINATION_TABLE_KEY = \"destination.tableName\";\\n[DEL] public static final String DESTINATION_DB_KEY = \"destination.dbName\";\\n[DEL] public static final String DESTINATION_DATA_PATH_KEY = \"destination.dataPath\";\\n[DEL] public static final String DESTINATION_CONVERSION_FORMATS_KEY = \"destination.formats\";\\n[ADD] public static final String DESTINATION_CONVERSION_FORMATS_KEY = \"destinationFormats\";\\n[ADD] \\n[ADD] // Destination formats\\n[ADD] @Getter\\n[ADD] private final Set<String> destFormats;\\n \\n[DEL] private static final String HIVE_RUNTIME_PROPERTIES_KEY_PREFIX = \"hiveRuntime\";\\n[DEL] private final Optional<String> destinationTableName;\\n[DEL] private final Optional<String> destinationDbName;\\n[DEL] private final Optional<String> destinationDataPath;\\n[DEL] private final Optional<HashSet<String>> formats;\\n[DEL] private final Properties hiveProperties;\\n[ADD] // Mapping for destination format to it\\'s Conversion config\\n[ADD] private final Map<String, ConversionConfig> destConversionConfigs;\\n \\n[ADD] /**\\n[ADD] * <ul>\\n[ADD] *  <li> The constructor takes in a dataset {@link Config} which MUST have a comma separated list of destination formats at key,\\n[ADD] *  {@value #DESTINATION_CONVERSION_FORMATS_KEY}\\n[ADD] *  <li> Conversion configuration for a format can be set by using destination format as prefix.\\n[ADD] *  <li> E.g. If {@value #DESTINATION_CONVERSION_FORMATS_KEY}=flattenedOrc,nestedOrc.<br>\\n[ADD] *  The destination table name for flattened ORC is set at flattenedOrc.tableName<br>\\n[ADD] *  And the destination table name for nested ORC is set at nestedOrc.tableName\\n[ADD] * </ul>\\n[ADD] * @param fs\\n[ADD] * @param clientPool\\n[ADD] * @param table\\n[ADD] * @param config\\n[ADD] */\\n   public ConvertibleHiveDataset(FileSystem fs, HiveMetastoreClientPool clientPool, Table table, Config config) {\\n     super(fs, clientPool, table, config);\\n[DEL] this.destinationTableName = Optional.fromNullable(resolveTemplate(ConfigUtils.getString(config, DESTINATION_TABLE_KEY, null), table));\\n[DEL] this.destinationDbName = Optional.fromNullable(resolveTemplate(ConfigUtils.getString(config, DESTINATION_DB_KEY, null), table));\\n[DEL] this.destinationDataPath = Optional.fromNullable(resolveTemplate(ConfigUtils.getString(config, DESTINATION_DATA_PATH_KEY, null), table));\\n[DEL] \\n[DEL] if (config.hasPath(DESTINATION_CONVERSION_FORMATS_KEY)) {\\n[DEL] this.formats = Optional.of(Sets.<String> newHashSet());\\n[DEL] try {\\n[DEL] this.formats.get().addAll(config.getStringList(DESTINATION_CONVERSION_FORMATS_KEY));\\n[DEL] } catch (ConfigException.WrongType e) {\\n[DEL] Splitter tokenSplitter = Splitter.on(\",\").omitEmptyStrings().trimResults();\\n[DEL] this.formats.get().addAll(tokenSplitter.splitToList(config.getString(DESTINATION_CONVERSION_FORMATS_KEY)));\\n[ADD] \\n[ADD] Preconditions.checkArgument(config.hasPath(DESTINATION_CONVERSION_FORMATS_KEY), String.format(\\n[ADD] \"Atleast one destination format should be specified at %s. If you do not intend to convert this dataset set %s to true\",\\n[ADD] DESTINATION_CONVERSION_FORMATS_KEY, HiveDatasetFinder.HIVE_DATASET_IS_BLACKLISTED_KEY));\\n[ADD] \\n[ADD] // value for DESTINATION_CONVERSION_FORMATS_KEY can be a TypeSafe list or a comma separated list of string\\n[ADD] this.destFormats = Sets.newHashSet(ConfigUtils.getStringList(config, DESTINATION_CONVERSION_FORMATS_KEY));\\n[ADD] \\n[ADD] // For each format create ConversionConfig and store it in a Map<format,conversionConfig>\\n[ADD] this.destConversionConfigs = Maps.newHashMap();\\n[ADD] \\n[ADD] for (String format : this.destFormats) {\\n[ADD] if (config.hasPath(format)) {\\n[ADD] this.destConversionConfigs.put(format, new ConversionConfig(config.getConfig(format), table, format));\\n[ADD] \\n       }\\n[DEL] } else {\\n[DEL] this.formats = Optional.absent();\\n     }\\n[ADD] }\\n \\n[DEL] this.hiveProperties = ConfigUtils.configToProperties(ConfigUtils.getConfig(config, HIVE_RUNTIME_PROPERTIES_KEY_PREFIX, ConfigFactory.empty()));\\n[ADD] /**\\n[ADD] * Return the {@link ConversionConfig} for a destination format if available. If not return {@link Optional#absent()}\\n[ADD] * @param format for which {@link ConversionConfig} needs to be returned\\n[ADD] */\\n[ADD] public Optional<ConversionConfig> getConversionConfigForFormat(String format) {\\n[ADD] return Optional.fromNullable(this.destConversionConfigs.get(format));\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * The Conversion configuration for converting from source format to each destination format.\\n[ADD] * <p>\\n[ADD] * <b>Required properties</b>\\n[ADD] *  <ul>\\n[ADD] *    <li>{@value #DESTINATION_DB_KEY}\\n[ADD] *    <li>{@value #DESTINATION_TABLE_KEY}\\n[ADD] *    <li>{@value #DESTINATION_DATA_PATH_KEY}\\n[ADD] *  </ul>\\n[ADD] * <b>Optional properties</b>\\n[ADD] *  <ul>\\n[ADD] *    <li>{@value #CLUSTER_BY_KEY}\\n[ADD] *    <li>{@value #NUM_BUCKETS_KEY}\\n[ADD] *    <li>Any properties with a prefix of {@value #HIVE_RUNTIME_PROPERTIES_KEY_PREFIX} will be available at\\n[ADD] *    {@link #getHiveRuntimeProperties()} without the prefix.\\n[ADD] *  </ul>\\n[ADD] * <p>\\n[ADD] */\\n[ADD] @Getter\\n[ADD] @ToString\\n[ADD] public static class ConversionConfig {\\n[ADD] public static final String DESTINATION_TABLE_KEY = \"destination.tableName\";\\n[ADD] public static final String DESTINATION_DB_KEY = \"destination.dbName\";\\n[ADD] public static final String DESTINATION_DATA_PATH_KEY = \"destination.dataPath\";\\n[ADD] public static final String CLUSTER_BY_KEY = \"clusterByList\";\\n[ADD] public static final String NUM_BUCKETS_KEY = \"numBuckets\";\\n[ADD] \\n[ADD] private static final String HIVE_RUNTIME_PROPERTIES_KEY_PREFIX = \"hiveRuntime\";\\n[ADD] private final String destinationFormat;\\n[ADD] private final String destinationTableName;\\n[ADD] private final String destinationDbName;\\n[ADD] private final String destinationDataPath;\\n[ADD] private final List<String> clusterBy;\\n[ADD] private final Optional<Integer> numBuckets;\\n[ADD] private final Properties hiveRuntimeProperties;\\n[ADD] \\n[ADD] private ConversionConfig(Config config, Table table, String destinationFormat) {\\n \\n[ADD] Preconditions.checkArgument(config.hasPath(DESTINATION_TABLE_KEY), String.format(\"Key %s.%s is not specified\", destinationFormat, DESTINATION_TABLE_KEY));\\n[ADD] Preconditions.checkArgument(config.hasPath(DESTINATION_DB_KEY), String.format(\"Key %s.%s is not specified\", destinationFormat, DESTINATION_DB_KEY));\\n[ADD] Preconditions.checkArgument(config.hasPath(DESTINATION_DATA_PATH_KEY),\\n[ADD] String.format(\"Key %s.%s is not specified\", destinationFormat, DESTINATION_DATA_PATH_KEY));\\n[ADD] \\n[ADD] // Required\\n[ADD] this.destinationFormat = destinationFormat;\\n[ADD] this.destinationTableName = resolveTemplate(config.getString(DESTINATION_TABLE_KEY), table);\\n[ADD] this.destinationDbName = resolveTemplate(config.getString(DESTINATION_DB_KEY), table);\\n[ADD] this.destinationDataPath = resolveTemplate(config.getString(DESTINATION_DATA_PATH_KEY), table);\\n[ADD] \\n[ADD] // Optional\\n[ADD] this.clusterBy = ConfigUtils.getStringList(config, CLUSTER_BY_KEY);\\n[ADD] this.numBuckets = Optional.fromNullable(ConfigUtils.getInt(config, NUM_BUCKETS_KEY, null));\\n[ADD] this.hiveRuntimeProperties = ConfigUtils.configToProperties(ConfigUtils.getConfig(config, HIVE_RUNTIME_PROPERTIES_KEY_PREFIX, ConfigFactory.empty()));\\n[ADD] }\\n   }\\n[ADD] \\n }',\n",
              " '     Collection<String> allPages = getPages(startDate, endDate, requestedDimensions, countryFilter, jobs,\\n         Math.min(rowLimit, GoogleWebmasterClient.API_ROW_LIMIT));\\n     int actualSize = allPages.size();\\n[DEL] log.info(String\\n[DEL] .format(\"A total of %d pages fetched for property %s at country-%s from %s to %s\", actualSize, _siteProperty,\\n[DEL] country, startDate, endDate));\\n[ADD] log.info(String.format(\"A total of %d pages fetched for property %s at country-%s from %s to %s\", actualSize,\\n[ADD] _siteProperty, country, startDate, endDate));\\n \\n     if (expectedSize != -1 && actualSize != expectedSize) {\\n       log.warn(String.format(\"Expected page size is %d, but only able to get %d\", expectedSize, actualSize));',\n",
              " ' import java.util.concurrent.TimeUnit;\\n import java.util.concurrent.atomic.AtomicBoolean;\\n import java.util.concurrent.atomic.AtomicInteger;\\n[ADD] import java.util.function.Function;\\n \\n import javax.net.ServerSocketFactory;\\n import javax.net.SocketFactory;',\n",
              " ' import org.apache.dubbo.rpc.RpcException;\\n \\n import java.util.List;\\n[ADD] import java.util.Objects;\\n import java.util.concurrent.CopyOnWriteArrayList;\\n \\n /**',\n",
              " '     return workUnits;\\n   }\\n \\n[ADD] @Override\\n[ADD] public WorkUnitStream getWorkunitStream(SourceState state) {\\n[ADD] service = Executors.newSingleThreadExecutor();\\n[ADD] \\n[ADD] try {\\n[ADD] fs = getSourceFileSystem(state);\\n[ADD] suite = CompactionSuiteUtils.getCompactionSuiteFactory(state).createSuite(state);\\n[ADD] \\n[ADD] initJobDir(state);\\n[ADD] copyJarDependencies(state);\\n[ADD] List<CompactionVerifier> verifiers = suite.getDatasetsFinderVerifiers();\\n[ADD] DatasetsFinder finder = DatasetUtils.instantiateDatasetFinder(state.getProperties(),\\n[ADD] getSourceFileSystem(state),\\n[ADD] DefaultFileSystemGlobFinder.class.getName());\\n[ADD] \\n[ADD] List<Dataset> datasets = finder.findDatasets();\\n[ADD] CompactionDatasetIterator iterator = new CompactionDatasetIterator(datasets, verifiers);\\n[ADD] service.submit(iterator.getDatasetProcessor());\\n[ADD] return new BasicWorkUnitStream.Builder (iterator).build();\\n[ADD] } catch (IOException e) {\\n[ADD] throw new RuntimeException(e);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] \\n[ADD] /**\\n[ADD] * Iterator that provides {@link WorkUnit}s for all verified {@link Dataset}s\\n[ADD] */\\n[ADD] public class CompactionDatasetIterator implements Iterator<WorkUnit> {\\n[ADD] private LinkedBlockingDeque<WorkUnit> workUnits;\\n[ADD] \\n[ADD] private List<Dataset> datasets;\\n[ADD] private List<CompactionVerifier> verifiers;\\n[ADD] private WorkUnit last;\\n[ADD] private volatile int unProcessed;\\n[ADD] private volatile int success;\\n[ADD] \\n[ADD] /**\\n[ADD] * Constructor\\n[ADD] */\\n[ADD] public CompactionDatasetIterator (List<Dataset> datasets, List<CompactionVerifier> verifiers) {\\n[ADD] this.datasets = datasets;\\n[ADD] this.workUnits = new LinkedBlockingDeque<>();\\n[ADD] this.verifiers = verifiers;\\n[ADD] this.unProcessed = datasets.size();\\n[ADD] this.success = 0;\\n[ADD] this.last = null;\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Check if any {@link WorkUnit} is available. The producer is {@link CompactionDatasetIterator#getDatasetProcessor()}\\n[ADD] * @return true when a new {@link WorkUnit}  is available\\n[ADD] *         false when producer exits\\n[ADD] */\\n[ADD] public boolean hasNext () {\\n[ADD] try {\\n[ADD] while (true) {\\n[ADD] if (last != null) {\\n[ADD] log.debug (\"hasNext() is true because cache is not empty\");\\n[ADD] return true;\\n[ADD] }\\n[ADD] if (this.unProcessed > 0) {\\n[ADD] last = this.workUnits.poll(1, TimeUnit.SECONDS);\\n[ADD] if (last == null)\\n[ADD] log.debug (\"Waiting for producer to complete...\");\\n[ADD] } else {\\n[ADD] if (workUnits.size() > 0) {\\n[ADD] log.debug (\"hasNext() has new element\");\\n[ADD] return true;\\n[ADD] }\\n[ADD] log.debug (\"hasNext() returns false because producer is complete\");\\n[ADD] return false;\\n[ADD] }\\n[ADD] }\\n[ADD] } catch (InterruptedException e) {\\n[ADD] log.error(e.toString());\\n[ADD] return false;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Obtain next available {@link WorkUnit}.\\n[ADD] * Block the consumer thread until a new {@link WorkUnit} is provided. Otherwise throw an exception\\n[ADD] */\\n[ADD] public WorkUnit next () {\\n[ADD] if (hasNext()) {\\n[ADD] if (last != null) {\\n[ADD] WorkUnit tmp = last;\\n[ADD] last = null;\\n[ADD] log.debug (\"next() pops out a workunit\");\\n[ADD] return tmp;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] throw new NoSuchElementException (\"work units queue has been exhausted\");\\n[ADD] }\\n[ADD] \\n[ADD] public void remove() {\\n[ADD] throw new UnsupportedOperationException(\"No remove supported on \" + this.getClass().getName());\\n[ADD] }\\n[ADD] \\n[ADD] public Callable<Integer> getDatasetProcessor () {\\n[ADD] return new Callable<Integer>() {\\n[ADD] @Override\\n[ADD] public Integer call() throws Exception {\\n[ADD] for (Dataset dataset: datasets) {\\n[ADD] // all verifier should be passed before we compact the dataset\\n[ADD] boolean verificationPassed = true;\\n[ADD] if (verifiers != null) {\\n[ADD] for (CompactionVerifier verifier : verifiers) {\\n[ADD] if (!verifier.verify(dataset)) {\\n[ADD] verificationPassed = false;\\n[ADD] break;\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] if (verificationPassed) {\\n[ADD] success++;\\n[ADD] workUnits.add (createWorkUnit(dataset));\\n[ADD] }\\n[ADD] unProcessed--;\\n[ADD] }\\n[ADD] \\n[ADD] return success;\\n[ADD] }\\n[ADD] };\\n[ADD] }\\n[ADD] }\\n[ADD] \\n   protected WorkUnit createWorkUnit (Dataset dataset) throws IOException {\\n     WorkUnit workUnit = new WorkUnit();\\n     TaskUtils.setTaskFactoryClass(workUnit, MRCompactionTaskFactory.class);',\n",
              " ' \\n \\t\\t\\tactualArguments = runAndGetArgumentList(pb);\\n \\t\\t\\tfinal String[] expectedArguments = new String[] {ibmJavaOptionsArg};\\n[DEL] HashMap<String, Integer> posns = checkArguments(actualArguments, expectedArguments);\\n[DEL] checkArgumentSequence(expectedArguments, posns, true);\\n[ADD] HashMap<String, Integer> argumentPositions = checkArguments(actualArguments, expectedArguments);\\n[ADD] checkArgumentSequence(expectedArguments, argumentPositions, true);\\n \\t\\t} catch (AssertionError e) {\\n \\t\\t\\tdumpDiagnostics(e,actualArguments);\\n \\t\\t\\tthrow e;',\n",
              " '   public String exceptionsCommaSeparated = EXCLUDED_EXCEPTION_TYPE;\\n \\n   private JavaFileScannerContext context;\\n[DEL] private Deque<Collection<IdentifierTree>> validUsagesStack;\\n[DEL] private Iterable<String> exceptions;\\n[ADD] private Deque<UsageStatus> usageStatusStack;\\n[ADD] private List<String> exceptions;\\n   private List<String> exceptionIdentifiers;\\n   private Set<CatchTree> excludedCatchTrees = new HashSet<>();\\n \\n   @Override\\n   public void scanFile(JavaFileScannerContext context) {\\n     this.context = context;\\n[DEL] validUsagesStack = new ArrayDeque<>();\\n[DEL] exceptions = Splitter.on(\",\").trimResults().split(exceptionsCommaSeparated);\\n[DEL] exceptionIdentifiers = Lists.newArrayList();\\n[DEL] for (String exception : exceptions) {\\n[DEL] exceptionIdentifiers.add(exception.substring(exception.lastIndexOf(\\'.\\') + 1));\\n[DEL] }\\n[ADD] usageStatusStack = new ArrayDeque<>();\\n     if (context.getSemanticModel() != null) {\\n       scan(context.getTree());\\n     }',\n",
              " ' package org.apache.gobblin.service.monitoring;\\n \\n import java.util.Arrays;\\n[ADD] import java.util.Collections;\\n import java.util.Iterator;\\n import java.util.List;\\n[DEL] import java.util.function.Supplier;\\n[DEL] \\n[DEL] import com.google.common.collect.Lists;\\n \\n import org.mockito.Mockito;\\n import org.testng.Assert;\\n import org.testng.annotations.Test;\\n \\n[ADD] import com.google.common.collect.Lists;\\n[ADD] \\n import org.apache.gobblin.service.ExecutionStatus;\\n import org.apache.gobblin.test.matchers.service.monitoring.FlowStatusMatch;\\n import org.apache.gobblin.test.matchers.service.monitoring.JobStatusMatch;\\n \\n import static org.hamcrest.MatcherAssert.assertThat;\\n[ADD] import static org.mockito.Mockito.when;\\n \\n \\n public class FlowStatusGeneratorTest {',\n",
              " '         assertTrue(apple.isBitten());\\n         assertTrue(banana.isBitten());\\n         assertTrue(orange.isBitten());\\n[DEL] \\n[DEL] assertNotNull(results[0].getProperty(\"key\", PropertyScope.INVOCATION));\\n[DEL] assertNotNull(results[1].getProperty(\"key\", PropertyScope.INVOCATION));\\n[DEL] assertNotNull(results[2].getProperty(\"key\", PropertyScope.INVOCATION));\\n     }\\n \\n     @Test',\n",
              " '         return filterBuilder;\\n     }\\n \\n[DEL] public static class FieldTypeException extends ElasticsearchException {\\n[DEL] public FieldTypeException(Throwable e) {\\n[DEL] super(e);\\n[DEL] }\\n[DEL] }\\n[DEL] \\n \\n     /**\\n      * Extracts the last stream id from the filter string passed as part of the elasticsearch query. This is used later',\n",
              " '         if (!style.equals(\"_none_\"))\\n             xml = Xml.transform(xml, stylePath.resolve(style));\\n \\n[DEL] String schema = Util.getParam(params, Params.SCHEMA, null);\\n         if (schema == null) {\\n             schema = dataMan.autodetectSchema(xml);\\n             if (schema == null) {\\n[DEL] throw new BadParameterEx(\"Can\\'t detect schema for metadata automatically.\", \"schema is unknown\");\\n[ADD] throw new BadParameterEx(\\n[ADD] \"Can\\'t detect schema for metadata automatically.\",\\n[ADD] \"schema is unknown\");\\n             }\\n         }\\n[DEL] if (validate) DataManager.validateMetadata(schema, xml, context);\\n[ADD] if (validate)\\n[ADD] DataManager.validateMetadata(schema, xml, context);\\n \\n[DEL] //-----------------------------------------------------------------------\\n[DEL] //--- if the uuid does not exist we generate it for metadata and templates\\n[ADD] // -----------------------------------------------------------------------\\n[ADD] // --- if the uuid does not exist we generate it for metadata and\\n[ADD] // templates\\n         String uuid;\\n         if (metadataType == MetadataType.SUB_TEMPLATE) {\\n             uuid = UUID.randomUUID().toString();',\n",
              " ' \\t\\t}\\n \\t}\\n \\n[ADD] @Override\\n[ADD] public String getComponentType() {\\n[ADD] return \"stream:outbound-channel-adapter\";\\n[ADD] }\\n \\n[ADD] @Override\\n \\tprotected void handleMessageInternal(Message<?> message) {\\n \\t\\tObject payload = message.getPayload();\\n \\t\\tif (payload == null) {',\n",
              " '             }\\n             else\\n             {\\n[DEL] FileUtils.copyInputStreamToFile(zip.getInputStream(entry), destFile);\\n[ADD] copyInputStreamToFile(zip.getInputStream(entry), destFile);\\n                 chmodRwx(destFile);\\n             }\\n         }',\n",
              " '       formals = typeSubstitutionSolver.applySiteSubstitutionToFormalParameters(formals, defSite);\\n     }\\n     formals = typeSubstitutionSolver.applySubstitutionToFormalParameters(formals, substitution);\\n[DEL] if (!isArgumentsAcceptable(env, argTypes, formals, methodJavaSymbol.isVarArgs(), autoboxing)) {\\n[ADD] if (!isArgumentsAcceptable(env, argTypes, formals, methodJavaSymbol.isVarArgs(), looseInvocation)) {\\n       return bestSoFar;\\n     }\\n     // TODO ambiguity, errors, ...',\n",
              " '         // Delivery failed so message should be back on the queue\\n         Message msg = readMessageFromQueue(\"sanity\");\\n         assertNotNull(msg);\\n[ADD] msg.acknowledge();\\n     }\\n }',\n",
              " '  */\\n package org.mule.extension.ftp;\\n \\n[DEL] import static java.util.stream.Collectors.toList;\\n[DEL] import static org.hamcrest.CoreMatchers.equalTo;\\n[DEL] import static org.hamcrest.CoreMatchers.is;\\n[DEL] import static org.junit.Assert.assertThat;\\n[DEL] import static org.mule.functional.util.sftp.SftpServer.PASSWORD;\\n[DEL] import static org.mule.functional.util.sftp.SftpServer.USERNAME;\\n[DEL] import static org.mule.extension.file.common.api.FileWriteMode.APPEND;\\n[DEL] import static org.mule.extension.file.common.api.FileWriteMode.OVERWRITE;\\n[ADD] import org.junit.rules.TemporaryFolder;\\n[ADD] import org.junit.rules.TestRule;\\n import org.mule.extension.AbstractFtpTestHarness;\\n import org.mule.extension.FtpTestHarness;\\n[ADD] import org.mule.extension.file.common.api.FileAttributes;\\n import org.mule.extension.ftp.api.FtpFileAttributes;\\n import org.mule.extension.ftp.api.sftp.SftpFileAttributes;\\n import org.mule.extension.ftp.internal.sftp.connection.SftpClient;\\n import org.mule.extension.ftp.internal.sftp.connection.SftpClientFactory;\\n import org.mule.functional.util.sftp.SftpServer;\\n[DEL] import org.mule.extension.file.common.api.FileAttributes;\\n import org.mule.tck.junit4.rule.DynamicPort;\\n \\n import java.io.ByteArrayInputStream;',\n",
              " '    {\\n       return editButton_;\\n    }\\n[DEL] \\n[ADD] \\n[ADD] public void focus()\\n[ADD] {\\n[ADD] homeButton_.setFocus(true);\\n[ADD] }\\n[ADD] \\n    private class SlidesPopupMenu extends ScrollableToolbarPopupMenu\\n    {\\n       public SlidesPopupMenu()',\n",
              " ' import org.mule.runtime.extension.api.annotation.Extension;\\n import org.mule.runtime.extension.api.annotation.ExtensionOf;\\n import org.mule.runtime.extension.api.annotation.Operations;\\n[DEL] import org.mule.runtime.extension.api.annotation.Parameter;\\n import org.mule.runtime.extension.api.annotation.RestrictedTo;\\n import org.mule.runtime.extension.api.annotation.dsl.xml.XmlHints;\\n import org.mule.runtime.extension.api.annotation.metadata.MetadataKeyId;\\n import org.mule.runtime.extension.api.annotation.param.Connection;\\n[ADD] import org.mule.runtime.extension.api.annotation.param.NullSafe;\\n[ADD] import org.mule.runtime.extension.api.annotation.param.Parameter;\\n[ADD] import org.mule.runtime.extension.api.annotation.param.ParameterGroup;\\n import org.mule.runtime.extension.api.annotation.param.UseConfig;\\n[ADD] import org.mule.runtime.extension.api.annotation.source.EmitsResponse;\\n import org.mule.runtime.extension.api.exception.IllegalModelDefinitionException;\\n import org.mule.runtime.extension.api.introspection.declaration.DescribingContext;\\n import org.mule.runtime.extension.api.introspection.declaration.spi.Describer;',\n",
              " ' \\t\\t}\\n \\t}\\n \\n[DEL] \\n[DEL] private static class UuidToStringConverter implements Converter<UUID, String> {\\n[DEL] public String convert(UUID source) {\\n[DEL] return source.toString();\\n[ADD] private static class UuidToDBObjectConverter implements Converter<UUID, DBObject> {\\n[ADD] public DBObject convert(UUID source) {\\n[ADD] BasicDBObject dbObject = new BasicDBObject();\\n[ADD] dbObject.put(\"_value\", source.toString());\\n[ADD] dbObject.put(\"_class\", source.getClass().getName());\\n[ADD] return dbObject;\\n \\t\\t}\\n \\t}\\n[DEL] \\n[DEL] \\n[DEL] private static class StringToUuidConverter implements Converter<String, UUID> {\\n[DEL] public UUID convert(String source) {\\n[DEL] return UUID.fromString(source);\\n[ADD] \\n[ADD] private static class DBObjectToUUIDConverter implements Converter<DBObject, UUID> {\\n[ADD] public UUID convert(DBObject source) {\\n[ADD] UUID id = UUID.fromString((String) source.get(\"_value\"));\\n[ADD] return id;\\n \\t\\t}\\n \\t}\\n ',\n",
              " '             if (mAllModelIds.get(pos) != noteModelId) {\\n                 // Initialize mapping between fields of old model -> new model\\n                 int itemsLength = mEditorNote.items().length;\\n[DEL] mModelChangeFieldMap = new HashMap<>(itemsLength);\\n[ADD] mModelChangeFieldMap = HashUtil.HashMapInit(itemsLength);\\n                 for (int i=0; i < itemsLength; i++) {\\n                     mModelChangeFieldMap.put(i, i);\\n                 }\\n                 // Initialize mapping between cards new model -> old model\\n                 int templatesLength = newModel.getJSONArray(\"tmpls\").length();\\n[DEL] mModelChangeCardMap = new HashMap<>(templatesLength);\\n[ADD] mModelChangeCardMap = HashUtil.HashMapInit(templatesLength);\\n                 for (int i = 0; i < templatesLength ; i++) {\\n                     if (i < mEditorNote.numberOfCards()) {\\n                         mModelChangeCardMap.put(i, i);',\n",
              " ' \\t\\tMessageHeaders headers = message.getHeaders();\\n \\t\\treturn getMessageBuilderFactory()\\n \\t\\t\\t\\t.fromMessage(message)\\n[DEL] .setHeader(MessageHeaders.REPLY_CHANNEL, headers.get(ORIGINAL_REPLY_CHANNEL))\\n[DEL] .setHeader(MessageHeaders.ERROR_CHANNEL, headers.get(ORIGINAL_ERROR_CHANNEL))\\n[DEL] .removeHeaders(ORIGINAL_REPLY_CHANNEL, ORIGINAL_ERROR_CHANNEL)\\n[ADD] .setHeader(MessageHeaders.ERROR_CHANNEL, headers.get(GATHER_RESULT_CHANNEL))\\n \\t\\t\\t\\t.build();\\n \\t}\\n ',\n",
              " '       this.state.setProp(WRITER_LATEST_SCHEMA, builder.getSchema());\\n     }\\n     long cacheExpiryInterval = this.state.getPropAsLong(PARTITIONED_WRITER_CACHE_TTL_SECONDS, DEFAULT_PARTITIONED_WRITER_CACHE_TTL_SECONDS);\\n[DEL] this.writeTimeoutInterval = cacheExpiryInterval / 3;\\n[ADD] // Increase the timeout value to make it less sensitive to HDFS slow writer\\n[ADD] this.writeTimeoutInterval = cacheExpiryInterval / 3 * 2;\\n     log.debug(\"PartitionedDataWriter: Setting cache expiry interval to {} seconds\", cacheExpiryInterval);\\n \\n     this.partitionWriters = CacheBuilder.newBuilder()',\n",
              " '     RelNode outer = call.rel(1);\\n     RelNode uncollect = call.rel(2);\\n \\n[DEL] if (correlate.getCorrelationId().getId() != 0) {\\n[DEL] // Only one level of correlation nesting is supported\\n[DEL] return;\\n[DEL] }\\n     if (correlate.getRequiredColumns().cardinality() != 1) {\\n       // can only unnest a single column\\n       return;',\n",
              " '      */\\n     public boolean removeStream(Stream stream) {\\n         final boolean removed = streams.remove(stream);\\n[ADD] sizeCounter.dec(8);\\n[ADD] if (LOG.isTraceEnabled()) {\\n[ADD] LOG.trace(\"[Message size update][{}] stream removed: {}\", getId(), sizeCounter.getCount());\\n[ADD] }\\n \\n         if (removed) {\\n             indexSets.clear();',\n",
              " ' public class TwoInputProcessor<T, O> extends StreamProcessor<Record, TwoInputOperator<T, O>> {\\n   private static final Logger LOGGER = LoggerFactory.getLogger(TwoInputProcessor.class);\\n \\n[DEL] private String leftStream;\\n[DEL] private String rightStream;\\n[ADD] private int leftStreamJobVertexId;\\n[ADD] private int rightStreamJobVertexId;\\n \\n   public TwoInputProcessor(TwoInputOperator<T, O> operator) {\\n     super(operator);',\n",
              " '     }\\n   }\\n \\n[DEL] public static MessageElement createMessageElement( String name, Object value, boolean useExternalKey ) throws Exception {\\n[ADD] public static XmlObject createMessageElement( String name, Object value, boolean useExternalKey ) throws Exception {\\n \\n[DEL] MessageElement me = null;\\n[ADD] XmlObject me = null;\\n \\n     if ( useExternalKey ) {\\n       // We use an external key',\n",
              " '       }\\n    }\\n    \\n[DEL] private class FormatComment\\n[ADD] private class FormatConfig\\n    {\\n[DEL] public FormatComment(PanmirrorUIToolsFormat formatTools)\\n[ADD] public FormatConfig(PanmirrorUIToolsFormat formatTools)\\n       {\\n          formatTools_ = formatTools;\\n[DEL] comment_ = formatTools_.parseFormatComment(getEditorCode());\\n[ADD] config_ = formatTools_.parseFormatConfig(getEditorCode(), true);\\n       }\\n       \\n[ADD] @SuppressWarnings(\"unused\")\\n       public boolean hasChanged()\\n       {\\n[DEL] PanmirrorFormatComment comment = formatTools_.parseFormatComment(getEditorCode());\\n[DEL] return !PanmirrorFormatComment.areEqual(comment,  comment_);\\n[ADD] PanmirrorPandocFormatConfig config = formatTools_.parseFormatConfig(getEditorCode(), true);\\n[ADD] return !PanmirrorPandocFormatConfig.areEqual(config,  config_);\\n[ADD] }\\n[ADD] \\n[ADD] public boolean requiresReload()\\n[ADD] {\\n[ADD] PanmirrorPandocFormatConfig config = formatTools_.parseFormatConfig(getEditorCode(), true);\\n[ADD] return !PanmirrorPandocFormatConfig.editorBehaviorConfigEqual(config,  config_);\\n       }\\n       \\n[ADD] \\n       private final PanmirrorUIToolsFormat formatTools_;\\n[DEL] private final PanmirrorFormatComment comment_;\\n[ADD] private final PanmirrorPandocFormatConfig config_;\\n    }\\n    \\n    ',\n",
              " '  */\\n @Getter\\n @Setter\\n[DEL] @AllArgsConstructor(access = AccessLevel.PROTECTED)\\n[DEL] @NoArgsConstructor(access = AccessLevel.PRIVATE)\\n[DEL] @EqualsAndHashCode\\n[DEL] @Builder(builderClassName = \"Builder\", builderMethodName = \"_hiddenBuilder\")\\n[DEL] public class CopyableFile implements File, HasGuid {\\n[ADD] @NoArgsConstructor(access = AccessLevel.PROTECTED)\\n[ADD] @EqualsAndHashCode(callSuper = true)\\n[ADD] public class CopyableFile extends CopyEntity implements File {\\n \\n   private static final Gson GSON = new Gson();\\n \\n   /** {@link FileStatus} of the existing origin file. */\\n   private FileStatus origin;\\n \\n[DEL] /** Complete destination {@link Path} of the file. Dataset\\'s final publish directory + {@link #relativeDestination} */\\n[ADD] /** Complete destination {@link Path} of the file. */\\n   private Path destination;\\n \\n   /** Desired {@link OwnerAndPermission} of the destination path. */',\n",
              " '             throw new NotImplementedException();\\n         }\\n     }\\n[ADD] \\n[ADD] /// <summary>\\n[ADD] /// Converts an ICollection<AnnotationViewModel> to a string\\n[ADD] /// that displays how many AnnotationViewModels there is in the\\n[ADD] /// Collection.\\n[ADD] /// </summary>\\n[ADD] [ValueConversion(typeof(ICollection<AnnotationViewModel>), typeof(string))]\\n[ADD] public class NestedGroupsLabelConverter : IValueConverter\\n[ADD] {\\n[ADD] public object Convert(object value, Type targetType, object parameter, CultureInfo culture)\\n[ADD] {\\n[ADD] if (!(value is ICollection<AnnotationViewModel> viewModels) ||\\n[ADD] !viewModels.Any())\\n[ADD] {\\n[ADD] return string.Empty;\\n[ADD] }\\n[ADD] \\n[ADD] var numberOfNestedGroups = viewModels.Count;\\n[ADD] if (numberOfNestedGroups > 1)\\n[ADD] {\\n[ADD] return $\"{numberOfNestedGroups} Groups\";\\n[ADD] }\\n[ADD] \\n[ADD] return viewModels.FirstOrDefault().AnnotationText;\\n[ADD] }\\n[ADD] \\n[ADD] public object ConvertBack(object value, Type targetType, object parameter, CultureInfo culture)\\n[ADD] {\\n[ADD] throw new NotImplementedException();\\n[ADD] }\\n[ADD] }\\n }',\n",
              " '       setPendingQuit(DesktopFrame.PENDING_QUIT_AND_RESTART);\\n       \\n       final TimedProgressIndicator progress = new TimedProgressIndicator(\\n[DEL] globalDisplay_.getProgressIndicator(\"Error\"));\\n[DEL] progress.onTimedProgress(\"Restarting R...\", 1000);\\n[ADD] globalDisplay_.getProgressIndicator(constants_.progressErrorCaption()));\\n[ADD] progress.onTimedProgress(constants_.restartingRMessage(), 1000);\\n       \\n       final Operation onRestartComplete = () -> {\\n          suspendingAndRestarting_ = false;',\n",
              " ' \\n package io.opentelemetry.javaagent.instrumentation.netty.v4_1.client;\\n \\n[DEL] import static io.opentelemetry.javaagent.instrumentation.netty.v4_1.client.NettyHttpClientTracer.tracer;\\n[ADD] import static io.opentelemetry.javaagent.instrumentation.netty.v4_1.client.NettyClientSingletons.HTTP_REQUEST;\\n[ADD] import static io.opentelemetry.javaagent.instrumentation.netty.v4_1.client.NettyClientSingletons.instrumenter;\\n \\n import io.netty.channel.ChannelHandlerContext;\\n import io.netty.channel.ChannelOutboundHandlerAdapter;',\n",
              " ' \\n   private Optional<ComponentModel> getTestConnectionGlobalElement(ConfigurationDeclarer configurationDeclarer,\\n                                                                   List<ComponentModel> globalElementsComponentModel,\\n[DEL] Set<ExtensionModel> extensions) {\\n[ADD] ExtensionModelHelper extensionModelHelper) {\\n     final List<ComponentModel> markedAsTestConnectionGlobalElements =\\n         globalElementsComponentModel.stream()\\n             .filter(globalElementComponentModel -> Boolean\\n                 .parseBoolean(globalElementComponentModel.getParameters().get(MODULE_CONNECTION_MARKER_ATTRIBUTE)))\\n[DEL] .collect(Collectors.toList());\\n[ADD] .collect(toList());\\n \\n     if (markedAsTestConnectionGlobalElements.size() > 1) {\\n       throw new MuleRuntimeException(createStaticMessage(format(\"It can only be one global element marked as test connectivity [%s] but found [%d], offended global elements are: [%s]\",',\n",
              " ' \\n \\tprivate final AbstractScriptExecutingMessageProcessor<?> scriptMessageProcessor;\\n \\n[DEL] public ScriptExecutingMessageSource(AbstractScriptExecutingMessageProcessor<?> scriptMessageProcessor) {\\n[ADD] ScriptExecutingMessageSource(AbstractScriptExecutingMessageProcessor<?> scriptMessageProcessor) {\\n \\t\\tthis.scriptMessageProcessor = scriptMessageProcessor;\\n \\t}\\n ',\n",
              " ' \\n   // Turn on inline compaction - after fw delta commits a inline compaction will be run\\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\\n[DEL] private static final String DEFAULT_INLINE_COMPACT = \"true\";\\n[ADD] private static final String DEFAULT_INLINE_COMPACT = \"false\";\\n \\n   // Run a compaction every N delta commits\\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";',\n",
              " ' \\t\\t}\\n \\t\\treturn protocol.getHexValue();\\n \\t}\\n[DEL] \\n[ADD] \\n[ADD] \\n[ADD] /* return ADDRESS_NOT_FOUND_IN_CACHE if the address is not in the shared cache.\\n[ADD] * If the address is in the share cache: 1. for old cache without layer number, return -1. 2. For cache with layer number, return the layer in which the address is in\\n[ADD] */\\n \\tprivate void dbgShrcInCache(PrintStream out, J9JavaVMPointer vm, J9SharedClassConfigPointer sharedClassConfig, VoidPointer address) throws CorruptDataException {\\n \\t\\tboolean found = false;\\n \\n \\t\\tShrcConfig dbgShrcReadConfig = dbgShrcReadConfig(sharedClassConfig, out);\\n[DEL] J9SharedCacheHeaderPointer cacheStartAddress = dbgShrcReadConfig.getCacheStartAddress();\\n[ADD] J9SharedCacheHeaderPointer[] cacheStartAddressArray = dbgShrcReadConfig.getCacheStartAddress();\\n[ADD] J9SharedCacheHeaderPointer cacheStartAddress = cacheStartAddressArray[0];\\n \\n \\t\\tUDATA containsCachelets = cacheStartAddress.containsCachelets();\\n \\t\\tif (!containsCachelets.eq(0)) {\\n \\t\\t\\tint index = 1;\\n \\t\\t\\tlong totalFreeBytes = 0;\\n[DEL] dbgShrcHeaderOperations(out, J9SharedCacheHeaderPointer.cast(cacheStartAddress), VoidPointer.NULL, 0);\\n[ADD] dbgShrcHeaderOperations(out, J9SharedCacheHeaderPointer.cast(cacheStartAddress), VoidPointer.NULL, 0, -1);\\n \\n \\t\\t\\tSharedClassMetadataIterator iterator = new SharedClassMetadataIterator(vm, TYPE_CACHELET, true, out);\\n \\t\\t\\twhile (iterator.hasNext()) {',\n",
              " '       ValidationUtils.checkArgument(!cfg.filterDupes || cfg.operation != Operation.UPSERT,\\n           \"\\'--filter-dupes\\' needs to be disabled when \\'--op\\' is \\'UPSERT\\' to ensure updates are not missed.\");\\n \\n[DEL] this.props = properties != null ? properties : UtilHelpers.readConfig(\\n[DEL] FSUtils.getFs(cfg.propsFilePath, jssc.hadoopConfiguration()),\\n[DEL] new Path(cfg.propsFilePath), cfg.configs).getConfig();\\n[ADD] this.props = properties;\\n       LOG.info(\"Creating delta streamer with configs : \" + props.toString());\\n       this.schemaProvider = UtilHelpers.createSchemaProvider(cfg.schemaProviderClassName, props, jssc);\\n ',\n",
              " ' \\n     private final ValueProvider<X> value;\\n     private final SerializableFunction<X, T> translator;\\n[ADD] private transient T cachedValue;\\n \\n     NestedValueProvider(ValueProvider<X> value, SerializableFunction<X, T> translator) {\\n       this.value = checkNotNull(value);',\n",
              " ' import org.mule.api.store.ObjectStoreManager;\\n import org.mule.tck.SerializationTestUtils;\\n import org.mule.tck.junit4.AbstractMuleTestCase;\\n[DEL] import org.mule.tck.junit4.rule.SystemProperty;\\n import org.mule.util.concurrent.Latch;\\n import org.mule.util.lock.MuleLockFactory;\\n import org.mule.util.lock.SingleServerLockProvider;',\n",
              " '  * ClassGenerator\\n  */\\n public final class ClassGenerator {\\n[DEL] private static final AtomicLong CLASS_NAME_COUNTER = new AtomicLong(0);\\n[DEL] private static final String SIMPLE_NAME_TAG = \"<init>\";\\n[DEL] private static final Map<ClassLoader, ClassPool> POOL_MAP = new ConcurrentHashMap<ClassLoader, ClassPool>(); //ClassLoader - ClassPool\\n[DEL] private ClassPool mPool;\\n[DEL] private CtClass mCtc;\\n[DEL] private String mClassName, mSuperClass;\\n[DEL] private Set<String> mInterfaces;\\n[DEL] private List<String> mFields, mConstructors, mMethods;\\n[DEL] private Map<String, Method> mCopyMethods; // <method desc,method instance>\\n[DEL] private Map<String, Constructor<?>> mCopyConstructors; // <constructor desc,constructor instance>\\n[DEL] private boolean mDefaultConstructor = false;\\n[DEL] \\n[DEL] private ClassGenerator() {\\n[DEL] }\\n[DEL] \\n[DEL] private ClassGenerator(ClassPool pool) {\\n[DEL] mPool = pool;\\n[DEL] }\\n[DEL] \\n[DEL] public static ClassGenerator newInstance() {\\n[DEL] return new ClassGenerator(getClassPool(Thread.currentThread().getContextClassLoader()));\\n[DEL] }\\n[DEL] \\n[DEL] public static ClassGenerator newInstance(ClassLoader loader) {\\n[DEL] return new ClassGenerator(getClassPool(loader));\\n[DEL] }\\n[DEL] \\n[DEL] public static boolean isDynamicClass(Class<?> cl) {\\n[DEL] return ClassGenerator.DC.class.isAssignableFrom(cl);\\n[DEL] }\\n[DEL] \\n[DEL] public static ClassPool getClassPool(ClassLoader loader) {\\n[DEL] if (loader == null)\\n[DEL] return ClassPool.getDefault();\\n \\n[DEL] ClassPool pool = POOL_MAP.get(loader);\\n[DEL] if (pool == null) {\\n[DEL] pool = new ClassPool(true);\\n[DEL] pool.appendClassPath(new LoaderClassPath(loader));\\n[DEL] POOL_MAP.put(loader, pool);\\n[DEL] }\\n[DEL] return pool;\\n[DEL] }\\n[DEL] \\n[DEL] private static String modifier(int mod) {\\n[DEL] StringBuilder modifier = new StringBuilder();\\n[DEL] if (Modifier.isPublic(mod)) modifier.append(\"public\");\\n[DEL] if (Modifier.isProtected(mod)) modifier.append(\"protected\");\\n[DEL] if (Modifier.isPrivate(mod)) modifier.append(\"private\");\\n[DEL] \\n[DEL] if (Modifier.isStatic(mod)) modifier.append(\" static\");\\n[DEL] if (Modifier.isVolatile(mod)) modifier.append(\" volatile\");\\n[ADD] private static final AtomicLong CLASS_NAME_COUNTER = new AtomicLong(0);\\n[ADD] private static final String SIMPLE_NAME_TAG = \"<init>\";\\n[ADD] private static final Map<ClassLoader, ClassPool> POOL_MAP = new ConcurrentHashMap<ClassLoader, ClassPool>(); //ClassLoader - ClassPool\\n[ADD] private ClassPool mPool;\\n[ADD] private CtClass mCtc;\\n[ADD] private String mClassName;\\n[ADD] private String mSuperClass;\\n[ADD] private Set<String> mInterfaces;\\n[ADD] private List<String> mFields;\\n[ADD] private List<String> mConstructors;\\n[ADD] private List<String> mMethods;\\n[ADD] private Map<String, Method> mCopyMethods; // <method desc,method instance>\\n[ADD] private Map<String, Constructor<?>> mCopyConstructors; // <constructor desc,constructor instance>\\n[ADD] private boolean mDefaultConstructor = false;\\n \\n[DEL] return modifier.toString();\\n[DEL] }\\n[ADD] private ClassGenerator() {\\n[ADD] }\\n \\n[DEL] public String getClassName() {\\n[DEL] return mClassName;\\n[DEL] }\\n[ADD] private ClassGenerator(ClassPool pool) {\\n[ADD] mPool = pool;\\n[ADD] }\\n \\n[DEL] public ClassGenerator setClassName(String name) {\\n[DEL] mClassName = name;\\n[DEL] return this;\\n[DEL] }\\n[ADD] public static ClassGenerator newInstance() {\\n[ADD] return new ClassGenerator(getClassPool(Thread.currentThread().getContextClassLoader()));\\n[ADD] }\\n \\n[DEL] public ClassGenerator addInterface(String cn) {\\n[DEL] if (mInterfaces == null)\\n[DEL] mInterfaces = new HashSet<String>();\\n[DEL] mInterfaces.add(cn);\\n[DEL] return this;\\n[DEL] }\\n[ADD] public static ClassGenerator newInstance(ClassLoader loader) {\\n[ADD] return new ClassGenerator(getClassPool(loader));\\n[ADD] }\\n \\n[DEL] public ClassGenerator addInterface(Class<?> cl) {\\n[DEL] return addInterface(cl.getName());\\n[DEL] }\\n[DEL] \\n[DEL] public ClassGenerator setSuperClass(String cn) {\\n[DEL] mSuperClass = cn;\\n[DEL] return this;\\n[DEL] }\\n[DEL] \\n[DEL] public ClassGenerator setSuperClass(Class<?> cl) {\\n[DEL] mSuperClass = cl.getName();\\n[DEL] return this;\\n[DEL] }\\n[DEL] \\n[DEL] public ClassGenerator addField(String code) {\\n[DEL] if (mFields == null)\\n[DEL] mFields = new ArrayList<String>();\\n[DEL] mFields.add(code);\\n[DEL] return this;\\n[DEL] }\\n[ADD] public static boolean isDynamicClass(Class<?> cl) {\\n[ADD] return ClassGenerator.DC.class.isAssignableFrom(cl);\\n[ADD] }\\n \\n[DEL] public ClassGenerator addField(String name, int mod, Class<?> type) {\\n[DEL] return addField(name, mod, type, null);\\n[ADD] public static ClassPool getClassPool(ClassLoader loader) {\\n[ADD] if (loader == null) {\\n[ADD] return ClassPool.getDefault();\\n     }\\n[DEL] \\n[DEL] public ClassGenerator addField(String name, int mod, Class<?> type, String def) {\\n[DEL] StringBuilder sb = new StringBuilder();\\n[DEL] sb.append(modifier(mod)).append(\\' \\').append(ReflectUtils.getName(type)).append(\\' \\');\\n[DEL] sb.append(name);\\n[DEL] if (def != null && def.length() > 0) {\\n[DEL] sb.append(\\'=\\');\\n[DEL] sb.append(def);\\n[ADD] \\n[ADD] ClassPool pool = POOL_MAP.get(loader);\\n[ADD] if (pool == null) {\\n[ADD] pool = new ClassPool(true);\\n[ADD] pool.appendClassPath(new LoaderClassPath(loader));\\n[ADD] POOL_MAP.put(loader, pool);\\n[ADD] }\\n[ADD] return pool;\\n[ADD] }\\n[ADD] \\n[ADD] private static String modifier(int mod) {\\n[ADD] StringBuilder modifier = new StringBuilder();\\n[ADD] if (Modifier.isPublic(mod)) {\\n[ADD] modifier.append(\"public\");\\n[ADD] }\\n[ADD] if (Modifier.isProtected(mod)) {\\n[ADD] modifier.append(\"protected\");\\n[ADD] }\\n[ADD] if (Modifier.isPrivate(mod)) {\\n[ADD] modifier.append(\"private\");\\n[ADD] }\\n[ADD] \\n[ADD] if (Modifier.isStatic(mod)) {\\n[ADD] modifier.append(\" static\");\\n[ADD] }\\n[ADD] if (Modifier.isVolatile(mod)) {\\n[ADD] modifier.append(\" volatile\");\\n[ADD] }\\n[ADD] \\n[ADD] return modifier.toString();\\n[ADD] }\\n[ADD] \\n[ADD] public String getClassName() {\\n[ADD] return mClassName;\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator setClassName(String name) {\\n[ADD] mClassName = name;\\n[ADD] return this;\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator addInterface(String cn) {\\n[ADD] if (mInterfaces == null) {\\n[ADD] mInterfaces = new HashSet<String>();\\n[ADD] }\\n[ADD] mInterfaces.add(cn);\\n[ADD] return this;\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator addInterface(Class<?> cl) {\\n[ADD] return addInterface(cl.getName());\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator setSuperClass(String cn) {\\n[ADD] mSuperClass = cn;\\n[ADD] return this;\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator setSuperClass(Class<?> cl) {\\n[ADD] mSuperClass = cl.getName();\\n[ADD] return this;\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator addField(String code) {\\n[ADD] if (mFields == null) {\\n[ADD] mFields = new ArrayList<String>();\\n[ADD] }\\n[ADD] mFields.add(code);\\n[ADD] return this;\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator addField(String name, int mod, Class<?> type) {\\n[ADD] return addField(name, mod, type, null);\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator addField(String name, int mod, Class<?> type, String def) {\\n[ADD] StringBuilder sb = new StringBuilder();\\n[ADD] sb.append(modifier(mod)).append(\\' \\').append(ReflectUtils.getName(type)).append(\\' \\');\\n[ADD] sb.append(name);\\n[ADD] if (def != null && def.length() > 0) {\\n[ADD] sb.append(\\'=\\');\\n[ADD] sb.append(def);\\n[ADD] }\\n[ADD] sb.append(\\';\\');\\n[ADD] return addField(sb.toString());\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator addMethod(String code) {\\n[ADD] if (mMethods == null) {\\n[ADD] mMethods = new ArrayList<String>();\\n[ADD] }\\n[ADD] mMethods.add(code);\\n[ADD] return this;\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator addMethod(String name, int mod, Class<?> rt, Class<?>[] pts, String body) {\\n[ADD] return addMethod(name, mod, rt, pts, null, body);\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator addMethod(String name, int mod, Class<?> rt, Class<?>[] pts, Class<?>[] ets,\\n[ADD] String body) {\\n[ADD] StringBuilder sb = new StringBuilder();\\n[ADD] sb.append(modifier(mod)).append(\\' \\').append(ReflectUtils.getName(rt)).append(\\' \\').append(name);\\n[ADD] sb.append(\\'(\\');\\n[ADD] for (int i = 0; i < pts.length; i++) {\\n[ADD] if (i > 0) {\\n[ADD] sb.append(\\',\\');\\n[ADD] }\\n[ADD] sb.append(ReflectUtils.getName(pts[i]));\\n[ADD] sb.append(\" arg\").append(i);\\n[ADD] }\\n[ADD] sb.append(\\')\\');\\n[ADD] if (ets != null && ets.length > 0) {\\n[ADD] sb.append(\" throws \");\\n[ADD] for (int i = 0; i < ets.length; i++) {\\n[ADD] if (i > 0) {\\n[ADD] sb.append(\\',\\');\\n         }\\n[DEL] sb.append(\\';\\');\\n[DEL] return addField(sb.toString());\\n[DEL] }\\n[DEL] \\n[DEL] public ClassGenerator addMethod(String code) {\\n[DEL] if (mMethods == null)\\n[DEL] mMethods = new ArrayList<String>();\\n[DEL] mMethods.add(code);\\n[DEL] return this;\\n[DEL] }\\n[DEL] \\n[DEL] public ClassGenerator addMethod(String name, int mod, Class<?> rt, Class<?>[] pts, String body) {\\n[DEL] return addMethod(name, mod, rt, pts, null, body);\\n[DEL] }\\n[DEL] \\n[DEL] public ClassGenerator addMethod(String name, int mod, Class<?> rt, Class<?>[] pts, Class<?>[] ets, String body) {\\n[DEL] StringBuilder sb = new StringBuilder();\\n[DEL] sb.append(modifier(mod)).append(\\' \\').append(ReflectUtils.getName(rt)).append(\\' \\').append(name);\\n[DEL] sb.append(\\'(\\');\\n[DEL] for (int i = 0; i < pts.length; i++) {\\n[DEL] if (i > 0)\\n[DEL] sb.append(\\',\\');\\n[DEL] sb.append(ReflectUtils.getName(pts[i]));\\n[DEL] sb.append(\" arg\").append(i);\\n[ADD] sb.append(ReflectUtils.getName(ets[i]));\\n[ADD] }\\n[ADD] }\\n[ADD] sb.append(\\'{\\').append(body).append(\\'}\\');\\n[ADD] return addMethod(sb.toString());\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator addMethod(Method m) {\\n[ADD] addMethod(m.getName(), m);\\n[ADD] return this;\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator addMethod(String name, Method m) {\\n[ADD] String desc = name + ReflectUtils.getDescWithoutMethodName(m);\\n[ADD] addMethod(\\':\\' + desc);\\n[ADD] if (mCopyMethods == null) {\\n[ADD] mCopyMethods = new ConcurrentHashMap<String, Method>(8);\\n[ADD] }\\n[ADD] mCopyMethods.put(desc, m);\\n[ADD] return this;\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator addConstructor(String code) {\\n[ADD] if (mConstructors == null) {\\n[ADD] mConstructors = new LinkedList<String>();\\n[ADD] }\\n[ADD] mConstructors.add(code);\\n[ADD] return this;\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator addConstructor(int mod, Class<?>[] pts, String body) {\\n[ADD] return addConstructor(mod, pts, null, body);\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator addConstructor(int mod, Class<?>[] pts, Class<?>[] ets, String body) {\\n[ADD] StringBuilder sb = new StringBuilder();\\n[ADD] sb.append(modifier(mod)).append(\\' \\').append(SIMPLE_NAME_TAG);\\n[ADD] sb.append(\\'(\\');\\n[ADD] for (int i = 0; i < pts.length; i++) {\\n[ADD] if (i > 0) {\\n[ADD] sb.append(\\',\\');\\n[ADD] }\\n[ADD] sb.append(ReflectUtils.getName(pts[i]));\\n[ADD] sb.append(\" arg\").append(i);\\n[ADD] }\\n[ADD] sb.append(\\')\\');\\n[ADD] if (ets != null && ets.length > 0) {\\n[ADD] sb.append(\" throws \");\\n[ADD] for (int i = 0; i < ets.length; i++) {\\n[ADD] if (i > 0) {\\n[ADD] sb.append(\\',\\');\\n         }\\n[DEL] sb.append(\\')\\');\\n[DEL] if (ets != null && ets.length > 0) {\\n[DEL] sb.append(\" throws \");\\n[DEL] for (int i = 0; i < ets.length; i++) {\\n[DEL] if (i > 0)\\n[DEL] sb.append(\\',\\');\\n[DEL] sb.append(ReflectUtils.getName(ets[i]));\\n[DEL] }\\n[ADD] sb.append(ReflectUtils.getName(ets[i]));\\n[ADD] }\\n[ADD] }\\n[ADD] sb.append(\\'{\\').append(body).append(\\'}\\');\\n[ADD] return addConstructor(sb.toString());\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator addConstructor(Constructor<?> c) {\\n[ADD] String desc = ReflectUtils.getDesc(c);\\n[ADD] addConstructor(\":\" + desc);\\n[ADD] if (mCopyConstructors == null) {\\n[ADD] mCopyConstructors = new ConcurrentHashMap<String, Constructor<?>>(4);\\n[ADD] }\\n[ADD] mCopyConstructors.put(desc, c);\\n[ADD] return this;\\n[ADD] }\\n[ADD] \\n[ADD] public ClassGenerator addDefaultConstructor() {\\n[ADD] mDefaultConstructor = true;\\n[ADD] return this;\\n[ADD] }\\n[ADD] \\n[ADD] public ClassPool getClassPool() {\\n[ADD] return mPool;\\n[ADD] }\\n[ADD] \\n[ADD] public Class<?> toClass() {\\n[ADD] return toClass(ClassHelper.getClassLoader(ClassGenerator.class),\\n[ADD] getClass().getProtectionDomain());\\n[ADD] }\\n[ADD] \\n[ADD] public Class<?> toClass(ClassLoader loader, ProtectionDomain pd) {\\n[ADD] if (mCtc != null) {\\n[ADD] mCtc.detach();\\n[ADD] }\\n[ADD] long id = CLASS_NAME_COUNTER.getAndIncrement();\\n[ADD] try {\\n[ADD] CtClass ctcs = mSuperClass == null ? null : mPool.get(mSuperClass);\\n[ADD] if (mClassName == null) {\\n[ADD] mClassName = (mSuperClass == null || javassist.Modifier.isPublic(ctcs.getModifiers())\\n[ADD] ? ClassGenerator.class.getName() : mSuperClass + \"$sc\") + id;\\n[ADD] }\\n[ADD] mCtc = mPool.makeClass(mClassName);\\n[ADD] if (mSuperClass != null) {\\n[ADD] mCtc.setSuperclass(ctcs);\\n[ADD] }\\n[ADD] mCtc.addInterface(mPool.get(DC.class.getName())); // add dynamic class tag.\\n[ADD] if (mInterfaces != null) {\\n[ADD] for (String cl : mInterfaces) {\\n[ADD] mCtc.addInterface(mPool.get(cl));\\n         }\\n[DEL] sb.append(\\'{\\').append(body).append(\\'}\\');\\n[DEL] return addMethod(sb.toString());\\n[DEL] }\\n[DEL] \\n[DEL] public ClassGenerator addMethod(Method m) {\\n[DEL] addMethod(m.getName(), m);\\n[DEL] return this;\\n[DEL] }\\n[DEL] \\n[DEL] public ClassGenerator addMethod(String name, Method m) {\\n[DEL] String desc = name + ReflectUtils.getDescWithoutMethodName(m);\\n[DEL] addMethod(\\':\\' + desc);\\n[DEL] if (mCopyMethods == null)\\n[DEL] mCopyMethods = new ConcurrentHashMap<String, Method>(8);\\n[DEL] mCopyMethods.put(desc, m);\\n[DEL] return this;\\n[DEL] }\\n[DEL] \\n[DEL] public ClassGenerator addConstructor(String code) {\\n[DEL] if (mConstructors == null)\\n[DEL] mConstructors = new LinkedList<String>();\\n[DEL] mConstructors.add(code);\\n[DEL] return this;\\n[DEL] }\\n[DEL] \\n[DEL] public ClassGenerator addConstructor(int mod, Class<?>[] pts, String body) {\\n[DEL] return addConstructor(mod, pts, null, body);\\n[DEL] }\\n[DEL] \\n[DEL] public ClassGenerator addConstructor(int mod, Class<?>[] pts, Class<?>[] ets, String body) {\\n[DEL] StringBuilder sb = new StringBuilder();\\n[DEL] sb.append(modifier(mod)).append(\\' \\').append(SIMPLE_NAME_TAG);\\n[DEL] sb.append(\\'(\\');\\n[DEL] for (int i = 0; i < pts.length; i++) {\\n[DEL] if (i > 0)\\n[DEL] sb.append(\\',\\');\\n[DEL] sb.append(ReflectUtils.getName(pts[i]));\\n[DEL] sb.append(\" arg\").append(i);\\n[ADD] }\\n[ADD] if (mFields != null) {\\n[ADD] for (String code : mFields) {\\n[ADD] mCtc.addField(CtField.make(code, mCtc));\\n         }\\n[DEL] sb.append(\\')\\');\\n[DEL] if (ets != null && ets.length > 0) {\\n[DEL] sb.append(\" throws \");\\n[DEL] for (int i = 0; i < ets.length; i++) {\\n[DEL] if (i > 0)\\n[DEL] sb.append(\\',\\');\\n[DEL] sb.append(ReflectUtils.getName(ets[i]));\\n[DEL] }\\n[ADD] }\\n[ADD] if (mMethods != null) {\\n[ADD] for (String code : mMethods) {\\n[ADD] if (code.charAt(0) == \\':\\') {\\n[ADD] mCtc.addMethod(CtNewMethod.copy(getCtMethod(mCopyMethods.get(code.substring(1))),\\n[ADD] code.substring(1, code.indexOf(\\'(\\')), mCtc, null));\\n[ADD] } else {\\n[ADD] mCtc.addMethod(CtNewMethod.make(code, mCtc));\\n[ADD] }\\n         }\\n[DEL] sb.append(\\'{\\').append(body).append(\\'}\\');\\n[DEL] return addConstructor(sb.toString());\\n[ADD] }\\n[ADD] if (mDefaultConstructor) {\\n[ADD] mCtc.addConstructor(CtNewConstructor.defaultConstructor(mCtc));\\n[ADD] }\\n[ADD] if (mConstructors != null) {\\n[ADD] for (String code : mConstructors) {\\n[ADD] if (code.charAt(0) == \\':\\') {\\n[ADD] mCtc.addConstructor(CtNewConstructor\\n[ADD] .copy(getCtConstructor(mCopyConstructors.get(code.substring(1))), mCtc, null));\\n[ADD] } else {\\n[ADD] String[] sn = mCtc.getSimpleName().split(\"\\\\\\\\$+\"); // inner class name include $.\\n[ADD] mCtc.addConstructor(\\n[ADD] CtNewConstructor.make(code.replaceFirst(SIMPLE_NAME_TAG, sn[sn.length - 1]), mCtc));\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] return mCtc.toClass(loader, pd);\\n[ADD] } catch (RuntimeException e) {\\n[ADD] throw e;\\n[ADD] } catch (NotFoundException e) {\\n[ADD] throw new RuntimeException(e.getMessage(), e);\\n[ADD] } catch (CannotCompileException e) {\\n[ADD] throw new RuntimeException(e.getMessage(), e);\\n     }\\n[ADD] }\\n \\n[DEL] public ClassGenerator addConstructor(Constructor<?> c) {\\n[DEL] String desc = ReflectUtils.getDesc(c);\\n[DEL] addConstructor(\":\" + desc);\\n[DEL] if (mCopyConstructors == null)\\n[DEL] mCopyConstructors = new ConcurrentHashMap<String, Constructor<?>>(4);\\n[DEL] mCopyConstructors.put(desc, c);\\n[DEL] return this;\\n[ADD] public void release() {\\n[ADD] if (mCtc != null) {\\n[ADD] mCtc.detach();\\n     }\\n[DEL] \\n[DEL] public ClassGenerator addDefaultConstructor() {\\n[DEL] mDefaultConstructor = true;\\n[DEL] return this;\\n[ADD] if (mInterfaces != null) {\\n[ADD] mInterfaces.clear();\\n     }\\n[DEL] \\n[DEL] public ClassPool getClassPool() {\\n[DEL] return mPool;\\n[ADD] if (mFields != null) {\\n[ADD] mFields.clear();\\n     }\\n[DEL] \\n[DEL] public Class<?> toClass() {\\n[DEL] return toClass(ClassHelper.getClassLoader(ClassGenerator.class), getClass().getProtectionDomain());\\n[ADD] if (mMethods != null) {\\n[ADD] mMethods.clear();\\n     }\\n[DEL] \\n[DEL] public Class<?> toClass(ClassLoader loader, ProtectionDomain pd) {\\n[DEL] if (mCtc != null)\\n[DEL] mCtc.detach();\\n[DEL] long id = CLASS_NAME_COUNTER.getAndIncrement();\\n[DEL] try {\\n[DEL] CtClass ctcs = mSuperClass == null ? null : mPool.get(mSuperClass);\\n[DEL] if (mClassName == null)\\n[DEL] mClassName = (mSuperClass == null || javassist.Modifier.isPublic(ctcs.getModifiers())\\n[DEL] ? ClassGenerator.class.getName() : mSuperClass + \"$sc\") + id;\\n[DEL] mCtc = mPool.makeClass(mClassName);\\n[DEL] if (mSuperClass != null)\\n[DEL] mCtc.setSuperclass(ctcs);\\n[DEL] mCtc.addInterface(mPool.get(DC.class.getName())); // add dynamic class tag.\\n[DEL] if (mInterfaces != null)\\n[DEL] for (String cl : mInterfaces) mCtc.addInterface(mPool.get(cl));\\n[DEL] if (mFields != null)\\n[DEL] for (String code : mFields) mCtc.addField(CtField.make(code, mCtc));\\n[DEL] if (mMethods != null) {\\n[DEL] for (String code : mMethods) {\\n[DEL] if (code.charAt(0) == \\':\\')\\n[DEL] mCtc.addMethod(CtNewMethod.copy(getCtMethod(mCopyMethods.get(code.substring(1))), code.substring(1, code.indexOf(\\'(\\')), mCtc, null));\\n[DEL] else\\n[DEL] mCtc.addMethod(CtNewMethod.make(code, mCtc));\\n[DEL] }\\n[DEL] }\\n[DEL] if (mDefaultConstructor)\\n[DEL] mCtc.addConstructor(CtNewConstructor.defaultConstructor(mCtc));\\n[DEL] if (mConstructors != null) {\\n[DEL] for (String code : mConstructors) {\\n[DEL] if (code.charAt(0) == \\':\\') {\\n[DEL] mCtc.addConstructor(CtNewConstructor.copy(getCtConstructor(mCopyConstructors.get(code.substring(1))), mCtc, null));\\n[DEL] } else {\\n[DEL] String[] sn = mCtc.getSimpleName().split(\"\\\\\\\\$+\"); // inner class name include $.\\n[DEL] mCtc.addConstructor(CtNewConstructor.make(code.replaceFirst(SIMPLE_NAME_TAG, sn[sn.length - 1]), mCtc));\\n[DEL] }\\n[DEL] }\\n[DEL] }\\n[DEL] return mCtc.toClass(loader, pd);\\n[DEL] } catch (RuntimeException e) {\\n[DEL] throw e;\\n[DEL] } catch (NotFoundException e) {\\n[DEL] throw new RuntimeException(e.getMessage(), e);\\n[DEL] } catch (CannotCompileException e) {\\n[DEL] throw new RuntimeException(e.getMessage(), e);\\n[DEL] }\\n[ADD] if (mConstructors != null) {\\n[ADD] mConstructors.clear();\\n     }\\n[DEL] \\n[DEL] public void release() {\\n[DEL] if (mCtc != null) mCtc.detach();\\n[DEL] if (mInterfaces != null) mInterfaces.clear();\\n[DEL] if (mFields != null) mFields.clear();\\n[DEL] if (mMethods != null) mMethods.clear();\\n[DEL] if (mConstructors != null) mConstructors.clear();\\n[DEL] if (mCopyMethods != null) mCopyMethods.clear();\\n[DEL] if (mCopyConstructors != null) mCopyConstructors.clear();\\n[ADD] if (mCopyMethods != null) {\\n[ADD] mCopyMethods.clear();\\n     }\\n[DEL] \\n[DEL] private CtClass getCtClass(Class<?> c) throws NotFoundException {\\n[DEL] return mPool.get(c.getName());\\n[ADD] if (mCopyConstructors != null) {\\n[ADD] mCopyConstructors.clear();\\n     }\\n[ADD] }\\n \\n[DEL] private CtMethod getCtMethod(Method m) throws NotFoundException {\\n[DEL] return getCtClass(m.getDeclaringClass()).getMethod(m.getName(), ReflectUtils.getDescWithoutMethodName(m));\\n[DEL] }\\n[ADD] private CtClass getCtClass(Class<?> c) throws NotFoundException {\\n[ADD] return mPool.get(c.getName());\\n[ADD] }\\n \\n[DEL] private CtConstructor getCtConstructor(Constructor<?> c) throws NotFoundException {\\n[DEL] return getCtClass(c.getDeclaringClass()).getConstructor(ReflectUtils.getDesc(c));\\n[DEL] }\\n[ADD] private CtMethod getCtMethod(Method m) throws NotFoundException {\\n[ADD] return getCtClass(m.getDeclaringClass())\\n[ADD] .getMethod(m.getName(), ReflectUtils.getDescWithoutMethodName(m));\\n[ADD] }\\n[ADD] \\n[ADD] private CtConstructor getCtConstructor(Constructor<?> c) throws NotFoundException {\\n[ADD] return getCtClass(c.getDeclaringClass()).getConstructor(ReflectUtils.getDesc(c));\\n[ADD] }\\n[ADD] \\n[ADD] public static interface DC {\\n \\n[DEL] public static interface DC {\\n[DEL] } // dynamic class tag interface.\\n[ADD] } // dynamic class tag interface.\\n }\\n\\\\ No newline at end of file',\n",
              " ' import org.apache.hadoop.fs.Path;\\n import org.apache.hadoop.fs.permission.FsPermission;\\n import org.apache.hadoop.security.token.Token;\\n[ADD] import org.apache.log4j.Logger;\\n \\n[DEL] import com.google.common.base.Optional;\\n[DEL] import com.google.common.base.Preconditions;\\n[DEL] import com.google.common.base.Strings;\\n[ADD] import java.io.FileNotFoundException;\\n[ADD] import java.io.IOException;\\n[ADD] import java.net.URI;\\n[ADD] import java.util.concurrent.ExecutionException;\\n[ADD] import java.util.concurrent.TimeUnit;\\n \\n import lombok.extern.slf4j.Slf4j;\\n ',\n",
              " '  * SPDX-License-Identifier: EPL-2.0 OR Apache-2.0 OR GPL-2.0 WITH Classpath-exception-2.0 OR LicenseRef-GPL-2.0 WITH Assembly-exception\\n  *******************************************************************************/\\n \\n[ADD] package com.ibm.jvm;\\n[ADD] \\n[ADD] import openj9.management.internal.InvalidDumpOptionExceptionBase;\\n[ADD] \\n /**\\n  * This exception is thrown when an invalid option is passed\\n  * to methods on the com.ibm.jvm.Dump class.',\n",
              " ' \\t}\\n \\t\\n \\tprivate File loadProperties(Builder builder) throws Exception {\\n[ADD] Map<String, Xpp3Dom> tracker = new HashMap<>();\\n \\t\\t// Load parent project properties first\\n[DEL] loadParentProjectProperties(builder, project);\\n[ADD] loadParentProjectProperties(builder, project, tracker);\\n \\n \\t\\t// Load current project properties\\n \\t\\tXpp3Dom configuration = Optional.ofNullable(project.getBuildPlugins())\\n \\t\\t\\t.flatMap(this::getConfiguration)\\n \\t\\t\\t.orElseGet(this::defaultConfiguration);\\n[DEL] return loadProjectProperties(builder, project, project, configuration);\\n[ADD] return loadProjectProperties(builder, project, project, configuration, tracker);\\n \\t}\\n \\n[DEL] private void loadParentProjectProperties(Builder builder, MavenProject currentProject) throws Exception {\\n[ADD] private void loadParentProjectProperties(Builder builder, MavenProject currentProject,\\n[ADD] Map<String, Xpp3Dom> tracker)\\n[ADD] throws Exception {\\n \\t\\tMavenProject parentProject = currentProject.getParent();\\n \\t\\tif (parentProject == null) {\\n \\t\\t\\treturn;\\n \\t\\t}\\n[DEL] loadParentProjectProperties(builder, parentProject);\\n[ADD] loadParentProjectProperties(builder, parentProject, tracker);\\n \\n \\t\\t// Get configuration from parent project\\n \\t\\tXpp3Dom configuration = Optional.ofNullable(parentProject.getBuildPlugins())',\n",
              " ' package org.mule.runtime.core.internal.processor.strategy;\\n \\n import static java.lang.Integer.MAX_VALUE;\\n[ADD] import static java.lang.Thread.currentThread;\\n import static java.util.Arrays.asList;\\n import static org.hamcrest.CoreMatchers.equalTo;\\n[ADD] import static org.hamcrest.CoreMatchers.is;\\n import static org.hamcrest.CoreMatchers.not;\\n import static org.hamcrest.MatcherAssert.assertThat;\\n import static org.hamcrest.Matchers.hasSize;',\n",
              " '     return config.withValue(GEARPUMP_SERIALIZERS, ConfigValueFactory.fromMap(serializers));\\n   }\\n \\n[ADD] \\n[ADD] \\n[ADD] // The following codes are forked from DataflowRunner for View translator\\n[ADD] /**\\n[ADD] * Specialized implementation for\\n[ADD] * {@link org.apache.beam.sdk.transforms.View.AsMap View.AsMap}\\n[ADD] * for the Gearpump runner.\\n[ADD] */\\n[ADD] private static class StreamingViewAsMap<K, V>\\n[ADD] extends PTransform<PCollection<KV<K, V>>, PCollectionView<Map<K, V>>> {\\n[ADD] \\n[ADD] private static final long serialVersionUID = 4791080760092950304L;\\n[ADD] \\n[ADD] public StreamingViewAsMap(View.AsMap<K, V> transform) {}\\n[ADD] \\n[ADD] @Override\\n[ADD] public PCollectionView<Map<K, V>> expand(PCollection<KV<K, V>> input) {\\n[ADD] PCollectionView<Map<K, V>> view =\\n[ADD] PCollectionViews.mapView(\\n[ADD] input.getPipeline(),\\n[ADD] input.getWindowingStrategy(),\\n[ADD] input.getCoder());\\n[ADD] \\n[ADD] @SuppressWarnings({\"rawtypes\", \"unchecked\"})\\n[ADD] KvCoder<K, V> inputCoder = (KvCoder) input.getCoder();\\n[ADD] try {\\n[ADD] inputCoder.getKeyCoder().verifyDeterministic();\\n[ADD] } catch (Coder.NonDeterministicException e) {\\n[ADD] // throw new RuntimeException(e);\\n[ADD] }\\n[ADD] \\n[ADD] return input\\n[ADD] .apply(Combine.globally(new Concatenate<KV<K, V>>()).withoutDefaults())\\n[ADD] .apply(CreateGearpumpPCollectionView.<KV<K, V>, Map<K, V>>of(view));\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] protected String getKindString() {\\n[ADD] return \"StreamingViewAsMap\";\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Specialized expansion for {@link\\n[ADD] * org.apache.beam.sdk.transforms.View.AsMultimap View.AsMultimap} for the\\n[ADD] * Gearpump runner.\\n[ADD] */\\n[ADD] private static class StreamingViewAsMultimap<K, V>\\n[ADD] extends PTransform<PCollection<KV<K, V>>, PCollectionView<Map<K, Iterable<V>>>> {\\n[ADD] \\n[ADD] private static final long serialVersionUID = 5854899081751333352L;\\n[ADD] \\n[ADD] public StreamingViewAsMultimap(View.AsMultimap<K, V> transform) {}\\n[ADD] \\n[ADD] @Override\\n[ADD] public PCollectionView<Map<K, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\\n[ADD] PCollectionView<Map<K, Iterable<V>>> view =\\n[ADD] PCollectionViews.multimapView(\\n[ADD] input.getPipeline(),\\n[ADD] input.getWindowingStrategy(),\\n[ADD] input.getCoder());\\n[ADD] \\n[ADD] @SuppressWarnings({\"rawtypes\", \"unchecked\"})\\n[ADD] KvCoder<K, V> inputCoder = (KvCoder) input.getCoder();\\n[ADD] try {\\n[ADD] inputCoder.getKeyCoder().verifyDeterministic();\\n[ADD] } catch (Coder.NonDeterministicException e) {\\n[ADD] // throw new RuntimeException(e);\\n[ADD] }\\n[ADD] \\n[ADD] return input\\n[ADD] .apply(Combine.globally(new Concatenate<KV<K, V>>()).withoutDefaults())\\n[ADD] .apply(CreateGearpumpPCollectionView.<KV<K, V>, Map<K, Iterable<V>>>of(view));\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] protected String getKindString() {\\n[ADD] return \"StreamingViewAsMultimap\";\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Specialized implementation for\\n[ADD] * {@link org.apache.beam.sdk.transforms.View.AsIterable View.AsIterable} for the\\n[ADD] * Gearpump runner.\\n[ADD] */\\n[ADD] private static class StreamingViewAsIterable<T>\\n[ADD] extends PTransform<PCollection<T>, PCollectionView<Iterable<T>>> {\\n[ADD] \\n[ADD] private static final long serialVersionUID = -3399860618995613421L;\\n[ADD] \\n[ADD] public StreamingViewAsIterable(View.AsIterable<T> transform) {}\\n[ADD] \\n[ADD] @Override\\n[ADD] public PCollectionView<Iterable<T>> expand(PCollection<T> input) {\\n[ADD] PCollectionView<Iterable<T>> view =\\n[ADD] PCollectionViews.iterableView(\\n[ADD] input.getPipeline(),\\n[ADD] input.getWindowingStrategy(),\\n[ADD] input.getCoder());\\n[ADD] \\n[ADD] return input.apply(Combine.globally(new Concatenate<T>()).withoutDefaults())\\n[ADD] .apply(CreateGearpumpPCollectionView.<T, Iterable<T>>of(view));\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] protected String getKindString() {\\n[ADD] return \"StreamingViewAsIterable\";\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Specialized implementation for\\n[ADD] * {@link org.apache.beam.sdk.transforms.View.AsList View.AsList} for the\\n[ADD] * Gearpump runner.\\n[ADD] */\\n[ADD] private static class StreamingViewAsList<T>\\n[ADD] extends PTransform<PCollection<T>, PCollectionView<List<T>>> {\\n[ADD] \\n[ADD] private static final long serialVersionUID = -5018631473886330629L;\\n[ADD] \\n[ADD] public StreamingViewAsList(View.AsList<T> transform) {}\\n[ADD] \\n[ADD] @Override\\n[ADD] public PCollectionView<List<T>> expand(PCollection<T> input) {\\n[ADD] PCollectionView<List<T>> view =\\n[ADD] PCollectionViews.listView(\\n[ADD] input.getPipeline(),\\n[ADD] input.getWindowingStrategy(),\\n[ADD] input.getCoder());\\n[ADD] \\n[ADD] return input.apply(Combine.globally(new Concatenate<T>()).withoutDefaults())\\n[ADD] .apply(CreateGearpumpPCollectionView.<T, List<T>>of(view));\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] protected String getKindString() {\\n[ADD] return \"StreamingViewAsList\";\\n[ADD] }\\n[ADD] }\\n[ADD] private static class StreamingCombineGloballyAsSingletonView<InputT, OutputT>\\n[ADD] extends PTransform<PCollection<InputT>, PCollectionView<OutputT>> {\\n[ADD] \\n[ADD] private static final long serialVersionUID = 9064900748869035738L;\\n[ADD] private final Combine.GloballyAsSingletonView<InputT, OutputT> transform;\\n[ADD] \\n[ADD] public StreamingCombineGloballyAsSingletonView(\\n[ADD] Combine.GloballyAsSingletonView<InputT, OutputT> transform) {\\n[ADD] this.transform = transform;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public PCollectionView<OutputT> expand(PCollection<InputT> input) {\\n[ADD] PCollection<OutputT> combined =\\n[ADD] input.apply(Combine.globally(transform.getCombineFn())\\n[ADD] .withoutDefaults()\\n[ADD] .withFanout(transform.getFanout()));\\n[ADD] \\n[ADD] PCollectionView<OutputT> view = PCollectionViews.singletonView(\\n[ADD] combined.getPipeline(),\\n[ADD] combined.getWindowingStrategy(),\\n[ADD] transform.getInsertDefault(),\\n[ADD] transform.getInsertDefault()\\n[ADD] ? transform.getCombineFn().defaultValue() : null,\\n[ADD] combined.getCoder());\\n[ADD] return combined\\n[ADD] .apply(ParDo.of(new WrapAsList<OutputT>()))\\n[ADD] .apply(CreateGearpumpPCollectionView.<OutputT, OutputT>of(view));\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] protected String getKindString() {\\n[ADD] return \"StreamingCombineGloballyAsSingletonView\";\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private static class StreamingViewAsSingleton<T>\\n[ADD] extends PTransform<PCollection<T>, PCollectionView<T>> {\\n[ADD] \\n[ADD] private static final long serialVersionUID = 5870455965625071546L;\\n[ADD] private final View.AsSingleton<T> transform;\\n[ADD] \\n[ADD] public StreamingViewAsSingleton(View.AsSingleton<T> transform) {\\n[ADD] this.transform = transform;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public PCollectionView<T> expand(PCollection<T> input) {\\n[ADD] Combine.Globally<T, T> combine = Combine.globally(\\n[ADD] new SingletonCombine<>(transform.hasDefaultValue(), transform.defaultValue()));\\n[ADD] if (!transform.hasDefaultValue()) {\\n[ADD] combine = combine.withoutDefaults();\\n[ADD] }\\n[ADD] return input.apply(combine.asSingletonView());\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] protected String getKindString() {\\n[ADD] return \"StreamingViewAsSingleton\";\\n[ADD] }\\n[ADD] \\n[ADD] private static class SingletonCombine<T> extends Combine.BinaryCombineFn<T> {\\n[ADD] private boolean hasDefaultValue;\\n[ADD] private T defaultValue;\\n[ADD] \\n[ADD] SingletonCombine(boolean hasDefaultValue, T defaultValue) {\\n[ADD] this.hasDefaultValue = hasDefaultValue;\\n[ADD] this.defaultValue = defaultValue;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public T apply(T left, T right) {\\n[ADD] throw new IllegalArgumentException(\"PCollection with more than one element \"\\n[ADD] + \"accessed as a singleton view. Consider using Combine.globally().asSingleton() to \"\\n[ADD] + \"combine the PCollection into a single value\");\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public T identity() {\\n[ADD] if (hasDefaultValue) {\\n[ADD] return defaultValue;\\n[ADD] } else {\\n[ADD] throw new IllegalArgumentException(\\n[ADD] \"Empty PCollection accessed as a singleton view. \"\\n[ADD] + \"Consider setting withDefault to provide a default value\");\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private static class WrapAsList<T> extends DoFn<T, List<T>> {\\n[ADD] @ProcessElement\\n[ADD] public void processElement(ProcessContext c) {\\n[ADD] c.output(Collections.singletonList(c.element()));\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Creates a primitive {@link PCollectionView}.\\n[ADD] *\\n[ADD] * <p>For internal use only by runner implementors.\\n[ADD] *\\n[ADD] * @param <ElemT> The type of the elements of the input PCollection\\n[ADD] * @param <ViewT> The type associated with the {@link PCollectionView} used as a side input\\n[ADD] */\\n[ADD] public static class CreateGearpumpPCollectionView<ElemT, ViewT>\\n[ADD] extends PTransform<PCollection<List<ElemT>>, PCollectionView<ViewT>> {\\n[ADD] private static final long serialVersionUID = -2637073020800540542L;\\n[ADD] private PCollectionView<ViewT> view;\\n[ADD] \\n[ADD] private CreateGearpumpPCollectionView(PCollectionView<ViewT> view) {\\n[ADD] this.view = view;\\n[ADD] }\\n[ADD] \\n[ADD] public static <ElemT, ViewT> CreateGearpumpPCollectionView<ElemT, ViewT> of(\\n[ADD] PCollectionView<ViewT> view) {\\n[ADD] return new CreateGearpumpPCollectionView<>(view);\\n[ADD] }\\n[ADD] \\n[ADD] public PCollectionView<ViewT> getView() {\\n[ADD] return view;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public PCollectionView<ViewT> expand(PCollection<List<ElemT>> input) {\\n[ADD] return view;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Combiner that combines {@code T}s into a single {@code List<T>} containing all inputs.\\n[ADD] *\\n[ADD] * <p>For internal use by {@link StreamingViewAsMap}, {@link StreamingViewAsMultimap},\\n[ADD] * {@link StreamingViewAsList}, {@link StreamingViewAsIterable}.\\n[ADD] * They require the input {@link PCollection} fits in memory.\\n[ADD] * For a large {@link PCollection} this is expected to crash!\\n[ADD] *\\n[ADD] * @param <T> the type of elements to concatenate.\\n[ADD] */\\n[ADD] private static class Concatenate<T> extends Combine.CombineFn<T, List<T>, List<T>> {\\n[ADD] @Override\\n[ADD] public List<T> createAccumulator() {\\n[ADD] return new ArrayList<>();\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public List<T> addInput(List<T> accumulator, T input) {\\n[ADD] accumulator.add(input);\\n[ADD] return accumulator;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public List<T> mergeAccumulators(Iterable<List<T>> accumulators) {\\n[ADD] List<T> result = createAccumulator();\\n[ADD] for (List<T> accumulator : accumulators) {\\n[ADD] result.addAll(accumulator);\\n[ADD] }\\n[ADD] return result;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public List<T> extractOutput(List<T> accumulator) {\\n[ADD] return accumulator;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public Coder<List<T>> getAccumulatorCoder(CoderRegistry registry, Coder<T> inputCoder) {\\n[ADD] return ListCoder.of(inputCoder);\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public Coder<List<T>> getDefaultOutputCoder(CoderRegistry registry, Coder<T> inputCoder) {\\n[ADD] return ListCoder.of(inputCoder);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n }',\n",
              " ' import org.apache.gobblin.configuration.ConfigurationKeys;\\n import org.apache.gobblin.configuration.WorkUnitState;\\n import org.apache.gobblin.crypto.EncryptionConfigParser;\\n[ADD] import org.apache.gobblin.crypto.GPGFileDecryptor;\\n import org.apache.gobblin.data.management.copy.CopyConfiguration;\\n import org.apache.gobblin.data.management.copy.CopySource;\\n import org.apache.gobblin.data.management.copy.CopyableDatasetMetadata;',\n",
              " '         /// Returns authentication manager object for oxygen authentication.\\n         /// </summary>\\n         public AuthenticationManager AuthenticationManager { get; set; }\\n[ADD] public object DependencyView { get; private set; }\\n \\n         #endregion\\n ',\n",
              " '       return this;\\n     }\\n \\n[ADD] public Builder withMarkersType(String mode) {\\n[ADD] writeConfig.setValue(MARKERS_TYPE_PROP, mode);\\n[ADD] return this;\\n[ADD] }\\n[ADD] \\n[ADD] public Builder withMarkersTimelineServerBasedBatchNumThreads(int numThreads) {\\n[ADD] writeConfig.setValue(MARKERS_TIMELINE_SERVER_BASED_BATCH_NUM_THREADS_PROP, String.valueOf(numThreads));\\n[ADD] return this;\\n[ADD] }\\n[ADD] \\n[ADD] public Builder withMarkersTimelineServerBasedBatchIntervalMs(long intervalMs) {\\n[ADD] writeConfig.setValue(MARKERS_TIMELINE_SERVER_BASED_BATCH_INTERVAL_MS_PROP, String.valueOf(intervalMs));\\n[ADD] return this;\\n[ADD] }\\n[ADD] \\n     public Builder withMarkersDeleteParallelism(int parallelism) {\\n       writeConfig.setValue(MARKERS_DELETE_PARALLELISM, String.valueOf(parallelism));\\n       return this;',\n",
              " '     undeployLatch = new Latch();\\n   }\\n \\n[ADD] public void setUseMockedListeners(boolean useMockedListeners) {\\n[ADD] this.useMockedListeners = useMockedListeners;\\n[ADD] }\\n[ADD] \\n   private static class TestMuleDeploymentService extends MuleDeploymentService {\\n \\n     public TestMuleDeploymentService(DefaultDomainFactory domainFactory, DefaultApplicationFactory applicationFactory,',\n",
              " '       // Create a new batch because previous one has no space\\n       BytesBoundedBatch batch = new BytesBoundedBatch(this.memSizeLimit, this.expireInMilliSecond);\\n       LOG.debug(\"Batch \" + batch.getId() + \" is generated\");\\n[DEL] Future<RecordMetadata> future = batch.tryAppend(record, callback);\\n[ADD] Future<RecordMetadata> future = null;\\n[ADD] try {\\n[ADD] future = batch.tryAppend(record, callback, this.largeMessagePolicy);\\n[ADD] } catch (RecordTooLargeException e) {\\n[ADD] // If a new batch also wasn\\'t able to accomodate the new message\\n[ADD] throw new RuntimeException(\"Failed due to a message that was too large\", e);\\n[ADD] }\\n \\n[DEL] // Even single record can exceed the batch size limit\\n[DEL] // Ignore the record because Eventhub can only accept payload less than 256KB\\n[ADD] // The future might be null, since the largeMessagePolicy might be set to DROP\\n       if (future == null) {\\n[DEL] LOG.error(\"Batch \" + batch.getId() + \" is marked as complete because it contains a huge record: \"\\n[ADD] assert largeMessagePolicy.equals(LargeMessagePolicy.DROP);\\n[ADD] LOG.error(\"Batch \" + batch.getId() + \" is silently marked as complete, dropping a huge record: \"\\n                 + record);\\n         future = Futures.immediateFuture(new RecordMetadata(0));\\n         callback.onSuccess(WriteResponse.EMPTY);',\n",
              " '   }\\n \\n   @Nullable\\n[DEL] private static Type getTypeArgument(Type collectionType) {\\n[DEL] if (collectionType.is(\"java.util.Collection\") && JUtils.isParametrized(collectionType)) {\\n[DEL] return JUtils.typeArguments(collectionType).get(0);\\n[ADD] private static Type findSuperTypeMatching(Type type, String genericTypeName) {\\n[ADD] if (type.is(genericTypeName)) {\\n[ADD] return type;\\n     }\\n[DEL] return JUtils.directSuperTypes(collectionType)\\n[ADD] return JUtils.superTypes(type.symbol())\\n       .stream()\\n[DEL] .map(CollectionInappropriateCallsCheck::getTypeArgument)\\n[DEL] .filter(Objects::nonNull)\\n[ADD] .filter(superType -> superType.is(genericTypeName))\\n       .findFirst()\\n       .orElse(null);\\n   }',\n",
              " ' import javax.annotation.concurrent.NotThreadSafe;\\n import lombok.extern.slf4j.Slf4j;\\n \\n[DEL] import org.apache.gobblin.configuration.State;\\n[DEL] import org.apache.gobblin.instrumented.Instrumentable;\\n[DEL] import org.apache.gobblin.instrumented.Instrumented;\\n[DEL] import org.apache.gobblin.metrics.GobblinMetrics;\\n[DEL] import org.apache.gobblin.metrics.MetricContext;\\n[DEL] import org.apache.gobblin.metrics.MetricNames;\\n[DEL] import org.apache.gobblin.metrics.Tag;\\n[DEL] import org.apache.gobblin.source.extractor.CheckpointableWatermark;\\n[DEL] import org.apache.gobblin.util.ConfigUtils;\\n[DEL] import org.apache.gobblin.util.ExecutorsUtils;\\n[DEL] \\n \\n /**\\n  * A class to handle fine-grain watermarks.',\n",
              " '      */\\n     private DirectDownloadCommand getDirectDownloadCommandFromProtocol(DownloadProtocol protocol, String url, Long templateId, PrimaryDataStoreTO destPool,\\n                                                                        String checksum, Map<String, String> httpHeaders) {\\n[ADD] int connectTimeout = DEFAULT_DIRECT_DOWNLOAD_CONNECT_TIMEOUT;\\n[ADD] int soTimeout = DEFAULT_DIRECT_DOWNLOAD_SOCKET_TIMEOUT;\\n[ADD] int connectionRequestTimeout = DEFAULT_DIRECT_DOWNLOAD_CONNECTION_REQUEST_TIMEOUT;\\n[ADD] if (DownloadProtocol.HTTP.equals(protocol) ||\\n[ADD] DownloadProtocol.HTTPS.equals(protocol) ||\\n[ADD] DownloadProtocol.METALINK.equals(protocol)) {\\n[ADD] try {\\n[ADD] connectTimeout = Integer.parseInt(configDao.getValue(DirectDownloadConnectTimeout.key()));\\n[ADD] } catch (NumberFormatException nfe) {\\n[ADD] s_logger.warn(String.format(\"Unable to retrieve configuration: %s value\", DirectDownloadConnectTimeout.key()), nfe);\\n[ADD] }\\n[ADD] try {\\n[ADD] soTimeout = Integer.parseInt(configDao.getValue(DirectDownloadSocketTimeout.key()));\\n[ADD] } catch (NumberFormatException nfe) {\\n[ADD] s_logger.warn(String.format(\"Unable to retrieve configuration: %s value\", DirectDownloadSocketTimeout.key()), nfe);\\n[ADD] }\\n[ADD] }\\n[ADD] if (DownloadProtocol.HTTPS.equals(protocol)) {\\n[ADD] try {\\n[ADD] connectionRequestTimeout = Integer.parseInt(configDao.getValue(DirectDownloadConnectionRequestTimeout.key()));\\n[ADD] } catch (NumberFormatException nfe) {\\n[ADD] s_logger.warn(String.format(\"Unable to retrieve configuration: %s value\", DirectDownloadConnectionRequestTimeout.key()), nfe);\\n[ADD] }\\n[ADD] }\\n         if (protocol.equals(DownloadProtocol.HTTP)) {\\n[DEL] return new HttpDirectDownloadCommand(url, templateId, destPool, checksum, httpHeaders);\\n[ADD] return new HttpDirectDownloadCommand(url, templateId, destPool, checksum, httpHeaders, connectTimeout, soTimeout);\\n         } else if (protocol.equals(DownloadProtocol.HTTPS)) {\\n[DEL] return new HttpsDirectDownloadCommand(url, templateId, destPool, checksum, httpHeaders);\\n[ADD] return new HttpsDirectDownloadCommand(url, templateId, destPool, checksum, httpHeaders, connectTimeout, soTimeout, connectionRequestTimeout);\\n         } else if (protocol.equals(DownloadProtocol.NFS)) {\\n             return new NfsDirectDownloadCommand(url, templateId, destPool, checksum, httpHeaders);\\n         } else if (protocol.equals(DownloadProtocol.METALINK)) {\\n[DEL] return new MetalinkDirectDownloadCommand(url, templateId, destPool, checksum, httpHeaders);\\n[ADD] return new MetalinkDirectDownloadCommand(url, templateId, destPool, checksum, httpHeaders, connectTimeout, soTimeout);\\n         } else {\\n             return null;\\n         }',\n",
              " ' /**\\n  * {@code PTransform}s for estimating the number of distinct elements in a {@code PCollection}, or\\n  * the number of distinct values associated with each key in a {@code PCollection} of {@code KV}s.\\n[ADD] *\\n[ADD] * <p>Consider using {@code HllCount} in the {@code zetasketch} extension module if you need better\\n[ADD] * performance or need to save intermediate aggregation result into a sketch for later processing.\\n[ADD] *\\n[ADD] * <p>For example, to estimate the number of distinct elements in a {@code PCollection<String>}:\\n[ADD] *\\n[ADD] * <pre>{@code\\n[ADD] * PCollection<String> input = ...;\\n[ADD] * PCollection<Long> countDistinct =\\n[ADD] *     input.apply(HllCount.Init.forStrings().globally()).apply(HllCount.Extract.globally());\\n[ADD] * }</pre>\\n[ADD] *\\n[ADD] * For more details about using {@code HllCount} and the {@code zetasketch} extension module, see\\n[ADD] * https://s.apache.org/hll-in-beam#bookmark=id.v6chsij1ixo7.\\n  */\\n public class ApproximateUnique {\\n ',\n",
              " '         LOGGER.error(\"Failed to close the \" + GobblinYarnAppLauncher.class.getSimpleName(), ioe);\\n       } catch (TimeoutException te) {\\n         LOGGER.error(\"Timeout in stopping the service manager\", te);\\n[ADD] } finally {\\n[ADD] if (this.emailNotificationOnShutdown) {\\n[ADD] sendEmailOnShutdown(Optional.of(applicationReport));\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] @Subscribe\\n[ADD] public void handleGetApplicationReportFailureEvent(\\n[ADD] @SuppressWarnings(\"unused\") GetApplicationReportFailureEvent getApplicationReportFailureEvent) {\\n[ADD] int numConsecutiveFailures = this.getApplicationReportFailureCount.incrementAndGet();\\n[ADD] if (numConsecutiveFailures > this.maxGetApplicationReportFailures) {\\n[ADD] LOGGER.warn(String\\n[ADD] .format(\"Number of consecutive failures to get the ApplicationReport %d exceeds the threshold %d\",\\n[ADD] numConsecutiveFailures, this.maxGetApplicationReportFailures));\\n[ADD] \\n[ADD] try {\\n[ADD] stop();\\n[ADD] } catch (IOException ioe) {\\n[ADD] LOGGER.error(\"Failed to close the \" + GobblinYarnAppLauncher.class.getSimpleName(), ioe);\\n[ADD] } catch (TimeoutException te) {\\n[ADD] LOGGER.error(\"Timeout in stopping the service manager\", te);\\n[ADD] } finally {\\n[ADD] if (this.emailNotificationOnShutdown) {\\n[ADD] sendEmailOnShutdown(Optional.<ApplicationReport>absent());\\n[ADD] }\\n       }\\n     }\\n   }',\n",
              " '   }\\n \\n   private static class ZeroConstraint extends ObjectConstraint {\\n[DEL] private ZeroConstraint(Tree syntaxNode, Status status) {\\n[DEL] super(false, false, syntaxNode, status);\\n[ADD] private ZeroConstraint(Status status) {\\n[ADD] super(false, false, status);\\n     }\\n \\n     @Override',\n",
              " ' \\n \\t@Override\\n \\tpublic void initialize(ConfigurableListableBeanFactory beanFactory) throws BeansException {\\n[DEL] Collection<ChannelSecurityInterceptor> securityInterceptors = beanFactory.getBeansOfType(ChannelSecurityInterceptor.class).values();\\n[DEL] beanFactory.addBeanPostProcessor(new ChannelSecurityInterceptorBeanPostProcessor(securityInterceptors));\\n[ADD] beanFactory.addBeanPostProcessor(new ChannelSecurityInterceptorBeanPostProcessor(beanFactory.getBeansOfType(ChannelSecurityInterceptor.class).values()));\\n \\t}\\n \\n }',\n",
              " ' \\n     LoopHeader *FunctionBody::GetLoopHeader(uint index) const\\n     {\\n[DEL] Assert(this->loopHeaderArray != nullptr);\\n[ADD] Assert(this->GetLoopHeaderArray() != nullptr);\\n         Assert(index < loopCount);\\n[DEL] return &this->loopHeaderArray[index];\\n[ADD] return &this->GetLoopHeaderArray()[index];\\n     }\\n \\n     FunctionEntryPointInfo *FunctionBody::GetSimpleJitEntryPointInfo() const\\n     {\\n[DEL] return simpleJitEntryPointInfo;\\n[ADD] return static_cast<FunctionEntryPointInfo *>(this->GetAuxPtr(AuxPointerType::SimpleJitEntryPointInfo));\\n     }\\n \\n     void FunctionBody::SetSimpleJitEntryPointInfo(FunctionEntryPointInfo *const entryPointInfo)\\n     {\\n[DEL] simpleJitEntryPointInfo = entryPointInfo;\\n[ADD] this->SetAuxPtr(AuxPointerType::SimpleJitEntryPointInfo, entryPointInfo);\\n     }\\n \\n     void FunctionBody::VerifyExecutionMode(const ExecutionMode executionMode) const',\n",
              " ' import com.google.common.collect.ImmutableSet;\\n import com.google.common.collect.Lists;\\n import com.google.common.collect.Maps;\\n[DEL] \\n import org.sonar.java.ast.api.JavaKeyword;\\n import org.sonar.java.model.AbstractTypedTree;\\n import org.sonar.java.model.declaration.VariableTreeImpl;',\n",
              " '       if (SourceWindowManager.isMainSourceWindow() && !excludeActive)\\n       {\\n          // if this is the main window, close docs in the satellites first \\n[DEL] windowManager_.closeAllSatelliteDocs(caption, new Command()\\n[ADD] pWindowManager_.get().closeAllSatelliteDocs(caption, new Command()\\n          {\\n             @Override\\n             public void execute()',\n",
              " '     }\\n \\n     @Override\\n[DEL] public WorkItemCommitRequest persistDirectly(ForKey cache) throws IOException {\\n[ADD] public WorkItemCommitRequest persistDirectly(WindmillStateCache.ForKeyAndFamily cache)\\n[ADD] throws IOException {\\n       WorkItemCommitRequest.Builder commitBuilder = WorkItemCommitRequest.newBuilder();\\n       TagSortedListUpdateRequest.Builder updatesBuilder =\\n[DEL] commitBuilder.addSortedListUpdatesBuilder().setStateFamily(stateFamily).setTag(stateKey);\\n[ADD] commitBuilder\\n[ADD] .addSortedListUpdatesBuilder()\\n[ADD] .setStateFamily(cache.getStateFamily())\\n[ADD] .setTag(stateKey);\\n       try {\\n         if (cleared) {\\n           // Default range.',\n",
              " '     }\\n \\n     public static List<URL> parseURLs(String address, Map<String, String> defaults) {\\n[DEL] if (address == null || address.length() == 0) {\\n[ADD] if (StringUtils.isEmpty(address)) {\\n             throw new IllegalArgumentException(\"Address is not allowed to be empty, please re-enter.\");\\n         }\\n         String[] addresses = REGISTRY_SPLIT_PATTERN.split(address);',\n",
              " '    private final Provider<WorkbenchContext> pWorkbenchContext_;\\n    private final SourceServerOperations server_;\\n    private final GlobalDisplay display_;\\n[DEL] private final SourceShim sourceShim_;\\n[ADD] private final Source source_;\\n    private final UserPrefs userPrefs_;\\n \\n    private HashMap<String, Integer> sourceWindows_ = ',\n",
              " '             .apply(Window.<String>into(FixedWindows.of(windowDuration)));\\n \\n     try {\\n[DEL] PAssertStreaming.runAndAssertContents(pipeline, output, new String[0]);\\n[ADD] PAssertStreaming.runAndAssertContents(pipeline, output, new String[0],\\n[ADD] Duration.standardSeconds(1L));\\n     } catch (AssertionError e) {\\n       assertTrue(\"Expected error message: \" + EXPECTED_ERR + \" but got: \" + e.getMessage(),\\n           e.getMessage().equals(EXPECTED_ERR));',\n",
              " '     return enemies;\\n   }\\n \\n[ADD] @Deprecated(message = \"The usage of this operation must be replaced by the knock operation.\", since = \"1.5.0\",\\n[ADD] removedIn = \"2.0.0\")\\n   @Stereotype(KillingStereotype.class)\\n   @MediaType(TEXT_PLAIN)\\n[DEL] public String kill(@Optional(defaultValue = PAYLOAD) String victim, String goodbyeMessage) throws Exception {\\n[ADD] public String kill(@Optional(defaultValue = PAYLOAD) String victim, @Deprecated(\\n[ADD] message = \"There is now a standarized way to say goodbye to your enemies before knocking them up, using a different message will only be supported until the next mayor release\",\\n[ADD] since = \"1.4.0\") @Optional(\\n[ADD] defaultValue = \"We are done\") String goodbyeMessage)\\n[ADD] throws Exception {\\n     KillParameters killParameters = new KillParameters(victim, goodbyeMessage);\\n     return format(\"%s, %s\", killParameters.getGoodbyeMessage(), killParameters.getVictim());\\n   }',\n",
              " ' \\n     \\'\\'\\'\\n \\n[ADD] class SaveDataTool(Action):\\n[ADD] \"\"\"\\n[ADD] A tool for downloading data from the plot.\\n[ADD] \"\"\"\\n[ADD] \\n[ADD] source = Instance(ColumnDataSource, help=\"\"\"\\n[ADD] The data-source that should be used to extract the data from and download the selected data.\\n[ADD] \"\"\")\\n[ADD] \\n[ADD] column_formatters = Dict(String, Either(Enum(TooltipFieldFormatter), Tuple(Enum(TooltipFieldFormatter), String)),\\n[ADD] default=lambda: dict(), help=\"\"\"\\n[ADD] Specify the formatting scheme for data source columns alongside that format\\'s\\n[ADD] specifications, e.g.\\n[ADD] \\n[ADD] .. code-block:: python\\n[ADD] \\n[ADD] tool.formatters = {\"date\": (\"datetime\", \"%F %T.%3N\")}\\n[ADD] \\n[ADD] will cause format specifications for the \"date\" column to be interpreted\\n[ADD] according to the \"datetime\" formatting scheme. The following schemes are\\n[ADD] available:\\n[ADD] \\n[ADD] :``\"numeral\"``:\\n[ADD] Provides a wide variety of formats for numbers, currency, bytes, times,\\n[ADD] and percentages. The full set of formats can be found in the\\n[ADD] |NumeralTickFormatter| reference documentation.\\n[ADD] \\n[ADD] :``\"datetime\"``:\\n[ADD] Provides formats for date and time values. The full set of formats is\\n[ADD] listed in the |DatetimeTickFormatter| reference documentation.\\n[ADD] \\n[ADD] :``\"printf\"``:\\n[ADD] Provides formats similar to C-style \"printf\" type specifiers. See the\\n[ADD] |PrintfTickFormatter| reference documentation for complete details.\\n[ADD] \\n[ADD] If no formatter is specified for a column name, the default ``\"numeral\"``\\n[ADD] formatter is assumed.\\n[ADD] \\n[ADD] .. |NumeralTickFormatter| replace:: :class:`~bokeh.models.formatters.NumeralTickFormatter`\\n[ADD] .. |DatetimeTickFormatter| replace:: :class:`~bokeh.models.formatters.DatetimeTickFormatter`\\n[ADD] .. |PrintfTickFormatter| replace:: :class:`~bokeh.models.formatters.PrintfTickFormatter`\\n[ADD] \\n[ADD] \"\"\")\\n[ADD] \\n[ADD] separator = String(default=\",\", help=\"\"\"\\n[ADD] The separator to use between the columns of the data. Can be used,\\n[ADD] for example, to extract the tabular data in TSV (Tab-Separated Value)\\n[ADD] format rather than Comma-Separated\"\"\")\\n[ADD] \\n[ADD] download_selected = Bool(default=True, help=\"\"\"\\n[ADD] Whether only the selected data should be downloaded, or all data that\\'s in the data-source.\"\"\")\\n[ADD] \\n[ADD] \\n class ResetTool(Action):\\n     \\'\\'\\' *toolbar icon*: |reset_icon|\\n ',\n",
              " '   protected FileSystem getFileSystem() {\\n     return hoodieTable.getMetaClient().getFs();\\n   }\\n[ADD] \\n[ADD] protected int getPartitionId() {\\n[ADD] return suppliers.getPartitionIdSupplier().get();\\n[ADD] }\\n[ADD] \\n[ADD] protected int getStageId() {\\n[ADD] return suppliers.getStageIdSupplier().get();\\n[ADD] }\\n[ADD] \\n[ADD] protected long getAttemptId() {\\n[ADD] return suppliers.getAttemptIdSupplier().get();\\n[ADD] }\\n }',\n",
              " '     }\\n   }\\n \\n[ADD] private void validateGenerics(ConnectableComponentModel model, ProblemsReporter problemsReporter,\\n[ADD] Class<? extends SampleDataProvider> providerClass) {\\n[ADD] String providerGenerics = asGenericSignature(getInterfaceGenerics(providerClass, SampleDataProvider.class));\\n[ADD] String outputGenerics = asGenericSignature(getOutputTypes(model, providerClass.getClassLoader()));\\n[ADD] \\n[ADD] if (!Objects.equals(providerGenerics, outputGenerics)) {\\n[ADD] problemsReporter.addError(new Problem(model, format(\\n[ADD] \"SampleDataProvider [%s] was expecting to define \\'%s\\' generics signature but \\'%s\\' was found instead\",\\n[ADD] providerClass.getName(), outputGenerics, providerGenerics)));\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private List<Type> getOutputTypes(ConnectableComponentModel model, ClassLoader classLoader) {\\n[ADD] return asList(JavaTypeUtils.getType(model.getOutput().getType(), classLoader),\\n[ADD] JavaTypeUtils.getType(model.getOutputAttributes().getType(), classLoader));\\n[ADD] }\\n[ADD] \\n[ADD] private String asGenericSignature(List<Type> types) {\\n[ADD] return \"<\" + types.stream()\\n[ADD] .map(this::asString)\\n[ADD] .collect(joining(\",\")) + \">\";\\n[ADD] }\\n[ADD] \\n[ADD] private String asString(Type type) {\\n[ADD] if (type instanceof ParameterizedTypeImpl) {\\n[ADD] ParameterizedTypeImpl parameterizedType = (ParameterizedTypeImpl) type;\\n[ADD] return parameterizedType.getRawType().getName() + asGenericSignature(asList(parameterizedType.getActualTypeArguments()));\\n[ADD] } else {\\n[ADD] return type != null ? type.getTypeName() : Object.class.getName();\\n[ADD] }\\n[ADD] }\\n[ADD] \\n   private static class SampleDataProviderInfo {\\n \\n     private SampleDataProviderModel sampleDataProviderModel;',\n",
              " '         connectionManager.setDefaultMaxPerRoute(url.getParameter(Constants.CONNECTIONS_KEY, HTTPCLIENTCONNECTIONMANAGER_MAXPERROUTE));\\n \\n         connectionMonitor.addConnectionManager(connectionManager);\\n[ADD] connectionMonitor.run();\\n         RequestConfig requestConfig = RequestConfig.custom()\\n                 .setConnectTimeout(url.getParameter(Constants.CONNECT_TIMEOUT_KEY, Constants.DEFAULT_CONNECT_TIMEOUT))\\n                 .setSocketTimeout(url.getParameter(Constants.TIMEOUT_KEY, Constants.DEFAULT_TIMEOUT))',\n",
              " ' \\n import javax.annotation.Nonnull;\\n import java.util.Collection;\\n[ADD] import java.util.Optional;\\n \\n import static com.codahale.metrics.MetricRegistry.name;\\n ',\n",
              " ' import java.net.URI;\\n import java.util.HashMap;\\n import java.util.Map;\\n[DEL] \\n import javax.xml.transform.TransformerException;\\n \\n import org.springframework.expression.Expression;',\n",
              " '         inflightInstant = cleanInstant;\\n       }\\n \\n[ADD] // Update Metadata Table before even finishing clean. This ensures that an async clean operation in the\\n[ADD] // background does not lead to stale metadata being returned from Metadata Table.\\n[ADD] HoodieMetadata.update(config, cleanerPlan, cleanInstant.getTimestamp());\\n[ADD] \\n       List<HoodieCleanStat> cleanStats = clean(jsc, cleanerPlan);\\n       if (cleanStats.isEmpty()) {\\n         return HoodieCleanMetadata.newBuilder().build();',\n",
              " '     return destination instanceof Topic;\\n   }\\n \\n[DEL] private void waitForMessageToBeProcesed() {\\n[DEL] if (synchronous) {\\n[DEL] jmsLock.lock();\\n[ADD] private JmsListenerLock createJmsLock() {\\n[ADD] return synchronous ? new DefaultJmsListenerLock() : new NullJmsListenerLock();\\n[ADD] }\\n[ADD] \\n[ADD] private int getValidNumberOfConsumers(int numberOfConsumers) {\\n[ADD] if (numberOfConsumers > 1 && consumerType.isTopic()) {\\n[ADD] TopicConsumer topicConsumer = (TopicConsumer) consumerType;\\n[ADD] \\n[ADD] if (!isCapableOfMultiConsumersOnTopic(topicConsumer)) {\\n[ADD] LOGGER.warn(\"Destination [\" + destination + \"] is a topic, but \" + numberOfConsumers\\n[ADD] + \" receivers have been requested. Will configure only 1\");\\n[ADD] return 1;\\n[ADD] }\\n     }\\n[ADD] \\n[ADD] return numberOfConsumers;\\n[ADD] }\\n[ADD] \\n[ADD] private boolean isCapableOfMultiConsumersOnTopic(TopicConsumer topicConsumer) {\\n[ADD] return jmsSupport.getSpecification().equals(JMS_2_0) && topicConsumer.isShared();\\n   }\\n }',\n",
              " ' \\n     private static void checkForExtraOptions(String fileNamePattern) throws InvalidDumpOptionException {\\n \\t\\t// Check no-one has tried to sneak options onto the end of a filename.\\n[DEL] if( fileNamePattern.contains(\",\")) { //$NON-NLS-1$\\n[ADD] if (fileNamePattern.contains(\",\")) { //$NON-NLS-1$\\n     \\t\\tthrow new InvalidDumpOptionException(\"Invalid dump filename specified.\"); //$NON-NLS-1$\\n     \\t}\\n \\t}',\n",
              " ' package org.mule.runtime.module.deployment.impl.internal.policy;\\n \\n import static java.lang.String.format;\\n[ADD] import static java.util.Collections.emptySet;\\n import static java.util.stream.Collectors.toList;\\n import static java.util.stream.Collectors.toSet;\\n import static org.mule.runtime.api.util.Preconditions.checkArgument;\\n import static org.mule.runtime.deployment.model.internal.DefaultRegionPluginClassLoadersFactory.getArtifactPluginId;\\n import static org.mule.runtime.module.artifact.api.classloader.DefaultArtifactClassLoaderFilter.NULL_CLASSLOADER_FILTER;\\n import static org.mule.runtime.module.deployment.impl.internal.artifact.ArtifactFactoryUtils.validateArtifactLicense;\\n[ADD] \\n import org.mule.runtime.deployment.model.api.application.Application;\\n import org.mule.runtime.deployment.model.api.plugin.ArtifactPlugin;\\n import org.mule.runtime.deployment.model.api.plugin.ArtifactPluginDescriptor;',\n",
              " ' \\t\\t\\t\\t\\t\\t\\tlogger.debug(\"Read exception \" +\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t this.getConnectionId() + \" \" +\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t e.getClass().getSimpleName() +\\n[DEL] \":\" + e.getCause() + \":\" + e.getMessage());\\n[ADD] \":\" + (e.getCause() != null ? e.getCause() + \":\" : \"\") + e.getMessage());\\n \\t\\t\\t\\t\\t\\t}\\n[DEL] } else if (logger.isTraceEnabled()) {\\n[ADD] }\\n[ADD] else if (logger.isTraceEnabled()) {\\n \\t\\t\\t\\t\\t\\tlogger.error(\"Read exception \" +\\n \\t\\t\\t\\t\\t\\t\\t\\t this.getConnectionId(), e);\\n[DEL] } else {\\n[ADD] }\\n[ADD] else {\\n \\t\\t\\t\\t\\t\\tlogger.error(\"Read exception \" +\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t this.getConnectionId() + \" \" +\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t e.getClass().getSimpleName() +\\n[DEL] \":\" + e.getCause() + \":\" + e.getMessage());\\n[ADD] \":\" + (e.getCause() != null ? e.getCause() + \":\" : \"\") + e.getMessage());\\n \\t\\t\\t\\t\\t}\\n \\t\\t\\t\\t}\\n \\t\\t\\t}',\n",
              " ' \\t\\tAssert.hasText(replyChannelName, \"\\'replyChannelName\\' must not be empty\");\\n \\t\\tthis.replyChannelName = replyChannelName;\\n \\t}\\n[ADD] \\n[ADD] /**\\n[ADD] * Sets the content enricher\\'s error channel. If set it will allow the return\\n[ADD] * of an alternative object to use for enrichment if exceptions occur in the\\n[ADD] * downstream flow.\\n[ADD] * @param errorChannel The error channel.\\n[ADD] * @since 4.1\\n[ADD] */\\n[ADD] public void setErrorChannel(MessageChannel errorChannel) {\\n[ADD] this.errorChannel = errorChannel;\\n[ADD] }\\n[ADD] \\n[ADD] public void setErrorChannelName(String errorChannelName) {\\n[ADD] Assert.hasText(errorChannelName, \"\\'errorChannelName\\' must not be empty\");\\n[ADD] this.errorChannelName = errorChannelName;\\n[ADD] }\\n \\n \\t/**\\n \\t * Set the timeout value for sending request messages. If not explicitly configured,',\n",
              " '   /** If true, dump the heap when thrashing or requested. */\\n   private final boolean canDumpHeap;\\n \\n[ADD] /**\\n[ADD] * The GC thrashing threshold (0.00 - 100.00) for every period. If the time spent on garbage\\n[ADD] * collection in one period exceeds this threshold, that period is considered to be in GC\\n[ADD] * thrashing.\\n[ADD] */\\n[ADD] private final double GCThrashingPercentagePerPeriod;\\n[ADD] \\n   private final AtomicBoolean isThrashing = new AtomicBoolean(false);\\n \\n   private final AtomicBoolean isRunning = new AtomicBoolean(false);',\n",
              " '   protected PreparedStatement insertPstmtForFixedBatch;\\n   private final Retryer<Boolean> retryer;\\n \\n[ADD] // If this config is true, the inserter can insert duplicate primary records according to the specific language\\n[ADD] protected final boolean replaceExistingValues;\\n[ADD] \\n   public BaseJdbcBufferedInserter(State state, Connection conn) {\\n     this.conn = conn;\\n     this.batchSize = state.getPropAsInt(WRITER_JDBC_INSERT_BATCH_SIZE, DEFAULT_WRITER_JDBC_INSERT_BATCH_SIZE);',\n",
              " ' \\t\\tsuper.onInit();\\n \\t\\tBeanFactory beanFactory = this.getBeanFactory();\\n \\t\\tif (beanFactory != null) {\\n[DEL] this.messagingTemplate.setBeanFactory(beanFactory);\\n[DEL] if (StringUtils.hasText(this.discardChannelName)) {\\n[DEL] Assert.isNull(this.discardChannel, \"\\'outputChannelName\\' and \\'discardChannel\\' are mutually exclusive.\");\\n[DEL] try {\\n[DEL] this.discardChannel = beanFactory.getBean(this.discardChannelName, MessageChannel.class);\\n[DEL] }\\n[DEL] catch (BeansException e) {\\n[DEL] throw new DestinationResolutionException(\"Failed to look up MessageChannel with name \\'\"\\n[DEL] + this.discardChannelName + \"\\' in the BeanFactory.\");\\n[DEL] }\\n[DEL] }\\n[ADD] getMessagingTemplate().setBeanFactory(beanFactory);\\n[ADD] Assert.state(!(this.discardChannelName != null && this.discardChannel != null),\\n[ADD] \"\\'discardChannelName\\' and \\'discardChannel\\' are mutually exclusive.\");\\n \\n[DEL] if (StringUtils.hasText(this.outputChannelName)) {\\n[DEL] Assert.isNull(this.outputChannel, \"\\'outputChannelName\\' and \\'outputChannel\\' are mutually exclusive.\");\\n[DEL] try {\\n[DEL] this.outputChannel = this.getBeanFactory().getBean(this.outputChannelName, MessageChannel.class);\\n[DEL] }\\n[DEL] catch (BeansException e) {\\n[DEL] throw new DestinationResolutionException(\"Failed to look up MessageChannel with name \\'\"\\n[DEL] + this.outputChannelName + \"\\' in the BeanFactory.\");\\n[DEL] }\\n[DEL] }\\n[ADD] Assert.state(!(getOutputChannelName() != null && getOutputChannel() != null),\\n[ADD] \"\\'outputChannelName\\' and \\'outputChannel\\' are mutually exclusive.\");\\n \\n \\t\\t\\tif (this.outputProcessor instanceof BeanFactoryAware) {\\n \\t\\t\\t\\t((BeanFactoryAware) this.outputProcessor).setBeanFactory(beanFactory);',\n",
              " '       }\\n     }\\n \\n[ADD] private static boolean canSkipClassLoaderByPackagePrefix(ClassLoader loader) {\\n[ADD] String name = loader.getClass().getName();\\n[ADD] if (name.startsWith(\"datadog.\")\\n[ADD] || name.startsWith(\"com.dynatrace.\")\\n[ADD] || name.startsWith(\"com.appdynamics.\")\\n[ADD] || name.startsWith(\"com.newrelic.\")\\n[ADD] || name.startsWith(\"com.nr.agent.\")) {\\n[ADD] return true;\\n[ADD] }\\n[ADD] return false;\\n[ADD] }\\n[ADD] \\n     /**\\n      * TODO: this turns out to be useless with OSGi: {@code\\n      * org.eclipse.osgi.internal.loader.BundleLoader#isRequestFromVM} returns {@code true} when',\n",
              " '             return false;\\n         }\\n \\n[ADD] public class onCardBrowserAppearanceContract extends ActivityResultContract<Intent, Intent>{\\n[ADD] @NonNull\\n[ADD] @Override\\n[ADD] public Intent createIntent(@NonNull Context context, Intent input) {\\n[ADD] return input;\\n[ADD] }\\n \\n[DEL] @Override\\n[DEL] public void onActivityResult(int requestCode, int resultCode, @Nullable Intent data) {\\n[DEL] super.onActivityResult(requestCode, resultCode, data);\\n[DEL] if (requestCode == REQUEST_CARD_BROWSER_APPEARANCE) {\\n[DEL] onCardBrowserAppearanceResult(resultCode, data);\\n[DEL] return;\\n[ADD] \\n[ADD] @Override\\n[ADD] public Intent parseResult(int resultCode, @Nullable Intent intent) {\\n[ADD] if(resultCode != Activity.RESULT_OK || intent == null) {\\n[ADD] return null;\\n[ADD] }\\n[ADD] return intent;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] ActivityResultLauncher<Intent> onCardBrowserAppearanceResult = registerForActivityResult(new onCardBrowserAppearanceContract(), new ActivityResultCallback<Intent>() {\\n[ADD] @Override\\n[ADD] public void onActivityResult(Intent data) {\\n[ADD] onCardBrowserAppearanceResult(data);\\n             }\\n[ADD] });\\n \\n[DEL] if (requestCode == REQUEST_PREVIEWER) {\\n[ADD] public class onRequestPreviewContract extends ActivityResultContract<Intent, Intent>{\\n[ADD] @NonNull\\n[ADD] @Override\\n[ADD] public Intent createIntent(@NonNull Context context, Intent input) {\\n[ADD] return input;\\n[ADD] }\\n[ADD] @Override\\n[ADD] public Intent parseResult(int resultCode, @Nullable Intent intent) {\\n[ADD] if(resultCode != Activity.RESULT_OK || intent == null) {\\n[ADD] return null;\\n[ADD] }\\n[ADD] return intent;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] ActivityResultLauncher<Intent> onRequestPreviewResult = registerForActivityResult(new onRequestPreviewContract(), new ActivityResultCallback<Intent>() {\\n[ADD] @Override\\n[ADD] public void onActivityResult(Intent result) {\\n                 TemporaryModel.clearTempModelFiles();\\n                 // Make sure the fragments reinitialize, otherwise there is staleness on return\\n                 ((TemplatePagerAdapter)mTemplateEditor.mViewPager.getAdapter()).ordinalShift();\\n                 mTemplateEditor.mViewPager.getAdapter().notifyDataSetChanged();\\n             }\\n[DEL] }\\n[DEL] \\n[DEL] \\n[DEL] private void onCardBrowserAppearanceResult(int resultCode, @Nullable Intent data) {\\n[DEL] if (resultCode != RESULT_OK) {\\n[DEL] Timber.i(\"Activity Cancelled: Card Template Browser Appearance\");\\n[DEL] return;\\n[DEL] }\\n[ADD] });\\n \\n[ADD] private void onCardBrowserAppearanceResult(@Nullable Intent data) {\\n             CardTemplateBrowserAppearanceEditor.Result result = CardTemplateBrowserAppearanceEditor.Result.fromIntent(data);\\n             if (result == null) {\\n                 Timber.w(\"Error processing Card Template Browser Appearance result\");',\n",
              " ' import io.opentelemetry.OpenTelemetry;\\n import io.opentelemetry.auto.bootstrap.ContextStore;\\n import io.opentelemetry.auto.bootstrap.InstrumentationContext;\\n[ADD] import io.opentelemetry.auto.bootstrap.instrumentation.jdbc.DbSystem;\\n import io.opentelemetry.auto.tooling.Instrumenter;\\n import io.opentelemetry.trace.Span;\\n import io.opentelemetry.trace.Tracer;',\n",
              " ' \\t\\treturn reply;\\n \\t}\\n \\n[DEL] private void sendRequestMessage(javax.jms.Message jmsRequest, MessageProducer messageProducer, int priority) throws JMSException {\\n[ADD] private SettableListenableFuture<Message<?>> createFuture(final String correlationId) {\\n[ADD] SettableListenableFuture<Message<?>> future = new SettableListenableFuture<Message<?>>();\\n[ADD] this.futures.put(correlationId, future);\\n[ADD] if (this.receiveTimeout > 0) {\\n[ADD] getTaskScheduler().schedule(new Runnable() {\\n[ADD] \\n[ADD] @Override\\n[ADD] public void run() {\\n[ADD] expire(correlationId);\\n[ADD] }\\n[ADD] }, new Date(System.currentTimeMillis() + this.receiveTimeout));\\n[ADD] }\\n[ADD] return future;\\n[ADD] }\\n[ADD] \\n[ADD] private void expire(String correlationId) {\\n[ADD] final SettableListenableFuture<Message<?>> future = JmsOutboundGateway.this.futures\\n[ADD] .remove(correlationId);\\n[ADD] if (future != null) {\\n[ADD] try {\\n[ADD] if (getRequiresReply()) {\\n[ADD] future.setException(new JmsTimeoutException(\"No reply in \" + this.receiveTimeout + \" ms\"));\\n[ADD] }\\n[ADD] else {\\n[ADD] if (logger.isDebugEnabled()) {\\n[ADD] logger.debug(\"Reply expired and reply not required for \" + correlationId);\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] catch (Exception e) {\\n[ADD] logger.error(\"Exception while expiring future\");\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private void sendRequestMessage(javax.jms.Message jmsRequest, MessageProducer messageProducer, int priority)\\n[ADD] throws JMSException {\\n \\t\\tif (this.explicitQosEnabled) {\\n \\t\\t\\tmessageProducer.send(jmsRequest, this.deliveryMode, priority, this.timeToLive);\\n \\t\\t}',\n",
              " '         {\\n             for (ComponentModel innerComponent : innerComponents)\\n             {\\n[DEL] if (hasDefinition(innerComponent.getIdentifier()))\\n[ADD] if (hasDefinition(innerComponent.getIdentifier(), of(innerComponent.getParent().getIdentifier())))\\n                 {\\n                     resolveComponentRecursively(componentModel, innerComponent, registry, componentModelPostProcessor, oldParsingMechanism);\\n                 }',\n",
              " ' \\n   @Override\\n   public void close() throws IOException {\\n[ADD] ((ExternalSpillableMap) deltaRecordMap).close();\\n     parquetReader.close();\\n   }\\n ',\n",
              " '   @Test\\n   @Description(\"Tests that a task submitted to a Scheduler after calling shutdown() is rejected\")\\n   public void submitAfterShutdownSameExecutor() throws InterruptedException, ExecutionException {\\n[DEL] final ScheduledExecutorService executor = createExecutor();\\n[DEL] \\n[DEL] executor.shutdown();\\n[ADD] executor1.shutdown();\\n \\n[DEL] assertRejected(executor, SUBMIT_EMPTY_RUNNABLE);\\n[ADD] assertRejected(executor1, SUBMIT_EMPTY_RUNNABLE);\\n   }\\n \\n   @Test\\n   @Description(\"Tests that a task submitted to a Scheduler after calling shutdown() on another Scheduler is NOT rejected\")\\n   public void submitAfterShutdownOtherExecutor() throws InterruptedException, ExecutionException, TimeoutException {\\n[DEL] final ScheduledExecutorService executor1 = createExecutor();\\n[DEL] final ScheduledExecutorService executor2 = createExecutor();\\n[DEL] \\n     executor1.shutdown();\\n \\n     final CountDownLatch latch = new CountDownLatch(1);',\n",
              " '     {\\n         elog->LoadPreservedBPInfo();\\n \\n[ADD] if(elog->GetPerservedBPInfoCount() != 0)\\n[ADD] {\\n[ADD] TTD::TTDebuggerSourceLocation** locationList = elog->GetPerservedBPInfoLocationArray();\\n[ADD] for(uint32 i = 0; i < elog->GetPerservedBPInfoCount(); ++i)\\n[ADD] {\\n[ADD] TTD::TTDebuggerSourceLocation* bpLocation = locationList[i];\\n[ADD] bpLocation->EnsureTopLevelBodyCtrPreInflate();\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] if(elog->HasPendingTTDBP())\\n[ADD] {\\n[ADD] elog->EnsureTTDBPInfoTopLevelBodyCtrPreInflate();\\n[ADD] }\\n[ADD] \\n         elog->DoSnapshotInflate(snapTime);\\n \\n         if(elog->GetPerservedBPInfoCount() != 0)\\n         {\\n             JsrtDebugManager* jsrtDebugManager = runtime->GetJsrtDebugManager();\\n \\n[ADD] bool bpNotMapped = false;\\n             TTD_LOG_PTR_ID* ctxIdList = elog->GetPerservedBPInfoScriptArray();\\n             TTD::TTDebuggerSourceLocation** locationList = elog->GetPerservedBPInfoLocationArray();\\n             for(uint32 i = 0; i < elog->GetPerservedBPInfoCount(); ++i)\\n             {\\n[DEL] const TTD::TTDebuggerSourceLocation* bpLocation = locationList[i];\\n[ADD] TTD::TTDebuggerSourceLocation* bpLocation = locationList[i];\\n                 Js::ScriptContext* bpContext = threadContext->TTDContext->LookupContextForScriptId(ctxIdList[i]);\\n \\n[DEL] if(bpContext != nullptr)\\n[ADD] //\\n[ADD] //TODO: When we travel back some script may not be loaded (so no place to put BP). We need to update this\\n[ADD] //      to do a more extensive maintaining of the preserved breakpoints and put them back as we add new script -- instead of just here.\\n[ADD] //      However, for now just print a warn if the BP cannot be resolved.\\n[ADD] //\\n[ADD] bool thisBPNotMapped = (bpContext == nullptr || bpLocation->LoadFunctionBodyIfPossible(bpContext) == nullptr);\\n[ADD] bpNotMapped |= thisBPNotMapped;\\n[ADD] \\n[ADD] if(!thisBPNotMapped)\\n                 {\\n[DEL] Js::FunctionBody* body = bpLocation->ResolveAssociatedSourceInfo(bpContext);\\n[DEL] Js::Utf8SourceInfo* utf8SourceInfo = body->GetUtf8SourceInfo();\\n[ADD] Js::Utf8SourceInfo* utf8SourceInfo = bpLocation->LoadFunctionBodyIfPossible(bpContext)->GetUtf8SourceInfo();\\n \\n                     bool isNewBP = false;\\n                     jsrtDebugManager->SetBreakpointHelper_TTD(bpContext, utf8SourceInfo, bpLocation->GetLine(), bpLocation->GetColumn(), &isNewBP);\\n                 }\\n             }\\n[ADD] \\n[ADD] if(bpNotMapped)\\n[ADD] {\\n[ADD] fprintf(stderr, \"Could not map a breakpoint after inflate -- some breakpoints may be disabled.\\\\n\");\\n[ADD] }\\n         }\\n         elog->UnLoadPreservedBPInfo();\\n ',\n",
              " ' import org.mule.runtime.core.api.MuleEvent;\\n import org.mule.runtime.core.api.MuleException;\\n import org.mule.runtime.core.api.MuleMessage;\\n[DEL] import org.mule.runtime.core.api.ThreadSafeAccess;\\n import org.mule.runtime.core.api.connector.ReplyToHandler;\\n import org.mule.runtime.core.config.i18n.CoreMessages;\\n import org.mule.runtime.core.construct.Flow;',\n",
              " '     if (!(javaagentFile.exists() || javaagentFile.isFile())) {\\n       throw new RuntimeException(\"Unable to find javaagent file: \" + javaagentFile);\\n     }\\n[DEL] bootstrapURL = javaagentFile.toURI().toURL();\\n[ADD] javaAgentJarURL = javaagentFile.toURI().toURL();\\n[ADD] checkJarManifestMainClassIsThis(javaAgentJarURL);\\n     inst.appendToBootstrapClassLoaderSearch(new JarFile(javaagentFile));\\n \\n[DEL] return bootstrapURL;\\n[ADD] return javaAgentJarURL;\\n   }\\n \\n   private static List<String> getVMArgumentsThroughReflection() {\\n     try {\\n       // Try Oracle-based\\n       final Class managementFactoryHelperClass =\\n[DEL] AgentBootstrap.class.getClassLoader().loadClass(\"sun.management.ManagementFactoryHelper\");\\n[ADD] thisClass.getClassLoader().loadClass(\"sun.management.ManagementFactoryHelper\");\\n \\n       final Class vmManagementClass =\\n[DEL] AgentBootstrap.class.getClassLoader().loadClass(\"sun.management.VMManagement\");\\n[ADD] thisClass.getClassLoader().loadClass(\"sun.management.VMManagement\");\\n \\n       Object vmManagement;\\n ',\n",
              " '       }\\n     }\\n \\n[DEL] private class ResidualElements {\\n[DEL] private final List<TimestampedValue<T>> elementsList;\\n[DEL] private @Nullable Iterator<TimestampedValue<T>> elementsIterator;\\n[DEL] private @Nullable TimestampedValue<T> currentT;\\n[DEL] private boolean hasCurrent;\\n[DEL] private boolean done;\\n[DEL] \\n[DEL] ResidualElements(List<TimestampedValue<T>> residualElementsList) {\\n[DEL] this.elementsList = checkNotNull(residualElementsList, \"residualElementsList\");\\n[DEL] this.elementsIterator = null;\\n[DEL] this.currentT = null;\\n[DEL] this.hasCurrent = false;\\n[DEL] this.done = false;\\n[DEL] }\\n[DEL] \\n[DEL] public boolean advance() {\\n[DEL] if (elementsIterator == null) {\\n[DEL] elementsIterator = elementsList.iterator();\\n[DEL] }\\n[DEL] if (elementsIterator.hasNext()) {\\n[DEL] currentT = elementsIterator.next();\\n[DEL] hasCurrent = true;\\n[DEL] return true;\\n[DEL] } else {\\n[DEL] done = true;\\n[DEL] hasCurrent = false;\\n[DEL] return false;\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] boolean hasCurrent() {\\n[DEL] return hasCurrent;\\n[DEL] }\\n[ADD] /**\\n[ADD] * A marker representing the progress and state of an {@link BoundedToUnboundedSourceAdapter}.\\n[ADD] */\\n[ADD] @VisibleForTesting\\n[ADD] public static class Checkpoint<T> implements UnboundedSource.CheckpointMark {\\n \\n[DEL] boolean done() {\\n[DEL] return done;\\n[DEL] }\\n[ADD] private final @Nullable List<TimestampedValue<T>> residualElements;\\n[ADD] private @Nullable ArrayDeque<BoundedSource<T>> residualSources;\\n \\n[DEL] TimestampedValue<T> getCurrentTimestampedValue() {\\n[DEL] if (!hasCurrent) {\\n[DEL] throw new NoSuchElementException();\\n[DEL] }\\n[DEL] return currentT;\\n[ADD] public Checkpoint(\\n[ADD] @Nullable List<TimestampedValue<T>> residualElements,\\n[ADD] ArrayDeque<BoundedSource<T>> residualSources) {\\n[ADD] this.residualElements = residualElements;\\n[ADD] this.residualSources = residualSources;\\n       }\\n \\n[DEL] T getCurrent() {\\n[DEL] return getCurrentTimestampedValue().getValue();\\n[DEL] }\\n[ADD] @Override\\n[ADD] public void finalizeCheckpoint() {}\\n \\n[DEL] Instant getCurrentTimestamp() {\\n[DEL] return getCurrentTimestampedValue().getTimestamp();\\n[ADD] @VisibleForTesting\\n[ADD] @Nullable\\n[ADD] List<TimestampedValue<T>> getResidualElements() {\\n[ADD] return residualElements;\\n       }\\n \\n[DEL] List<TimestampedValue<T>> getRestElements() {\\n[DEL] if (elementsIterator == null) {\\n[DEL] return elementsList;\\n[DEL] } else {\\n[DEL] List<TimestampedValue<T>> newResidualElements = Lists.newArrayList();\\n[DEL] while (elementsIterator.hasNext()) {\\n[DEL] newResidualElements.add(elementsIterator.next());\\n[DEL] }\\n[DEL] return newResidualElements;\\n[DEL] }\\n[ADD] @VisibleForTesting\\n[ADD] @Nullable\\n[ADD] ArrayDeque<BoundedSource<T>> getResidualSources() {\\n[ADD] return residualSources;\\n       }\\n     }\\n \\n[DEL] private class ResidualSource {\\n[DEL] private BoundedSource<T> residualSource;\\n[ADD] private class ResidualSources {\\n[ADD] private ArrayDeque<BoundedSource<T>> residualSources;\\n       private PipelineOptions options;\\n       private @Nullable BoundedReader<T> reader;\\n       private boolean closed;\\n       private boolean readerDone;\\n[ADD] private boolean currentResidualSourceDone;\\n[ADD] private @Nullable BoundedSource<T> currentResidualSource;\\n \\n[DEL] public ResidualSource(BoundedSource<T> residualSource, PipelineOptions options) {\\n[DEL] this.residualSource = checkNotNull(residualSource, \"residualSource\");\\n[ADD] public ResidualSources(\\n[ADD] ArrayDeque<BoundedSource<T>> residualSources, PipelineOptions options) {\\n[ADD] this.residualSources = residualSources;\\n         this.options = checkNotNull(options, \"options\");\\n         this.reader = null;\\n         this.closed = false;\\n         this.readerDone = false;\\n[ADD] this.currentResidualSourceDone = false;\\n       }\\n \\n[DEL] private boolean advance() throws IOException {\\n[DEL] checkArgument(!closed, \"advance() call on closed %s\", getClass().getName());\\n[ADD] private boolean checkpointAdvanceHelper(boolean onlyFinishReadingCurrentSource)\\n[ADD] throws IOException {\\n[ADD] return helper(onlyFinishReadingCurrentSource);\\n[ADD] }\\n[ADD] \\n[ADD] private boolean helper(boolean onlyFinishReadingCurrentSource) throws IOException {\\n[ADD] checkArgument(!closed, \"advance() call on closed %s,\", getClass().getName());\\n         if (readerDone) {\\n           return false;\\n         }\\n[DEL] if (reader == null) {\\n[DEL] reader = residualSource.createReader(options);\\n[DEL] readerDone = !reader.start();\\n[ADD] \\n[ADD] if (reader == null && !residualSources.isEmpty()) {\\n[ADD] currentResidualSource = residualSources.poll();\\n[ADD] reader = currentResidualSource.createReader(options);\\n[ADD] currentResidualSourceDone = !reader.start();\\n         } else {\\n[DEL] readerDone = !reader.advance();\\n[ADD] if (reader == null || residualSources == null) {\\n[ADD] return false;\\n[ADD] }\\n[ADD] currentResidualSourceDone = !reader.advance();\\n[ADD] }\\n[ADD] \\n[ADD] if (currentResidualSourceDone\\n[ADD] && !residualSources.isEmpty()\\n[ADD] && !onlyFinishReadingCurrentSource) {\\n[ADD] reader.close();\\n[ADD] reader = null;\\n[ADD] currentResidualSource = residualSources.poll();\\n[ADD] reader = currentResidualSource.createReader(options);\\n[ADD] currentResidualSourceDone = !reader.start();\\n[ADD] }\\n[ADD] \\n[ADD] if (residualSources.isEmpty() && currentResidualSourceDone) {\\n[ADD] readerDone = true;\\n         }\\n         return !readerDone;\\n       }\\n \\n[ADD] private boolean advance() throws IOException {\\n[ADD] return helper(false);\\n[ADD] }\\n[ADD] \\n       T getCurrent() throws NoSuchElementException {\\n         if (reader == null) {\\n           throw new NoSuchElementException();',\n",
              " ' \\n   @Override\\n   public ResultSet execute(String query) {\\n[DEL] Context context = tracer().startSpan(Context.current(), session, query);\\n[ADD] CassandraRequest request = new CassandraRequest(session, query);\\n[ADD] Context context = instrumenter().start(Context.current(), request);\\n     ResultSet resultSet;\\n     try (Scope ignored = context.makeCurrent()) {\\n       resultSet = session.execute(query);\\n     } catch (Throwable t) {\\n[DEL] tracer().endExceptionally(context, t);\\n[ADD] instrumenter().end(context, request, null, t);\\n       throw t;\\n     }\\n[DEL] tracer().end(context, resultSet.getExecutionInfo());\\n[ADD] request.setExecutionInfo(resultSet.getExecutionInfo());\\n[ADD] instrumenter().end(context, request, null, null);\\n     return resultSet;\\n   }\\n \\n   @Override\\n   public ResultSet execute(String query, Object... values) {\\n[DEL] Context context = tracer().startSpan(Context.current(), session, query);\\n[ADD] CassandraRequest request = new CassandraRequest(session, query);\\n[ADD] Context context = instrumenter().start(Context.current(), request);\\n     ResultSet resultSet;\\n     try (Scope ignored = context.makeCurrent()) {\\n       resultSet = session.execute(query, values);\\n     } catch (Throwable t) {\\n[DEL] tracer().endExceptionally(context, t);\\n[ADD] instrumenter().end(context, request, null, t);\\n       throw t;\\n     }\\n[DEL] tracer().end(context, resultSet.getExecutionInfo());\\n[ADD] request.setExecutionInfo(resultSet.getExecutionInfo());\\n[ADD] instrumenter().end(context, request, null, null);\\n     return resultSet;\\n   }\\n \\n   @Override\\n   public ResultSet execute(String query, Map<String, Object> values) {\\n[DEL] Context context = tracer().startSpan(Context.current(), session, query);\\n[ADD] CassandraRequest request = new CassandraRequest(session, query);\\n[ADD] Context context = instrumenter().start(Context.current(), request);\\n     ResultSet resultSet;\\n     try (Scope ignored = context.makeCurrent()) {\\n       resultSet = session.execute(query, values);\\n     } catch (Throwable t) {\\n[DEL] tracer().endExceptionally(context, t);\\n[ADD] instrumenter().end(context, request, null, t);\\n       throw t;\\n     }\\n[DEL] tracer().end(context, resultSet.getExecutionInfo());\\n[ADD] request.setExecutionInfo(resultSet.getExecutionInfo());\\n[ADD] instrumenter().end(context, request, null, null);\\n     return resultSet;\\n   }\\n \\n   @Override\\n   public ResultSet execute(Statement statement) {\\n[DEL] Context context = tracer().startSpan(Context.current(), session, getQuery(statement));\\n[ADD] String query = getQuery(statement);\\n[ADD] CassandraRequest request = new CassandraRequest(session, query);\\n[ADD] Context context = instrumenter().start(Context.current(), request);\\n     ResultSet resultSet;\\n     try (Scope ignored = context.makeCurrent()) {\\n       resultSet = session.execute(statement);\\n     } catch (Throwable t) {\\n[DEL] tracer().endExceptionally(context, t);\\n[ADD] instrumenter().end(context, request, null, t);\\n       throw t;\\n     }\\n[DEL] tracer().end(context, resultSet.getExecutionInfo());\\n[ADD] request.setExecutionInfo(resultSet.getExecutionInfo());\\n[ADD] instrumenter().end(context, request, null, null);\\n     return resultSet;\\n   }\\n \\n   @Override\\n   public ResultSetFuture executeAsync(String query) {\\n[DEL] Context context = tracer().startSpan(Context.current(), session, query);\\n[ADD] CassandraRequest request = new CassandraRequest(session, query);\\n[ADD] Context context = instrumenter().start(Context.current(), request);\\n     try (Scope ignored = context.makeCurrent()) {\\n       ResultSetFuture future = session.executeAsync(query);\\n[DEL] addCallbackToEndSpan(future, context);\\n[ADD] addCallbackToEndSpan(future, context, request);\\n       return future;\\n     }\\n   }\\n \\n   @Override\\n   public ResultSetFuture executeAsync(String query, Object... values) {\\n[DEL] Context context = tracer().startSpan(Context.current(), session, query);\\n[ADD] CassandraRequest request = new CassandraRequest(session, query);\\n[ADD] Context context = instrumenter().start(Context.current(), request);\\n     try (Scope ignored = context.makeCurrent()) {\\n       ResultSetFuture future = session.executeAsync(query, values);\\n[DEL] addCallbackToEndSpan(future, context);\\n[ADD] addCallbackToEndSpan(future, context, request);\\n       return future;\\n     }\\n   }\\n \\n   @Override\\n   public ResultSetFuture executeAsync(String query, Map<String, Object> values) {\\n[DEL] Context context = tracer().startSpan(Context.current(), session, query);\\n[ADD] CassandraRequest request = new CassandraRequest(session, query);\\n[ADD] Context context = instrumenter().start(Context.current(), request);\\n     try (Scope ignored = context.makeCurrent()) {\\n       ResultSetFuture future = session.executeAsync(query, values);\\n[DEL] addCallbackToEndSpan(future, context);\\n[ADD] addCallbackToEndSpan(future, context, request);\\n       return future;\\n     }\\n   }',\n",
              " ' \\t\\t\\tif (this.replyContainerProperties.getRecoveryInterval() != null) {\\n \\t\\t\\t\\tcontainer.setRecoveryInterval(this.replyContainerProperties.getRecoveryInterval());\\n \\t\\t\\t}\\n[DEL] if (this.replyContainerProperties.getSessionAcknowledgeMode() != null) {\\n[DEL] container.setSessionAcknowledgeMode(this.replyContainerProperties.getSessionAcknowledgeMode());\\n[ADD] if (StringUtils.hasText(this.replyContainerProperties.getSessionAcknowledgeModeName())) {\\n[ADD] Integer acknowledgeMode = JmsAdapterUtils.parseAcknowledgeMode(this.replyContainerProperties.getSessionAcknowledgeModeName());\\n[ADD] if (acknowledgeMode != null) {\\n[ADD] if (JmsAdapterUtils.SESSION_TRANSACTED == acknowledgeMode) {\\n[ADD] container.setSessionTransacted(true);\\n[ADD] }\\n[ADD] else {\\n[ADD] container.setSessionAcknowledgeMode(acknowledgeMode);\\n[ADD] }\\n[ADD] }\\n \\t\\t\\t}\\n \\t\\t\\tif (this.replyContainerProperties.getTaskExecutor() != null) {\\n \\t\\t\\t\\tcontainer.setTaskExecutor(this.replyContainerProperties.getTaskExecutor());',\n",
              " ' \\n     private void setState(Map<String, String> extraParameters, MuleEvent event)\\n     {\\n[DEL] String state = String.format(OAuthProperties.EVENT_STATE_TEMPLATE, event.getId());\\n[ADD] String state = String.format(BASE_EVENT_STATE_TEMPLATE + getSuffix(), event.getId());\\n \\n         if (this.getState() != null)\\n         {',\n",
              " '   @Test\\n   public void testChoice() throws Exception {\\n     ChoiceRouter choiceRouter = new ChoiceRouter();\\n[DEL] choiceRouter.addRoute(getAppendingMP(\"1\"), new AcceptAllFilter());\\n[DEL] choiceRouter.addRoute(getAppendingMP(\"2\"), new AcceptAllFilter());\\n[DEL] choiceRouter.addRoute(getAppendingMP(\"3\"), new AcceptAllFilter());\\n[ADD] choiceRouter.addRoute(newChain(getAppendingMP(\"1\")), new AcceptAllFilter());\\n[ADD] choiceRouter.addRoute(newChain(getAppendingMP(\"2\")), new AcceptAllFilter());\\n[ADD] choiceRouter.addRoute(newChain(getAppendingMP(\"3\")), new AcceptAllFilter());\\n \\n     assertThat(process(newChain(choiceRouter), getTestEventUsingFlow(\"0\")).getMessage().getPayload().getValue(), equalTo(\"01\"));\\n[DEL] \\n[DEL] assertEquals(isMultipleThreadsUsed() ? 2 : 1, threads);\\n   }\\n \\n   @Test(expected = MessagingException.class)',\n",
              " '   protected void doSetUp() throws Exception {\\n     capturedEvents = new LinkedList<>();\\n     ReconnectableConnectionProvider.fail = false;\\n[ADD] FallibleReconnectableSource.fail = false;\\n   }\\n \\n   @Override\\n   protected void doTearDown() throws Exception {\\n     capturedEvents = null;\\n     ReconnectableConnectionProvider.fail = false;\\n[ADD] FallibleReconnectableSource.fail = false;\\n   }\\n \\n   @Test',\n",
              " '    */\\n   Optional<HoodieInstant> lastInstant();\\n \\n[ADD] \\n[ADD] /**\\n[ADD] * Get hash of timeline\\n[ADD] * @return\\n[ADD] */\\n[ADD] String getTimelineHash();\\n[ADD] \\n   /**\\n    * @return nth completed instant going back from the last completed instant\\n    */',\n",
              " ' \\n     int numInstrumenters = 0;\\n \\n[DEL] for (InstrumentationModule instrumentationModule : loadInstrumentationModules()) {\\n[DEL] log.debug(\"Loading instrumentation {}\", instrumentationModule.getClass().getName());\\n[ADD] for (AgentExtension agentExtension : loadAgentExtensions()) {\\n[ADD] log.debug(\"Loading extension {}\", agentExtension.getClass().getName());\\n       try {\\n[DEL] agentBuilder = instrumentationModule.instrument(agentBuilder);\\n[ADD] agentBuilder = agentExtension.extend(agentBuilder);\\n         numInstrumenters++;\\n       } catch (Exception | LinkageError e) {\\n[DEL] log.error(\\n[DEL] \"Unable to load instrumentation {}\", instrumentationModule.getClass().getName(), e);\\n[ADD] log.error(\"Unable to load extension {}\", agentExtension.getClass().getName(), e);\\n       }\\n     }\\n \\n[DEL] agentBuilder = customizeByteBuddyAgent(agentBuilder);\\n     log.debug(\"Installed {} instrumenter(s)\", numInstrumenters);\\n     ResettableClassFileTransformer resettableClassFileTransformer = agentBuilder.installOn(inst);\\n     installComponentsAfterByteBuddy(componentInstallers, config);',\n",
              " ' \\t\\treturn \"KIND_#\"+kind; //$NON-NLS-1$\\n \\t}\\n \\n[DEL] /*[IF Sidecar19-SE-OpenJ9]*/\\n[ADD] /*[IF Sidecar18-SE-OpenJ9]*/\\n \\tMethodHandle(MethodType mt, LambdaForm lf) {\\n \\t\\tthrow OpenJDKCompileStub.OpenJDKCompileStubThrowError();\\n \\t}',\n",
              " ' \\n         Map<String, Object> proxiesOftype = proxies.get(type);\\n         if (CollectionUtils.isNotEmptyMap(proxiesOftype)) {\\n[DEL] proxiesOftype.remove(key);\\n[ADD] Destroyable proxy = (Destroyable) proxiesOftype.remove(key);\\n[ADD] proxy.$destroy();\\n             if (proxiesOftype.isEmpty()) {\\n                 proxies.remove(type);\\n             }',\n",
              " '    */\\n   static void setCurrentEvent(Event event) {\\n     CurrentEventHolder.currentEvent.set(event);\\n[ADD] if (event != null) {\\n[ADD] ThreadContext.put(\"correlationId\", event.getCorrelationId());\\n[ADD] ThreadContext.put(\"originatingFlowName\", event.getContext().getOriginatingFlowName());\\n[ADD] }\\n   }\\n \\n }',\n",
              " ' import org.pentaho.di.trans.step.StepMetaInterface;\\n import org.pentaho.di.trans.steps.memgroupby.MemoryGroupByData.HashEntry;\\n \\n[DEL] import java.util.ArrayList;\\n[DEL] import java.util.HashMap;\\n[DEL] import java.util.List;\\n[DEL] import java.util.Set;\\n[DEL] import java.util.TreeSet;\\n[DEL] \\n /**\\n  * Groups information based on aggregation rules. (sum, count, ...)\\n  *',\n",
              " '     if (params.length > 0) {\\n       contextToken = fnClass.resolveType(params[0]);\\n     }\\n[DEL] checkArgument(\\n[ADD] errors.checkArgument(\\n         contextToken != null && contextToken.equals(processContextToken),\\n[DEL] \"%s must take a %s as its first argument\",\\n[DEL] format(m),\\n[ADD] \"Must take %s as its first argument\",\\n         formatType(processContextToken));\\n \\n[DEL] List<DoFnSignature.ProcessElementMethod.Parameter> extraParameters = new ArrayList<>();\\n[ADD] List<DoFnSignature.Parameter> extraParameters = new ArrayList<>();\\n[ADD] \\n     TypeToken<?> expectedInputProviderT = inputProviderTypeOf(inputT);\\n     TypeToken<?> expectedOutputReceiverT = outputReceiverTypeOf(outputT);\\n     for (int i = 1; i < params.length; ++i) {\\n[DEL] TypeToken<?> param = fnClass.resolveType(params[i]);\\n[DEL] Class<?> rawType = param.getRawType();\\n[ADD] TypeToken<?> paramT = fnClass.resolveType(params[i]);\\n[ADD] Class<?> rawType = paramT.getRawType();\\n       if (rawType.equals(BoundedWindow.class)) {\\n[DEL] checkArgument(\\n[DEL] !extraParameters.contains(DoFnSignature.ProcessElementMethod.Parameter.BOUNDED_WINDOW),\\n[DEL] \"Multiple BoundedWindow parameters in %s\",\\n[DEL] format(m));\\n[DEL] extraParameters.add(DoFnSignature.ProcessElementMethod.Parameter.BOUNDED_WINDOW);\\n[ADD] errors.checkArgument(\\n[ADD] !extraParameters.contains(DoFnSignature.Parameter.BOUNDED_WINDOW),\\n[ADD] \"Multiple BoundedWindow parameters\");\\n[ADD] extraParameters.add(DoFnSignature.Parameter.BOUNDED_WINDOW);\\n       } else if (rawType.equals(DoFn.InputProvider.class)) {\\n[DEL] checkArgument(\\n[DEL] !extraParameters.contains(DoFnSignature.ProcessElementMethod.Parameter.INPUT_PROVIDER),\\n[DEL] \"Multiple InputProvider parameters in %s\",\\n[DEL] format(m));\\n[DEL] checkArgument(\\n[DEL] param.equals(expectedInputProviderT),\\n[DEL] \"Wrong type of InputProvider parameter for method %s: %s, should be %s\",\\n[DEL] format(m),\\n[DEL] formatType(param),\\n[ADD] errors.checkArgument(\\n[ADD] !extraParameters.contains(DoFnSignature.Parameter.INPUT_PROVIDER),\\n[ADD] \"Multiple InputProvider parameters\");\\n[ADD] errors.checkArgument(\\n[ADD] paramT.equals(expectedInputProviderT),\\n[ADD] \"Wrong type of InputProvider parameter: %s, should be %s\",\\n[ADD] formatType(paramT),\\n             formatType(expectedInputProviderT));\\n[DEL] extraParameters.add(DoFnSignature.ProcessElementMethod.Parameter.INPUT_PROVIDER);\\n[ADD] extraParameters.add(DoFnSignature.Parameter.INPUT_PROVIDER);\\n       } else if (rawType.equals(DoFn.OutputReceiver.class)) {\\n[DEL] checkArgument(\\n[DEL] !extraParameters.contains(DoFnSignature.ProcessElementMethod.Parameter.OUTPUT_RECEIVER),\\n[DEL] \"Multiple OutputReceiver parameters in %s\",\\n[DEL] format(m));\\n[DEL] checkArgument(\\n[DEL] param.equals(expectedOutputReceiverT),\\n[DEL] \"Wrong type of OutputReceiver parameter for method %s: %s, should be %s\",\\n[DEL] format(m),\\n[DEL] formatType(param),\\n[ADD] errors.checkArgument(\\n[ADD] !extraParameters.contains(DoFnSignature.Parameter.OUTPUT_RECEIVER),\\n[ADD] \"Multiple OutputReceiver parameters\");\\n[ADD] errors.checkArgument(\\n[ADD] paramT.equals(expectedOutputReceiverT),\\n[ADD] \"Wrong type of OutputReceiver parameter: %s, should be %s\",\\n[ADD] formatType(paramT),\\n             formatType(expectedOutputReceiverT));\\n[DEL] extraParameters.add(DoFnSignature.ProcessElementMethod.Parameter.OUTPUT_RECEIVER);\\n[ADD] extraParameters.add(DoFnSignature.Parameter.OUTPUT_RECEIVER);\\n       } else {\\n         List<String> allowedParamTypes =\\n             Arrays.asList(formatType(new TypeToken<BoundedWindow>() {}));\\n[DEL] checkArgument(\\n[DEL] false,\\n[DEL] \"%s is not a valid context parameter for method %s. Should be one of %s\",\\n[DEL] formatType(param),\\n[DEL] format(m),\\n[DEL] allowedParamTypes);\\n[ADD] errors.throwIllegalArgument(\\n[ADD] \"%s is not a valid context parameter. Should be one of %s\",\\n[ADD] formatType(paramT), allowedParamTypes);\\n       }\\n     }\\n ',\n",
              " '    public ConnectionsPane(Commands commands, EventBus eventBus, UserPrefs userPrefs)\\n    {\\n       // initialize\\n[DEL] super(\"Connections\");\\n[ADD] super(\"Connections\", eventBus);\\n       commands_ = commands;\\n[DEL] eventBus_ = eventBus;\\n       userPrefs_ = userPrefs;\\n \\n       // track activation events to update the toolbar\\n[DEL] eventBus_.addHandler(ActiveConnectionsChangedEvent.TYPE, this);\\n[ADD] events_.addHandler(ActiveConnectionsChangedEvent.TYPE, this);\\n       \\n       // create data grid\\n       keyProvider_ = new ProvidesKey<Connection>() {',\n",
              " ' \\t\\tassertThat(pool.getAllocatedCount()).isEqualTo(2);\\n \\t}\\n \\n[DEL] @Test(expected = IllegalArgumentException.class)\\n[ADD] @Test\\n \\tpublic void testForeignObject() {\\n \\t\\tfinal Set<String> strings = new HashSet<String>();\\n \\t\\tfinal AtomicBoolean stale = new AtomicBoolean();\\n \\t\\tSimplePool<String> pool = stringPool(2, strings, stale);\\n \\t\\tpool.getItem();\\n[DEL] pool.releaseItem(\"Hello, world!\");\\n[ADD] assertThatIllegalArgumentException().isThrownBy(() -> pool.releaseItem(\"Hello, world!\"));\\n \\t}\\n \\n \\t@Test',\n",
              " '         // This special InputStream closes the SftpClient when the stream is closed.\\n         // The stream will be materialized in a Message Dispatcher or Service\\n         // Component\\n[DEL] return new SftpInputStream(client, fileInputStream, fileName, connector.isAutoDelete(), endpoint);\\n[ADD] return new SftpInputStream(client, fileInputStream, fileName, determineAutoDelete(), endpoint);\\n[ADD] }\\n[ADD] \\n[ADD] private boolean determineAutoDelete()\\n[ADD] {\\n[ADD] Boolean autoDelete = Boolean.valueOf((String) endpoint.getProperty(\"autoDelete\"));\\n[ADD] if (autoDelete == null)\\n[ADD] {\\n[ADD] autoDelete = connector.isAutoDelete();\\n[ADD] }\\n[ADD] return autoDelete;\\n     }\\n \\n     private InputStream archiveFileUsingTempDirs(String archive,',\n",
              " '     return MULE_DOMAIN_CLASSIFIER;\\n   }\\n \\n[ADD] @Override\\n[ADD] public String getScope() {\\n[ADD] return PROVIDED_SCOPE;\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Adds a property into the plugin properties file.\\n[ADD] *\\n[ADD] * @param propertyName name fo the property to add. Non empty\\n[ADD] * @param propertyValue value of the property to add. Non null.\\n[ADD] * @return the same builder instance\\n[ADD] */\\n[ADD] public DomainFileBuilder configuredWith(String propertyName, String propertyValue) {\\n[ADD] checkImmutable();\\n[ADD] Preconditions.checkArgument(!isEmpty(propertyName), \"Property name cannot be empty\");\\n[ADD] Preconditions.checkArgument(propertyValue != null, \"Property value cannot be null\");\\n[ADD] properties.put(propertyName, propertyValue);\\n[ADD] return this;\\n[ADD] }\\n[ADD] \\n   @Override\\n   protected DomainFileBuilder getThis() {\\n     return this;',\n",
              " '     @Test\\n     public void deploysInvalidExplodedDomainAfterStartup() throws Exception\\n     {\\n[DEL] deploymentService.start();\\n[ADD] startDeployment();\\n \\n         addExplodedDomainFromBuilder(emptyDomainFileBuilder, \"domain with spaces\");\\n ',\n",
              " '   /**\\n    * Drop records already present in the dataset.\\n    *\\n[DEL] * @param jssc                  JavaSparkContext\\n[ADD] * @param jssc JavaSparkContext\\n    * @param incomingHoodieRecords HoodieRecords to deduplicate\\n[DEL] * @param writeConfig           HoodieWriteConfig\\n[ADD] * @param writeConfig HoodieWriteConfig\\n    */\\n   @SuppressWarnings(\"unchecked\")\\n   public static JavaRDD<HoodieRecord> dropDuplicates(JavaSparkContext jssc, JavaRDD<HoodieRecord> incomingHoodieRecords,\\n[DEL] HoodieWriteConfig writeConfig) {\\n[ADD] HoodieWriteConfig writeConfig) {\\n     try {\\n       HoodieReadClient client = new HoodieReadClient<>(jssc, writeConfig);\\n       return client.tagLocation(incomingHoodieRecords)',\n",
              " '    * @return {@link RDDCustomColumnsSortPartitioner} if sort columns are provided, otherwise empty.\\n    */\\n   protected Option<BulkInsertPartitioner<T>> getPartitioner(Map<String, String> strategyParams, Schema schema) {\\n[DEL] if (getWriteConfig().isLayoutOptimizationEnabled()) {\\n[DEL] // sort input records by z-order/hilbert\\n[DEL] return Option.of(new RDDSpatialCurveOptimizationSortPartitioner((HoodieSparkEngineContext) getEngineContext(),\\n[DEL] getWriteConfig(), HoodieAvroUtils.addMetadataFields(schema)));\\n[DEL] } else if (strategyParams.containsKey(PLAN_STRATEGY_SORT_COLUMNS.key())) {\\n[DEL] return Option.of(new RDDCustomColumnsSortPartitioner(strategyParams.get(PLAN_STRATEGY_SORT_COLUMNS.key()).split(\",\"),\\n[DEL] HoodieAvroUtils.addMetadataFields(schema), getWriteConfig().isConsistentLogicalTimestampEnabled()));\\n[DEL] } else {\\n[DEL] return Option.empty();\\n[DEL] }\\n[ADD] Option<String[]> orderByColumnsOpt =\\n[ADD] Option.ofNullable(strategyParams.get(PLAN_STRATEGY_SORT_COLUMNS.key()))\\n[ADD] .map(listStr -> listStr.split(\",\"));\\n[ADD] \\n[ADD] return orderByColumnsOpt.map(orderByColumns -> {\\n[ADD] HoodieClusteringConfig.LayoutOptimizationStrategy layoutOptStrategy = getWriteConfig().getLayoutOptimizationStrategy();\\n[ADD] switch (layoutOptStrategy) {\\n[ADD] case ZORDER:\\n[ADD] case HILBERT:\\n[ADD] return new RDDSpatialCurveSortPartitioner(\\n[ADD] (HoodieSparkEngineContext) getEngineContext(),\\n[ADD] orderByColumns,\\n[ADD] layoutOptStrategy,\\n[ADD] getWriteConfig().getLayoutOptimizationCurveBuildMethod(),\\n[ADD] HoodieAvroUtils.addMetadataFields(schema));\\n[ADD] case LINEAR:\\n[ADD] return new RDDCustomColumnsSortPartitioner(orderByColumns, HoodieAvroUtils.addMetadataFields(schema),\\n[ADD] getWriteConfig().isConsistentLogicalTimestampEnabled());\\n[ADD] default:\\n[ADD] throw new UnsupportedOperationException(String.format(\"Layout optimization strategy \\'%s\\' is not supported\", layoutOptStrategy));\\n[ADD] }\\n[ADD] });\\n   }\\n \\n   /**',\n",
              " '  */\\n package org.mule.extension.oauth2.internal.authorizationcode;\\n \\n[ADD] import static org.mule.extension.http.api.HttpHeaders.Names.AUTHORIZATION;\\n import static org.mule.runtime.api.i18n.I18nMessageFactory.createStaticMessage;\\n[ADD] import static org.mule.runtime.core.api.lifecycle.LifecycleUtils.initialiseIfNeeded;\\n \\n import org.mule.extension.oauth2.api.RequestAuthenticationException;\\n import org.mule.extension.oauth2.internal.AbstractGrantType;\\n import org.mule.extension.oauth2.internal.authorizationcode.state.ConfigOAuthContext;\\n import org.mule.extension.oauth2.internal.authorizationcode.state.ResourceOwnerOAuthContext;\\n import org.mule.extension.oauth2.internal.tokenmanager.TokenManagerConfig;\\n[DEL] import org.mule.runtime.api.tls.TlsContextFactory;\\n[DEL] import org.mule.runtime.core.api.MuleContext;\\n[DEL] import org.mule.runtime.core.api.Event;\\n[ADD] import org.mule.runtime.api.connection.ConnectionException;\\n import org.mule.runtime.api.exception.MuleException;\\n import org.mule.runtime.api.exception.MuleRuntimeException;\\n[DEL] import org.mule.runtime.core.api.context.MuleContextAware;\\n import org.mule.runtime.api.lifecycle.Initialisable;\\n import org.mule.runtime.api.lifecycle.InitialisationException;\\n import org.mule.runtime.api.lifecycle.Startable;\\n[ADD] import org.mule.runtime.api.lifecycle.Stoppable;\\n[ADD] import org.mule.runtime.api.tls.TlsContextFactory;\\n[ADD] import org.mule.runtime.core.api.DefaultMuleException;\\n[ADD] import org.mule.runtime.core.api.Event;\\n[ADD] import org.mule.runtime.core.api.MuleContext;\\n[ADD] import org.mule.runtime.core.api.context.MuleContextAware;\\n[ADD] import org.mule.runtime.core.api.registry.RegistrationException;\\n import org.mule.runtime.core.util.AttributeEvaluator;\\n[DEL] import org.mule.runtime.module.http.api.HttpHeaders;\\n import org.mule.runtime.module.http.api.listener.HttpListenerConfig;\\n[ADD] import org.mule.service.http.api.HttpService;\\n import org.mule.service.http.api.domain.message.request.HttpRequestBuilder;\\n[ADD] import org.mule.service.http.api.server.HttpServer;\\n[ADD] import org.mule.service.http.api.server.HttpServerConfiguration;\\n[ADD] \\n[ADD] import java.io.IOException;\\n[ADD] import java.net.MalformedURLException;\\n[ADD] import java.net.URL;\\n \\n import org.apache.commons.lang.StringUtils;\\n[ADD] import org.slf4j.Logger;\\n[ADD] import org.slf4j.LoggerFactory;\\n \\n /**\\n  * Represents the config element for oauth:authentication-code-config.',\n",
              " ' /*\\n  * AriaLiveStatusWidget.java\\n  *\\n[DEL] * Copyright (C) 2019 by RStudio, Inc.\\n[ADD] * Copyright (C) 2019-20 by RStudio, Inc.\\n  *\\n  * Unless you have received this program directly from RStudio pursuant\\n  * to the terms of a commercial license agreement with RStudio, then',\n",
              " '     exception.expectMessage(\"MyCustomValidationEventHandler failure mesage\");\\n     try (Reader<WrongTrainType> reader = source.createReader(null)) {\\n \\n[DEL] List<WrongTrainType> results = new ArrayList<>();\\n       for (boolean available = reader.start(); available; available = reader.advance()) {\\n[DEL] WrongTrainType train = reader.getCurrent();\\n[DEL] results.add(train);\\n[ADD] reader.getCurrent();\\n       }\\n     }\\n   }',\n",
              " ' \\n       final Expression expression = list.append(list.newName(\"current\"), input);\\n \\n[DEL] FieldType fromType = schema.getField(index).getType();\\n[DEL] Class convertTo = null;\\n[DEL] if (storageType == Object.class) {\\n[DEL] convertTo = Object.class;\\n[DEL] } else if (fromType.getTypeName().isLogicalType()) {\\n[DEL] convertTo = LOGICAL_TYPE_TO_BASE_TYPE_MAP.get(fromType.getLogicalType().getIdentifier());\\n[DEL] } else {\\n[DEL] convertTo = TYPE_CONVERSION_MAP.get(fromType.getTypeName());\\n[DEL] }\\n[DEL] if (convertTo == null) {\\n[DEL] throw new UnsupportedOperationException(\"Unable to get \" + fromType.getTypeName());\\n[ADD] FieldType fieldType = schema.getField(index).getType();\\n[ADD] Expression value;\\n[ADD] switch (fieldType.getTypeName()) {\\n[ADD] case BYTE:\\n[ADD] value = Expressions.call(expression, \"getByte\", Expressions.constant(index));\\n[ADD] break;\\n[ADD] case INT16:\\n[ADD] value = Expressions.call(expression, \"getInt16\", Expressions.constant(index));\\n[ADD] break;\\n[ADD] case INT32:\\n[ADD] value = Expressions.call(expression, \"getInt32\", Expressions.constant(index));\\n[ADD] break;\\n[ADD] case INT64:\\n[ADD] value = Expressions.call(expression, \"getInt64\", Expressions.constant(index));\\n[ADD] break;\\n[ADD] case DECIMAL:\\n[ADD] value = Expressions.call(expression, \"getDecimal\", Expressions.constant(index));\\n[ADD] break;\\n[ADD] case FLOAT:\\n[ADD] value = Expressions.call(expression, \"getFloat\", Expressions.constant(index));\\n[ADD] break;\\n[ADD] case DOUBLE:\\n[ADD] value = Expressions.call(expression, \"getDouble\", Expressions.constant(index));\\n[ADD] break;\\n[ADD] case STRING:\\n[ADD] value = Expressions.call(expression, \"getString\", Expressions.constant(index));\\n[ADD] break;\\n[ADD] case DATETIME:\\n[ADD] value = Expressions.call(expression, \"getDateTime\", Expressions.constant(index));\\n[ADD] break;\\n[ADD] case BOOLEAN:\\n[ADD] value = Expressions.call(expression, \"getBoolean\", Expressions.constant(index));\\n[ADD] break;\\n[ADD] case BYTES:\\n[ADD] value = Expressions.call(expression, \"getBytes\", Expressions.constant(index));\\n[ADD] break;\\n[ADD] case ARRAY:\\n[ADD] value = Expressions.call(expression, \"getArray\", Expressions.constant(index));\\n[ADD] if (storageType == Object.class\\n[ADD] && TypeName.ROW.equals(fieldType.getCollectionElementType().getTypeName())) {\\n[ADD] // Workaround for missing row output support\\n[ADD] return Expressions.convert_(value, Object.class);\\n[ADD] }\\n[ADD] break;\\n[ADD] case MAP:\\n[ADD] value = Expressions.call(expression, \"getMap\", Expressions.constant(index));\\n[ADD] break;\\n[ADD] case ROW:\\n[ADD] value = Expressions.call(expression, \"getRow\", Expressions.constant(index));\\n[ADD] break;\\n[ADD] case LOGICAL_TYPE:\\n[ADD] String identifier = fieldType.getLogicalType().getIdentifier();\\n[ADD] if (CharType.IDENTIFIER.equals(identifier)) {\\n[ADD] value = Expressions.call(expression, \"getString\", Expressions.constant(index));\\n[ADD] } else if (TimeWithLocalTzType.IDENTIFIER.equals(identifier)) {\\n[ADD] value = Expressions.call(expression, \"getDateTime\", Expressions.constant(index));\\n[ADD] } else if (SqlTypes.DATE.getIdentifier().equals(identifier)) {\\n[ADD] value =\\n[ADD] Expressions.convert_(\\n[ADD] Expressions.call(\\n[ADD] expression,\\n[ADD] \"getLogicalTypeValue\",\\n[ADD] Expressions.constant(index),\\n[ADD] Expressions.constant(LocalDate.class)),\\n[ADD] LocalDate.class);\\n[ADD] } else if (SqlTypes.TIME.getIdentifier().equals(identifier)) {\\n[ADD] value =\\n[ADD] Expressions.convert_(\\n[ADD] Expressions.call(\\n[ADD] expression,\\n[ADD] \"getLogicalTypeValue\",\\n[ADD] Expressions.constant(index),\\n[ADD] Expressions.constant(LocalTime.class)),\\n[ADD] LocalTime.class);\\n[ADD] } else if (SqlTypes.DATETIME.getIdentifier().equals(identifier)) {\\n[ADD] value =\\n[ADD] Expressions.convert_(\\n[ADD] Expressions.call(\\n[ADD] expression,\\n[ADD] \"getLogicalTypeValue\",\\n[ADD] Expressions.constant(index),\\n[ADD] Expressions.constant(LocalDateTime.class)),\\n[ADD] LocalDateTime.class);\\n[ADD] } else {\\n[ADD] throw new UnsupportedOperationException(\"Unable to get logical type \" + identifier);\\n[ADD] }\\n[ADD] break;\\n[ADD] default:\\n[ADD] throw new UnsupportedOperationException(\"Unable to get \" + fieldType.getTypeName());\\n       }\\n \\n[DEL] Expression value =\\n[DEL] Expressions.convert_(\\n[DEL] Expressions.call(\\n[DEL] expression,\\n[DEL] \"getBaseValue\",\\n[DEL] Expressions.constant(index),\\n[DEL] Expressions.constant(convertTo)),\\n[DEL] convertTo);\\n[DEL] return (storageType != Object.class) ? value(value, fromType) : value;\\n[ADD] return value(value, fieldType);\\n     }\\n \\n[DEL] private static Expression value(Expression value, Schema.FieldType type) {\\n[DEL] if (type.getTypeName().isLogicalType()) {\\n[DEL] String logicalId = type.getLogicalType().getIdentifier();\\n[DEL] if (SqlTypes.TIME.getIdentifier().equals(logicalId)) {\\n[ADD] private static Expression value(Expression value, FieldType fieldType) {\\n[ADD] switch (fieldType.getTypeName()) {\\n[ADD] case BYTE:\\n[ADD] return Expressions.convert_(value, Byte.class);\\n[ADD] case INT16:\\n[ADD] return Expressions.convert_(value, Short.class);\\n[ADD] case INT32:\\n[ADD] return Expressions.convert_(value, Integer.class);\\n[ADD] case INT64:\\n[ADD] return Expressions.convert_(value, Long.class);\\n[ADD] case DECIMAL:\\n[ADD] return Expressions.convert_(value, BigDecimal.class);\\n[ADD] case FLOAT:\\n[ADD] return Expressions.convert_(value, Float.class);\\n[ADD] case DOUBLE:\\n[ADD] return Expressions.convert_(value, Double.class);\\n[ADD] case STRING:\\n[ADD] return Expressions.convert_(value, String.class);\\n[ADD] case BOOLEAN:\\n[ADD] return Expressions.convert_(value, Boolean.class);\\n[ADD] case DATETIME:\\n           return nullOr(\\n[DEL] value, Expressions.divide(value, Expressions.constant(NANOS_PER_MILLISECOND)));\\n[DEL] } else if (SqlTypes.DATE.getIdentifier().equals(logicalId)) {\\n[DEL] return value;\\n[DEL] } else if (SqlTypes.DATETIME.getIdentifier().equals(logicalId)) {\\n[DEL] Expression dateValue =\\n[DEL] Expressions.call(value, \"getInt64\", Expressions.constant(DateTime.DATE_FIELD_NAME));\\n[DEL] Expression timeValue =\\n[DEL] Expressions.call(value, \"getInt64\", Expressions.constant(DateTime.TIME_FIELD_NAME));\\n[DEL] Expression returnValue =\\n[DEL] Expressions.add(\\n[DEL] Expressions.multiply(dateValue, Expressions.constant(MILLIS_PER_DAY)),\\n[DEL] Expressions.divide(timeValue, Expressions.constant(NANOS_PER_MILLISECOND)));\\n[DEL] return nullOr(value, returnValue);\\n[DEL] } else if (!CharType.IDENTIFIER.equals(logicalId)) {\\n[DEL] throw new UnsupportedOperationException(\\n[DEL] \"Unknown LogicalType \" + type.getLogicalType().getIdentifier());\\n[DEL] }\\n[DEL] } else if (type.getTypeName().isMapType()) {\\n[DEL] return nullOr(value, map(value, type.getMapValueType()));\\n[DEL] } else if (CalciteUtils.isDateTimeType(type)) {\\n[DEL] return nullOr(value, Expressions.call(value, \"getMillis\"));\\n[DEL] } else if (type.getTypeName().isCompositeType()) {\\n[DEL] return nullOr(value, row(value, type.getRowSchema()));\\n[DEL] } else if (type.getTypeName().isCollectionType()) {\\n[DEL] return nullOr(value, list(value, type.getCollectionElementType()));\\n[DEL] } else if (type.getTypeName() == TypeName.BYTES) {\\n[DEL] return nullOr(\\n[DEL] value, Expressions.new_(ByteString.class, Types.castIfNecessary(byte[].class, value)));\\n[ADD] value, Expressions.call(Expressions.convert_(value, DateTime.class), \"getMillis\"));\\n[ADD] case BYTES:\\n[ADD] return nullOr(\\n[ADD] value, Expressions.new_(ByteString.class, Expressions.convert_(value, byte[].class)));\\n[ADD] case ARRAY:\\n[ADD] return nullOr(value, list(value, fieldType.getCollectionElementType()));\\n[ADD] case MAP:\\n[ADD] return nullOr(value, map(value, fieldType.getMapValueType()));\\n[ADD] case ROW:\\n[ADD] return nullOr(value, row(value, fieldType.getRowSchema()));\\n[ADD] case LOGICAL_TYPE:\\n[ADD] String identifier = fieldType.getLogicalType().getIdentifier();\\n[ADD] if (CharType.IDENTIFIER.equals(identifier)) {\\n[ADD] return Expressions.convert_(value, String.class);\\n[ADD] } else if (TimeWithLocalTzType.IDENTIFIER.equals(identifier)) {\\n[ADD] return nullOr(\\n[ADD] value, Expressions.call(Expressions.convert_(value, DateTime.class), \"getMillis\"));\\n[ADD] } else if (SqlTypes.DATE.getIdentifier().equals(identifier)) {\\n[ADD] return nullOr(\\n[ADD] value,\\n[ADD] Expressions.call(\\n[ADD] Expressions.box(\\n[ADD] Expressions.call(\\n[ADD] Expressions.convert_(value, LocalDate.class), \"toEpochDay\")),\\n[ADD] \"intValue\"));\\n[ADD] } else if (SqlTypes.TIME.getIdentifier().equals(identifier)) {\\n[ADD] return nullOr(\\n[ADD] value,\\n[ADD] Expressions.call(\\n[ADD] Expressions.box(\\n[ADD] Expressions.divide(\\n[ADD] Expressions.call(\\n[ADD] Expressions.convert_(value, LocalTime.class), \"toNanoOfDay\"),\\n[ADD] Expressions.constant(NANOS_PER_MILLISECOND))),\\n[ADD] \"intValue\"));\\n[ADD] } else if (SqlTypes.DATETIME.getIdentifier().equals(identifier)) {\\n[ADD] value = Expressions.convert_(value, LocalDateTime.class);\\n[ADD] Expression dateValue =\\n[ADD] Expressions.call(Expressions.call(value, \"toLocalDate\"), \"toEpochDay\");\\n[ADD] Expression timeValue =\\n[ADD] Expressions.call(Expressions.call(value, \"toLocalTime\"), \"toNanoOfDay\");\\n[ADD] Expression returnValue =\\n[ADD] Expressions.add(\\n[ADD] Expressions.multiply(dateValue, Expressions.constant(MILLIS_PER_DAY)),\\n[ADD] Expressions.divide(timeValue, Expressions.constant(NANOS_PER_MILLISECOND)));\\n[ADD] return nullOr(value, returnValue);\\n[ADD] } else {\\n[ADD] throw new UnsupportedOperationException(\"Unable to convert logical type \" + identifier);\\n[ADD] }\\n[ADD] default:\\n[ADD] throw new UnsupportedOperationException(\"Unable to convert \" + fieldType.getTypeName());\\n       }\\n[DEL] \\n[DEL] return value;\\n     }\\n \\n     private static Expression list(Expression input, FieldType elementType) {',\n",
              " ' \\n             ApplyUpdateCachedItems();\\n             if (IsHandleCreated && !ListViewHandleDestroyed) {\\n[DEL] Debug.Assert(listItemsArray == null, \"listItemsArray not null, even though handle created\");\\n[DEL] \\n                 NativeMethods.LVFINDINFO info = new NativeMethods.LVFINDINFO();\\n                 info.lParam = (IntPtr) item.ID;\\n                 info.flags = NativeMethods.LVFI_PARAM;',\n",
              " '   return HF_VERSION_CLSAG;\\n }\\n \\n[DEL] uint64_t calculate_fee(bool use_per_byte_fee, const cryptonote::transaction &tx, size_t blob_size, uint64_t base_fee, uint64_t fee_multiplier, uint64_t fee_quantization_mask)\\n[ADD] uint64_t calculate_fee(bool use_per_byte_fee, const cryptonote::transaction &tx, size_t blob_size, uint64_t base_fee, uint64_t fee_quantization_mask)\\n {\\n   if (use_per_byte_fee)\\n[DEL] return calculate_fee_from_weight(base_fee, cryptonote::get_transaction_weight(tx, blob_size), fee_multiplier, fee_quantization_mask);\\n[ADD] return calculate_fee_from_weight(base_fee, cryptonote::get_transaction_weight(tx, blob_size), fee_quantization_mask);\\n   else\\n[DEL] return calculate_fee(base_fee, blob_size, fee_multiplier);\\n[ADD] return calculate_fee(base_fee, blob_size);\\n }\\n \\n bool get_short_payment_id(crypto::hash8 &payment_id8, const tools::wallet2::pending_tx &ptx, hw::device &hwdev)',\n",
              " '     // Hadoop FileSystem\\n     fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\\n \\n[DEL] TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\\n[ADD] if (isMain) {\\n[ADD] TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\\n \\n[DEL] // writeClient\\n[DEL] writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg), true);\\n[ADD] // writeClient\\n[ADD] writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg), true);\\n \\n[DEL] // init table, create it if not exists.\\n[DEL] initTable();\\n[ADD] // init table, create it if not exists.\\n[ADD] initTable();\\n[ADD] \\n[ADD] // create instantGenerateTmpFolder\\n[ADD] createInstantGenerateTmpDir();\\n[ADD] }\\n   }\\n \\n   @Override\\n   public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\\n     super.prepareSnapshotPreBarrier(checkpointId);\\n[DEL] // check whether the last instant is completed, if not, wait 10s and then throws an exception\\n[DEL] if (!StringUtils.isNullOrEmpty(latestInstant)) {\\n[DEL] doCheck();\\n[DEL] // last instant completed, set it empty\\n[DEL] latestInstant = \"\";\\n[DEL] }\\n[ADD] int indexOfThisSubtask = getRuntimeContext().getIndexOfThisSubtask();\\n[ADD] String instantGenerateInfoFileName = String.format(\"%d_%d_%d\", indexOfThisSubtask, checkpointId, batchSize);\\n[ADD] Path path = new Path(INSTANT_GENERATE_FOLDER_NAME, instantGenerateInfoFileName);\\n[ADD] // mk generate file by each subtask\\n[ADD] fs.create(path, true);\\n[ADD] LOG.info(\"Subtask [{}] at checkpoint [{}] created generate file [{}]\", indexOfThisSubtask, checkpointId, instantGenerateInfoFileName);\\n[ADD] if (isMain) {\\n[ADD] boolean receivedDataInCurrentCP = checkReceivedData(checkpointId);\\n[ADD] // check whether the last instant is completed, if not, wait 10s and then throws an exception\\n[ADD] if (!StringUtils.isNullOrEmpty(latestInstant)) {\\n[ADD] doCheck();\\n[ADD] // last instant completed, set it empty\\n[ADD] latestInstant = \"\";\\n[ADD] }\\n \\n[DEL] // no data no new instant\\n[DEL] if (!bufferedRecords.isEmpty()) {\\n[DEL] latestInstant = startNewInstant(checkpointId);\\n[ADD] // no data no new instant\\n[ADD] if (receivedDataInCurrentCP) {\\n[ADD] latestInstant = startNewInstant(checkpointId);\\n[ADD] }\\n     }\\n   }\\n \\n   @Override\\n   public void initializeState(StateInitializationContext context) throws Exception {\\n[DEL] // instantState\\n[DEL] ListStateDescriptor<String> latestInstantStateDescriptor = new ListStateDescriptor<String>(\"latestInstant\", String.class);\\n[DEL] latestInstantState = context.getOperatorStateStore().getListState(latestInstantStateDescriptor);\\n[DEL] \\n[DEL] // recordState\\n[DEL] ListStateDescriptor<StreamRecord> recordsStateDescriptor = new ListStateDescriptor<StreamRecord>(\"recordsState\", StreamRecord.class);\\n[DEL] recordsState = context.getOperatorStateStore().getListState(recordsStateDescriptor);\\n[DEL] \\n[DEL] if (context.isRestored()) {\\n[DEL] Iterator<String> latestInstantIterator = latestInstantState.get().iterator();\\n[DEL] latestInstantIterator.forEachRemaining(x -> latestInstant = x);\\n[DEL] LOG.info(\"InstantGenerateOperator initializeState get latestInstant [{}]\", latestInstant);\\n[DEL] \\n[DEL] Iterator<StreamRecord> recordIterator = recordsState.get().iterator();\\n[DEL] bufferedRecords.clear();\\n[DEL] recordIterator.forEachRemaining(x -> bufferedRecords.add(x));\\n[ADD] isMain = getRuntimeContext().getIndexOfThisSubtask() == 0;\\n[ADD] if (isMain) {\\n[ADD] // instantState\\n[ADD] ListStateDescriptor<String> latestInstantStateDescriptor = new ListStateDescriptor<String>(\"latestInstant\", String.class);\\n[ADD] latestInstantState = context.getOperatorStateStore().getListState(latestInstantStateDescriptor);\\n[ADD] \\n[ADD] if (context.isRestored()) {\\n[ADD] Iterator<String> latestInstantIterator = latestInstantState.get().iterator();\\n[ADD] latestInstantIterator.forEachRemaining(x -> latestInstant = x);\\n[ADD] LOG.info(\"Restoring the latest instant [{}] from the state\", latestInstant);\\n[ADD] }\\n     }\\n   }\\n \\n   @Override\\n   public void snapshotState(StateSnapshotContext functionSnapshotContext) throws Exception {\\n[DEL] if (latestInstantList.isEmpty()) {\\n[DEL] latestInstantList.add(latestInstant);\\n[ADD] long checkpointId = functionSnapshotContext.getCheckpointId();\\n[ADD] if (isMain) {\\n[ADD] LOG.info(\"Update latest instant [{}] records size [{}] checkpointId [{}]\", latestInstant, batchSize, checkpointId);\\n[ADD] if (latestInstantList.isEmpty()) {\\n[ADD] latestInstantList.add(latestInstant);\\n[ADD] } else {\\n[ADD] latestInstantList.set(0, latestInstant);\\n[ADD] }\\n[ADD] latestInstantState.update(latestInstantList);\\n     } else {\\n[DEL] latestInstantList.set(0, latestInstant);\\n[ADD] LOG.info(\"Records size [{}] checkpointId [{}]\", batchSize, checkpointId);\\n     }\\n[DEL] latestInstantState.update(latestInstantList);\\n[DEL] LOG.info(\"Update latest instant [{}]\", latestInstant);\\n[DEL] \\n[DEL] recordsState.update(bufferedRecords);\\n[DEL] LOG.info(\"Update records state size = [{}]\", bufferedRecords.size());\\n[DEL] bufferedRecords.clear();\\n[ADD] batchSize = 0;\\n   }\\n \\n   /**',\n",
              " '         : 1;\\n \\n     assertException(exception -> {\\n[DEL] assertThat(exception, instanceOf(ConnectionException.class));\\n[ADD] assertErrorIsCausedByConnectionException(exception);\\n       try {\\n         verify(interceptor1, times(expectedRetries)).before(operationContext);\\n         verify(interceptor2, times(expectedRetries)).before(operationContext);',\n",
              " ' \\n \\tprivate final MethodHandleLookup methodHandleLookup = MethodHandleLookup.getMethodHandleLookup();\\n \\n[DEL] private final Map<Method, MethodHandle> methodHandleCache =\\n[DEL] new ConcurrentReferenceHashMap<>(10, ReferenceType.WEAK);\\n[ADD] private final Map<Method, MethodHandle> methodHandleCache = new ConcurrentReferenceHashMap<>(10, ReferenceType.WEAK);\\n \\n \\t@Override\\n \\tpublic Object invoke(MethodInvocation invocation) throws Throwable { // NOSONAR',\n",
              " '     } else {\\n       doc.getDocumentElement().setAttributeNS(\"http://www.w3.org/2000/xmlns/\",\\n                                               \"xmlns\", CORE_NAMESPACE);\\n[DEL] addNamespaceDeclarationIfNeeded(CORE_PREFIX, namespace, buildSchemaLocation(prefix, namespace));\\n       return doc.createElementNS(CORE_NAMESPACE, name);\\n     }\\n   }',\n",
              " ' import java.net.URISyntaxException;\\n import java.util.concurrent.Future;\\n \\n[ADD] import org.apache.commons.lang3.RandomStringUtils;\\n import org.apache.commons.lang3.StringUtils;\\n \\n import com.google.common.base.Joiner;',\n",
              " '   public static final String AUXILIARYFOLDER_NAME = METAFOLDER_NAME + Path.SEPARATOR + \".aux\";\\n   public static final String BOOTSTRAP_INDEX_ROOT_FOLDER_PATH = AUXILIARYFOLDER_NAME + Path.SEPARATOR + \".bootstrap\";\\n   public static final String HEARTBEAT_FOLDER_NAME = METAFOLDER_NAME + Path.SEPARATOR + \".heartbeat\";\\n[DEL] public static final String ZINDEX_NAME = \".zindex\";\\n[ADD] public static final String COLUMN_STATISTICS_INDEX_NAME = \".colstatsindex\";\\n   public static final String BOOTSTRAP_INDEX_BY_PARTITION_FOLDER_PATH = BOOTSTRAP_INDEX_ROOT_FOLDER_PATH\\n       + Path.SEPARATOR + \".partitions\";\\n   public static final String BOOTSTRAP_INDEX_BY_FILE_ID_FOLDER_PATH = BOOTSTRAP_INDEX_ROOT_FOLDER_PATH + Path.SEPARATOR',\n",
              " ' \\n import javax.annotation.Nullable;\\n \\n[DEL] import javax.annotation.Nullable;\\n[DEL] \\n import java.util.ArrayList;\\n import java.util.Collections;\\n import java.util.Deque;',\n",
              " '     }\\n \\n \\n[ADD] @Override\\n[ADD] protected void onPause() {\\n[ADD] super.onPause();\\n[ADD] getPreferenceScreen().getSharedPreferences().unregisterOnSharedPreferenceChangeListener(this);\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] protected void onResume() {\\n[ADD] super.onResume();\\n[ADD] getPreferenceScreen().getSharedPreferences().registerOnSharedPreferenceChangeListener(this);\\n[ADD] \\n[ADD] // syncAccount\\'s summary can change while preferences are still open (user logs\\n[ADD] // in from preferences screen), so we need to update it here.\\n[ADD] SharedPreferences preferences = AnkiDroidApp.getSharedPrefs(getBaseContext());\\n[ADD] String username = preferences.getString(\"username\", \"\");\\n[ADD] if (TextUtils.isEmpty(username)) {\\n[ADD] syncAccount.setSummary(R.string.sync_account_summ_logged_out);\\n[ADD] } else {\\n[ADD] syncAccount.setSummary(getString(R.string.sync_account_summ_logged_in, username));\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] \\n     private void updateListPreference(String key) {\\n         ListPreference listpref = (ListPreference) getPreferenceScreen().findPreference(key);\\n         String entry;',\n",
              " ' import org.apache.log4j.LogManager;\\n import org.apache.log4j.Logger;\\n \\n[ADD] import java.io.ByteArrayInputStream;\\n[ADD] import java.io.ByteArrayOutputStream;\\n import java.io.File;\\n import java.io.FileOutputStream;\\n import java.io.IOException;\\n[ADD] import java.io.InputStream;\\n import java.io.RandomAccessFile;\\n import java.io.Serializable;\\n import java.net.InetAddress;',\n",
              " '             });\\n       }\\n \\n[ADD] InstrumentationContext.internalSetContextStoreSupplier(\\n[ADD] (keyClass, contextClass) -> FieldBackedProvider.getContextStore(keyClass, contextClass));\\n     } else {\\n       logger.info(\"Tracing is disabled.\");\\n     }',\n",
              " '           }\\n         }\\n       }\\n[ADD] // notify discard log lines threads if log tab sniffer read buffer\\n[ADD] if ( Const.KETTLE_LOG_TAB_REFRESH_THREAD.equals( Thread.currentThread().getName() ) ) {\\n[ADD] isBufferReadBySpoonLogTabRefresher = true;\\n[ADD] buffer.notifyAll();\\n[ADD] }\\n     }\\n \\n     return lines;',\n",
              " '    * Factory for {@link ThrottledFileSystem}.\\n    */\\n   public static class Factory<S extends ScopeType<S>> extends FileSystemInstrumentationFactory<S> {\\n[ADD] private final String SERVICE_NAME_CONF_KEY = \"gobblin.broker.limiter.serviceName\";\\n     @Override\\n     public FileSystem instrumentFileSystem(FileSystem fs, SharedResourcesBroker<S> broker,\\n         ConfigView<S, FileSystemKey> config) {\\n       try {\\n[DEL] Limiter limiter =\\n[DEL] broker.getSharedResource(new SharedLimiterFactory<S>(), new FileSystemLimiterKey(config.getKey().getUri()));\\n[DEL] return new ThrottledFileSystem(fs, limiter);\\n[ADD] String serviceName = config.getConfig().getString(SERVICE_NAME_CONF_KEY);\\n[ADD] Limiter limiter;\\n[ADD] if (serviceName == null)\\n[ADD] limiter = broker.getSharedResource(new SharedLimiterFactory<S>(), new FileSystemLimiterKey(config.getKey().getUri()));\\n[ADD] else\\n[ADD] limiter = broker.getSharedResource(new SharedLimiterFactory<S>(), new FileSystemLimiterKey(config.getKey().getUri(), serviceName));\\n[ADD] return new ThrottledFileSystem(fs, limiter, serviceName);\\n       } catch (NotConfiguredException nce) {\\n         throw new RuntimeException(nce);\\n       }',\n",
              " ' import org.apache.hudi.avro.model.HoodieCompactionPlan;\\n import org.apache.hudi.cli.HoodieCLI;\\n import org.apache.hudi.cli.HoodiePrintHelper;\\n[ADD] import org.apache.hudi.cli.HoodieTableHeaderFields;\\n import org.apache.hudi.cli.TableHeader;\\n import org.apache.hudi.cli.commands.SparkMain.SparkCommand;\\n import org.apache.hudi.cli.utils.CommitUtil;',\n",
              " ' import org.springframework.integration.endpoint.SourcePollingChannelAdapter;\\n import org.springframework.integration.handler.AbstractMessageProducingHandler;\\n import org.springframework.integration.handler.AbstractReplyProducingMessageHandler;\\n[ADD] import org.springframework.integration.handler.LambdaMessageProcessor;\\n[ADD] import org.springframework.integration.handler.MessageProcessor;\\n import org.springframework.integration.handler.ReplyProducingMessageHandlerWrapper;\\n import org.springframework.integration.handler.advice.HandleMessageAdvice;\\n import org.springframework.integration.router.AbstractMessageRouter;\\n import org.springframework.integration.scheduling.PollerMetadata;\\n[ADD] import org.springframework.integration.util.ClassUtils;\\n import org.springframework.integration.util.MessagingAnnotationUtils;\\n import org.springframework.lang.Nullable;\\n import org.springframework.messaging.MessageChannel;',\n",
              " '           long beginLsTs = System.currentTimeMillis();\\n           FileStatus[] statuses = listPartition(partitionPath);\\n           long endLsTs = System.currentTimeMillis();\\n[DEL] LOG.info(\"#files found in partition (\" + partitionPathStr + \") =\" + statuses.length + \", Time taken =\"\\n[ADD] LOG.debug(\"#files found in partition (\" + partitionPathStr + \") =\" + statuses.length + \", Time taken =\"\\n               + (endLsTs - beginLsTs));\\n           List<HoodieFileGroup> groups = addFilesToView(statuses);\\n ',\n",
              " ' import java.util.Collection;\\n \\n import org.apache.commons.lang3.StringUtils;\\n[ADD] import org.apache.gobblin.runtime.api.FlowSpec;\\n import org.apache.hadoop.conf.Configuration;\\n import org.apache.hadoop.fs.FSDataInputStream;\\n import org.apache.hadoop.fs.FSDataOutputStream;',\n",
              " '    */\\n   List<String> getDeltaLogPaths();\\n \\n[ADD] List<FileStatus> getDeltaLogFileStatus();\\n[ADD] \\n   /**\\n    * Return Max Instant Time.\\n    * @return',\n",
              " '     }\\n }\\n \\n[DEL] \\n[ADD] @SuppressLint(\"FieldNamingPatternDetector\")\\n @SuppressWarnings({\"PMD.AvoidReassigningParameters\",\"PMD.DefaultPackage\",\\n         \"PMD.NPathComplexity\",\"PMD.MethodNamingConventions\",\"PMD.ExcessiveMethodLength\",\\n         \"PMD.EmptyIfStmt\",\"PMD.CollapsibleIfStatements\"})',\n",
              " ' import org.apache.hadoop.hive.ql.metadata.Table;\\n import org.apache.hadoop.mapred.InputFormat;\\n import org.apache.hadoop.mapred.InvalidInputException;\\n[ADD] import org.apache.hadoop.security.UserGroupInformation;\\n import org.apache.thrift.TException;\\n \\n import com.google.common.annotations.VisibleForTesting;',\n",
              " ' \\tpublic void test_isVarargsCollector_Using_asVarargsCollector_asTyped_arg_int_to_byte() throws Throwable {\\n \\t\\tMethodHandle mh = MethodHandles.lookup().findStatic(PackageExamples.class,\"addPublicStaticVariableArity\",MethodType.methodType(int.class, int[].class));\\n \\t\\t\\n[DEL] AssertJUnit.assertTrue(mh.isVarargsCollector());\\n[ADD] Assert.assertTrue(mh.isVarargsCollector());\\n \\t\\t\\n \\t\\tmh = mh.asType(MethodType.methodType(int.class,byte.class));\\n \\t\\t\\n[DEL] AssertJUnit.assertFalse(mh.isVarargsCollector());\\n[ADD] Assert.assertFalse(mh.isVarargsCollector());\\n \\t}\\n \\t\\n \\t/**',\n",
              " ' \\t\\tif (this.count == 0) {\\n \\t\\t\\treturn 0;\\n \\t\\t}\\n[DEL] double t0 = lastTime();\\n[DEL] return (System.nanoTime() / this.factor - t0);\\n[ADD] double currentT0 = lastTime();\\n[ADD] return (System.nanoTime() / this.factor - currentT0);\\n \\t}\\n \\n \\t/**',\n",
              " '   public static Field[] filterFields(Class<?> containingClass, Field[] fields) {\\n     if (fields.length == 0\\n         || !VirtualFieldInstalledMarker.class.isAssignableFrom(containingClass)) {\\n[ADD] // nothing to filter when class does not have any added virtual fields\\n       return fields;\\n     }\\n     List<Field> result = new ArrayList<>(fields.length);',\n",
              " ' import static java.util.Collections.singleton;\\n import static java.util.Optional.empty;\\n import static java.util.Optional.of;\\n[ADD] import static java.util.stream.Collectors.toSet;\\n[ADD] import static org.apache.commons.collections.CollectionUtils.intersection;\\n import static org.apache.commons.lang3.StringUtils.isNotEmpty;\\n import static org.mule.runtime.api.dsl.DslResolvingContext.getDefault;\\n[ADD] import static org.mule.runtime.api.i18n.I18nMessageFactory.createStaticMessage;\\n import static org.mule.runtime.core.api.el.ExpressionManager.DEFAULT_EXPRESSION_POSTFIX;\\n import static org.mule.runtime.core.api.el.ExpressionManager.DEFAULT_EXPRESSION_PREFIX;\\n import static org.mule.runtime.core.api.transaction.TransactionConfig.ACTION_ALWAYS_BEGIN;',\n",
              " ' \\t\\t\\t}\\n \\t\\t}\\n \\n[ADD] @Override\\n[ADD] public void handleTransportError(StompSession session, Throwable exception) {\\n[ADD] logger.error(\"STOMP transport error for session: [\" + session + \"]\", exception);\\n[ADD] }\\n[ADD] \\n \\t}\\n \\n }',\n",
              " ' import org.apache.dubbo.rpc.cluster.router.state.BitList;\\n import org.apache.dubbo.rpc.cluster.router.state.StateRouter;\\n import org.apache.dubbo.rpc.cluster.router.state.StateRouterFactory;\\n[DEL] import org.apache.dubbo.rpc.cluster.router.state.StateRouterResult;\\n[ADD] import org.apache.dubbo.rpc.cluster.router.state.TailStateRouter;\\n import org.apache.dubbo.rpc.model.ModuleModel;\\n \\n import java.util.ArrayList;',\n",
              " '             mAllDeckIds.add(thisDid);\\n         }\\n \\n[DEL] ArrayAdapter<String> noteDeckAdapter = new ArrayAdapter<>(this, android.R.layout.simple_spinner_item, deckNames);\\n[DEL] mNoteDeckSpinner.setAdapter(noteDeckAdapter);\\n[DEL] noteDeckAdapter.setDropDownViewResource(android.R.layout.simple_spinner_dropdown_item);\\n[DEL] mNoteDeckSpinner.setOnItemSelectedListener(new OnItemSelectedListener() {\\n[ADD] ArrayAdapter<String> noteDeckAdapter = new ArrayAdapter<String>(this, android.R.layout.simple_spinner_item, deckNames) {\\n             @Override\\n[DEL] public void onItemSelected(AdapterView<?> parent, View view, int pos, long id) {\\n[DEL] // Timber.i(\"NoteEditor:: onItemSelected() fired on mNoteDeckSpinner with pos = %d\", pos);\\n[DEL] mCurrentDid = mAllDeckIds.get(pos);\\n[DEL] }\\n[ADD] public View getDropDownView(int position, View convertView, ViewGroup parent){\\n \\n[DEL] @Override\\n[DEL] public void onNothingSelected(AdapterView<?> parent) {\\n[DEL] // Do Nothing\\n[ADD] mCurrentDid = mAllDeckIds.get(position);\\n[ADD] \\n[ADD] // Cast the drop down items (popup items) as text view\\n[ADD] TextView tv = (TextView) super.getDropDownView(position, convertView, parent);\\n[ADD] \\n[ADD] // If this item is selected\\n[ADD] if (position == mNoteDeckSpinner.getSelectedItemPosition()) {\\n[ADD] tv.setBackgroundColor(Color.LTGRAY);\\n[ADD] tv.setTextColor(Color.BLACK);\\n[ADD] }\\n[ADD] \\n[ADD] // Return the modified view\\n[ADD] return tv;\\n             }\\n[DEL] });\\n[ADD] };\\n[ADD] mNoteDeckSpinner.setAdapter(noteDeckAdapter);\\n[ADD] noteDeckAdapter.setDropDownViewResource(android.R.layout.simple_spinner_dropdown_item);\\n \\n         mCurrentDid = intent.getLongExtra(EXTRA_DID, mCurrentDid);\\n ',\n",
              " \" \\t\\treturn s1len - s2len;\\n \\t}\\n \\n[DEL] private int compareValue(int codepoint) {\\n[ADD] private static int compareValue(int codepoint) {\\n \\t\\tif ('A' <= codepoint && codepoint <= 'Z') {\\n \\t\\t\\treturn codepoint + ('a' - 'A');\\n \\t\\t}\",\n",
              " '   private long duration;\\n \\n   // Needed for serialization/deserialization\\n[DEL] public TaskState() {\\n[DEL] }\\n[ADD] public TaskState() {}\\n \\n   public TaskState(WorkUnitState workUnitState) {\\n     // Since getWorkunit() returns an immutable WorkUnit object,\\n     // the WorkUnit object in this object is also immutable.\\n[DEL] super(workUnitState.getWorkunit());\\n[ADD] super(workUnitState.getWorkunit(), workUnitState.getJobState());\\n     addAll(workUnitState);\\n     this.jobId = workUnitState.getProp(ConfigurationKeys.JOB_ID_KEY);\\n     this.taskId = workUnitState.getProp(ConfigurationKeys.TASK_ID_KEY);',\n",
              " ' \\n   @Override\\n   public final void initialise() throws InitialisationException {\\n[DEL] processorChain = new DefaultMessageProcessorChainBuilder().chain(this.processors).build();\\n[ADD] DefaultMessageProcessorChainBuilder chainBuilder = new DefaultMessageProcessorChainBuilder().chain(processors);\\n[ADD] \\n[ADD] List<Processor> beforeNext = new ArrayList<>();\\n[ADD] List<Processor> afterNext = new ArrayList<>();\\n[ADD] \\n[ADD] boolean seenNext = false;\\n[ADD] for (Processor processor : processors) {\\n[ADD] if (processor instanceof PolicyNextActionMessageProcessor) {\\n[ADD] seenNext = true;\\n[ADD] continue;\\n[ADD] }\\n[ADD] \\n[ADD] if (seenNext) {\\n[ADD] afterNext.add(processor);\\n[ADD] } else {\\n[ADD] beforeNext.add(processor);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] processingStrategy = new SourcePolicyProcessingStrategy(schedulerService, muleContext.getSchedulerBaseConfig(),\\n[ADD] getLocation().getLocation(), beforeNext, afterNext);\\n[ADD] \\n[ADD] chainBuilder.setProcessingStrategy(processingStrategy);\\n[ADD] processorChain = chainBuilder.build();\\n[ADD] chainWithPs = processingStrategy.onPipeline(processorChain);\\n     initialiseIfNeeded(processorChain, muleContext);\\n \\n     notificationHelper =',\n",
              " '     conf.setInteger(FlinkOptions.COMPACTION_DELTA_COMMITS, config.compactionDeltaCommits);\\n     conf.setInteger(FlinkOptions.COMPACTION_DELTA_SECONDS, config.compactionDeltaSeconds);\\n     conf.setInteger(FlinkOptions.COMPACTION_MAX_MEMORY, config.compactionMaxMemory);\\n[ADD] conf.setLong(FlinkOptions.COMPACTION_TARGET_IO, config.compactionTargetIo);\\n[ADD] conf.setInteger(FlinkOptions.COMPACTION_TASKS, config.compactionTasks);\\n     conf.setBoolean(FlinkOptions.CLEAN_ASYNC_ENABLED, config.cleanAsyncEnable);\\n     // use synchronous compaction always\\n     conf.setBoolean(FlinkOptions.COMPACTION_ASYNC_ENABLED, false);',\n",
              " ' from llnl.util.tty.color import colorize\\n from spack.filesystem_view import YamlFilesystemView\\n from spack.util.executable import which\\n[DEL] from spack.stage import Stage, ResourceStage, StageComposite\\n[ADD] from spack.stage import _stage_prefix, Stage, ResourceStage, StageComposite\\n from spack.util.environment import dump_environment\\n from spack.util.package_hash import package_hash\\n from spack.version import Version',\n",
              " '         final DefaultDomainManager domainManager = new DefaultDomainManager();\\n         domainManager.addDomain(createDefaultDomain());\\n \\n[DEL] TestApplicationFactory appFactory = createTestApplicationFactory(new MuleApplicationClassLoaderFactory(new DefaultNativeLibraryFinderFactory()), domainManager);\\n[ADD] TestApplicationFactory appFactory = createTestApplicationFactory(new MuleApplicationClassLoaderFactory(new DefaultNativeLibraryFinderFactory()), domainManager, mock(ServiceRepository.class, RETURNS_DEEP_STUBS));\\n         appFactory.setFailOnStopApplication(true);\\n \\n         deploymentService.setAppFactory(appFactory);\\n[DEL] deploymentService.start();\\n[ADD] startDeployment();\\n \\n         assertApplicationDeploymentSuccess(applicationDeploymentListener, emptyAppFileBuilder.getId());\\n         Application app = findApp(emptyAppFileBuilder.getId(), 1);',\n",
              " '             if (result) {\\n                 final String message = String.format(\"Transitioned host HA state from:%s to:%s due to event:%s for the host id:%d\",\\n                         currentHAState, nextState, event, haConfig.getResourceId());\\n[DEL] if (LOG.isTraceEnabled()) {\\n[DEL] LOG.trace(message);\\n[DEL] }\\n[ADD] LOG.debug(message);\\n[ADD] \\n                 if (nextState == HAConfig.HAState.Recovering || nextState == HAConfig.HAState.Fencing || nextState == HAConfig.HAState.Fenced) {\\n                     ActionEventUtils.onActionEvent(CallContext.current().getCallingUserId(), CallContext.current().getCallingAccountId(),\\n                             Domain.ROOT_DOMAIN, EventTypes.EVENT_HA_STATE_TRANSITION, message);',\n",
              " '     return recordsWithLocation.filter(v1 -> !v1.isCurrentLocationKnown());\\n   }\\n \\n[ADD] /**\\n[ADD] * Main API to run bootstrap to hudi.\\n[ADD] */\\n[ADD] public void bootstrap(Option<Map<String, String>> extraMetadata) {\\n[ADD] if (rollbackPending) {\\n[ADD] rollBackPendingBootstrap();\\n[ADD] }\\n[ADD] HoodieTable<T> table = getTableAndInitCtx(WriteOperationType.UPSERT);\\n[ADD] table.bootstrap(jsc, extraMetadata);\\n[ADD] }\\n[ADD] \\n[ADD] /**\\n[ADD] * Main API to rollback pending bootstrap.\\n[ADD] */\\n[ADD] protected void rollBackPendingBootstrap() {\\n[ADD] LOG.info(\"Rolling back pending bootstrap if present\");\\n[ADD] HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\\n[ADD] HoodieTimeline inflightTimeline = table.getMetaClient().getCommitsTimeline().filterPendingExcludingCompaction();\\n[ADD] Option<String> instant = Option.fromJavaOptional(\\n[ADD] inflightTimeline.getReverseOrderedInstants().map(HoodieInstant::getTimestamp).findFirst());\\n[ADD] if (instant.isPresent() && HoodieTimeline.compareTimestamps(instant.get(), HoodieTimeline.LESSER_THAN_OR_EQUALS,\\n[ADD] HoodieTimeline.FULL_BOOTSTRAP_INSTANT_TS)) {\\n[ADD] LOG.info(\"Found pending bootstrap instants. Rolling them back\");\\n[ADD] table.rollbackBootstrap(jsc, HoodieActiveTimeline.createNewInstantTime());\\n[ADD] LOG.info(\"Finished rolling back pending bootstrap\");\\n[ADD] }\\n[ADD] \\n[ADD] }\\n[ADD] \\n   /**\\n    * Upsert a batch of new records into Hoodie table at the supplied instantTime.\\n    *',\n",
              " '     /** Reference from a class to one of its interfaces */\\n     int REFERENCE_INTERFACE         = 7;\\n     /** Reference from a class to the value of one of its static fields */\\n[DEL] int REFERENCE_STATIC_FIELD      = 8;\\n[ADD] int REFERENCE_STATIC_FIELD      = 8;\\n     /** Reference from a class to a resolved entry in the constant pool */\\n     int REFERENCE_CONSTANT_POOL     = 9;\\n     /** Reference from a class to its superclass */',\n",
              " ' \\t */\\n \\t@Test(groups = { \"level.extended\" })\\n \\tpublic void testLookup_StaticInnerClassLookup_CrossPackage() throws Throwable {\\n[DEL] Lookup lookup = PackageExamples.getLookup();\\n[DEL] Lookup inObj = lookup.in( SamePackageInnerClass_Static.class );\\n[DEL] assertClassAndMode( inObj, SamePackageInnerClass_Static.class, NO_ACCESS );\\n[ADD] Lookup inObj = packageExamplesLookup.in(SamePackageInnerClass_Static.class);\\n[ADD] assertClassAndMode(inObj, SamePackageInnerClass_Static.class, NO_ACCESS);\\n \\t}\\n \\t\\n \\t/***************************************************',\n",
              " ' \\n package com.uber.hoodie.common.util;\\n \\n[ADD] import com.esotericsoftware.kryo.Kryo;\\n[ADD] import com.esotericsoftware.kryo.io.Input;\\n[ADD] import com.esotericsoftware.kryo.io.Output;\\n[ADD] import com.twitter.chill.EmptyScalaKryoInstantiator;\\n import com.uber.hoodie.exception.HoodieSerializationException;\\n[DEL] import java.io.ByteArrayInputStream;\\n import java.io.ByteArrayOutputStream;\\n import java.io.IOException;\\n[DEL] import java.io.InputStream;\\n[DEL] import java.io.ObjectInputStream;\\n[DEL] import java.io.ObjectOutputStream;\\n[DEL] import java.io.OutputStream;\\n import java.io.Serializable;\\n \\n[ADD] \\n /**\\n[DEL] * (NOTE: Adapted from Apache commons-lang3)\\n[DEL] * This class defines API\\'s to serde an object.\\n[ADD] * {@link SerializationUtils} class internally uses {@link Kryo} serializer for serializing /\\n[ADD] * deserializing objects.\\n  */\\n public class SerializationUtils {\\n[DEL] // Serialize\\n[DEL] //-----------------------------------------------------------------------\\n \\n[DEL] /**\\n[DEL] * <p>Serializes an {@code Object} to the specified stream.</p>\\n[DEL] *\\n[DEL] * <p>The stream will be closed once the object is written.\\n[DEL] * This avoids the need for a finally clause, and maybe also exception\\n[DEL] * handling, in the application code.</p>\\n[DEL] *\\n[DEL] * <p>The stream passed in is not buffered internally within this method.\\n[DEL] * This is the responsibility of your application if desired.</p>\\n[DEL] *\\n[DEL] * @param obj the object to serialize to bytes, may be null\\n[DEL] * @param outputStream the stream to write to, must not be null\\n[DEL] * @throws IllegalArgumentException if {@code outputStream} is {@code null}\\n[DEL] * @throws HoodieSerializationException (runtime) if the serialization fails\\n[DEL] */\\n[DEL] public static void serialize(final Serializable obj, final OutputStream outputStream) {\\n[DEL] if (outputStream == null) {\\n[DEL] throw new IllegalArgumentException(\"The OutputStream must not be null\");\\n[DEL] }\\n[DEL] ObjectOutputStream out = null;\\n[DEL] try {\\n[DEL] // stream closed in the finally\\n[DEL] out = new ObjectOutputStream(outputStream);\\n[DEL] out.writeObject(obj);\\n[ADD] // Caching kryo serializer to avoid creating kryo instance for every serde operation\\n[ADD] private static final ThreadLocal<KryoSerializerInstance> serializerRef =\\n[ADD] ThreadLocal.withInitial(() -> new KryoSerializerInstance());\\n \\n[DEL] } catch (final IOException ex) {\\n[DEL] throw new HoodieSerializationException(\"unable to serialize object\", ex);\\n[DEL] } finally {\\n[DEL] try {\\n[DEL] if (out != null) {\\n[DEL] out.close();\\n[DEL] }\\n[DEL] } catch (final IOException ex) { // NOPMD\\n[DEL] // ignore close exception\\n[DEL] }\\n[DEL] }\\n[DEL] }\\n[ADD] // Serialize\\n[ADD] //-----------------------------------------------------------------------\\n \\n   /**\\n[DEL] * <p>Serializes an {@code Object} to a byte array for\\n[DEL] * storage/serialization.</p>\\n[ADD] * <p>Serializes an {@code Object} to a byte array for storage/serialization.</p>\\n    *\\n    * @param obj the object to serialize to bytes\\n    * @return a byte[] with the converted Serializable\\n[DEL] * @throws HoodieSerializationException (runtime) if the serialization fails\\n[ADD] * @throws IOException if the serialization fails\\n    */\\n[DEL] public static byte[] serialize(final Serializable obj) {\\n[DEL] final ByteArrayOutputStream baos = new ByteArrayOutputStream(512);\\n[DEL] serialize(obj, baos);\\n[DEL] return baos.toByteArray();\\n[ADD] public static byte[] serialize(final Object obj) throws IOException {\\n[ADD] return serializerRef.get().serialize(obj);\\n   }\\n \\n   // Deserialize\\n   //-----------------------------------------------------------------------\\n \\n   /**\\n[DEL] * <p>\\n[DEL] * Deserializes an {@code Object} from the specified stream.\\n[DEL] * </p>\\n[ADD] * <p> Deserializes a single {@code Object} from an array of bytes. </p>\\n    *\\n[DEL] * <p>\\n[DEL] * The stream will be closed once the object is written. This avoids the need for a finally clause, and maybe also\\n[DEL] * exception handling, in the application code.\\n[DEL] * </p>\\n[DEL] *\\n[DEL] * <p>\\n[DEL] * The stream passed in is not buffered internally within this method. This is the responsibility of your\\n[DEL] * application if desired.\\n[DEL] * </p>\\n[DEL] *\\n[DEL] * <p>\\n[DEL] * If the call site incorrectly types the return value, a {@link ClassCastException} is thrown from the call site.\\n[DEL] * Without Generics in this declaration, the call site must type cast and can cause the same ClassCastException.\\n[DEL] * Note that in both cases, the ClassCastException is in the call site, not in this method.\\n[DEL] * </p>\\n[DEL] *\\n[DEL] * @param <T> the object type to be deserialized\\n[DEL] * @param inputStream the serialized object input stream, must not be null\\n[DEL] * @return the deserialized object\\n[DEL] * @throws IllegalArgumentException if {@code inputStream} is {@code null}\\n[DEL] * @throws HoodieSerializationException (runtime) if the serialization fails\\n[DEL] */\\n[DEL] public static <T> T deserialize(final InputStream inputStream) {\\n[DEL] if (inputStream == null) {\\n[DEL] throw new IllegalArgumentException(\"The InputStream must not be null\");\\n[DEL] }\\n[DEL] ObjectInputStream in = null;\\n[DEL] try {\\n[DEL] // stream closed in the finally\\n[DEL] in = new ObjectInputStream(inputStream);\\n[DEL] @SuppressWarnings(\"unchecked\") // may fail with CCE if serialised form is incorrect\\n[DEL] final T obj = (T) in.readObject();\\n[DEL] return obj;\\n[DEL] \\n[DEL] } catch (final ClassCastException ex) {\\n[DEL] throw new HoodieSerializationException(\"cannot cast class\", ex);\\n[DEL] } catch (final ClassNotFoundException ex) {\\n[DEL] throw new HoodieSerializationException(\"class not found\", ex);\\n[DEL] } catch (final IOException ex) {\\n[DEL] throw new HoodieSerializationException(\"unable to deserialize to object\", ex);\\n[DEL] } finally {\\n[DEL] try {\\n[DEL] if (in != null) {\\n[DEL] in.close();\\n[DEL] }\\n[DEL] } catch (final IOException ex) { // NOPMD\\n[DEL] // ignore close exception\\n[DEL] }\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] /**\\n[DEL] * <p>\\n[DEL] * Deserializes a single {@code Object} from an array of bytes.\\n[DEL] * </p>\\n[DEL] *\\n[DEL] * <p>\\n[DEL] * If the call site incorrectly types the return value, a {@link ClassCastException} is thrown from the call site.\\n[DEL] * Without Generics in this declaration, the call site must type cast and can cause the same ClassCastException.\\n[DEL] * Note that in both cases, the ClassCastException is in the call site, not in this method.\\n[DEL] * </p>\\n[ADD] * <p> If the call site incorrectly types the return value, a {@link ClassCastException} is thrown\\n[ADD] * from the call site. Without Generics in this declaration, the call site must type cast and can\\n[ADD] * cause the same ClassCastException. Note that in both cases, the ClassCastException is in the\\n[ADD] * call site, not in this method. </p>\\n    *\\n    * @param <T> the object type to be deserialized\\n    * @param objectData the serialized object, must not be null',\n",
              " '               new ValueWithRecordIdKeySelector<T>())\\n               .transform(\"debuping\", outputTypeInfo, new DedupingOperator<T>());\\n         } else {\\n[DEL] source = nonDedupSource.flatMap(new StripIdsMap<T>());\\n[ADD] source = nonDedupSource.flatMap(new StripIdsMap<T>()).returns(outputTypeInfo);\\n         }\\n       } catch (Exception e) {\\n         throw new RuntimeException(',\n",
              " ' import com.ichi2.async.TaskManager;\\n import com.ichi2.compat.CompatHelper;\\n import com.ichi2.libanki.Decks;\\n[DEL] import com.ichi2.libanki.Model;\\n[ADD] import com.ichi2.libanki.SoundOrVideoTag;\\n import com.ichi2.libanki.sched.AbstractSched;\\n import com.ichi2.libanki.Card;\\n import com.ichi2.libanki.Collection;',\n",
              " '  */\\n public class JobWorkerPool implements GracefulShutdownHook {\\n     public interface Factory {\\n[DEL] JobWorkerPool create(String name, int poolSize);\\n[ADD] JobWorkerPool create(String name, int poolSize, Runnable shutdownCallback);\\n     }\\n \\n     private static final Logger LOG = LoggerFactory.getLogger(JobWorkerPool.class);',\n",
              " '    * @return The loaded {@link Config}.\\n    * @throws IOException\\n    */\\n[DEL] public Config loadPullFile(Path path, Config sysProps, boolean loadGlobalProperties, boolean resolve) throws IOException {\\n[ADD] public Config loadPullFile(Path path, Config sysProps, boolean loadGlobalProperties, boolean resolve)\\n[ADD] throws IOException {\\n     Config fallback = loadGlobalProperties ? loadAncestorGlobalConfigs(path, sysProps) : sysProps;\\n     Config loadedConfig;\\n[DEL] \\n     if (this.javaPropsPullFileFilter.accept(path)) {\\n       loadedConfig = loadJavaPropsWithFallback(path, fallback);\\n     } else if (this.hoconPullFileFilter.accept(path)) {',\n",
              " '                     .build());\\n             createdLookupCaches.put(dto.id(), dto);\\n         }\\n[ADD] \\n[ADD] if (!createdLookupCaches.isEmpty()) {\\n[ADD] clusterBus.post(CachesUpdated.create(createdLookupCaches.keySet()));\\n[ADD] }\\n[ADD] \\n[ADD] final Collection<LookupCache> caches = lookupTableService.getCaches(createdLookupCaches.keySet());\\n[ADD] final CountDownLatch latch = new CountDownLatch(caches.size());\\n[ADD] caches.forEach(c -> c.addListener(new LatchUpdaterListener(latch), scheduler));\\n[ADD] \\n[ADD] latch.await(30, TimeUnit.SECONDS);\\n     }\\n \\n[DEL] private void createLookupDataAdapters(String bundleId, Set<LookupDataAdapterBundle> lookupDataAdapters) {\\n[ADD] private void createLookupDataAdapters(String bundleId, Set<LookupDataAdapterBundle> lookupDataAdapters) throws InterruptedException {\\n         for (LookupDataAdapterBundle bundle : lookupDataAdapters) {\\n             final DataAdapterDto dto = dbDataAdapterService.save(DataAdapterDto.builder()\\n                     .title(bundle.getTitle())',\n",
              " '         final Domain domain = findADomain(dummyDomainDescriptor.id, 1);\\n         assertNotNull(domain);\\n         assertNotNull(domain.getMuleContext());\\n[ADD] assertDomainAnchorFileExists(dummyDomainDescriptor.id);\\n     }\\n \\n     @Test',\n",
              " ' package org.ray.runtime.object;\\n \\n import java.util.Arrays;\\n[ADD] \\n[ADD] import org.apache.commons.lang3.tuple.Pair;\\n import org.ray.api.exception.RayActorException;\\n import org.ray.api.exception.RayTaskException;\\n import org.ray.api.exception.RayWorkerException;\\n import org.ray.api.exception.UnreconstructableException;\\n import org.ray.api.id.ObjectId;\\n import org.ray.runtime.generated.Gcs.ErrorType;\\n[DEL] import org.ray.runtime.util.Serializer;\\n[ADD] import org.ray.runtime.serializer.Serializer;\\n[ADD] import org.ray.runtime.serializer.Serializer.Meta;\\n \\n /**\\n  * Serialize to and deserialize from {@link NativeRayObject}. Metadata is generated during',\n",
              " ' import android.content.Context;\\n import android.content.Intent;\\n import android.content.SharedPreferences;\\n[ADD] import android.graphics.Bitmap;\\n[ADD] import android.graphics.Canvas;\\n[ADD] import android.graphics.Color;\\n import android.content.pm.PackageManager;\\n import android.graphics.drawable.Drawable;\\n import android.os.Build;',\n",
              " ' \\n   @Test\\n   @Category(NeedsRunner.class)\\n[DEL] public void testCustomSink() throws Exception {\\n[ADD] public void testCustomWrite() throws Exception {\\n     FakeBigQueryServices fakeBqServices = new FakeBigQueryServices()\\n         .withJobService(new FakeJobService()\\n             .startJobReturns(\"done\", \"done\", \"done\")\\n[DEL] .pollJobReturns(Status.FAILED, Status.FAILED, Status.SUCCEEDED))\\n[DEL] .withDatasetService(mockDatasetService);\\n[ADD] .pollJobReturns(Status.FAILED, Status.FAILED, Status.SUCCEEDED));\\n \\n     Pipeline p = TestPipeline.create(bqOptions);\\n     p.apply(Create.of(',\n",
              " ' \\n   @Override\\n   public CoreEvent process(CoreEvent event) {\\n[ADD] evaluateCorrectArguments();\\n[ADD] \\n     String result = muleContext.getExpressionManager().parseLogTemplate(content, event, getLocation(), NULL_BINDING_CONTEXT);\\n     Message.Builder messageBuilder = Message.builder(event.getMessage()).value(result).nullAttributesValue();\\n     MediaType configuredMediaType = buildMediaType();',\n",
              " ' \\t\\t\\treturn this;\\n \\t\\t}\\n \\n[DEL] if (StrHWAvailable() && language == \"en\") { //$NON-NLS-1$\\n[DEL] String output = new String(lengthInternal());\\n[DEL] if (toUpperHWOptimized(output))\\n[DEL] return output;\\n[ADD] if (helpers.supportsIntrinsicCaseConversion() && language == \"en\") { //$NON-NLS-1$\\n[ADD] int sLength = lengthInternal();\\n[ADD] \\n[ADD] if (enableCompression && (null == compressionFlag || count >= 0)) {\\n[ADD] char[] output = new char[(sLength + 1) / 2];\\n[ADD] if (helpers.toUpperIntrinsicLatin1(value, output, sLength)){\\n[ADD] return new String(output, 0, sLength, true);\\n[ADD] }\\n[ADD] } else {\\n[ADD] char[] output = new char[sLength];\\n[ADD] if (helpers.toUpperIntrinsicUTF16(value, output, sLength * 2)){\\n[ADD] return new String(output, 0, sLength, false);\\n[ADD] }\\n[ADD] }\\n \\t\\t}\\n \\n \\t\\treturn toUpperCaseCore(language);',\n",
              " ' \\n     public static String KEY_WRITE_TIMESTAMP = \"WRITE_TIMESTAMP\";\\n \\n[ADD] // The whole dubbo service use only one hashedWheelTimer for heartbeat task and reconnect task\\n[ADD] private static HashedWheelTimer hashedWheelTimer = new HashedWheelTimer(new NamedThreadFactory(\"dubbo-heartbeat\", true),\\n[ADD] 1000,\\n[ADD] TimeUnit.MILLISECONDS,\\n[ADD] Constants.TICKS_PER_WHEEL);\\n[ADD] \\n     public HeartbeatHandler(ChannelHandler handler) {\\n         super(handler);\\n     }',\n",
              " '             {\\n                 internetHeaders.addHeader(CONTENT_DISPOSITION, getContentDisposition(part));\\n             }\\n[ADD] \\n[ADD] if (contentTypeSubType.equals(RELATED) && part.getName() != null)\\n[ADD] {\\n[ADD] internetHeaders.addHeader(CONTENT_ID, part.getName());\\n[ADD] }\\n[ADD] \\n             if (internetHeaders.getHeader(CONTENT_TYPE) == null && part.getContentType() != null)\\n             {\\n                 internetHeaders.addHeader(CONTENT_TYPE, part.getContentType());',\n",
              " '   }\\n \\n   public Span startSpan(Statement statement, String query) {\\n[ADD] return startSpan(statement, normalizeAndExtractInfo(query));\\n[ADD] }\\n[ADD] \\n[ADD] public Span startSpan(Statement statement, SqlStatementInfo queryInfo) {\\n     Connection connection = connectionFromStatement(statement);\\n     if (connection == null) {\\n       return null;',\n",
              " '         Map<String, String> keyMap = methodParameters.get(method);\\n         String value = null;\\n         if (keyMap != null) {\\n[DEL] value =  keyMap.get(key);\\n[ADD] value = keyMap.get(key);\\n         }\\n         if (StringUtils.isEmpty(value)) {\\n             value = parameters.get(key);',\n",
              " ' \\t\\tVersion: version.Version,\\n \\t}, nil\\n }\\n[ADD] \\n[ADD] // validateVersion checks that python is running a valid version. If a version\\n[ADD] // is invalid, it prints to os.Stderr. This is interpreted as diagnostic message\\n[ADD] // by the Pulumi CLI program.\\n[ADD] func validateVersion(virtualEnvPath string) {\\n[ADD] var versionCmd *exec.Cmd\\n[ADD] var err error\\n[ADD] versionArgs := []string{\"--version\"}\\n[ADD] if virtualEnvPath == \"\" {\\n[ADD] versionCmd = python.VirtualEnvCommand(virtualEnvPath, \"python\", versionArgs...)\\n[ADD] } else if versionCmd, err = python.Command(versionArgs...); err != nil {\\n[ADD] fmt.Fprintf(os.Stderr, \"Failed to find python executable\\\\n\")\\n[ADD] return\\n[ADD] }\\n[ADD] var out []byte\\n[ADD] if out, err = versionCmd.Output(); err != nil {\\n[ADD] fmt.Fprintf(os.Stderr, \"Failed to resolve python version command: %s\\\\n\", err.Error())\\n[ADD] return\\n[ADD] }\\n[ADD] version := strings.TrimSpace(strings.TrimPrefix(string(out), \"Python \"))\\n[ADD] parsed, err := semver.Parse(version)\\n[ADD] if err != nil {\\n[ADD] fmt.Fprintf(os.Stderr, \"Failed to parse python version: \\'%s\\'\\\\n\", version)\\n[ADD] return\\n[ADD] }\\n[ADD] if parsed.LT(mimimumPythonVersion) {\\n[ADD] fmt.Fprintf(os.Stderr, \"Python %d.%d is approaching EOL and will not be supported in Pulumi soon.\"+\\n[ADD] \" Check %s for more details\\\\n\", mimimumPythonVersion.Major,\\n[ADD] mimimumPythonVersion.Minor, minimumPythonVersionIssue)\\n[ADD] }\\n[ADD] }',\n",
              " '  */\\n package org.mule.runtime.core.internal.el;\\n \\n[ADD] import static java.util.Collections.singletonMap;\\n import static org.hamcrest.Matchers.hasItem;\\n import static org.hamcrest.Matchers.hasItems;\\n import static org.hamcrest.Matchers.hasSize;',\n",
              " ' \\n package org.apache.gobblin.converter;\\n \\n[DEL] import com.google.common.base.Optional;\\n[DEL] import com.google.common.cache.CacheBuilder;\\n[DEL] import com.google.common.cache.CacheLoader;\\n[DEL] import com.google.common.cache.LoadingCache;\\n[DEL] import org.apache.gobblin.configuration.ConfigurationKeys;\\n[DEL] import org.apache.gobblin.configuration.WorkUnitState;\\n[DEL] import org.apache.gobblin.converter.filter.AvroProjectionConverter;\\n[DEL] import org.apache.gobblin.converter.filter.AvroSchemaFieldRemover;\\n[DEL] import org.apache.gobblin.metrics.kafka.KafkaSchemaRegistry;\\n[DEL] import org.apache.gobblin.metrics.kafka.KafkaSchemaRegistryFactory;\\n[DEL] import org.apache.gobblin.metrics.kafka.SchemaRegistryException;\\n[DEL] import org.apache.gobblin.util.AvroUtils;\\n[DEL] import java.io.IOException;\\n import java.nio.ByteBuffer;\\n import java.nio.charset.StandardCharsets;\\n[DEL] import java.util.concurrent.ExecutionException;\\n[DEL] import javax.xml.bind.DatatypeConverter;\\n[ADD] \\n import org.apache.avro.Schema;\\n import org.apache.avro.generic.GenericDatumReader;\\n import org.apache.avro.generic.GenericRecord;\\n import org.apache.avro.io.Decoder;\\n import org.apache.avro.io.DecoderFactory;\\n[ADD] import org.apache.gobblin.configuration.WorkUnitState;\\n[ADD] import org.apache.gobblin.metrics.kafka.KafkaSchemaRegistry;\\n[ADD] import org.apache.gobblin.metrics.kafka.KafkaSchemaRegistryFactory;\\n[ADD] import org.apache.gobblin.util.AvroUtils;\\n[ADD] \\n[ADD] import com.google.common.base.Optional;\\n[ADD] import com.google.common.cache.CacheBuilder;\\n[ADD] import com.google.common.cache.CacheLoader;\\n[ADD] import com.google.common.cache.LoadingCache;\\n[ADD] \\n[ADD] import javax.xml.bind.DatatypeConverter;\\n[ADD] \\n \\n /**\\n[DEL] * A converter for extracting schema/records from an envelope schema.\\n[DEL] * Input schema: envelope schema - must have fields payloadSchemaId (the schema registry key of the output\\n[DEL] *               schema) and payload (byte data for output record)\\n[DEL] * Input record: record corresponding to input schema\\n[DEL] * Output schema: schema obtained from schema registry using key provided in input record\\'s {@link #PAYLOAD_SCHEMA_ID_FIELD}\\n[DEL] * Output record: record corresponding to output schema obtained from input record\\'s {@link #PAYLOAD_FIELD} as bytes\\n[ADD] * Base class for an envelope schema converter using {@link KafkaSchemaRegistry}\\n  */\\n[DEL] public class EnvelopeSchemaConverter extends Converter<Schema, String, GenericRecord, GenericRecord> {\\n[ADD] public abstract class EnvelopeSchemaConverter<P> extends Converter<Schema, Schema, GenericRecord, GenericRecord> {\\n[ADD] public static final String PAYLOAD_SCHEMA_ID_FIELD = \"converter.envelopeSchemaConverter.schemaIdField\";\\n[ADD] public static final String PAYLOAD_FIELD = \"converter.envelopeSchemaConverter.payloadField\";\\n[ADD] public static final String PAYLOAD_SCHEMA_TOPIC = \"converter.envelopeSchemaConverter.payloadSchemaTopic\";\\n[ADD] public static final String KAFKA_REGISTRY_FACTORY = \"converter.envelopeSchemaConverter.kafkaRegistryFactory\";\\n \\n[DEL] public static final String PAYLOAD_SCHEMA_ID_FIELD = \"EnvelopeSchemaConverter.schemaIdField\";\\n[DEL] public static final String PAYLOAD_FIELD = \"EnvelopeSchemaConverter.payloadField\";\\n[DEL] public static final String DEFAULT_PAYLOAD_SCHEMA_ID_FIELD =\"payloadSchemaId\";\\n   public static final String DEFAULT_PAYLOAD_FIELD = \"payload\";\\n[DEL] public static final String DEFAULT_KAFKA_SCHEMA_REGISTRY_FACTORY_CLASS = \"org.apache.gobblin.metrics.kafka.KafkaAvroSchemaRegistryFactory\";\\n[ADD] public static final String DEFAULT_PAYLOAD_SCHEMA_ID_FIELD = \"payloadSchemaId\";\\n[ADD] public static final String DEFAULT_KAFKA_SCHEMA_REGISTRY_FACTORY_CLASS =\\n[ADD] \"org.apache.gobblin.metrics.kafka.KafkaAvroSchemaRegistryFactory\";\\n \\n[DEL] private Optional<AvroSchemaFieldRemover> fieldRemover;\\n[DEL] private KafkaSchemaRegistry registry;\\n[DEL] private DecoderFactory decoderFactory;\\n[DEL] private LoadingCache<Schema, GenericDatumReader<GenericRecord>> readers;\\n[ADD] protected String payloadSchemaIdField;\\n[ADD] protected String payloadField;\\n[ADD] protected String payloadSchemaTopic;\\n[ADD] protected GenericDatumReader<P> latestPayloadReader;\\n[ADD] protected KafkaSchemaRegistry registry;\\n[ADD] \\n[ADD] /** Cache the payload schema with schema id as its cache key */\\n[ADD] protected LoadingCache<String, Schema> schemaCache;\\n \\n[DEL] /**\\n[DEL] * To remove certain fields from the Avro schema or records of a topic/table, set property\\n[DEL] * {topic/table name}.remove.fields={comma-separated, fully qualified field names} in workUnit.\\n[DEL] */\\n   @Override\\n   public EnvelopeSchemaConverter init(WorkUnitState workUnit) {\\n[DEL] if (workUnit.contains(ConfigurationKeys.EXTRACT_TABLE_NAME_KEY)) {\\n[DEL] String removeFieldsPropName = workUnit.getProp(ConfigurationKeys.EXTRACT_TABLE_NAME_KEY) + AvroProjectionConverter.REMOVE_FIELDS;\\n[DEL] if (workUnit.contains(removeFieldsPropName)) {\\n[DEL] this.fieldRemover = Optional.of(new AvroSchemaFieldRemover(workUnit.getProp(removeFieldsPropName)));\\n[DEL] } else {\\n[DEL] this.fieldRemover = Optional.absent();\\n[DEL] }\\n[ADD] super.init(workUnit);\\n[ADD] \\n[ADD] payloadSchemaIdField = workUnit.getProp(PAYLOAD_SCHEMA_ID_FIELD, DEFAULT_PAYLOAD_SCHEMA_ID_FIELD);\\n[ADD] payloadField = workUnit.getProp(PAYLOAD_FIELD, DEFAULT_PAYLOAD_FIELD);\\n[ADD] \\n[ADD] // Get the schema specific topic to fetch the schema in the registry\\n[ADD] if (!workUnit.contains(PAYLOAD_SCHEMA_TOPIC)) {\\n[ADD] throw new RuntimeException(\"Configuration not found: \" + PAYLOAD_SCHEMA_TOPIC);\\n     }\\n[DEL] String registryFactoryField = workUnit.contains(KafkaSchemaRegistryFactory.KAFKA_SCHEMA_REGISTRY_FACTORY_CLASS) ?\\n[DEL] workUnit.getProp(KafkaSchemaRegistryFactory.KAFKA_SCHEMA_REGISTRY_FACTORY_CLASS) : DEFAULT_KAFKA_SCHEMA_REGISTRY_FACTORY_CLASS;\\n[ADD] payloadSchemaTopic = workUnit.getProp(PAYLOAD_SCHEMA_TOPIC);\\n[ADD] \\n[ADD] String registryFactoryField = workUnit.getProp(KAFKA_REGISTRY_FACTORY, DEFAULT_KAFKA_SCHEMA_REGISTRY_FACTORY_CLASS);\\n     try {\\n[DEL] KafkaSchemaRegistryFactory registryFactory = ((Class<? extends KafkaSchemaRegistryFactory>) Class.forName(registryFactoryField)).newInstance();\\n[DEL] this.registry = registryFactory.create(workUnit.getProperties());\\n[ADD] KafkaSchemaRegistryFactory registryFactory =\\n[ADD] ((Class<? extends KafkaSchemaRegistryFactory>) Class.forName(registryFactoryField)).newInstance();\\n[ADD] registry = registryFactory.create(workUnit.getProperties());\\n     } catch (ClassNotFoundException | IllegalAccessException | InstantiationException e) {\\n[DEL] return null;\\n[ADD] throw new RuntimeException(e);\\n     }\\n[DEL] this.decoderFactory = DecoderFactory.get();\\n[DEL] this.readers = CacheBuilder.newBuilder().build(new CacheLoader<Schema, GenericDatumReader<GenericRecord>>() {\\n[ADD] \\n[ADD] schemaCache = CacheBuilder.newBuilder().build(new CacheLoader<String, Schema>() {\\n       @Override\\n[DEL] public GenericDatumReader<GenericRecord> load(final Schema key) throws Exception {\\n[DEL] return new GenericDatumReader<>(key);\\n[ADD] public Schema load(String key)\\n[ADD] throws Exception {\\n[ADD] return (Schema) registry.getSchemaByKey(key);\\n       }\\n     });\\n     return this;\\n   }\\n \\n   /**\\n[DEL] * Do nothing, actual schema must be obtained from records.\\n[DEL] */\\n[DEL] @Override\\n[DEL] public String convertSchema(Schema inputSchema, WorkUnitState workUnit) throws SchemaConversionException {\\n[DEL] return EnvelopeSchemaConverter.class.getName();\\n[DEL] }\\n[DEL] \\n[DEL] /**\\n[DEL] * Get actual schema from registry and deserialize payload using it.\\n[ADD] * Get the payload schema\\n[ADD] *\\n[ADD] * @param inputRecord the input record which has the payload\\n[ADD] * @return the current schema of the payload\\n    */\\n[DEL] @Override\\n[DEL] public Iterable<GenericRecord> convertRecord(String outputSchema, GenericRecord inputRecord, WorkUnitState workUnit)\\n[DEL] throws DataConversionException {\\n[DEL] try {\\n[DEL] String schemaIdField = workUnit.contains(PAYLOAD_SCHEMA_ID_FIELD) ?\\n[DEL] workUnit.getProp(PAYLOAD_SCHEMA_ID_FIELD) : DEFAULT_PAYLOAD_SCHEMA_ID_FIELD;\\n[DEL] String payloadField = workUnit.contains(PAYLOAD_FIELD) ?\\n[DEL] workUnit.getProp(PAYLOAD_FIELD) : DEFAULT_PAYLOAD_FIELD;\\n[DEL] String schemaKey = String.valueOf(inputRecord.get(schemaIdField));\\n[DEL] Schema payloadSchema = (Schema) this.registry.getSchemaByKey(schemaKey);\\n[DEL] byte[] payload = getPayload(inputRecord, payloadField);\\n[DEL] GenericRecord outputRecord = deserializePayload(payload, payloadSchema);\\n[DEL] if (this.fieldRemover.isPresent()) {\\n[DEL] payloadSchema = this.fieldRemover.get().removeFields(payloadSchema);\\n[DEL] }\\n[DEL] return new SingleRecordIterable<>(AvroUtils.convertRecordSchema(outputRecord, payloadSchema));\\n[DEL] } catch (IOException | SchemaRegistryException | ExecutionException e) {\\n[DEL] throw new DataConversionException(e);\\n[ADD] protected Schema getPayloadSchema(GenericRecord inputRecord)\\n[ADD] throws Exception {\\n[ADD] Optional<Object> schemaIdValue = AvroUtils.getFieldValue(inputRecord, payloadSchemaIdField);\\n[ADD] if (!schemaIdValue.isPresent()) {\\n[ADD] throw new Exception(\"Schema id with key \" + payloadSchemaIdField + \" not found in the record\");\\n     }\\n[ADD] String schemaKey = String.valueOf(schemaIdValue.get());\\n[ADD] return schemaCache.get(schemaKey);\\n   }\\n \\n   /**\\n[DEL] * Get payload field from GenericRecord and convert to byte array\\n[ADD] * Get payload field and convert to byte array\\n[ADD] *\\n[ADD] * @param inputRecord the input record which has the payload\\n[ADD] * @return the byte array of the payload in the input record\\n    */\\n[DEL] public byte[] getPayload(GenericRecord inputRecord, String payloadFieldName) {\\n[DEL] ByteBuffer bb = (ByteBuffer) inputRecord.get(payloadFieldName);\\n[DEL] byte[] payloadBytes;\\n[ADD] protected byte[] getPayloadBytes(GenericRecord inputRecord) {\\n[ADD] ByteBuffer bb = (ByteBuffer) inputRecord.get(payloadField);\\n     if (bb.hasArray()) {\\n[DEL] payloadBytes = bb.array();\\n[ADD] return bb.array();\\n     } else {\\n[DEL] payloadBytes = new byte[bb.remaining()];\\n[ADD] byte[] payloadBytes = new byte[bb.remaining()];\\n       bb.get(payloadBytes);\\n[ADD] String hexString = new String(payloadBytes, StandardCharsets.UTF_8);\\n[ADD] return DatatypeConverter.parseHexBinary(hexString);\\n     }\\n[DEL] String hexString = new String(payloadBytes, StandardCharsets.UTF_8);\\n[DEL] return DatatypeConverter.parseHexBinary(hexString);\\n[ADD] }\\n[ADD] \\n[ADD] protected Schema fetchLatestPayloadSchema() throws Exception {\\n[ADD] Schema latestPayloadSchema = (Schema)registry.getLatestSchemaByTopic(payloadSchemaTopic);\\n[ADD] latestPayloadReader = new GenericDatumReader<>(latestPayloadSchema);\\n[ADD] return latestPayloadSchema;\\n   }\\n \\n   /**\\n[DEL] * Deserialize payload using payload schema\\n[ADD] * Convert the payload in the input record to a deserialized object with the latest schema\\n[ADD] *\\n[ADD] * @param inputRecord the input record\\n[ADD] * @return the schema\\'ed payload object\\n    */\\n[DEL] public GenericRecord deserializePayload(byte[] payload, Schema payloadSchema) throws IOException, ExecutionException {\\n[DEL] Decoder decoder = this.decoderFactory.binaryDecoder(payload, null);\\n[DEL] GenericDatumReader<GenericRecord> reader = this.readers.get(payloadSchema);\\n[DEL] return reader.read(null, decoder);\\n[ADD] protected P upConvertPayload(GenericRecord inputRecord) throws DataConversionException {\\n[ADD] try {\\n[ADD] Schema payloadSchema = getPayloadSchema(inputRecord);\\n[ADD] // Set writer schema\\n[ADD] latestPayloadReader.setSchema(payloadSchema);\\n[ADD] \\n[ADD] byte[] payloadBytes = getPayloadBytes(inputRecord);\\n[ADD] Decoder decoder = DecoderFactory.get().binaryDecoder(payloadBytes, null);\\n[ADD] \\n[ADD] // \\'latestPayloadReader.read\\' will convert the record from \\'payloadSchema\\' to the latest payload schema\\n[ADD] return latestPayloadReader.read(null, decoder);\\n[ADD] } catch (Exception e) {\\n[ADD] throw new DataConversionException(e);\\n[ADD] }\\n   }\\n }',\n",
              " '             } catch (JSONException e) {\\n                 throw new RuntimeException(e);\\n             }\\n[DEL] mDecks.load(cursor.getString(7), cursor.getString(8));\\n[DEL] mTags.load(cursor.getString(9));\\n[ADD] deckConf = cursor.getString(7);\\n[ADD] mTags.load(cursor.getString(8));\\n         } finally {\\n             if (cursor != null) {\\n                 cursor.close();\\n             }\\n         }\\n[DEL] loadModels();\\n[ADD] mModels.load(loadColumn(\"models\"));\\n[ADD] mDecks.load(loadColumn(\"decks\"), deckConf);\\n     }\\n \\n[DEL] public void loadModels() {\\n[ADD] public String loadColumn(String columnName) {\\n         int pos = 1;\\n         int chunk = 256*1024;\\n         String buf = \"\";',\n",
              " '         assertNotSame(mp3.event, mp2.resultEvent);\\n         assertEquals(mp2.resultEvent.getMessage().getPayload(), mp3.event.getMessage().getPayload());\\n         assertEquals(mp3.event.getMessage().getPayload(), \"012\");\\n[ADD] \\n[ADD] assertEquals(isMultipleThreadsUsed() ? 4 : 1, threads);\\n     }\\n \\n     @Test\\n     public void testMPChainWithNullReturnAtEnd() throws MuleException, Exception\\n     {\\n         DefaultMessageProcessorChainBuilder builder = new DefaultMessageProcessorChainBuilder();\\n[DEL] builder.chain(new AppendingMP(\"1\"), new AppendingMP(\"2\"), new AppendingMP(\"3\"), new ReturnNullMP());\\n[DEL] assertNull(builder.build().process(getTestEventUsingFlow(\"0\")));\\n[ADD] builder.chain(getAppendingMP(\"1\"), getAppendingMP(\"2\"), getAppendingMP(\"3\"), new ReturnNullMP());\\n[ADD] assertNull(process(builder.build(), getTestEventUsingFlow(\"0\")));\\n[ADD] \\n[ADD] assertEquals(isMultipleThreadsUsed() ? 4 : 1, threads);\\n     }\\n \\n     @Test\\n     public void testMPChainWithVoidReturnAtEnd() throws MuleException, Exception\\n     {\\n         DefaultMessageProcessorChainBuilder builder = new DefaultMessageProcessorChainBuilder();\\n[DEL] builder.chain(new AppendingMP(\"1\"), new AppendingMP(\"2\"), new AppendingMP(\"3\"), new ReturnVoidMP());\\n[DEL] assertEquals(\"0123\", builder.build().process(getTestEventUsingFlow(\"0\")).getMessage().getPayload());\\n[ADD] builder.chain(getAppendingMP(\"1\"), getAppendingMP(\"2\"), getAppendingMP(\"3\"), new ReturnVoidMP());\\n[ADD] assertEquals(\"0123\", process(builder.build(), getTestEventUsingFlow(\"0\")).getMessage().getPayload());\\n[ADD] \\n[ADD] assertEquals(isMultipleThreadsUsed() ? 4 : 1, threads);\\n     }\\n \\n     @Test\\n     public void testMPChainWithBuilder() throws MuleException, Exception\\n     {\\n         DefaultMessageProcessorChainBuilder builder = new DefaultMessageProcessorChainBuilder();\\n[DEL] builder.chain(new AppendingMP(\"1\"));\\n[ADD] builder.chain(getAppendingMP(\"1\"));\\n         builder.chain(new MessageProcessorBuilder()\\n         {\\n             public MessageProcessor build()\\n             {\\n[DEL] return new AppendingMP(\"2\");\\n[ADD] return getAppendingMP(\"2\");\\n             }\\n         });\\n[DEL] builder.chain(new AppendingMP(\"3\"));\\n[DEL] assertEquals(\"0123\", builder.build().process(getTestEventUsingFlow(\"0\")).getMessage().getPayload());\\n[ADD] builder.chain(getAppendingMP(\"3\"));\\n[ADD] assertEquals(\"0123\", process(builder.build(), getTestEventUsingFlow(\"0\")).getMessage().getPayload());\\n[ADD] \\n[ADD] assertEquals(isMultipleThreadsUsed() ? 4 : 1, threads);\\n     }\\n \\n     @Test',\n",
              " ' \\n @Priority(Priorities.AUTHENTICATION)\\n public class ShiroSecurityContextFilter implements ContainerRequestFilter {\\n[ADD] public static final String REQUEST_HEADERS = \"REQUEST_HEADERS\";\\n     private final DefaultSecurityManager securityManager;\\n     private Provider<Request> grizzlyRequestProvider;\\n     private final Set<IpSubnet> trustedProxies;\\n \\n[ADD] public static final String X_GRAYLOG_REMOTE_ADDRESS = \"x-graylog-remote-address\";\\n[ADD] \\n     @Inject\\n     public ShiroSecurityContextFilter(DefaultSecurityManager securityManager,\\n                                       Provider<Request> grizzlyRequestProvider,',\n",
              " '   private static final JsonParser PARSER = new JsonParser();\\n \\n   private final String baseUrl;\\n[ADD] private final String topicQueryString;\\n   private final String startQueryString;\\n   private final String endQueryString;\\n   private final int maxNumTries;\\n[ADD] private final int retryBackOffSecs;\\n[ADD] \\n   /**\\n    * Constructor\\n    */',\n",
              " ' import org.sonar.plugins.java.api.semantic.Type;\\n import org.sonar.plugins.java.api.tree.AnnotationTree;\\n import org.sonar.plugins.java.api.tree.Arguments;\\n[ADD] import org.sonar.plugins.java.api.tree.ArrayAccessExpressionTree;\\n import org.sonar.plugins.java.api.tree.AssignmentExpressionTree;\\n import org.sonar.plugins.java.api.tree.BaseTreeVisitor;\\n import org.sonar.plugins.java.api.tree.BinaryExpressionTree;',\n",
              " ' import org.mule.runtime.module.extension.internal.loader.java.property.MetadataResolverFactoryModelProperty;\\n import org.mule.runtime.module.extension.internal.loader.java.property.NullSafeModelProperty;\\n import org.mule.runtime.module.extension.internal.loader.java.property.SdkSourceFactoryModelProperty;\\n[ADD] import org.mule.runtime.module.extension.internal.runtime.ValueResolvingException;\\n import org.mule.runtime.module.extension.internal.runtime.config.MutableConfigurationStats;\\n import org.mule.runtime.module.extension.internal.runtime.execution.deprecated.ComponentExecutorCompletableAdapterFactory;\\n import org.mule.runtime.module.extension.internal.runtime.execution.deprecated.ReactiveOperationExecutorFactoryWrapper;\\n[ADD] import org.mule.runtime.module.extension.internal.runtime.resolver.ParameterValueResolver;\\n import org.mule.runtime.module.extension.internal.runtime.resolver.ResolverSet;\\n import org.mule.runtime.module.extension.internal.runtime.resolver.ValueResolver;\\n import org.mule.runtime.module.extension.internal.runtime.resolver.ValueResolvingContext;\\n \\n[ADD] import java.util.Collection;\\n[ADD] import java.util.HashSet;\\n import java.util.List;\\n import java.util.Map;\\n import java.util.Map.Entry;\\n import java.util.Optional;\\n[ADD] import java.util.Set;\\n import java.util.concurrent.Callable;\\n import java.util.function.Function;\\n import java.util.function.Supplier;\\n[ADD] import java.util.stream.Stream;\\n \\n[ADD] import com.google.common.base.Joiner;\\n import com.google.common.collect.ImmutableList;\\n import com.google.common.collect.ImmutableMap;\\n ',\n",
              " \"  */\\n final Class<?> loadClass(Module module, String className) {\\n \\tClass<?> localClass = null;\\n[DEL] try {\\n[DEL] localClass = loadClassHelper(className, false, false, module);\\n[DEL] } catch (ClassNotFoundException e) {\\n[DEL] // returns null if the class can't be found\\n[ADD] \\n[ADD] if ((bootstrapClassLoader == null) || (this == bootstrapClassLoader)) {\\n[ADD] localClass = VMAccess.findClassOrNull(className, bootstrapClassLoader);\\n[ADD] } else {\\n[ADD] try {\\n[ADD] localClass = loadClassHelper(className, false, false, module);\\n[ADD] } catch (ClassNotFoundException e) {\\n[ADD] // returns null if the class can't be found\\n[ADD] }\\n \\t}\\n \\treturn localClass;\\n }\",\n",
              " '     // Get the parquet schema for this table looking at the latest commit\\n     MessageType schema = hoodieHiveClient.getDataSchema();\\n     // Sync schema if needed\\n[DEL] syncSchema(tableName, tableExists, useRealtimeInputFormat, schema);\\n[ADD] syncSchema(tableName, tableExists, useRealtimeInputFormat, readAsOptimized, schema);\\n \\n     LOG.info(\"Schema sync complete. Syncing partitions for \" + tableName);\\n     // Get the last time we successfully synced partitions',\n",
              " '         return null;\\n       }\\n     });\\n[ADD] \\n     pipeline.run();\\n   }\\n \\n[ADD] @Test\\n[ADD] public void testSplit() throws Exception {\\n[ADD] PipelineOptions options = PipelineOptionsFactory.create();\\n[ADD] MongoDbGridFSIO.Read<String> read = MongoDbGridFSIO.read()\\n[ADD] .withUri(\"mongodb://localhost:\" + PORT)\\n[ADD] .withDatabase(DATABASE);\\n[ADD] \\n[ADD] BoundedGridFSSource src = read.getSource();\\n[ADD] \\n[ADD] // make sure 2 files can fit in\\n[ADD] long desiredBundleSizeBytes = src.getEstimatedSizeBytes(options) * 2L / 5L + 1000;\\n[ADD] List<? extends BoundedSource<ObjectId>> splits = src.splitIntoBundles(\\n[ADD] desiredBundleSizeBytes, options);\\n[ADD] \\n[ADD] int expectedNbSplits = 3;\\n[ADD] assertEquals(expectedNbSplits, splits.size());\\n[ADD] SourceTestUtils.\\n[ADD] assertSourcesEqualReferenceSource(src, splits, options);\\n[ADD] int nonEmptySplits = 0;\\n[ADD] int count = 0;\\n[ADD] for (BoundedSource<ObjectId> subSource : splits) {\\n[ADD] List<ObjectId> result = SourceTestUtils.readFromSource(subSource, options);\\n[ADD] if (result.size() > 0) {\\n[ADD] nonEmptySplits += 1;\\n[ADD] }\\n[ADD] count += result.size();\\n[ADD] }\\n[ADD] assertEquals(expectedNbSplits, nonEmptySplits);\\n[ADD] assertEquals(5, count);\\n[ADD] }\\n[ADD] \\n }',\n",
              " ' \\n \\t@Test\\n \\tpublic void testNetSingleUseNoInbound() throws Exception {\\n[DEL] final AtomicReference<ServerSocket> serverSocket = new AtomicReference<ServerSocket>();\\n[ADD] final AtomicReference<ServerSocket> serverSocket = new AtomicReference<>();\\n \\t\\tfinal CountDownLatch latch = new CountDownLatch(1);\\n \\t\\tfinal Semaphore semaphore = new Semaphore(0);\\n \\t\\tfinal AtomicBoolean done = new AtomicBoolean();',\n",
              " ' import org.apache.hudi.common.table.HoodieTableMetaClient;\\n import org.apache.hudi.common.util.Option;\\n import org.apache.hudi.common.util.TypedProperties;\\n[ADD] import org.apache.hudi.exception.HoodieIOException;\\n import org.apache.hudi.hive.HiveSyncConfig;\\n import org.apache.hudi.hive.HoodieHiveClient;\\n import org.apache.hudi.hive.util.HiveTestService;\\n import org.apache.hudi.utilities.sources.TestDataSource;\\n \\n[ADD] import com.fasterxml.jackson.databind.JsonNode;\\n[ADD] import com.fasterxml.jackson.databind.ObjectMapper;\\n[ADD] import com.fasterxml.jackson.databind.ObjectWriter;\\n[ADD] import com.fasterxml.jackson.databind.node.ArrayNode;\\n[ADD] import com.fasterxml.jackson.databind.node.ObjectNode;\\n[ADD] import com.fasterxml.jackson.dataformat.csv.CsvMapper;\\n[ADD] import com.fasterxml.jackson.dataformat.csv.CsvSchema;\\n[ADD] import com.fasterxml.jackson.dataformat.csv.CsvSchema.Builder;\\n import com.google.common.collect.ImmutableList;\\n import org.apache.avro.generic.GenericRecord;\\n import org.apache.avro.generic.IndexedRecord;',\n",
              " \"               for (;;) {\\n                 JobMetrics metrics = getJobMetrics(job);\\n                 Optional<Boolean> success = checkForPAssertSuccess(job, metrics);\\n[ADD] if (messageHandler.hasSeenError()) {\\n[ADD] return Optional.of(false);\\n[ADD] }\\n[ADD] \\n                 if (success.isPresent() && (!success.get() || atMaxWatermark(job, metrics))) {\\n                   // It's possible that the streaming pipeline doesn't use PAssert.\\n                   // So checkForSuccess() will return true before job is finished.\",\n",
              " '                event.stopPropagation();\\n                commands_.interruptR().execute();\\n             }\\n[ADD] \\n[ADD] else if (mod == (KeyboardShortcut.ALT + KeyboardShortcut.SHIFT)\\n[ADD] && ne.getKeyCode() == 190) // period\\n[ADD] {\\n[ADD] event.preventDefault();\\n[ADD] event.stopPropagation();\\n[ADD] \\n[ADD] int currentRow = docDisplay_.getCursorPosition().getRow();\\n[ADD] String currentLine = docDisplay_.getLine(currentRow);\\n[ADD] \\n[ADD] // move cursor to end of line\\n[ADD] docDisplay_.setCursorPosition(\\n[ADD] Position.create(currentRow, currentLine.length()));\\n[ADD] \\n[ADD] // insert a space if there isn\\'t a space behind the cursor\\n[ADD] if (!currentLine.endsWith(\" \"))\\n[ADD] {\\n[ADD] docDisplay_.insertCode(\" %>%\\\\n\", false);\\n[ADD] }\\n[ADD] else\\n[ADD] {\\n[ADD] docDisplay_.insertCode(\"%>%\\\\n\", false);\\n[ADD] }\\n[ADD] \\n[ADD] docDisplay_.reindent(docDisplay_.getSelectionRange());\\n[ADD] \\n[ADD] }\\n[ADD] \\n          }\\n       });\\n       ',\n",
              " '       MethodMatcher.create().typeDefinition(JAVA_IO_FILE).name(INIT).parameters(JAVA_LANG_STRING),\\n       MethodMatcher.create().typeDefinition(JAVA_IO_FILE).name(INIT).parameters(JAVA_LANG_STRING, JAVA_LANG_STRING),\\n       MethodMatcher.create().typeDefinition(JAVA_IO_FILE).name(INIT).parameters(\"java.net.URI\"),\\n[ADD] MethodMatcher.create().typeDefinition(JAVA_IO_FILE).name(\"createTempFile\").parameters(JAVA_LANG_STRING, JAVA_LANG_STRING),\\n       MethodMatcher.create().typeDefinition(\"java.nio.file.Paths\").name(\"get\").parameters(JAVA_LANG_STRING, JAVA_LANG_STRING_ARRAY),\\n       MethodMatcher.create().typeDefinition(\"java.nio.file.Paths\").name(\"get\").parameters(\"java.net.URI\"),\\n       MethodMatcher.create().typeDefinition(\"java.nio.file.FileSystem\").name(\"getRootDirectories\").withoutParameter(),',\n",
              " ' package org.apache.gobblin.compaction.mapreduce.orc;\\n \\n import java.io.IOException;\\n[DEL] import org.apache.gobblin.compaction.mapreduce.CompactionJobConfigurator;\\n[DEL] import org.apache.gobblin.compaction.mapreduce.CompactorOutputCommitter;\\n[ADD] \\n import org.apache.hadoop.conf.Configuration;\\n import org.apache.hadoop.fs.Path;\\n import org.apache.hadoop.mapreduce.OutputCommitter;',\n",
              " '         return this.listenerRegistry.addRequestHandler(this, requestHandler, listenerRequestMatcher);\\n     }\\n \\n[DEL] public void cleanIdleConnections()\\n[ADD] public void setCleanIdleConnections()\\n     {\\n[DEL] transport.setKeepAlive(false);\\n[ADD] shouldCleanIdleConnectionsOnStop = true;\\n     }\\n }',\n",
              " ' \\t\\t\\tfor (Object next : (Iterable<?>) processorResult) {\\n \\t\\t\\t\\tthis.sendReplyMessage(next, replyChannel);\\n \\t\\t\\t}\\n[DEL] } else {\\n[ADD] }\\n[ADD] else {\\n \\t\\t\\tthis.sendReplyMessage(processorResult, replyChannel);\\n \\t\\t}\\n \\t}',\n",
              " '     future = res.getKey();\\n     executor = res.getValue();\\n     started = true;\\n[DEL] monitorThreads(onShutdownCallback);\\n[ADD] shutdownCallback(onShutdownCallback);\\n   }\\n \\n   /**',\n",
              " '         correctJsonObjectBasicCopy.putOpt(\"boolean_key\", (boolean) true);\\n         correctJsonObjectBasicCopy.putOpt(\"object_key\", correctJsonBasic);\\n \\n[DEL] Assert.assertEquals(6, correctJsonObjectBasicCopy.getInt(\"int_key\"));\\n[DEL] Assert.assertEquals(2L, correctJsonObjectBasicCopy.getLong(\"long_key\"));\\n[DEL] Assert.assertEquals(2d, correctJsonObjectBasicCopy.getDouble(\"double_key\"), 1e-10);\\n[ADD] assertEquals(6, correctJsonObjectBasicCopy.getInt(\"int_key\"));\\n[ADD] assertEquals(2L, correctJsonObjectBasicCopy.getLong(\"long_key\"));\\n[ADD] assertEquals(2d, correctJsonObjectBasicCopy.getDouble(\"double_key\"), 1e-10);\\n         Assert.assertTrue(correctJsonObjectBasicCopy.getBoolean(\"boolean_key\"));\\n[DEL] Assert.assertEquals(correctJsonBasic, correctJsonObjectBasicCopy.get(\"object_key\"));\\n[ADD] assertEquals(correctJsonBasic, correctJsonObjectBasicCopy.get(\"object_key\"));\\n \\n         // Check that putOpt doesn\\'t add pair when one is null\\n         correctJsonObjectBasicCopy.putOpt(\"boolean_key_2\", null);',\n",
              " ' \\t\\t}\\n \\n \\t\\t@Override\\n[DEL] public void onApplicationEvent(ContextRefreshedEvent event) {\\n[DEL] if (event.getApplicationContext() == getApplicationContext() && !this.initialized.getAndSet(true)) {\\n[DEL] ConversionService conversionService = getConversionService();\\n[DEL] if (conversionService == null) {\\n[DEL] conversionService = DefaultConversionService.getSharedInstance();\\n[ADD] protected void onInit() throws Exception {\\n[ADD] super.onInit();\\n[ADD] ConversionService conversionService = getConversionService();\\n[ADD] if (conversionService == null) {\\n[ADD] conversionService = DefaultConversionService.getSharedInstance();\\n[ADD] }\\n[ADD] for (Map.Entry<Object, NamedComponent> entry : this.mapping.entrySet()) {\\n[ADD] Object key = entry.getKey();\\n[ADD] String channelKey;\\n[ADD] if (key instanceof String) {\\n[ADD] channelKey = (String) key;\\n[ADD] }\\n[ADD] else if (key instanceof Class) {\\n[ADD] channelKey = ((Class<?>) key).getName();\\n \\t\\t\\t\\t}\\n[DEL] for (Map.Entry<Object, NamedComponent> entry : this.mapping.entrySet()) {\\n[DEL] Object key = entry.getKey();\\n[DEL] String channelKey;\\n[DEL] if (key instanceof String) {\\n[DEL] channelKey = (String) key;\\n[DEL] }\\n[DEL] else if (key instanceof Class) {\\n[DEL] channelKey = ((Class<?>) key).getName();\\n[DEL] }\\n[DEL] else if (conversionService.canConvert(key.getClass(), String.class)) {\\n[DEL] channelKey = conversionService.convert(key, String.class);\\n[DEL] }\\n[DEL] else {\\n[DEL] throw new MessagingException(\"Unsupported channel mapping type for router [\"\\n[DEL] + key.getClass() + \"]\");\\n[DEL] }\\n[DEL] \\n[DEL] this.router.setChannelMapping(channelKey, entry.getValue().getComponentName());\\n[ADD] else if (conversionService.canConvert(key.getClass(), String.class)) {\\n[ADD] channelKey = conversionService.convert(key, String.class);\\n \\t\\t\\t\\t}\\n[ADD] else {\\n[ADD] throw new MessagingException(\"Unsupported channel mapping type for router [\"\\n[ADD] + key.getClass() + \"]\");\\n[ADD] }\\n[ADD] \\n[ADD] this.router.setChannelMapping(channelKey, entry.getValue().getComponentName());\\n \\t\\t\\t}\\n \\t\\t}\\n ',\n",
              " '     };\\n   }\\n \\n[DEL] @Before\\n[DEL] public void setUp() {\\n[DEL] processorWasDisposed.set(false);\\n[DEL] messageSentTimer = new CountDownLatch(1);\\n[DEL] }\\n[DEL] \\n   @Test\\n   public void parsesPolicy() throws Exception {\\n     assertThat(policyProvider, instanceOf(TestPolicyProvider.class));',\n",
              " '     checkState(stateAddressWindows != null,\\n                              \"Cannot ensure window %s is active since it is neither ACTIVE nor NEW\",\\n                              window);\\n[DEL] if (stateAddressWindows.isEmpty()) {\\n[ADD] if (stateAddressWindows != null && stateAddressWindows.isEmpty()) {\\n       // Window was NEW, make it ACTIVE with itself as its state address window.\\n       stateAddressWindows.add(window);\\n     }',\n",
              " '             return false;\\n           }\\n \\n[ADD] // OrderedExecutor$1 is a worker class that processes tasks submitted to OrderedExecutor\\n[ADD] if (taskClass\\n[ADD] .getName()\\n[ADD] .equals(\"org.hornetq.utils.OrderedExecutorFactory$OrderedExecutor$1\")) {\\n[ADD] return false;\\n[ADD] }\\n[ADD] \\n           Class<?> enclosingClass = taskClass.getEnclosingClass();\\n           if (enclosingClass != null) {\\n             // Avoid context leak on jetty. Runnable submitted from SelectChannelEndPoint is used to',\n",
              " ' import java.io.FileReader;\\n import java.io.FileWriter;\\n import java.io.IOException;\\n[ADD] \\n[ADD] import org.apache.commons.io.IOUtils;\\n[ADD] \\n[ADD] import javax.annotation.Nonnull;\\n[ADD] \\n import java.io.BufferedInputStream;\\n[ADD] import java.io.BufferedReader;\\n import java.io.BufferedWriter;\\n import java.io.InputStream;\\n import java.io.InputStreamReader;\\n import java.io.Reader;\\n[ADD] import java.nio.file.Path;\\n import java.io.FileReader;\\n import java.io.BufferedWriter;\\n import java.util.Formatter;\\n import java.util.jar.JarFile;\\n import java.io.DataInputStream;\\n[ADD] import java.io.File;\\n \\n public class A {\\n   private final static int MAX_LOOP = 42;',\n",
              " '     }\\n   }\\n \\n[DEL] /**\\n[DEL] * Propagates this lifecycle phase into the the {@link #value} and each item in {@link #getInterceptors()}\\n[DEL] *\\n[DEL] * @throws MuleException if an exception is found\\n[DEL] */\\n   @Override\\n   public synchronized void start() throws MuleException {\\n     if (!started) {',\n",
              " \"     # depending on which one exists (there is a possibility, of course, to\\n     # get something like 'libabcXabc.so, but for now we consider this\\n     # unlikely).\\n[DEL] name = 'lib' + spec.name.replace('-', '?')\\n[ADD] name = spec.name.replace('-', '?')\\n[ADD] \\n[ADD] # Avoid double 'lib' for packages whose name already starts with lib\\n[ADD] if not name.startswith('lib'):\\n[ADD] name = 'lib' + name\\n \\n     # To speedup the search for external packages configured e.g. in /usr,\\n     # perform first non-recursive search in prefix.lib then in prefix.lib64 and\",\n",
              " '    *\\n    * <p>\\n    *   Removing a {@link JobMetrics} instance for a job will also remove the {@link TaskMetrics}s\\n[DEL] *   of every tasks of the job.\\n[ADD] *   of every tasks of the job. This is only used by job driver where there is no {@link ForkMetrics}.\\n    * </p>\\n    * @param jobState the given {@link JobState} instance\\n    */',\n",
              " ' \\n     private ScheduledExecutorService serviceExporterExecutor;\\n \\n[ADD] private ScheduledExecutorService serviceRefererExecutor;\\n[ADD] \\n     public ScheduledExecutorService registryNotificationExecutor;\\n \\n     private ScheduledExecutorService reconnectScheduledExecutor;\\n \\n[DEL] private ScheduledExecutorService serviceDiscveryAddressNotificationExecutor;\\n[ADD] private ScheduledExecutorService serviceDiscoveryAddressNotificationExecutor;\\n \\n     private ScheduledExecutorService metadataRetryExecutor;\\n ',\n",
              " '         Mockito.verify(networkModel, Mockito.times(times)).getNextAvailableMacAddressInNetwork(Mockito.anyLong());\\n         Assert.assertEquals(expectedMacAddress, returnedMacAddress);\\n     }\\n[ADD] \\n[ADD] @Test\\n[ADD] public void validateRemoveTagsWhenExists() {\\n[ADD] \\n[ADD] List<ResourceTag> resourceTags = Arrays.asList(new ResourceTagVO(\"test\", \"test\", 1l, 2l,\\n[ADD] Long.valueOf(123), ResourceTag.ResourceObjectType.UserVm, \"\", \"test\"));\\n[ADD] \\n[ADD] List<String> resourceIds = Arrays.asList(\"123\");\\n[ADD] \\n[ADD] Mockito.when(taggedResourceManagerMock.getUuid(\"123\", ResourceTag.ResourceObjectType.UserVm)).thenReturn(\"123\");\\n[ADD] Mockito.when(taggedResourceManagerMock.getResourceId(\"123\", ResourceTag.ResourceObjectType.UserVm)).thenReturn(Long.valueOf(123));\\n[ADD] \\n[ADD] Mockito.<List<? extends ResourceTag>>when(taggedResourceManagerMock.listByResourceTypeAndId(ResourceTag.ResourceObjectType.UserVm, Long.valueOf(123))).thenReturn(resourceTags);\\n[ADD] Mockito.when(taggedResourceManagerMock.deleteTags(resourceIds, ResourceTag.ResourceObjectType.UserVm, null)).thenReturn(true);\\n[ADD] \\n[ADD] Mockito.when(userVmDao.findById(123l)).thenReturn(userVmVoMock);\\n[ADD] Mockito.when(userVmVoMock.getUuid()).thenReturn(\"123\");\\n[ADD] \\n[ADD] \\n[ADD] userVmManagerImpl.removeTagsFromVm(Long.valueOf(123));\\n[ADD] Assert.assertTrue(taggedResourceManagerMock.deleteTags(resourceIds, ResourceTag.ResourceObjectType.UserVm, null));\\n[ADD] }\\n[ADD] \\n[ADD] \\n[ADD] @Test\\n[ADD] public void validateWhenRemoveTagsNoExists() {\\n[ADD] \\n[ADD] List<ResourceTag> resourceTags = Arrays.asList();\\n[ADD] \\n[ADD] Mockito.when(taggedResourceManagerMock.getResourceId(\"1l\", ResourceTag.ResourceObjectType.UserVm)).thenReturn(1l);\\n[ADD] Mockito.<List<? extends ResourceTag>>when(taggedResourceManagerMock.listByResourceTypeAndId(ResourceTag.ResourceObjectType.UserVm, 1l)).thenReturn(resourceTags);\\n[ADD] \\n[ADD] Mockito.when(userVmVoMock.getUuid()).thenReturn(\"1l\");\\n[ADD] \\n[ADD] Mockito.when(userVmDao.findById(Mockito.eq(vmId))).thenReturn(userVmVoMock);\\n[ADD] \\n[ADD] userVmManagerImpl.removeTagsFromVm(1l);\\n[ADD] Assert.assertEquals(0, taggedResourceManagerMock.listByResourceTypeAndId(ResourceTag.ResourceObjectType.UserVm, 1l).size());\\n[ADD] }\\n }',\n",
              " ' \\n \\tpublic static final long DEFAULT_SEND_TIMEOUT = 1000L;\\n \\n[DEL] \\n \\tprotected volatile MessageGroupStore messageStore;\\n \\n \\tprivate final MessageGroupProcessor outputProcessor;',\n",
              " '             if (currentStackEntry != null) {\\n               // For flow-ref to flows, avoid creating a second MessagingException, an instead mutate the thrown on so it has the\\n               // proper state when is bubbled.\\n[DEL] ((DefaultFlowCallStack) parentContextEvent.getFlowCallStack()).push(currentStackEntry);\\n[ADD] DefaultFlowCallStack parentCallStack = (DefaultFlowCallStack) parentContextEvent.getFlowCallStack();\\n[ADD] List<FlowStackElement> childFlowStacks = eventChildCtx.getFlowCallStack().getElements();\\n[ADD] Stack<FlowStackElement> remaining = new Stack();\\n[ADD] \\n[ADD] for (FlowStackElement e : childFlowStacks) {\\n[ADD] if (parentCallStack.peek().equals(e)) {\\n[ADD] break;\\n[ADD] }\\n[ADD] remaining.push(e);\\n[ADD] }\\n[ADD] \\n[ADD] while (!remaining.isEmpty()) {\\n[ADD] parentCallStack.push(remaining.pop());\\n[ADD] }\\n             }\\n             error.setProcessedEvent(parentContextEvent);\\n           }',\n",
              " '     {\\n         return name;\\n     }\\n[ADD] \\n[ADD] private boolean isEqualityAssertBehavior ()\\n[ADD] {\\n[ADD] return getBoolean(equalityAssertBehavior);\\n[ADD] }\\n }\\n \\n ',\n",
              " ' \\n   @Override\\n   public void dispose() throws Exception {\\n[DEL] // DoFnOperator generates another \"bundle\" for the final watermark\\n[DEL] super.dispose();\\n     // Remove the reference to stageContext and make stageContext available for garbage collection.\\n     try (@SuppressWarnings(\"unused\")\\n             AutoCloseable bundleFactoryCloser = stageBundleFactory;\\n         @SuppressWarnings(\"unused\")\\n[DEL] AutoCloseable closable = stageContext) {}\\n[ADD] AutoCloseable closable = stageContext) {\\n[ADD] // DoFnOperator generates another \"bundle\" for the final watermark -- see BEAM-5816 for more context\\n[ADD] super.dispose();\\n[ADD] }\\n     stageContext = null;\\n   }\\n ',\n",
              " '     span.recordException(throwable);\\n   }\\n \\n[DEL] public static void setPeer(final Span span, String peerName, String peerIp) {\\n[DEL] BaseDecorator.setPeer(span, peerName, peerIp);\\n[ADD] public void onPeerConnection(final Span span, final InetSocketAddress remoteConnection) {\\n[ADD] assert span != null;\\n[ADD] if (remoteConnection != null) {\\n[ADD] InetAddress remoteAddress = remoteConnection.getAddress();\\n[ADD] if (remoteAddress != null) {\\n[ADD] onPeerConnection(span, remoteAddress);\\n[ADD] } else {\\n[ADD] // Failed DNS lookup, the host string is the name.\\n[ADD] setPeer(span, remoteConnection.getHostString(), null);\\n[ADD] }\\n[ADD] span.setAttribute(SemanticAttributes.NET_PEER_PORT.key(), remoteConnection.getPort());\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] public void onPeerConnection(final Span span, final InetAddress remoteAddress) {\\n[ADD] assert span != null;\\n[ADD] setPeer(span, remoteAddress.getHostName(), remoteAddress.getHostAddress());\\n[ADD] }\\n[ADD] \\n[ADD] public void setPeer(final Span span, String peerName, String peerIp) {\\n[ADD] assert span != null;\\n[ADD] if (peerName != null && !peerName.equals(peerIp)) {\\n[ADD] SemanticAttributes.NET_PEER_NAME.set(span, peerName);\\n[ADD] }\\n[ADD] if (peerIp != null) {\\n[ADD] SemanticAttributes.NET_PEER_IP.set(span, peerIp);\\n[ADD] }\\n[ADD] String peerService = mapToPeer(peerName);\\n[ADD] if (peerService == null) {\\n[ADD] peerService = mapToPeer(peerIp);\\n[ADD] }\\n[ADD] if (peerService != null) {\\n[ADD] SemanticAttributes.PEER_SERVICE.set(span, peerService);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] protected String mapToPeer(String endpoint) {\\n[ADD] if (endpoint == null) {\\n[ADD] return null;\\n[ADD] }\\n[ADD] \\n[ADD] return Config.get().getEndpointPeerServiceMapping().get(endpoint);\\n   }\\n }',\n",
              " '         logger.setLevel(newLevel);\\n     }\\n \\n[ADD] @GET\\n[ADD] @Timed\\n[ADD] @ApiOperation(value = \"Get recent log messages\")\\n[ADD] @ApiResponses(value = {\\n[ADD] @ApiResponse(code = 404, message = \"Memory appender not configured.\")\\n[ADD] })\\n[ADD] @Path(\"/messages/recent\")\\n[ADD] @Produces(MediaType.APPLICATION_JSON)\\n[ADD] public LogMessagesSummary messages(@ApiParam(name = \"max\", required = true) @QueryParam(\"max\") int max) {\\n[ADD] List<LogMessage> messages = new ArrayList<>();\\n[ADD] \\n[ADD] Logger logger = Logger.getRootLogger();\\n[ADD] if (logger.getAppender(\"memory\") == null) {\\n[ADD] throw new WebApplicationException(404);\\n[ADD] }\\n[ADD] \\n[ADD] MemoryAppender appender = (MemoryAppender) logger.getAppender(\"memory\");\\n[ADD] for (LoggingEvent loggingEvent : appender.getLogMessages(max)) {\\n[ADD] List<String> throwable = new ArrayList<>();\\n[ADD] if(loggingEvent.getThrowableStrRep() != null) {\\n[ADD] throwable = Arrays.asList(loggingEvent.getThrowableStrRep());\\n[ADD] }\\n[ADD] \\n[ADD] messages.add(LogMessage.create(\\n[ADD] loggingEvent.getRenderedMessage(),\\n[ADD] loggingEvent.getLoggerName(),\\n[ADD] loggingEvent.getLevel().toString(),\\n[ADD] new DateTime(loggingEvent.getTimeStamp(), DateTimeZone.UTC).toString(),\\n[ADD] throwable\\n[ADD] ));\\n[ADD] }\\n[ADD] \\n[ADD] \\n[ADD] return LogMessagesSummary.create(messages);\\n[ADD] }\\n[ADD] \\n[ADD] \\n     private static class Subsystem {\\n \\n         private final String title;',\n",
              " '       collectExpectedIssues(syntaxTrivia.comment(), syntaxTrivia.startLine());\\n     }\\n \\n[DEL] private void collectExpectedIssues(String comment, int line) {\\n[DEL] if (comment.startsWith(NONCOMPLIANT_COMMENT)) {\\n[DEL] parseIssue(comment, line);\\n[ADD] @VisibleForTesting\\n[ADD] void collectExpectedIssues(String comment, int line) {\\n[ADD] if (NONCOMPLIANT_COMMENT.matcher(comment).find()) {\\n[ADD] ParsedComment parsedComment = parseIssue(comment, line);\\n[ADD] issues.put(LINE.get(parsedComment.issue), parsedComment.issue);\\n[ADD] parsedComment.flows.forEach(f -> flows.put(f.id, f));\\n[ADD] }\\n[ADD] if (FLOW_COMMENT.matcher(comment).find()) {\\n[ADD] parseFlows(comment, line).forEach(f -> flows.put(f.id, f));\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] @VisibleForTesting\\n[ADD] static Set<FlowComment> parseFlows(@Nullable String comment, int line) {\\n[ADD] if (comment == null) {\\n[ADD] return Collections.emptySet();\\n       }\\n[DEL] if (comment.startsWith(FLOW_COMMENT)) {\\n[DEL] parseFlow(comment, line);\\n[ADD] List<List<String>> flowIds = new ArrayList<>();\\n[ADD] List<Integer> flowStarts = new ArrayList<>();\\n[ADD] Matcher matcher = FLOW.matcher(comment);\\n[ADD] while (matcher.find()) {\\n[ADD] List<String> ids = Arrays.asList(matcher.group(\"ids\").split(\",\"));\\n[ADD] flowIds.add(ids);\\n[ADD] flowStarts.add(matcher.start());\\n       }\\n[ADD] // add one more fake start at the end, so the boundary of comment i is flowStarts[i],flowStarts[i+1] also for the last one\\n[ADD] flowStarts.add(comment.length());\\n[ADD] \\n[ADD] return IntStream.range(0, flowIds.size())\\n[ADD] .mapToObj(i -> createFlows(flowIds.get(i), line, comment.substring(flowStarts.get(i), flowStarts.get(i + 1))))\\n[ADD] .flatMap(Function.identity())\\n[ADD] .collect(Collectors.toSet());\\n     }\\n \\n[DEL] private void parseFlow(String comment, int line) {\\n[DEL] int atIdx = comment.indexOf(\\'@\\');\\n[DEL] int followingSpaceIdx = comment.indexOf(\\' \\', atIdx);\\n[DEL] String flowIdString = comment.substring(atIdx + 1, followingSpaceIdx == -1 ? comment.length() : followingSpaceIdx);\\n[DEL] String[] flowIds = flowIdString.split(\",\");\\n[DEL] String message = extractMessage(comment);\\n[DEL] Map<IssueAttribute, Object> attributes = extractAttributes(comment);\\n[ADD] private static Stream<FlowComment> createFlows(List<String> ids, int line, String flow) {\\n[ADD] Map<IssueAttribute, Object> attributes = new EnumMap<>(IssueAttribute.class);\\n[ADD] attributes.putAll(parseAttributes(flow));\\n[ADD] String message = parseMessage(flow, flow.length());\\n       attributes.put(MESSAGE, message);\\n[DEL] Arrays.stream(flowIds)\\n[DEL] .map(id -> new FlowComment(id, line, attributes))\\n[DEL] .forEach(f -> flows.put(f.id, f));\\n[ADD] return ids.stream().map(id -> new FlowComment(id, line, attributes));\\n     }\\n \\n[DEL] private void parseIssue(String comment, int line) {\\n[DEL] String cleanedComment = StringUtils.remove(comment, NONCOMPLIANT_COMMENT);\\n \\n[DEL] EnumMap<IssueAttribute, Object> attr = new EnumMap<>(IssueAttribute.class);\\n[DEL] String expectedMessage = extractMessage(cleanedComment);\\n[DEL] if (StringUtils.isNotEmpty(expectedMessage)) {\\n[DEL] attr.put(MESSAGE, expectedMessage);\\n[ADD] \\n[ADD] @VisibleForTesting\\n[ADD] static ParsedComment parseIssue(String comment, int line) {\\n[ADD] Matcher shiftMatcher = SHIFT.matcher(comment);\\n[ADD] Matcher flowMatcher = FLOW.matcher(comment);\\n[ADD] return createIssue(line,\\n[ADD] shiftMatcher.find() ? shiftMatcher.group(1) : null,\\n[ADD] comment,\\n[ADD] parseMessage(comment, flowMatcher.find() ? flowMatcher.start() : comment.length()),\\n[ADD] comment);\\n[ADD] }\\n[ADD] \\n[ADD] private static ParsedComment createIssue(int line, @Nullable String shift, @Nullable String attributes, @Nullable String message, @Nullable String flow) {\\n[ADD] Issue issue = Issue.create();\\n[ADD] issue.put(LINE, parseLineShifting(shift).getLine(line));\\n[ADD] Map<IssueAttribute, Object> attrs = parseAttributes(attributes);\\n[ADD] attrs = adjustEndLine(attrs, line);\\n[ADD] issue.putAll(attrs);\\n[ADD] if (message != null) {\\n[ADD] issue.put(MESSAGE, message);\\n       }\\n[DEL] int expectedLine = line;\\n[DEL] String attributesSubstr = extractAttributes(comment, attr);\\n[ADD] Set<FlowComment> flows = parseFlows(flow, line);\\n[ADD] return new ParsedComment(issue, flows);\\n[ADD] }\\n \\n[DEL] cleanedComment = StringUtils.stripEnd(StringUtils.remove(StringUtils.remove(cleanedComment, \"[[\" + attributesSubstr + \"]]\"), \"{{\" + expectedMessage + \"}}\"), \" \\\\t\");\\n[DEL] expectedLine = parseLineShifting(cleanedComment, expectedLine);\\n[DEL] updateEndLine(expectedLine, attr);\\n[DEL] issues.put(expectedLine, attr);\\n[ADD] private static LineRef parseLineShifting(@Nullable String shift) {\\n[ADD] if (shift == null) {\\n[ADD] return new LineRef.RelativeLineRef(0);\\n[ADD] }\\n[ADD] try {\\n[ADD] return LineRef.fromString(shift);\\n[ADD] } catch (NumberFormatException e) {\\n[ADD] Fail.fail(\"Use only \\'@+N\\' or \\'@-N\\' to shifts messages.\");\\n[ADD] return null;\\n[ADD] }\\n     }\\n \\n[DEL] private String extractMessage(String cleanedComment) {\\n[DEL] return StringUtils.substringBetween(cleanedComment, \"{{\", \"}}\");\\n[ADD] private static Map<IssueAttribute, Object> parseAttributes(@Nullable String comment) {\\n[ADD] comment = StringUtils.substringBetween(comment, \"[[\", \"]]\");\\n[ADD] if (comment == null) {\\n[ADD] return Collections.emptyMap();\\n[ADD] }\\n[ADD] return Arrays.stream(comment.split(\";\"))\\n[ADD] .map(Parser::parseAttribute)\\n[ADD] .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\\n     }\\n \\n[DEL] private int parseLineShifting(String cleanedComment, int expectedLine) {\\n[DEL] if (StringUtils.startsWith(cleanedComment, \"@\")) {\\n[DEL] final int lineAdjustment;\\n[DEL] final char firstChar = cleanedComment.charAt(1);\\n[DEL] final int endIndex = cleanedComment.indexOf(\\' \\');\\n[DEL] if (endIndex == -1) {\\n[DEL] lineAdjustment = Integer.parseInt(cleanedComment.substring(2));\\n[DEL] } else {\\n[DEL] lineAdjustment = Integer.parseInt(cleanedComment.substring(2, endIndex));\\n[DEL] }\\n[DEL] if (firstChar == \\'+\\') {\\n[DEL] expectedLine += lineAdjustment;\\n[DEL] } else if (firstChar == \\'-\\') {\\n[DEL] expectedLine -= lineAdjustment;\\n[DEL] } else {\\n[DEL] Fail.fail(\"Use only \\'@+N\\' or \\'@-N\\' to shifts messages.\");\\n[ADD] private static Map<IssueAttribute, Object> adjustEndLine(Map<IssueAttribute, Object> attributes, int line) {\\n[ADD] Object endLine = attributes.get(END_LINE);\\n[ADD] if (endLine != null && endLine instanceof LineRef.RelativeLineRef) {\\n[ADD] LineRef.RelativeLineRef relativeLineRef = (LineRef.RelativeLineRef) endLine;\\n[ADD] if (relativeLineRef.offset < 0) {\\n[ADD] Fail.fail(\"endLine attribute should be relative to the line and must be +N with N integer\");\\n         }\\n[ADD] EnumMap<IssueAttribute, Object> copy = new EnumMap<>(attributes);\\n[ADD] copy.put(END_LINE, new LineRef.AbsoluteLineRef(relativeLineRef.getLine(line)));\\n[ADD] return copy;\\n[ADD] }\\n[ADD] return attributes;\\n[ADD] }\\n[ADD] \\n[ADD] private static Map.Entry<IssueAttribute, Object> parseAttribute(String attribute) {\\n[ADD] Scanner scanner = new Scanner(attribute).useDelimiter(\"[=]+\");\\n[ADD] String name = scanner.next();\\n[ADD] if (!ATTRIBUTE_MAP.containsKey(name)) {\\n[ADD] Fail.fail(\"// Noncompliant attributes not valid: \" + attribute);\\n       }\\n[DEL] return expectedLine;\\n[ADD] IssueAttribute key = ATTRIBUTE_MAP.get(name);\\n[ADD] Object value = key.setter.apply(scanner.hasNext() ? scanner.next() : null);\\n[ADD] return new AbstractMap.SimpleImmutableEntry<>(key, value);\\n     }\\n \\n[DEL] private Map<IssueAttribute, Object> extractAttributes(String comment) {\\n[DEL] final Map<Expectations.IssueAttribute, Object> attributes = new EnumMap<>(IssueAttribute.class);\\n[DEL] extractAttributes(comment, attributes);\\n[DEL] return attributes;\\n[ADD] private static String parseMessage(String cleanedComment, int horizon) {\\n[ADD] return StringUtils.substringBetween(cleanedComment.substring(0, horizon), \"{{\", \"}}\");\\n     }\\n \\n[DEL] private String extractAttributes(String comment, Map<IssueAttribute, Object> attr) {\\n[DEL] String attributesSubstr = StringUtils.substringBetween(comment, \"[[\", \"]]\");\\n[DEL] if (StringUtils.isEmpty(attributesSubstr)) {\\n[DEL] return attributesSubstr;\\n[ADD] static class ParsedComment {\\n[ADD] final Issue issue;\\n[ADD] final Set<FlowComment> flows;\\n[ADD] \\n[ADD] private ParsedComment(Issue issue, Set<FlowComment> flows) {\\n[ADD] this.issue = issue;\\n[ADD] this.flows = flows;\\n       }\\n[DEL] Iterable<String> attributes = Splitter.on(\";\").split(attributesSubstr);\\n[DEL] for (String attribute : attributes) {\\n[DEL] String attributeValue = \"\";\\n[DEL] if (attribute.indexOf(\\'=\\') != -1) {\\n[DEL] String[] split = StringUtils.split(attribute, \\'=\\');\\n[DEL] attribute = split[0];\\n[DEL] attributeValue = split.length == 2 ? split[1] : \"\";\\n[DEL] }\\n[DEL] if (ATTRIBUTE_MAP.containsKey(attribute)) {\\n[DEL] IssueAttribute issueAttribute = ATTRIBUTE_MAP.get(attribute);\\n[DEL] Object value = issueAttribute.setter.apply(attributeValue);\\n[DEL] attr.put(issueAttribute, value);\\n[ADD] }\\n[ADD] \\n[ADD] abstract static class LineRef {\\n[ADD] abstract int getLine(int ref);\\n[ADD] \\n[ADD] static LineRef fromString(String input) {\\n[ADD] if (input.startsWith(\"+\") || input.startsWith(\"-\")) {\\n[ADD] return new RelativeLineRef(Integer.valueOf(input));\\n         } else {\\n[DEL] Fail.fail(\"// Noncompliant attributes not valid: \" + attributesSubstr);\\n[ADD] return new AbsoluteLineRef(Integer.valueOf(input));\\n         }\\n       }\\n[DEL] return attributesSubstr;\\n[DEL] }\\n \\n[DEL] private void updateEndLine(int expectedLine, EnumMap<IssueAttribute, Object> attr) {\\n[DEL] if (attr.containsKey(END_LINE)) {\\n[DEL] LineRef endLine = (LineRef) attr.get(END_LINE);\\n[DEL] if (endLine instanceof LineRef.RelativeLineRef) {\\n[DEL] attr.put(END_LINE, new LineRef.AbsoluteLineRef(endLine.getLine(expectedLine)));\\n[DEL] } else {\\n[DEL] Fail.fail(\"endLine attribute should be relative to the line and must be +N with N integer\");\\n[ADD] static int toLine(Object ref) {\\n[ADD] return ((LineRef) ref).getLine(0);\\n[ADD] }\\n[ADD] \\n[ADD] static class AbsoluteLineRef extends LineRef {\\n[ADD] final int line;\\n[ADD] \\n[ADD] public AbsoluteLineRef(int line) {\\n[ADD] this.line = line;\\n[ADD] }\\n[ADD] \\n[ADD] public int getLine(int ref) {\\n[ADD] return line;\\n         }\\n       }\\n[ADD] \\n[ADD] static class RelativeLineRef extends LineRef {\\n[ADD] final int offset;\\n[ADD] \\n[ADD] RelativeLineRef(int offset) {\\n[ADD] this.offset = offset;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] int getLine(int ref) {\\n[ADD] return ref + offset;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public int hashCode() {\\n[ADD] return Objects.hash(getLine(0));\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public boolean equals(Object obj) {\\n[ADD] return LineRef.class.isAssignableFrom(obj.getClass()) && Objects.equals(getLine(0), ((LineRef) obj).getLine(0));\\n[ADD] }\\n     }\\n   }\\n ',\n",
              " ' \\n package org.mule.runtime.module.artifact.classloader;\\n \\n[ADD] import static java.util.Collections.emptyList;\\n import static org.mule.runtime.core.util.Preconditions.checkArgument;\\n import org.mule.runtime.core.util.ClassUtils;\\n[ADD] import org.mule.runtime.module.artifact.descriptor.ArtifactDescriptor;\\n \\n import java.io.IOException;\\n import java.net.URL;',\n",
              " '             return all(set(self[f]) == set(other[f])\\n                        for f in other if (other[f] != [] and f in self))\\n \\n[ADD] def compatible(self, other):\\n[ADD] if self.spec and self.spec._concrete:\\n[ADD] return all(k in other and set(self[k]) == set(other[k])\\n[ADD] for k in self)\\n[ADD] \\n[ADD] return all(set(self[k]) == set(other[k])\\n[ADD] for k in self if k in other)\\n[ADD] \\n     def constrain(self, other):\\n         \"\"\"Add all flags in other that aren\\'t in self to self.\\n ',\n",
              " \"         {\\n             // Load actuals count, LoadHeapArguments will reuse the generated instructions here\\n             IR::Instr      *loadInputParamCountInstr = this->m_lowererMD.LoadInputParamCount(ldElem, -1 /* don't include 'this' while counting actuals. */);\\n[DEL] actualParamOpnd = loadInputParamCountInstr->GetDst()->AsRegOpnd();\\n[ADD] actualParamOpnd = loadInputParamCountInstr->GetDst()->UseWithNewType(TyInt32,this->m_func);\\n         }\\n \\n         if (hasIntConstIndex)\",\n",
              " '     };\\n   }\\n \\n[ADD] @Override\\n[ADD] public boolean equals(Object o) {\\n[ADD] if (this == o) return true;\\n[ADD] if (o == null || getClass() != o.getClass()) return false;\\n[ADD] HoodieLogFile that = (HoodieLogFile) o;\\n[ADD] return path != null ? path.equals(that.path) : that.path == null;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public int hashCode() {\\n[ADD] return path != null ? path.hashCode() : 0;\\n[ADD] }\\n[ADD] \\n   @Override\\n   public String toString() {\\n     return \"HoodieLogFile {\" + path + \\'}\\';',\n",
              " '         #endregion\\n     }\\n \\n[ADD] public class ConsoleHeightToBooleanConverter : IValueConverter\\n[ADD] {\\n[ADD] #region IValueConverter Members\\n[ADD] \\n[ADD] public object Convert(object value, Type targetType, object parameter,\\n[ADD] System.Globalization.CultureInfo culture)\\n[ADD] {\\n[ADD] if ((int)value > 0)\\n[ADD] {\\n[ADD] return true;\\n[ADD] }\\n[ADD] else\\n[ADD] {\\n[ADD] return false;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] public object ConvertBack(object value, Type targetType, object parameter,\\n[ADD] System.Globalization.CultureInfo culture)\\n[ADD] {\\n[ADD] return null;\\n[ADD] }\\n[ADD] \\n[ADD] #endregion\\n[ADD] }\\n[ADD] \\n[ADD] \\n     /// <summary>\\n     /// Obsolete class of ShowHidePreviewBubblesConverter\\n     /// Using of this class will produce compile warnings',\n",
              " '                 format = template.getFormat();\\n             }\\n \\n[DEL] if (volume.getVolumeType().equals(Volume.Type.ROOT) && diskOffering.getDiskSize() > 0 && format != null && format != ImageFormat.ISO) {\\n[DEL] throw new InvalidParameterValueException(\\n[DEL] \"Failed to resize Root volume. The service offering of this Volume has been configured with a root disk size; \"\\n[DEL] + \"on such case a Root Volume can only be resized when changing to another Service Offering with a Root disk size. \"\\n[DEL] + \"For more details please check out the Official Resizing Volumes documentation.\");\\n[DEL] }\\n             newSize = cmd.getSize();\\n             newHypervisorSnapshotReserve = volume.getHypervisorSnapshotReserve();\\n ',\n",
              " '                 }\\n                 backlog = eventBacklogService.getMessagesForEvent(ctx.event(), backlogSize);\\n             } else {\\n[DEL] backlog = eventBacklogService.getMessagesForEvent(ctx.event(), 50);\\n[ADD] backlog = eventBacklogService.getMessagesForEvent(ctx.event(), eventsConfiguration.eventNotificationsBacklog());\\n             }\\n         } catch (NotFoundException e) {\\n             LOG.error(\"Failed to fetch backlog for event {}\", ctx.event().id());',\n",
              " '         // the flow, preventing the enforcement(s) from being activated.\\n         // The motivation for this is avoiding enforcements over faulty pipelines.\\n         statement.evaluate();\\n[DEL] enforcement.get().afterUserCodeFinished();\\n[ADD] \\n[ADD] // If construction failed, then to reach here the test code must have caught the exception\\n[ADD] // and succeeded. This suggests the test was verifying a specific construction error, and\\n[ADD] // thus is expected.\\n[ADD] if (!constructionHasFailed) {\\n[ADD] enforcement.get().afterUserCodeFinished();\\n[ADD] }\\n       }\\n     };\\n   }',\n",
              " ' \\n   @Override\\n   public RayObject call(RayFunc func, Object[] args, CallOptions options) {\\n[ADD] Preconditions.checkState(!workerContext.getCurrentDriverId().isNil());\\n     TaskSpec spec = createTaskSpec(func, RayActorImpl.NIL, args, false, options);\\n     rayletClient.submitTask(spec);\\n     return new RayObjectImpl(spec.returnIds[0]);',\n",
              " ' #endif\\n #ifdef REJIT_STATS\\n         , rejitStatsMap(nullptr)\\n[ADD] , bailoutReasonCounts(nullptr)\\n[ADD] , bailoutReasonCountsCap(nullptr)\\n #endif\\n #ifdef ENABLE_BASIC_TELEMETRY\\n         , telemetry(nullptr)',\n",
              " '   private final HashMap<String, Object> kafkaParams;\\n   private final TypedProperties props;\\n   protected final String topicName;\\n[ADD] private final String kafkaCheckpointTimestamp;\\n \\n   public KafkaOffsetGen(TypedProperties props) {\\n     this.props = props;',\n",
              " '     // Do nothing by default\\n   }\\n \\n[ADD] protected void onMethodReferenceFound(MethodReferenceTree methodReferenceTree) {\\n[ADD] // Do nothing by default\\n[ADD] }\\n[ADD] \\n   private List<MethodMatcher> matchers() {\\n     if (matchers == null) {\\n       matchers = getMethodInvocationMatchers();',\n",
              " ' \\n   /**\\n    * Login to salesforce\\n[DEL] * @return login status\\n[ADD] * @return login status\\n    */\\n   public boolean bulkApiLogin() throws Exception {\\n[DEL] this.log.info(\"Authenticating salesforce bulk api\");\\n[ADD] log.info(\"Authenticating salesforce bulk api\");\\n     boolean success = false;\\n     String hostName = this.workUnit.getProp(ConfigurationKeys.SOURCE_CONN_HOST_NAME);\\n     String apiVersion = this.workUnit.getProp(ConfigurationKeys.SOURCE_CONN_VERSION);',\n",
              " '                     onCheck(position, view);\\n                 } else {\\n                     // load up the card selected on the list\\n[DEL] mCurrentCardId = Long.parseLong(getCards().get(position).get(\"id\"));\\n[DEL] sCardBrowserCard = getCol().getCard(mCurrentCardId);\\n[ADD] sCardBrowserCard = getCards().get(position);\\n[ADD] mCurrentCardId = sCardBrowserCard.getId();\\n                     // start note editor using the card we just loaded\\n                     Intent editCard = new Intent(CardBrowser.this, NoteEditor.class);\\n                     editCard.putExtra(NoteEditor.EXTRA_CALLER, NoteEditor.CALLER_CARDBROWSER_EDIT);',\n",
              " ' \\n     @Override\\n     public String getEventDescription() {\\n[DEL] if (getHostId() != null) {\\n[DEL] return \"Attempting to migrate VM Id: \" + this._uuidMgr.getUuid(VirtualMachine.class, getVirtualMachineId()) + \" to host Id: \" + this._uuidMgr.getUuid(Host.class, getHostId());\\n[DEL] } else if (getStoragePoolId() != null) {\\n[DEL] return \"Attempting to migrate VM Id: \" + this._uuidMgr.getUuid(VirtualMachine.class, getVirtualMachineId()) + \" to storage pool Id: \" + this._uuidMgr.getUuid(StoragePool.class, getStoragePoolId());\\n[DEL] } else {\\n[DEL] return \"Attempting to migrate VM Id: \" + this._uuidMgr.getUuid(VirtualMachine.class, getVirtualMachineId());\\n[DEL] }\\n[ADD] return \"Attempting to migrate VM Id: \" + getVirtualMachineId() + \" to host Id: \" + getHostId();\\n     }\\n \\n     @Override',\n",
              " '     }\\n   }\\n \\n[ADD] private static void getAllTransformations( RepositoryDirectoryInterface repositoryObject,\\n[ADD] List<RepositoryElementMetaInterface> objectsTransformations ) throws KettleException {\\n[ADD] //test if has sub-directories\\n[ADD] if ( repositoryObject.getChildren() != null && repositoryObject.getChildren().size() > 0 ) {\\n[ADD] for ( RepositoryDirectoryInterface subDirectory : repositoryObject.getChildren() ) {\\n[ADD] getAllTransformations( subDirectory, objectsTransformations );\\n[ADD] }\\n[ADD] }\\n[ADD] //getting all the transformations\\n[ADD] repositoryObject.getRepositoryObjects().stream()\\n[ADD] .filter( e -> RepositoryObjectType.TRANSFORMATION.equals( e.getObjectType() ) )\\n[ADD] .forEach( objectsTransformations::add );\\n[ADD] }\\n[ADD] \\n   public void setSelectedTrashFileItems( List<UIDeletedObject> selectedTrashFileItems ) {\\n     this.selectedTrashFileItems = selectedTrashFileItems;\\n   }',\n",
              " '         RootBeanDefinition beanDefinition = new RootBeanDefinition();\\n         beanDefinition.setBeanClass(beanClass);\\n         beanDefinition.setLazyInit(false);\\n[DEL] String id = resolveAttribute(element, \"id\", parserContext);\\n[DEL] if (StringUtils.isEmpty(id) && required) {\\n[DEL] String generatedBeanName = resolveAttribute(element, \"name\", parserContext);\\n[DEL] if (StringUtils.isEmpty(generatedBeanName)) {\\n[DEL] if (ProtocolConfig.class.equals(beanClass)) {\\n[DEL] generatedBeanName = \"dubbo\";\\n[DEL] } else {\\n[DEL] generatedBeanName = resolveAttribute(element, \"interface\", parserContext);\\n[DEL] }\\n[DEL] }\\n[DEL] if (StringUtils.isEmpty(generatedBeanName)) {\\n[DEL] generatedBeanName = beanClass.getName();\\n[DEL] }\\n[DEL] id = generatedBeanName;\\n[DEL] int counter = 2;\\n[DEL] while (parserContext.getRegistry().containsBeanDefinition(id)) {\\n[DEL] id = generatedBeanName + (counter++);\\n[DEL] }\\n[DEL] }\\n[ADD] String id = generateId(element, parserContext, beanClass, required);\\n[ADD] \\n         if (StringUtils.isNotEmpty(id)) {\\n             if (parserContext.getRegistry().containsBeanDefinition(id)) {\\n                 throw new IllegalStateException(\"Duplicate spring bean id \" + id);',\n",
              " ' import org.mule.runtime.api.metadata.MediaType;\\n import org.mule.runtime.api.metadata.TypedValue;\\n import org.mule.runtime.core.util.collection.ImmutableMapCollector;\\n[ADD] import org.mule.runtime.extension.api.runtime.source.OnTerminateResult;\\n import org.mule.runtime.extension.api.annotation.param.Config;\\n import org.mule.runtime.extension.api.annotation.param.Connection;\\n import org.mule.runtime.extension.api.annotation.param.ParameterGroup;',\n",
              " ' public class TimestampBasedCopyableDatasetTest {\\n \\n   private FileSystem localFs;\\n[ADD] private Path testTempPath;\\n \\n   @BeforeTest\\n[DEL] public void before()\\n[DEL] throws IOException {\\n[ADD] public void before() throws IOException {\\n     this.localFs = FileSystem.getLocal(new Configuration());\\n[ADD] this.testTempPath = new Path(Files.createTempDir().getAbsolutePath(), \"TbCopyDatasetTest\");\\n[ADD] this.localFs.mkdirs(this.testTempPath);\\n[ADD] }\\n[ADD] \\n[ADD] @AfterTest\\n[ADD] public void cleanUp() {\\n[ADD] try {\\n[ADD] this.localFs.delete(this.testTempPath, true);\\n[ADD] } catch (Exception e) {\\n[ADD] // ignore\\n[ADD] }\\n   }\\n \\n   /**',\n",
              " '       .sinceVersion(\"0.10.0\")\\n       .withDocumentation(\"Enable full scanning of log files while reading log records. If disabled, hudi does look up of only interested entries.\");\\n \\n[ADD] public static final ConfigProperty<Boolean> ENABLE_META_INDEX_BLOOM_FILTER = ConfigProperty\\n[ADD] .key(METADATA_PREFIX + \".index.bloomfilter.enable\")\\n[ADD] .defaultValue(false)\\n[ADD] .sinceVersion(\"0.11.0\")\\n[ADD] .withDocumentation(\"Enable indexing user data files bloom filters under metadata table. When enabled, \"\\n[ADD] + \"a new partition under metadata table will be created to store the bloom filter index and will be \"\\n[ADD] + \"used during the index lookups.\");\\n[ADD] \\n[ADD] public static final ConfigProperty<Boolean> ENABLE_META_INDEX_COLUMN_STATS = ConfigProperty\\n[ADD] .key(METADATA_PREFIX + \".index.column.stats.enable\")\\n[ADD] .defaultValue(false)\\n[ADD] .sinceVersion(\"0.11.0\")\\n[ADD] .withDocumentation(\"Enable indexing user data files column ranges under metadata table key lookups. When \"\\n[ADD] + \"enabled, a new partition under metadata table will be created to store the column ranges and will \"\\n[ADD] + \"used for pruning files during the index lookups.\");\\n[ADD] \\n[ADD] public static final ConfigProperty<Boolean> META_INDEX_COLUMN_STATS_FOR_ALL_COLUMNS = ConfigProperty\\n[ADD] .key(METADATA_PREFIX + \".index.column.stats.all_columns\")\\n[ADD] .defaultValue(false)\\n[ADD] .sinceVersion(\"0.11.0\")\\n[ADD] .withDocumentation(\"Enable indexing user data files column ranges under metadata table key lookups. When \"\\n[ADD] + \"enabled, a new partition under metadata table will be created to store the column ranges and will \"\\n[ADD] + \"used for pruning files during the index lookups.\");\\n[ADD] \\n[ADD] public static final ConfigProperty<Boolean> ENABLE_META_INDEX_BLOOM_FILTER_BATCH_LOAD_MODE = ConfigProperty\\n[ADD] .key(METADATA_PREFIX + \".index.bloomfilter.batchload.enable\")\\n[ADD] .defaultValue(true)\\n[ADD] .sinceVersion(\"0.11.0\")\\n[ADD] .withDocumentation(\"Enable batch/bulk loading of bloom filter index for the entire partition when looking \"\\n[ADD] + \"up index. This is useful when upserting large set of records under the same partition.\");\\n[ADD] \\n   public static final ConfigProperty<Boolean> POPULATE_META_FIELDS = ConfigProperty\\n       .key(METADATA_PREFIX + \".populate.meta.fields\")\\n       .defaultValue(true)',\n",
              " '         }\\n \\n         ContextHandlerCollection handlerCollection = new ContextHandlerCollection();\\n[DEL] Context context = new Context(handlerCollection, ROOT, Context.NO_SECURITY);\\n[DEL] context.setConnectorNames(new String[]{connector.getName()});\\n[DEL] context.addEventListener(new MuleServletContextListener(muleContext, getName()));\\n \\n         if (resourceBase != null)\\n         {\\n[DEL] Context resourceContext = new Context(handlerCollection, path, Context.NO_SECURITY);\\n[DEL] resourceContext.setResourceBase(resourceBase);\\n[ADD] ResourceHandler resourceHandler = new ResourceHandler();\\n[ADD] ContextHandler resourceContextHandler = new ContextHandler(handlerCollection, path);\\n[ADD] resourceHandler.setResourceBase(resourceBase);\\n[ADD] resourceContextHandler.setHandler(resourceHandler);\\n         }\\n \\n[DEL] context.addServlet(JarResourceServlet.class, JarResourceServlet.DEFAULT_PATH_SPEC);\\n[ADD] ServletContextHandler servletContext = new ServletContextHandler(handlerCollection, ROOT, ServletContextHandler.NO_SECURITY);\\n[ADD] servletContext.addEventListener(new MuleServletContextListener(muleContext, getName()));\\n \\n         ServletHolder holder = new ServletHolder();\\n         holder.setServlet(servlet);\\n[DEL] context.addServlet(holder, \"/*\");\\n[DEL] getHttpServer().addHandler(handlerCollection);\\n[ADD] servletContext.addServlet(holder, \"/*\");\\n[ADD] servletContext.addServlet(JarResourceServlet.class, JarResourceServlet.DEFAULT_PATH_SPEC);\\n[ADD] handlerCollection.addHandler(servletContext);\\n[ADD] \\n[ADD] addHandler(handlerCollection);\\n         return servlet;\\n     }\\n ',\n",
              " '       if (symbol.kind == JavaSymbol.VAR) {\\n         if(isAccessible(env, site, symbol)) {\\n           resolution.symbol = symbol;\\n[DEL] resolution.type = resolveTypeSubstitution(symbol.type, c.type);\\n[ADD] resolution.type = typeSubstitutionSolver.applySiteSubstitution(symbol.type, c.type);\\n           return resolution;\\n         } else {\\n           return Resolution.resolution(new AccessErrorJavaSymbol(symbol, Symbols.unknownType));',\n",
              " '     /*allowInObjectBeforeCollectCallback*/true);\\n }\\n \\n[ADD] CHAKRA_API JsGetProxyProperties (_In_ JsValueRef object, _Out_ bool* isProxy, _Out_opt_ JsValueRef* target, _Out_opt_ JsValueRef* handler)\\n[ADD] {\\n[ADD] return ContextAPINoScriptWrapper_NoRecord([&](Js::ScriptContext * scriptContext) -> JsErrorCode {\\n[ADD] VALIDATE_INCOMING_REFERENCE(object, scriptContext);\\n[ADD] PARAM_NOT_NULL(isProxy);\\n[ADD] \\n[ADD] if (target != nullptr)\\n[ADD] {\\n[ADD] *target = JS_INVALID_REFERENCE;\\n[ADD] }\\n[ADD] \\n[ADD] if (handler != nullptr)\\n[ADD] {\\n[ADD] *handler = JS_INVALID_REFERENCE;\\n[ADD] }\\n[ADD] \\n[ADD] *isProxy = Js::JavascriptProxy::Is(object);\\n[ADD] \\n[ADD] if (!*isProxy)\\n[ADD] {\\n[ADD] return JsNoError;\\n[ADD] }\\n[ADD] \\n[ADD] Js::JavascriptProxy* proxy = Js::JavascriptProxy::UnsafeFromVar(object);\\n[ADD] bool revoked = proxy->IsRevoked();\\n[ADD] \\n[ADD] if (target != nullptr && !revoked)\\n[ADD] {\\n[ADD] *target = static_cast<JsValueRef>(proxy->GetTarget());\\n[ADD] }\\n[ADD] \\n[ADD] if (handler != nullptr && !revoked)\\n[ADD] {\\n[ADD] *handler = static_cast<JsValueRef>(proxy->GetHandler());\\n[ADD] }\\n[ADD] \\n[ADD] return JsNoError;\\n[ADD] },\\n[ADD] /*allowInObjectBeforeCollectCallback*/true);\\n[ADD] }\\n[ADD] \\n #endif // _CHAKRACOREBUILD',\n",
              " '     return stats;\\n   }\\n \\n[ADD] @Override\\n[ADD] public void initializeFinalizeWrite() {\\n[ADD] if (!config.shouldUseTempFolderForCopyOnWrite()) {\\n[ADD] return;\\n[ADD] }\\n[ADD] \\n[ADD] // create temporary folder if needed\\n[ADD] final FileSystem fs = FSUtils.getFs();\\n[ADD] final Path temporaryFolder = new Path(config.getBasePath(), HoodieTableMetaClient.TEMPFOLDER_NAME);\\n[ADD] try {\\n[ADD] if (!fs.exists(temporaryFolder)) {\\n[ADD] fs.mkdirs(temporaryFolder);\\n[ADD] }\\n[ADD] } catch (IOException e) {\\n[ADD] throw new HoodieIOException(\"Failed to create temporary folder: \" + temporaryFolder);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] @SuppressWarnings(\"unchecked\")\\n[ADD] public Optional<Integer> finalizeWrite(JavaSparkContext jsc, List writeStatuses) {\\n[ADD] if (!config.shouldUseTempFolderForCopyOnWrite()) {\\n[ADD] return Optional.empty();\\n[ADD] }\\n[ADD] \\n[ADD] // This is to rename each data file from temporary path to its final location\\n[ADD] List<Tuple2<String, Boolean>> results = jsc.parallelize(writeStatuses, config.getFinalizeWriteParallelism())\\n[ADD] .map(writeStatus -> {\\n[ADD] Tuple2<String, HoodieWriteStat> writeStatTuple2 = (Tuple2<String, HoodieWriteStat>) writeStatus;\\n[ADD] HoodieWriteStat writeStat = writeStatTuple2._2();\\n[ADD] final FileSystem fs = FSUtils.getFs();\\n[ADD] final Path finalPath = new Path(config.getBasePath(), writeStat.getPath());\\n[ADD] \\n[ADD] if (writeStat.getTempPath() != null) {\\n[ADD] final Path tempPath = new Path(config.getBasePath(), writeStat.getTempPath());\\n[ADD] boolean success;\\n[ADD] try {\\n[ADD] logger.info(\"Renaming temporary file: \" + tempPath + \" to \" + finalPath);\\n[ADD] success = fs.rename(tempPath, finalPath);\\n[ADD] } catch (IOException e) {\\n[ADD] throw new HoodieIOException(\"Failed to rename file: \" + tempPath + \" to \" + finalPath);\\n[ADD] }\\n[ADD] \\n[ADD] if (!success) {\\n[ADD] throw new HoodieIOException(\"Failed to rename file: \" + tempPath + \" to \" + finalPath);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] return new Tuple2<>(writeStat.getPath(), true);\\n[ADD] }).collect();\\n[ADD] \\n[ADD] return Optional.of(results.size());\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public void cleanTemporaryDataFiles(JavaSparkContext jsc) {\\n[ADD] if (!config.shouldUseTempFolderForCopyOnWrite()) {\\n[ADD] return;\\n[ADD] }\\n[ADD] \\n[ADD] final FileSystem fs = FSUtils.getFs();\\n[ADD] final Path temporaryFolder = new Path(config.getBasePath(),\\n[ADD] HoodieTableMetaClient.TEMPFOLDER_NAME);\\n[ADD] try {\\n[ADD] if (!fs.exists(temporaryFolder)) {\\n[ADD] logger.info(\"Temporary folder does not exist: \" + temporaryFolder);\\n[ADD] return;\\n[ADD] }\\n[ADD] List<FileStatus> fileStatusesList = Arrays.asList(fs.listStatus(temporaryFolder));\\n[ADD] List<Tuple2<String, Boolean>> results = jsc\\n[ADD] .parallelize(fileStatusesList, config.getFinalizeWriteParallelism())\\n[ADD] .map(fileStatus -> {\\n[ADD] FileSystem fs1 = FSUtils.getFs();\\n[ADD] boolean success = fs1.delete(fileStatus.getPath(), false);\\n[ADD] logger.info(\"Deleting file in temporary folder\" + fileStatus.getPath() + \"\\\\t\"\\n[ADD] + success);\\n[ADD] return new Tuple2<>(fileStatus.getPath().toString(), success);\\n[ADD] }).collect();\\n[ADD] \\n[ADD] for (Tuple2<String, Boolean> result : results) {\\n[ADD] if (!result._2()) {\\n[ADD] logger.info(\"Failed to delete file: \" + result._1());\\n[ADD] throw new HoodieIOException(\\n[ADD] \"Failed to delete file in temporary folder: \" + result._1());\\n[ADD] }\\n[ADD] }\\n[ADD] } catch (IOException e) {\\n[ADD] throw new HoodieIOException(\\n[ADD] \"Failed to clean data files in temporary folder: \" + temporaryFolder);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n   private static class PartitionCleanStat implements Serializable {\\n \\n     private final String partitionPath;',\n",
              " '         serviceOfferingId = serviceOffering.getId();\\n     }\\n \\n[ADD] protected void findAccountServiceOfferingId(long accountId) {\\n[ADD] String accountRouterOffering = VirtualNetworkApplianceManager.VirtualRouterServiceOffering.valueIn(accountId);\\n[ADD] String globalRouterOffering = VirtualNetworkApplianceManager.VirtualRouterServiceOffering.value();\\n[ADD] if (accountRouterOffering != null) {\\n[ADD] verifyServiceOfferingByUuid(accountRouterOffering);\\n[ADD] }\\n[ADD] if (serviceOfferingId == null && globalRouterOffering != accountRouterOffering) {\\n[ADD] verifyServiceOfferingByUuid(globalRouterOffering);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private void verifyServiceOfferingByUuid(String offeringUuid) {\\n[ADD] logger.debug(\"Verifying router service offering with uuid : \" + offeringUuid);\\n[ADD] ServiceOfferingVO serviceOffering = serviceOfferingDao.findByUuid(offeringUuid);\\n[ADD] if (serviceOffering != null && serviceOffering.isSystemUse()) {\\n[ADD] boolean isLocalStorage = ConfigurationManagerImpl.SystemVMUseLocalStorage.valueIn(dest.getDataCenter().getId());\\n[ADD] if (isLocalStorage == serviceOffering.isUseLocalStorage()) {\\n[ADD] logger.debug(String.format(\"Service offering %s (uuid: %s) will be used on virtual router\", serviceOffering.getName(), serviceOffering.getUuid()));\\n[ADD] serviceOfferingId = serviceOffering.getId();\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] \\n     protected void findServiceOfferingId() {\\n         serviceOfferingId = networkOfferingDao.findById(guestNetwork.getNetworkOfferingId()).getServiceOfferingId();\\n[ADD] if (serviceOfferingId == null) {\\n[ADD] findAccountServiceOfferingId(guestNetwork.getAccountId());\\n[ADD] }\\n         if (serviceOfferingId == null) {\\n             findDefaultServiceOfferingId();\\n         }',\n",
              " ' \\n import java.io.BufferedReader;\\n import java.io.IOException;\\n[DEL] import java.io.InputStream;\\n[DEL] import java.io.InputStreamReader;\\n import java.io.OutputStream;\\n import java.net.URLDecoder;\\n import java.util.HashMap;\\n import java.util.List;\\n import java.util.Map;\\n[DEL] \\n[DEL] import com.sun.net.httpserver.Headers;\\n[DEL] import com.sun.net.httpserver.HttpExchange;\\n[DEL] import com.sun.net.httpserver.HttpHandler;\\n[DEL] \\n import com.cloud.consoleproxy.util.Logger;\\n[ADD] import org.eclipse.jetty.server.Request;\\n[ADD] import org.eclipse.jetty.server.handler.AbstractHandler;\\n[ADD] \\n[ADD] import javax.servlet.ServletException;\\n[ADD] import javax.servlet.http.HttpServletRequest;\\n[ADD] import javax.servlet.http.HttpServletResponse;\\n \\n[DEL] public class ConsoleProxyAjaxHandler implements HttpHandler {\\n[ADD] public class ConsoleProxyAjaxHandler extends AbstractHandler {\\n     private static final Logger s_logger = Logger.getLogger(ConsoleProxyAjaxHandler.class);\\n \\n     public ConsoleProxyAjaxHandler() {\\n     }\\n \\n     @Override\\n[DEL] public void handle(HttpExchange t) throws IOException {\\n[ADD] public void handle(String s, Request request, HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse) throws IOException, ServletException {\\n         try {\\n             if (s_logger.isTraceEnabled())\\n[DEL] s_logger.trace(\"AjaxHandler \" + t.getRequestURI());\\n[ADD] s_logger.trace(\"AjaxHandler \" + request.getRequestURI());\\n \\n             long startTick = System.currentTimeMillis();\\n \\n[DEL] doHandle(t);\\n[ADD] doHandle(request, httpServletResponse);\\n \\n             if (s_logger.isTraceEnabled())\\n[DEL] s_logger.trace(t.getRequestURI() + \" process time \" + (System.currentTimeMillis() - startTick) + \" ms\");\\n[ADD] s_logger.trace(request.getRequestURI() + \" process time \" + (System.currentTimeMillis() - startTick) + \" ms\");\\n         } catch (IOException e) {\\n             throw e;\\n         } catch (IllegalArgumentException e) {\\n             s_logger.warn(\"Exception, \", e);\\n[DEL] t.sendResponseHeaders(400, -1);     // bad request\\n[ADD] httpServletResponse.setStatus(400);\\n         } catch (Throwable e) {\\n             s_logger.error(\"Unexpected exception, \", e);\\n[DEL] t.sendResponseHeaders(500, -1);     // server error\\n[ADD] httpServletResponse.setStatus(500);\\n         } finally {\\n[DEL] t.close();\\n[ADD] request.setHandled(true);\\n         }\\n     }\\n \\n[DEL] private void doHandle(HttpExchange t) throws Exception, IllegalArgumentException {\\n[DEL] String queries = t.getRequestURI().getQuery();\\n[ADD] private void doHandle(Request request, HttpServletResponse response) throws Exception, IllegalArgumentException {\\n[ADD] String queries = request.getUri().getQuery();\\n         if (s_logger.isTraceEnabled())\\n             s_logger.trace(\"Handle AJAX request: \" + queries);\\n ',\n",
              " '  */\\n package org.graylog2.system.stats.elasticsearch;\\n \\n[DEL] import com.fasterxml.jackson.databind.JsonNode;\\n[DEL] import com.fasterxml.jackson.databind.node.ArrayNode;\\n[DEL] import com.fasterxml.jackson.databind.node.JsonNodeType;\\n[DEL] import com.google.common.collect.Lists;\\n import com.google.inject.Singleton;\\n[DEL] import io.searchbox.client.JestClient;\\n[DEL] import io.searchbox.client.JestResult;\\n[DEL] import io.searchbox.cluster.Health;\\n[DEL] import io.searchbox.cluster.PendingClusterTasks;\\n[DEL] import io.searchbox.cluster.Stats;\\n import org.graylog2.indexer.IndexSetRegistry;\\n[DEL] import org.graylog2.indexer.cluster.jest.JestUtils;\\n[ADD] import org.graylog2.indexer.cluster.ClusterAdapter;\\n[ADD] import org.graylog2.indexer.cluster.PendingTasksStats;\\n[ADD] import org.graylog2.indexer.indices.HealthStatus;\\n \\n import javax.inject.Inject;\\n import java.util.Arrays;\\n import java.util.List;\\n[DEL] import java.util.Locale;\\n \\n @Singleton\\n public class ElasticsearchProbe {\\n[DEL] private final JestClient jestClient;\\n     private final IndexSetRegistry indexSetRegistry;\\n[ADD] private final ClusterAdapter clusterAdapter;\\n \\n     @Inject\\n[DEL] public ElasticsearchProbe(JestClient jestClient, IndexSetRegistry indexSetRegistry) {\\n[DEL] this.jestClient = jestClient;\\n[ADD] public ElasticsearchProbe(IndexSetRegistry indexSetRegistry, ClusterAdapter clusterAdapter) {\\n         this.indexSetRegistry = indexSetRegistry;\\n[ADD] this.clusterAdapter = clusterAdapter;\\n     }\\n \\n     public ElasticsearchStats elasticsearchStats() {\\n[DEL] final JestResult clusterStatsResponse = JestUtils.execute(jestClient, new Stats.Builder().build(), () -> \"Couldn\\'t read Elasticsearch cluster stats\");\\n[DEL] final JsonNode clusterStatsResponseJson = clusterStatsResponse.getJsonObject();\\n[DEL] final String clusterName = clusterStatsResponseJson.path(\"cluster_name\").asText();\\n[ADD] final ClusterStats clusterStats = clusterAdapter.clusterStats();\\n \\n[DEL] String clusterVersion = null;\\n[DEL] if (clusterStatsResponseJson.path(\"nodes\").path(\"versions\").isArray()) {\\n[DEL] final ArrayNode versions = (ArrayNode) clusterStatsResponseJson.path(\"nodes\").path(\"versions\");\\n[DEL] // We just use the first version in the \"versions\" array. This is not correct if there are different\\n[DEL] // versions running in the cluster, but that is not recommended anyway.\\n[DEL] final JsonNode versionNode = versions.path(0);\\n[DEL] if (versionNode.getNodeType() != JsonNodeType.MISSING) {\\n[DEL] clusterVersion = versionNode.asText();\\n[DEL] }\\n[DEL] }\\n[ADD] final PendingTasksStats pendingTasksStats = clusterAdapter.pendingTasks();\\n \\n[DEL] final JsonNode countStats = clusterStatsResponseJson.path(\"nodes\").path(\"count\");\\n[ADD] final List<String> indices = Arrays.asList(indexSetRegistry.getIndexWildcards());\\n \\n[DEL] final NodesStats nodesStats = NodesStats.create(\\n[DEL] countStats.path(\"total\").asInt(-1),\\n[DEL] countStats.path(\"master_only\").asInt(-1),\\n[DEL] countStats.path(\"data_only\").asInt(-1),\\n[DEL] countStats.path(\"master_data\").asInt(-1),\\n[DEL] countStats.path(\"client\").asInt(-1)\\n[ADD] final ShardStats shardStats = clusterAdapter.shardStats(indices);\\n[ADD] final ClusterHealth clusterHealth = ClusterHealth.from(\\n[ADD] shardStats,\\n[ADD] pendingTasksStats\\n         );\\n[DEL] \\n[DEL] final JsonNode clusterIndicesStats = clusterStatsResponseJson.path(\"indices\");\\n[DEL] final IndicesStats indicesStats = IndicesStats.create(\\n[DEL] clusterIndicesStats.path(\"count\").asInt(-1),\\n[DEL] clusterIndicesStats.path(\"store\").path(\"size_in_bytes\").asLong(-1L),\\n[DEL] clusterIndicesStats.path(\"fielddata\").path(\"memory_size_in_bytes\").asLong(-1L)\\n[DEL] );\\n[DEL] \\n[DEL] final JestResult pendingClusterTasksResponse = JestUtils.execute(jestClient, new PendingClusterTasks.Builder().build(), () -> \"Couldn\\'t read Elasticsearch pending cluster tasks\");\\n[DEL] final JsonNode pendingClusterTasks = pendingClusterTasksResponse.getJsonObject().path(\"tasks\");\\n[DEL] final int pendingTasksSize = pendingClusterTasks.size();\\n[DEL] final List<Long> pendingTasksTimeInQueue = Lists.newArrayListWithCapacity(pendingTasksSize);\\n[DEL] for (JsonNode jsonElement : pendingClusterTasks) {\\n[DEL] if (jsonElement.has(\"time_in_queue_millis\")) {\\n[DEL] pendingTasksTimeInQueue.add(jsonElement.get(\"time_in_queue_millis\").asLong());\\n[DEL] }\\n[DEL] }\\n[DEL] \\n[DEL] final Health clusterHealthRequest = new Health.Builder()\\n[DEL] .addIndex(Arrays.asList(indexSetRegistry.getIndexWildcards()))\\n[DEL] .build();\\n[DEL] final JestResult clusterHealthResponse = JestUtils.execute(jestClient, clusterHealthRequest, () -> \"Couldn\\'t read Elasticsearch cluster health\");\\n[DEL] final JsonNode clusterHealthJson = clusterHealthResponse.getJsonObject();\\n[DEL] final ClusterHealth clusterHealth = ClusterHealth.create(\\n[DEL] clusterHealthJson.path(\"number_of_nodes\").asInt(-1),\\n[DEL] clusterHealthJson.path(\"number_of_data_nodes\").asInt(-1),\\n[DEL] clusterHealthJson.path(\"active_shards\").asInt(-1),\\n[DEL] clusterHealthJson.path(\"relocating_shards\").asInt(-1),\\n[DEL] clusterHealthJson.path(\"active_primary_shards\").asInt(-1),\\n[DEL] clusterHealthJson.path(\"initializing_shards\").asInt(-1),\\n[DEL] clusterHealthJson.path(\"unassigned_shards\").asInt(-1),\\n[DEL] clusterHealthJson.path(\"timed_out\").asBoolean(),\\n[DEL] pendingTasksSize,\\n[DEL] pendingTasksTimeInQueue\\n[DEL] );\\n[DEL] final ElasticsearchStats.HealthStatus healthStatus = getHealthStatus(clusterHealthJson.path(\"status\").asText(\"RED\"));\\n[ADD] final HealthStatus healthStatus = clusterAdapter.health(indices).orElseThrow(() -> new IllegalStateException(\"Unable to retrieve cluster health.\"));\\n \\n         return ElasticsearchStats.create(\\n[DEL] clusterName,\\n[DEL] clusterVersion,\\n[ADD] clusterStats.clusterName(),\\n[ADD] clusterStats.clusterVersion(),\\n                 healthStatus,\\n                 clusterHealth,\\n[DEL] nodesStats,\\n[DEL] indicesStats);\\n[DEL] }\\n[DEL] \\n[DEL] private ElasticsearchStats.HealthStatus getHealthStatus(String status) {\\n[DEL] return ElasticsearchStats.HealthStatus.valueOf(status.toUpperCase(Locale.ENGLISH));\\n[ADD] clusterStats.nodesStats(),\\n[ADD] clusterStats.indicesStats()\\n[ADD] );\\n     }\\n }',\n",
              " '  */\\n package org.graylog2.alerts;\\n \\n[ADD] import com.floreysoft.jmte.Engine;\\n import org.graylog2.configuration.EmailConfiguration;\\n import org.graylog2.notifications.NotificationService;\\n import org.graylog2.plugin.Message;',\n",
              " '       return;\\n     }\\n \\n[ADD] LOG.info(\"Processing input split : {}\", split);\\n[ADD] \\n     format.open(split);\\n     try {\\n       RowData nextElement = null;',\n",
              " ' \\n import org.apache.avro.Schema;\\n import org.apache.avro.file.CodecFactory;\\n[ADD] import org.apache.avro.file.DataFileConstants;\\n import org.apache.avro.file.DataFileWriter;\\n import org.apache.avro.generic.GenericDatumWriter;\\n import org.apache.avro.generic.GenericRecord;\\n import org.apache.avro.io.DatumWriter;\\n[ADD] \\n import org.apache.hadoop.fs.FSDataOutputStream;\\n import org.apache.hadoop.fs.Path;\\n[ADD] import org.apache.hadoop.fs.permission.FsPermission;\\n[ADD] \\n import org.slf4j.Logger;\\n import org.slf4j.LoggerFactory;\\n \\n[ADD] import com.google.common.base.Optional;\\n import com.google.common.base.Preconditions;\\n \\n import gobblin.configuration.ConfigurationKeys;',\n",
              " ' import org.mule.runtime.core.api.connectivity.ConnectivityTestingStrategy;\\n import org.mule.runtime.core.api.context.notification.MuleContextListener;\\n import org.mule.runtime.core.util.UUID;\\n[ADD] import org.mule.runtime.deployment.model.api.artifact.ArtifactContext;\\n import org.mule.runtime.deployment.model.api.plugin.ArtifactPlugin;\\n import org.mule.runtime.deployment.model.api.plugin.ArtifactPluginDescriptor;\\n[ADD] import org.mule.runtime.dsl.api.config.ArtifactConfiguration;\\n import org.mule.runtime.module.artifact.classloader.MuleDeployableArtifactClassLoader;\\n[DEL] import org.mule.runtime.module.deployment.internal.artifact.ArtifactContext;\\n import org.mule.runtime.module.deployment.internal.artifact.ArtifactContextBuilder;\\n import org.mule.runtime.module.deployment.internal.artifact.TemporaryArtifact;\\n import org.mule.runtime.module.deployment.internal.artifact.TemporaryArtifactBuilder;',\n",
              " '         /// </summary>\\n         protected override void SetItemsCore(IList value)\\n         {\\n[ADD] if (value == null)\\n[ADD] {\\n[ADD] throw new ArgumentNullException(nameof(value));\\n[ADD] }\\n[ADD] \\n             BeginUpdate();\\n             Items.ClearInternal();\\n             Items.AddRangeInternal(value);',\n",
              " ' import java.util.HashMap;\\n import java.util.List;\\n import java.util.Map;\\n[ADD] import java.util.concurrent.atomic.AtomicInteger;\\n[ADD] import org.aeonbits.owner.ConfigFactory;\\n import org.ray.api.RayActor;\\n[ADD] import org.ray.streaming.api.Language;\\n import org.ray.streaming.jobgraph.JobVertex;\\n import org.ray.streaming.jobgraph.VertexType;\\n import org.ray.streaming.operator.StreamOperator;\\n[DEL] import org.ray.streaming.runtime.master.JobRuntimeContext;\\n[ADD] import org.ray.streaming.runtime.config.master.ResourceConfig;\\n import org.ray.streaming.runtime.worker.JobWorker;\\n \\n /**',\n",
              " ' import org.mule.runtime.extension.api.annotation.Extension;\\n import org.mule.runtime.extension.api.annotation.ExtensionOf;\\n import org.mule.runtime.extension.api.annotation.Operations;\\n[DEL] import org.mule.runtime.extension.api.annotation.Parameter;\\n import org.mule.runtime.extension.api.annotation.RestrictedTo;\\n import org.mule.runtime.extension.api.annotation.dsl.xml.XmlHints;\\n import org.mule.runtime.extension.api.annotation.metadata.MetadataKeyId;\\n import org.mule.runtime.extension.api.annotation.param.Connection;\\n[ADD] import org.mule.runtime.extension.api.annotation.param.NullSafe;\\n[ADD] import org.mule.runtime.extension.api.annotation.param.Parameter;\\n[ADD] import org.mule.runtime.extension.api.annotation.param.ParameterGroup;\\n import org.mule.runtime.extension.api.annotation.param.UseConfig;\\n[ADD] import org.mule.runtime.extension.api.annotation.source.EmitsResponse;\\n import org.mule.runtime.extension.api.exception.IllegalModelDefinitionException;\\n import org.mule.runtime.extension.api.introspection.declaration.DescribingContext;\\n import org.mule.runtime.extension.api.introspection.declaration.spi.Describer;',\n",
              " '   }\\n \\n   private RexNode convertResolvedStructFieldAccess(ResolvedGetStructField resolvedGetStructField) {\\n[DEL] return rexBuilder()\\n[DEL] .makeFieldAccess(\\n[DEL] convertRexNodeFromResolvedExpr(resolvedGetStructField.getExpr()),\\n[DEL] (int) resolvedGetStructField.getFieldIdx());\\n[ADD] RexNode referencedExpr = convertRexNodeFromResolvedExpr(resolvedGetStructField.getExpr());\\n[ADD] return convertResolvedStructFieldAccessInternal(\\n[ADD] referencedExpr, (int) resolvedGetStructField.getFieldIdx());\\n   }\\n \\n   private RexNode convertResolvedStructFieldAccess(\\n       ResolvedGetStructField resolvedGetStructField,\\n       List<ResolvedColumn> columnList,\\n       List<RelDataTypeField> fieldList) {\\n[DEL] return rexBuilder()\\n[DEL] .makeFieldAccess(\\n[DEL] convertRexNodeFromResolvedExpr(resolvedGetStructField.getExpr(), columnList, fieldList),\\n[DEL] (int) resolvedGetStructField.getFieldIdx());\\n[ADD] RexNode referencedExpr =\\n[ADD] convertRexNodeFromResolvedExpr(resolvedGetStructField.getExpr(), columnList, fieldList);\\n[ADD] return convertResolvedStructFieldAccessInternal(\\n[ADD] referencedExpr, (int) resolvedGetStructField.getFieldIdx());\\n[ADD] }\\n[ADD] \\n[ADD] private RexNode convertResolvedStructFieldAccessInternal(RexNode referencedExpr, int fieldIdx) {\\n[ADD] // Calcite SQL does not allow the ROW constructor to be dereferenced directly, so do it here.\\n[ADD] if (referencedExpr instanceof RexCall\\n[ADD] && ((RexCall) referencedExpr).getOperator() instanceof SqlRowOperator) {\\n[ADD] return ((RexCall) referencedExpr).getOperands().get(fieldIdx);\\n[ADD] }\\n[ADD] return rexBuilder().makeFieldAccess(referencedExpr, fieldIdx);\\n   }\\n \\n   private RexBuilder rexBuilder() {',\n",
              " ' import android.widget.AdapterView;\\n import android.widget.ProgressBar;\\n import android.widget.Spinner;\\n[DEL] import android.widget.TextView;\\n \\n[ADD] import com.google.android.material.tabs.TabLayout;\\n[ADD] import com.google.android.material.tabs.TabLayoutMediator;\\n import com.ichi2.anim.ActivityTransitionAnimation;\\n import com.ichi2.anki.stats.AnkiStatsTaskHandler;\\n import com.ichi2.anki.stats.ChartView;',\n",
              " '         }.getType()))\\n         .defaultingTo(\"#[payload]\")\\n         .withExpressionSupport(REQUIRED)\\n[DEL] .describedAs(\"An expression to that returns a java collection, object array, map or DOM nodes.\");\\n[ADD] .describedAs(\"Expression that defines the collection to iterate over.\");\\n \\n     forEach.onDefaultParameterGroup()\\n         .withOptionalParameter(\"batchSize\")\\n         .ofType(typeLoader.load(Integer.class))\\n         .withExpressionSupport(NOT_SUPPORTED)\\n[DEL] .describedAs(\"An expression to that returns a java collection, object array, map or DOM nodes.\");\\n[ADD] .describedAs(\"Partitions the collection in sub-collections of the specified size.\");\\n \\n     forEach.onDefaultParameterGroup()\\n         .withOptionalParameter(\"rootMessageVariableName\")\\n         .ofType(typeLoader.load(String.class))\\n         .defaultingTo(\"rootMessage\")\\n         .withExpressionSupport(NOT_SUPPORTED)\\n[DEL] .describedAs(\"Property name where the parent message is stored.\");\\n[ADD] .describedAs(\"Variable name for the original message.\");\\n \\n     forEach.onDefaultParameterGroup()\\n         .withOptionalParameter(\"counterVariableName\")\\n         .ofType(typeLoader.load(String.class))\\n         .defaultingTo(\"counter\")\\n         .withExpressionSupport(NOT_SUPPORTED)\\n[DEL] .describedAs(\"Property name used to store the number of message being iterated.\");\\n[ADD] .describedAs(\"Variable name for the item number being processed.\");\\n \\n   }\\n \\n   private void declareUntilSuccessful(ExtensionDeclarer extensionDeclarer, ClassTypeLoader typeLoader) {\\n     ConstructDeclarer untilSuccessful = extensionDeclarer.withConstruct(\"untilSuccessful\")\\n         .describedAs(\"Attempts to route a message to the message processor it contains in an asynchronous manner. \" +\\n[DEL] \"Routing is considered successful if no exception has been raised and, optionally, if the response matches an expression\");\\n[ADD] \"Routing is considered successful if no error has been raised and, optionally, if the response matches an expression.\");\\n \\n     untilSuccessful.withChain();\\n ',\n",
              " ' import com.google.common.base.Strings;\\n import com.google.common.collect.ImmutableList;\\n import org.apache.commons.mail.DefaultAuthenticator;\\n[DEL] import org.apache.commons.mail.Email;\\n[ADD] import org.apache.commons.mail.HtmlEmail;\\n import org.apache.commons.mail.EmailConstants;\\n import org.apache.commons.mail.EmailException;\\n[DEL] import org.apache.commons.mail.SimpleEmail;\\n import org.graylog.events.notifications.EventBacklogService;\\n import org.graylog.events.notifications.EventNotificationContext;\\n import org.graylog.events.notifications.EventNotificationModelData;',\n",
              " '     }\\n   }\\n \\n[ADD] @Override\\n[ADD] public void processWatermark(Instant watermark, OpEmitter<OutT> emitter) {\\n[ADD] if (!isBundleStarted.get()) {\\n[ADD] doProcessWatermark(watermark, emitter);\\n[ADD] } else {\\n[ADD] // if there is a bundle in progress, hold back the watermark until end of the bundle\\n[ADD] this.bundleWatermarkHold = watermark;\\n[ADD] if (watermark.isEqual(BoundedWindow.TIMESTAMP_MAX_VALUE)) {\\n[ADD] // for batch mode, the max watermark should force the bundle to close\\n[ADD] finishBundle(emitter);\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n[ADD] \\n   @Override\\n   public void processSideInput(\\n       String id, WindowedValue<? extends Iterable<?>> elements, OpEmitter<OutT> emitter) {\\n[ADD] checkState(\\n[ADD] !isBundleStarted.get(),\\n[ADD] \"Side input not supported in bundling mode. Please disable bundling.\");\\n     @SuppressWarnings(\"unchecked\")\\n     final WindowedValue<Iterable<?>> retypedElements = (WindowedValue<Iterable<?>>) elements;\\n ',\n",
              " '         SwitchCase c = (SwitchCase) o;\\n         List<ExpressionTree> expressions = new ArrayList<>();\\n         for (Object oo : c.expressions()) {\\n[DEL] expressions.add(convertExpression((Expression) oo));\\n[ADD] Expression e = (Expression) oo;\\n[ADD] ExpressionTree expression = previewEnabled ? convertExpressionFromCase(e) : convertExpression(e);\\n[ADD] expressions.add(expression);\\n         }\\n \\n         caselabels.add(new CaseLabelTreeImpl(',\n",
              " \"     this.liveStoryManager_ = null;\\n \\n     /** @private @const {!LocalizationService} */\\n[DEL] this.localizationService_ = new LocalizationService(this.win);\\n[ADD] this.localizationService_ = new LocalizationService(this.win, this.viewer_);\\n     const localizationService = this.localizationService_;\\n     this.localizationService_\\n       .registerLocalizedStringBundle('default', LocalizedStringsDefault)\",\n",
              " ' import com.fasterxml.jackson.annotation.JsonValue;\\n import com.google.auto.value.AutoValue;\\n import org.apache.shiro.authz.Permission;\\n[ADD] import org.apache.shiro.authz.permission.WildcardPermission;\\n import org.graylog.grn.GRN;\\n \\n @AutoValue',\n",
              " '   public long getFileMTime(String filePath) throws FileBasedHelperException {\\n     try {\\n       ChannelSftp channelSftp = getSftpChannel();\\n[DEL] return channelSftp.lstat(filePath).getMTime();\\n[ADD] int modificationTime = channelSftp.lstat(filePath).getMTime();\\n[ADD] channelSftp.disconnect();\\n[ADD] return modificationTime;\\n     } catch (SftpException e) {\\n       throw new FileBasedHelperException(String\\n           .format(\"Failed to get modified timestamp for file at path %s due to error %s\", filePath, e.getMessage()), e);',\n",
              " ' \\n         String value = _configDao.getValue(Config.BackupSnapshotWait.toString());\\n \\n[DEL] Type.HOURLY.setMax(SnapshotHourlyMax.value());\\n[DEL] Type.DAILY.setMax(SnapshotDailyMax.value());\\n[DEL] Type.WEEKLY.setMax(SnapshotWeeklyMax.value());\\n[DEL] Type.MONTHLY.setMax(SnapshotMonthlyMax.value());\\n[ADD] Type.HOURLY.setMax(snapshotHourlyMax.value());\\n[ADD] Type.DAILY.setMax(snapshotDailyMax.value());\\n[ADD] Type.WEEKLY.setMax(snapshotWeeklyMax.value());\\n[ADD] Type.MONTHLY.setMax(snapshotMonthlyMax.value());\\n         _totalRetries = NumbersUtil.parseInt(_configDao.getValue(\"total.retries\"), 4);\\n         _pauseInterval = 2 * NumbersUtil.parseInt(_configDao.getValue(\"ping.interval\"), 60);\\n \\n[DEL] snapshotBackupRetries = BackupRetryAttempts.value();\\n[DEL] snapshotBackupRetryInterval = BackupRetryInterval.value();\\n[ADD] snapshotBackupRetries = backupRetryAttempts.value();\\n[ADD] snapshotBackupRetryInterval = backupRetryInterval.value();\\n         backupSnapshotExecutor = Executors.newScheduledThreadPool(10, new NamedThreadFactory(\"BackupSnapshotTask\"));\\n         s_logger.info(\"Snapshot Manager is configured.\");\\n ',\n",
              " ' \\t\\t}\\n \\t\\treturn entry;\\n \\t}\\n[ADD] /*[ENDIF] Sidecar19-SE */\\n \\t\\n \\tprivate ClassInfo scanClassFile(InputStream file, URL url) throws IOException {\\n \\t\\tClassScanner scanner = new ClassScanner(url, listeners);',\n",
              " ' \\t\\t\\t\\t.verify(Duration.ofSeconds(10));\\n \\t}\\n \\n[ADD] @Test\\n[ADD] @RedisAvailable\\n[ADD] public void testReadingPendingMessageWithNoAutoACK() {\\n[ADD] Address address = new Address(\"Winterfell, Westeros\");\\n[ADD] Person person = new Person(address, \"John Snow\");\\n[ADD] \\n[ADD] this.template.opsForStream()\\n[ADD] .createGroup(STREAM_KEY, this.redisStreamMessageProducer.getBeanName())\\n[ADD] .as(StepVerifier::create)\\n[ADD] .assertNext(message -> assertThat(message).isEqualTo(\"OK\"))\\n[ADD] .thenCancel()\\n[ADD] .verify(Duration.ofSeconds(10));\\n[ADD] \\n[ADD] this.redisStreamMessageProducer.setCreateConsumerGroup(false);\\n[ADD] this.redisStreamMessageProducer.setAutoAck(false);\\n[ADD] this.redisStreamMessageProducer.setConsumerName(CONSUMER);\\n[ADD] this.redisStreamMessageProducer.afterPropertiesSet();\\n[ADD] this.redisStreamMessageProducer.start();\\n[ADD] \\n[ADD] this.messageHandler.handleMessage(new GenericMessage<>(person));\\n[ADD] \\n[ADD] Mono<PendingMessagesSummary> pending = template.opsForStream().pending(STREAM_KEY, this.redisStreamMessageProducer.getBeanName());\\n[ADD] StepVerifier.create(pending)\\n[ADD] .expectSubscription()\\n[ADD] .expectNextCount(1)\\n[ADD] .verifyComplete();\\n[ADD] }\\n[ADD] \\n \\t@Configuration\\n \\tstatic class ContextConfig {\\n ',\n",
              " ' import org.mule.runtime.api.meta.model.declaration.fluent.ParameterGroupDeclaration;\\n import org.mule.runtime.api.meta.model.declaration.fluent.ParameterizedDeclaration;\\n import org.mule.runtime.api.meta.model.declaration.fluent.SourceDeclaration;\\n[ADD] import org.mule.runtime.api.meta.model.parameter.ParameterModel;\\n import org.mule.runtime.api.meta.model.parameter.ValueProviderModel;\\n import org.mule.runtime.api.util.Reference;\\n import org.mule.runtime.extension.api.annotation.param.Config;',\n",
              " '    */\\n   public static class DagManagerThread implements Runnable {\\n     private final Map<DagNode<JobExecutionPlan>, Dag<JobExecutionPlan>> jobToDag = new HashMap<>();\\n[ADD] private static final Map<String, Integer> proxyUserToJobCount = new ConcurrentHashMap<>();\\n[ADD] private static final Map<String, Integer> requesterToJobCount = new ConcurrentHashMap<>();\\n     private final Map<String, Dag<JobExecutionPlan>> dags = new HashMap<>();\\n     // dagToJobs holds a map of dagId to running jobs of that dag\\n     final Map<String, LinkedList<DagNode<JobExecutionPlan>>> dagToJobs = new HashMap<>();',\n",
              " '         try {\\n           schema = new Schema.Parser().parse(schemaString);\\n         } catch (Exception e) {\\n[DEL] throw new SchemaNotFoundException(String.format(\"Schema with ID = %s cannot be parsed\", id), e);\\n[ADD] throw new SchemaRegistryException(String.format(\"Schema with ID = %s cannot be parsed\", key), e);\\n         }\\n       } else {\\n[DEL] throw new SchemaNotFoundException(\\n[DEL] String.format(\"Schema with ID = %s cannot be parsed: schema should start with \\'{\\'\", id));\\n[ADD] throw new SchemaRegistryException(\\n[ADD] String.format(\"Schema with key %s cannot be parsed: schema should start with \\'{\\'\", key));\\n       }\\n \\n       return schema;',\n",
              " \" \\n         private void FindRecursivePoints()\\n         {\\n[DEL] foreach (FunctionCounter c in core.funcCounterTable)\\n[ADD] foreach (FunctionCounter c in exe.RuntimeData.funcCounterTable)\\n             {\\n[DEL] \\n                 if (c.times == Constants.kRecursionTheshold || c.times == Constants.kRecursionTheshold - 1)\\n                 {\\n[DEL] core.recursivePoint.Add(c);\\n[ADD] exe.RuntimeData.recursivePoint.Add(c);\\n                 }\\n[DEL] //else if (c.sharedCounter == Constants.kRepetationTheshold || (c.sharedCounter == Constants.kRepetationTheshold - 1 || c.sharedCounter == Constants.kRepetationTheshold + 1)\\n[DEL] //{\\n[DEL] core.recursivePoint.Add(c);\\n[DEL] //}\\n[ADD] exe.RuntimeData.recursivePoint.Add(c);\\n             }\\n[DEL] \\n         }\\n \\n \\n         private FunctionCounter FindCounter(int funcIndex, int classScope, string name)\\n         {\\n[DEL] foreach (FunctionCounter c in core.funcCounterTable)\\n[ADD] foreach (FunctionCounter c in exe.RuntimeData.funcCounterTable)\\n             {\\n[DEL] \\n                 if (c.classScope == classScope && c.functionIndex == funcIndex)\\n                 {\\n[DEL] // Comment it out. Looks this foreach loop is dead code.\\n[DEL] // - Yu Ke\\n[DEL] /*\\n[DEL] foreach (FunctionCounter c2 in core.funcCounterTable)\\n[DEL] {\\n[DEL] if (c.name.Equals(c2.name) && c2.name.ToCharArray()[0] != '%' && c2.name.ToCharArray()[0] != '_')\\n[DEL] {\\n[DEL] \\n[DEL] //c.sharedCounter++;\\n[DEL] if (c != c2)\\n[DEL] {\\n[DEL] //  c2.sharedCounter++;\\n[DEL] }\\n[DEL] }\\n[DEL] }\\n[DEL] */\\n[DEL] \\n                     return c;\\n                 }\\n[DEL] \\n             }\\n             FunctionCounter newC = new FunctionCounter(funcIndex, classScope, 0, name, 1);\\n[DEL] foreach (FunctionCounter c in core.funcCounterTable)\\n[DEL] {\\n[DEL] if (c.name.Equals(newC.name))\\n[DEL] {\\n[DEL] //c.sharedCounter++;\\n[DEL] }\\n[DEL] }\\n[DEL] core.funcCounterTable.Add(newC);\\n[ADD] exe.RuntimeData.funcCounterTable.Add(newC);\\n             return newC;\\n         }\\n \",\n",
              " '     }\\n   }\\n \\n[ADD] private static class BoundedReadSourceTranslator<T>\\n[ADD] implements FlinkStreamingPipelineTranslator.StreamTransformTranslator<Read.Bounded<T>> {\\n[ADD] \\n[ADD] @Override\\n[ADD] public void translateNode(Read.Bounded<T> transform, FlinkStreamingTranslationContext context) {\\n[ADD] \\n[ADD] BoundedSource<T> boundedSource = transform.getSource();\\n[ADD] PCollection<T> output = context.getOutput(transform);\\n[ADD] \\n[ADD] Coder<T> defaultOutputCoder = boundedSource.getDefaultOutputCoder();\\n[ADD] CoderTypeInformation<T> typeInfo = new CoderTypeInformation<>(defaultOutputCoder);\\n[ADD] \\n[ADD] DataStream<T> source = context.getExecutionEnvironment().createInput(\\n[ADD] new SourceInputFormat<>(\\n[ADD] boundedSource,\\n[ADD] context.getPipelineOptions()),\\n[ADD] typeInfo);\\n[ADD] \\n[ADD] DataStream<WindowedValue<T>> windowedStream = source.flatMap(\\n[ADD] new FlatMapFunction<T, WindowedValue<T>>() {\\n[ADD] @Override\\n[ADD] public void flatMap(T value, Collector<WindowedValue<T>> out) throws Exception {\\n[ADD] out.collect(WindowedValue.of(value, Instant.now(), GlobalWindow.INSTANCE, PaneInfo.NO_FIRING));\\n[ADD] }\\n[ADD] }).assignTimestampsAndWatermarks(new IngestionTimeExtractor<WindowedValue<T>>());\\n[ADD] \\n[ADD] context.setOutputDataStream(output, windowedStream);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n   private static class UnboundedReadSourceTranslator<T> implements FlinkStreamingPipelineTranslator.StreamTransformTranslator<Read.Unbounded<T>> {\\n \\n     @Override',\n",
              " ' \\n \\t\\treturn $role . \\':\\' . hash_hmac( \\'md5\\', \"{$role}|{$user_id}\", $token->secret );\\n \\t}\\n[ADD] \\n[ADD] /**\\n[ADD] * Some hosts disable the OpenSSL extension and so cannot make outgoing HTTPS requsets\\n[ADD] */\\n[ADD] public static function fix_url_for_bad_hosts( $url ) {\\n[ADD] if ( 0 !== strpos( $url, \\'https://\\' ) ) {\\n[ADD] return $url;\\n[ADD] }\\n[ADD] \\n[ADD] switch ( JETPACK_CLIENT__HTTPS ) {\\n[ADD] case \\'ALWAYS\\':\\n[ADD] return $url;\\n[ADD] case \\'NEVER\\':\\n[ADD] return set_url_scheme( $url, \\'http\\' );\\n[ADD] // default : case \\'AUTO\\' :\\n[ADD] }\\n[ADD] \\n[ADD] // we now return the unmodified SSL URL by default, as a security precaution\\n[ADD] return $url;\\n[ADD] }\\n }',\n",
              " '         end,\\n         getEndOffset());\\n \\n[DEL] FileBasedSource<T> source = createForSubrangeOfFile(fileOrPatternSpec, start, end);\\n[ADD] FileBasedSource<T> source = createForSubrangeOfFile(fileOrPatternSpec.get(), start, end);\\n     if (start > 0 || end != Long.MAX_VALUE) {\\n       checkArgument(source.getMode() == Mode.SINGLE_FILE_OR_SUBRANGE,\\n           \"Source created for the range [%s,%s) must be a subrange source\", start, end);',\n",
              " '   private List<ArtifactPluginDescriptor> collectContainerApplicationPluginDescriptors() throws IOException {\\n     File[] containerPlugins = getContainerAppPluginsFolder().listFiles();\\n     if (containerPlugins != null) {\\n[DEL] unzipPluginsIfNeeded();\\n       return createApplicationPluginDescriptors();\\n     } else {\\n       return EMPTY_LIST;\\n     }\\n   }\\n \\n[DEL] /**\\n[DEL] * Iterates the list of zip files in container application plugin folder, unzip them and once the plugin is expanded it deletes\\n[DEL] * the zip from the container app plugins folder.\\n[DEL] *\\n[DEL] * @throws IOException\\n[DEL] */\\n[DEL] private void unzipPluginsIfNeeded() throws IOException {\\n[DEL] for (File pluginZipFile : getContainerAppPluginsFolder()\\n[DEL] //TODO(fernandezlautaro): MULE-11383 all artifacts must be .jar files\\n[DEL] .listFiles((FileFilter) new SuffixFileFilter(asList(\".zip\", \".jar\"), INSENSITIVE))) {\\n[DEL] String pluginName = removeExtension(pluginZipFile.getName());\\n[DEL] \\n[DEL] final File pluginFolderExpanded = new File(getContainerAppPluginsFolder(), separator + pluginName);\\n[DEL] unzip(pluginZipFile, pluginFolderExpanded);\\n[DEL] \\n[DEL] forceDelete(pluginZipFile);\\n[DEL] }\\n[DEL] }\\n[DEL] \\n   /**\\n    * For each plugin expanded in container application plugins folder it creates an {@link ArtifactPluginDescriptor} for it and\\n    * adds the descriptor the given list.',\n",
              " '                                                  res.styles().paneLayoutTable());\\n       add(grid);\\n \\n[DEL] allPanePanels_ = new VerticalPanel[] {leftTopPanel_, leftBottomPanel_,\\n[ADD] visiblePanePanels_ = new VerticalPanel[] {leftTopPanel_, leftBottomPanel_,\\n                                             rightTopPanel_, rightBottomPanel_};\\n \\n       tabSet1ModuleList_ = new ModuleList();\\n       tabSet1ModuleList_.setValue(toArrayList(userPrefs.panes().getGlobalValue().getTabSet1()));\\n       tabSet2ModuleList_ = new ModuleList();\\n       tabSet2ModuleList_.setValue(toArrayList(userPrefs.panes().getGlobalValue().getTabSet2()));\\n[ADD] hiddenTabSetModuleList_ = new ModuleList();\\n[ADD] hiddenTabSetModuleList_.setValue(toArrayList(\\n[ADD] userPrefs.panes().getGlobalValue().getHiddenTabSet()));\\n \\n       ValueChangeHandler<ArrayList<Boolean>> vch = new ValueChangeHandler<ArrayList<Boolean>>()\\n       {',\n",
              " ' import static java.lang.Thread.currentThread;\\n import static java.nio.charset.StandardCharsets.UTF_8;\\n import static java.util.Collections.emptyList;\\n[DEL] import static java.util.Collections.emptyMap;\\n import static java.util.Collections.singletonList;\\n import static java.util.Optional.empty;\\n import static java.util.Optional.of;\\n[ADD] import static java.util.Optional.ofNullable;\\n import static java.util.stream.Collectors.joining;\\n import static java.util.stream.Collectors.toList;\\n import static java.util.stream.Collectors.toSet;',\n",
              " '         assertNullPayloadAndEmptyResponse(response);\\n     }\\n \\n[ADD] @Test\\n[ADD] public void serverClosesConnectionAfterSendingData() throws Exception\\n[ADD] {\\n[ADD] // Apache Fluent doesn\\'t fail while other clients such as curl, postman and this one do\\n[ADD] AsyncHttpClientConfig asyncHttpClientConfig = getAsyncHttpClientConfig();\\n[ADD] AsyncHttpClient asyncHttpClient = new AsyncHttpClient(new GrizzlyAsyncHttpProvider(asyncHttpClientConfig), asyncHttpClientConfig);\\n[ADD] ListenableFuture<com.ning.http.client.Response> responseFuture = asyncHttpClient.\\n[ADD] preparePost(getListenerUrl()).setBody(\"a=1&b=2\").\\n[ADD] addHeader(\"Content-Type\", \"application/x-www-form-urlencoded\").execute();\\n[ADD] com.ning.http.client.Response response = responseFuture.get();\\n[ADD] \\n[ADD] assertThat(response.getStatusCode(), is(200));\\n[ADD] }\\n[ADD] \\n[ADD] private AsyncHttpClientConfig getAsyncHttpClientConfig()\\n[ADD] {\\n[ADD] AsyncHttpClientConfig.Builder builder = new AsyncHttpClientConfig.Builder();\\n[ADD] builder.setAllowPoolingConnections(true);\\n[ADD] return builder.build();\\n[ADD] }\\n[ADD] \\n     private void assertNullPayloadAndEmptyResponse(Response response) throws Exception\\n     {\\n         final MuleMessage receivedMessage = muleContext.getClient().request(VM_OUTPUT_ENDPOINT, 1000);',\n",
              " '         if (!hasRunningJobs(dagId) && !this.failedDagIdsFinishRunning.contains(dagId)) {\\n           String status = TimingEvent.FlowTimings.FLOW_SUCCEEDED;\\n           if (this.failedDagIdsFinishAllPossible.contains(dagId)) {\\n[ADD] addFailedDag(dagId);\\n             status = TimingEvent.FlowTimings.FLOW_FAILED;\\n             this.failedDagIdsFinishAllPossible.remove(dagId);\\n           }',\n",
              " '             logger.warn(\"consumer app is startup\");\\n             Object target = referenceConfig.getServiceMetadata().getTarget();\\n             Assertions.assertNotNull(target);\\n[DEL] Greeting greetingService = (Greeting) target;\\n[DEL] String result = greetingService.hello();\\n[DEL] Assertions.assertEquals(\"local\", result);\\n[ADD] // provider app started != provider app registered\\n[ADD] //            Greeting greetingService = (Greeting) target;\\n[ADD] //            String result = greetingService.hello();\\n[ADD] //            Assertions.assertEquals(\"local\", result);\\n         } finally {\\n             providerBootstrap.stop();\\n             consumerBootstrap.stop();',\n",
              " ' \\n public class GZIPEncodingTestCase extends AbstractCxfOverHttpExtensionTestCase {\\n \\n[DEL] private static final HttpRequestOptions HTTP_REQUEST_OPTIONS = newOptions().method(POST.name()).build();\\n[DEL] \\n   private static final String GZIP = \"gzip\";\\n \\n   @Rule',\n",
              " '   public static final String SOURCE_PATH = \"SourcePath\";\\n   public static final String SIZE_IN_BYTES = \"SizeInBytes\";\\n \\n[DEL] static void submitSuccessfulDatasetPublish(EventSubmitter eventSubmitter, CopyableFile.DatasetAndPartition\\n[ADD] static void submitSuccessfulDatasetPublish(EventSubmitter eventSubmitter, CopyEntity.DatasetAndPartition\\n       datasetAndPartition, String originTimestamp, String upstreamTimestamp) {\\n     SlaEventSubmitter.builder().eventSubmitter(eventSubmitter).eventName(DATASET_PUBLISHED_EVENT_NAME)\\n         .datasetUrn(datasetAndPartition.getDataset().getDatasetURN())',\n",
              " '   @CliCommand(value = \"compaction unscheduleFileId\", help = \"UnSchedule Compaction for a fileId\")\\n   public String unscheduleCompactFile(\\n       @CliOption(key = \"fileId\", mandatory = true, help = \"File Id\") final String fileId,\\n[DEL] @CliOption(key = \"sparkMaster\", unspecifiedDefaultValue = \"\", help = \"Spark Master \") String master,\\n[ADD] @CliOption(key = \"partitionPath\", mandatory = true, help = \"partition path\") final String partitionPath,\\n[ADD] @CliOption(key = \"sparkMaster\", unspecifiedDefaultValue = \"local\", help = \"Spark Master\") String master,\\n       @CliOption(key = \"sparkMemory\", unspecifiedDefaultValue = \"2G\", help = \"executor memory\") String sparkMemory,\\n       @CliOption(key = {\"skipValidation\"}, help = \"skip validation\", unspecifiedDefaultValue = \"false\") boolean skipV,\\n       @CliOption(key = {\"dryRun\"}, help = \"Dry Run Mode\", unspecifiedDefaultValue = \"false\") boolean dryRun,',\n",
              " ' import org.mule.runtime.core.api.construct.FlowConstruct;\\n import org.mule.runtime.core.api.construct.Pipeline;\\n import org.mule.runtime.core.api.context.notification.FlowCallStack;\\n[ADD] import org.mule.runtime.core.api.message.InternalMessage;\\n import org.mule.runtime.core.api.processor.ProcessingDescriptor;\\n import org.mule.runtime.core.api.security.SecurityContext;\\n import org.mule.runtime.core.api.transformer.TransformerException;',\n",
              " '     CoderProperties.coderSerializable(RandomAccessDataCoder.of());\\n     CoderProperties.structuralValueConsistentWithEquals(\\n         RandomAccessDataCoder.of(), streamA, streamB);\\n[DEL] assertTrue(RandomAccessDataCoder.of().isRegisterByteSizeObserverCheap(streamA, Context.NESTED));\\n[DEL] assertTrue(RandomAccessDataCoder.of().isRegisterByteSizeObserverCheap(streamA, Context.OUTER));\\n[DEL] assertEquals(4, RandomAccessDataCoder.of().getEncodedElementByteSize(streamA, Context.NESTED));\\n[DEL] assertEquals(3, RandomAccessDataCoder.of().getEncodedElementByteSize(streamA, Context.OUTER));\\n[ADD] assertTrue(RandomAccessDataCoder.of().isRegisterByteSizeObserverCheap(streamA));\\n[ADD] assertTrue(RandomAccessDataCoder.of().isRegisterByteSizeObserverCheap(streamA));\\n[ADD] assertEquals(4, RandomAccessDataCoder.of().getEncodedElementByteSize(streamA));\\n[ADD] assertEquals(3, RandomAccessDataCoder.of().getEncodedElementByteSize(streamA));\\n   }\\n \\n   @Test',\n",
              " '         if (!mAddNote && mCurrentEditedCard != null) {\\n             Timber.i(\"NoteEditor:: Edit note activity successfully started with card id %d\", mCurrentEditedCard.getId());\\n         }\\n[ADD] \\n[ADD] //set focus to FieldEditText \\'front\\' on startup like Anki desktop\\n[ADD] if (mEditFields != null) {\\n[ADD] FieldEditText front = mEditFields.getFirst();\\n[ADD] front.requestFocus();\\n[ADD] }\\n     }\\n \\n ',\n",
              " ' import org.springframework.scheduling.TaskScheduler;\\n import org.springframework.util.Assert;\\n import org.springframework.util.CollectionUtils;\\n[DEL] import org.springframework.util.StringUtils;\\n \\n /**\\n  * Abstract Message handler that holds a buffer of correlated messages in a',\n",
              " \"    */\\n   public static Span getOrCreateSpan(String name, Tracer tracer) {\\n     Context context = Context.current();\\n[DEL] Span clientSpan = CONTEXT_CLIENT_SPAN_KEY.get(context);\\n[ADD] Span clientSpan = context.get(CONTEXT_CLIENT_SPAN_KEY);\\n \\n     if (clientSpan != null) {\\n       // We don't want to create two client spans for a given client call, suppress inner spans.\\n[DEL] return DefaultSpan.getInvalid();\\n[ADD] return Span.getInvalid();\\n     }\\n \\n     return tracer.spanBuilder(name).setSpanKind(Kind.CLIENT).setParent(context).startSpan();\",\n",
              " '         return false;\\n     }\\n \\n[ADD] public static void setClientStream(ChannelHandlerContext ctx, AbstractClientStream stream) {\\n[ADD] setClientStream(ctx.channel(), stream);\\n[ADD] }\\n[ADD] \\n[ADD] public static void setClientStream(Channel channel, AbstractClientStream stream) {\\n[ADD] channel.attr(CLIENT_STREAM_KEY).set(stream);\\n[ADD] }\\n[ADD] \\n[ADD] public static void setServerStream(ChannelHandlerContext ctx, AbstractServerStream stream) {\\n[ADD] setServerStream(ctx.channel(), stream);\\n[ADD] }\\n[ADD] \\n[ADD] public static void setServerStream(Channel channel, AbstractServerStream stream) {\\n[ADD] channel.attr(SERVER_STREAM_KEY).set(stream);\\n[ADD] }\\n[ADD] \\n     public static AbstractServerStream getServerStream(ChannelHandlerContext ctx) {\\n[DEL] return ctx.channel().attr(TripleUtil.SERVER_STREAM_KEY).get();\\n[ADD] return getServerStream(ctx.channel());\\n[ADD] }\\n[ADD] \\n[ADD] public static AbstractServerStream getServerStream(Channel channel) {\\n[ADD] return channel.attr(TripleUtil.SERVER_STREAM_KEY).get();\\n     }\\n \\n     public static AbstractClientStream getClientStream(ChannelHandlerContext ctx) {\\n[DEL] return ctx.channel().attr(TripleUtil.CLIENT_STREAM_KEY).get();\\n[ADD] return getClientStream(ctx.channel());\\n     }\\n \\n[DEL] public static Compressor getCompressor(ChannelHandlerContext ctx) {\\n[DEL] return ctx.channel().attr(COMPRESSOR_KEY).get();\\n[ADD] public static AbstractClientStream getClientStream(Channel channel) {\\n[ADD] return channel.attr(TripleUtil.CLIENT_STREAM_KEY).get();\\n     }\\n \\n[DEL] public static int calcCompressFlag(ChannelHandlerContext ctx) {\\n[DEL] Compressor compressor = getCompressor(ctx);\\n[ADD] public static Compressor getCompressor(ChannelHandlerContext ctx, boolean client) {\\n[ADD] AbstractStream stream = client ? getClientStream(ctx) : getServerStream(ctx);\\n[ADD] return stream.getCompressor();\\n[ADD] }\\n[ADD] \\n[ADD] public static Compressor getCompressor(Channel channel, boolean client) {\\n[ADD] AbstractStream stream = client ? getClientStream(channel) : getServerStream(channel);\\n[ADD] return stream.getCompressor();\\n[ADD] }\\n[ADD] \\n[ADD] public static Compressor getDeCompressor(ChannelHandlerContext ctx, boolean client) {\\n[ADD] AbstractStream stream = client ? getClientStream(ctx) : getServerStream(ctx);\\n[ADD] return stream.getDeCompressor();\\n[ADD] }\\n[ADD] \\n[ADD] public static int calcCompressFlag(ChannelHandlerContext ctx, boolean client) {\\n[ADD] Compressor compressor = getCompressor(ctx, client);\\n[ADD] return calcCompressFlag(compressor);\\n[ADD] }\\n[ADD] \\n[ADD] public static int calcCompressFlag(Channel channel, boolean client) {\\n[ADD] Compressor compressor = getCompressor(channel, client);\\n[ADD] return calcCompressFlag(compressor);\\n[ADD] }\\n[ADD] \\n[ADD] private static int calcCompressFlag(Compressor compressor) {\\n         if (null == compressor || IdentityCompressor.NONE.equals(compressor)) {\\n             return 0;\\n         }',\n",
              " ' \\n   public synchronized List<L> getListeners() {\\n     // Clone to protect against adding/removing listeners while running callbacks\\n[DEL] return new ArrayList<>(_listeners);\\n[ADD] ArrayList<L> res = new ArrayList<>(_listeners);\\n[ADD] \\n[ADD] // Scan any auto listeners\\n[ADD] Iterator<WeakReference<L>> autoIter = _autoListeners.iterator();\\n[ADD] while (autoIter.hasNext()) {\\n[ADD] WeakReference<L> ref = autoIter.next();\\n[ADD] L listener = ref.get();\\n[ADD] if (null != listener) {\\n[ADD] res.add(listener);\\n[ADD] }\\n[ADD] else {\\n[ADD] _log.info(\"Removing a weak listener:\" + ref);\\n[ADD] autoIter.remove();\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] return res;\\n   }\\n \\n   public synchronized void addListener(L listener) {',\n",
              " '     }\\n   },\\n \\n[ADD] /** LZO compression. */\\n[ADD] LZO(\".lzo\", \".lzo\") {\\n[ADD] @Override\\n[ADD] public ReadableByteChannel readDecompressed(ReadableByteChannel channel) throws IOException {\\n[ADD] return Channels.newChannel(new LzoCompressorInputStream(Channels.newInputStream(channel)));\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public WritableByteChannel writeCompressed(WritableByteChannel channel) throws IOException {\\n[ADD] return Channels.newChannel(new LzoCompressorOutputStream(Channels.newOutputStream(channel)));\\n[ADD] }\\n[ADD] },\\n[ADD] \\n[ADD] /** LZOP compression. */\\n[ADD] LZOP(\".lzo\", \".lzo\") {\\n[ADD] @Override\\n[ADD] public ReadableByteChannel readDecompressed(ReadableByteChannel channel) throws IOException {\\n[ADD] try {\\n[ADD] return Channels.newChannel(new LzopCompressorInputStream(Channels.newInputStream(channel)));\\n[ADD] } catch (IOException e) {\\n[ADD] if(e.getMessage().contains(\"Not an LZOP file\"))\\n[ADD] e = new IOException(\"Not an LZOP file. Please try using LZO Compression.\");\\n[ADD] throw e;\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public WritableByteChannel writeCompressed(WritableByteChannel channel) throws IOException {\\n[ADD] return Channels.newChannel(new LzopCompressorOutputStream(Channels.newOutputStream(channel)));\\n[ADD] }\\n[ADD] },\\n[ADD] \\n   /** Deflate compression. */\\n   DEFLATE(\".deflate\", \".deflate\", \".zlib\") {\\n     @Override',\n",
              " ' import static org.mule.runtime.core.DefaultEventContext.create;\\n import org.mule.runtime.api.component.location.ComponentLocation;\\n import org.mule.runtime.api.meta.AnnotatedObject;\\n[DEL] import org.mule.runtime.api.meta.NameableObject;\\n import org.mule.runtime.core.api.Event;\\n import org.mule.runtime.core.api.construct.FlowConstruct;\\n[DEL] import org.mule.runtime.core.api.construct.MessageProcessorPathResolver;\\n import org.mule.runtime.core.api.context.notification.ServerNotification;\\n import org.mule.runtime.core.api.context.notification.SynchronousServerEvent;\\n import org.mule.runtime.core.api.message.InternalMessage;\\n import org.mule.runtime.core.api.processor.Processor;\\n import org.mule.runtime.core.exception.MessagingException;\\n[DEL] import org.mule.runtime.core.util.ObjectUtils;\\n import org.mule.runtime.dsl.api.component.config.DefaultComponentLocation;\\n \\n public class MessageProcessorNotification extends ServerNotification implements SynchronousServerEvent {',\n",
              " '         MuleEvent muleEvent = getTestEvent(TEST, muleContext);\\n         muleEvent.setFlowVariable(TEST_PROPERTY, TEST);\\n \\n[DEL] assertVariableDataType(muleEvent, DataType.STRING);\\n[ADD] assertVariableDataType(muleEvent, STRING);\\n     }\\n \\n     @Test',\n",
              " '       }\\n     } else {\\n       HoodieTableMetaClient.initTableType(new Configuration(jssc.hadoopConfiguration()), cfg.targetBasePath,\\n[DEL] HoodieTableType.valueOf(cfg.tableType), cfg.targetTableName, \"archived\", cfg.payloadClassName, cfg.baseFileFormat);\\n[ADD] HoodieTableType.valueOf(cfg.tableType), cfg.targetTableName, \"archived\", cfg.payloadClassName, cfg.baseFileFormat,\\n[ADD] getHoodieClientConfig(this.schemaProvider).getIndexType().name());\\n     }\\n \\n     if (!resumeCheckpointStr.isPresent() && cfg.checkpoint != null) {',\n",
              " '     private <T> MetadataResult<T> mergeFailures(T descriptor, MetadataResult<?>... results)\\n     {\\n         List<MetadataResult<?>> failedResults = Stream.of(results).filter(result -> !result.isSuccess()).collect(toList());\\n[DEL] StringJoiner messageBuilder = new StringJoiner(\" and \");\\n[DEL] failedResults.forEach(failure -> messageBuilder.add(failure.getFailure().get().getMessage()));\\n[DEL] return failure(descriptor, messageBuilder.toString(), failedResults.size() == 1 ? results[0].getFailure().get().getFailureCode() : FailureCode.MULTIPLE, \"\");\\n[ADD] String messages = failedResults.stream().map(f -> f.getFailure().get().getMessage()).collect(joining(\" and \"));\\n[ADD] String stackTrace = failedResults.size() == 1 ? results[0].getFailure().get().getReason() : \"\";\\n[ADD] FailureCode failureCode = failedResults.size() == 1 ? results[0].getFailure().get().getFailureCode() : FailureCode.MULTIPLE;\\n[ADD] return failure(descriptor, messages, failureCode, stackTrace);\\n     }\\n }',\n",
              " ' \\n   @Override\\n   public SparkPipelineResult run(Pipeline pipeline) {\\n[DEL] // clear state of Aggregators, Metrics and Watermarks if exists.\\n[ADD] SparkPipelineOptions sparkOptions = pipeline.getOptions().as(SparkPipelineOptions.class);\\n[ADD] SparkPipelineResult result = null;\\n[ADD] // clear state of Aggregators, Metrics and Watermarks.\\n     AggregatorsAccumulator.clear();\\n     SparkMetricsContainer.clear();\\n     GlobalWatermarkHolder.clear();\\n \\n     TestPipelineOptions testPipelineOptions = pipeline.getOptions().as(TestPipelineOptions.class);\\n[DEL] SparkPipelineResult result = delegate.run(pipeline);\\n[DEL] result.waitUntilFinish();\\n[DEL] \\n[DEL] \\n[DEL] // make sure the test pipeline finished successfully.\\n[DEL] State resultState = result.getState();\\n[DEL] assertThat(\\n[DEL] String.format(\"Test pipeline result state was %s instead of %s\", resultState, State.DONE),\\n[DEL] resultState,\\n[DEL] is(State.DONE));\\n[DEL] assertThat(result, testPipelineOptions.getOnCreateMatcher());\\n[DEL] assertThat(result, testPipelineOptions.getOnSuccessMatcher());\\n[ADD] LOG.info(\"About to run test pipeline \" + sparkOptions.getJobName());\\n \\n     // if the pipeline was executed in streaming mode, validate aggregators.\\n     if (isForceStreaming) {\\n[DEL] // validate assertion succeeded (at least once).\\n[DEL] int success = result.getAggregatorValue(PAssert.SUCCESS_COUNTER, Integer.class);\\n[DEL] assertThat(\\n[DEL] String.format(\\n[DEL] \"Expected %d successful assertions, but found %d.\",\\n[DEL] expectedNumberOfAssertions, success),\\n[DEL] success,\\n[DEL] is(expectedNumberOfAssertions));\\n[DEL] // validate assertion didn\\'t fail.\\n[DEL] int failure = result.getAggregatorValue(PAssert.FAILURE_COUNTER, Integer.class);\\n[DEL] assertThat(\"Failure aggregator should be zero.\", failure, is(0));\\n[ADD] try {\\n[ADD] result = delegate.run(pipeline);\\n[ADD] long timeout = sparkOptions.getForcedTimeout();\\n[ADD] result.waitUntilFinish(Duration.millis(timeout));\\n[ADD] // validate assertion succeeded (at least once).\\n[ADD] int successAssertions = result.getAggregatorValue(PAssert.SUCCESS_COUNTER, Integer.class);\\n[ADD] assertThat(\\n[ADD] String.format(\\n[ADD] \"Expected %d successful assertions, but found %d.\",\\n[ADD] expectedNumberOfAssertions, successAssertions),\\n[ADD] successAssertions,\\n[ADD] is(expectedNumberOfAssertions));\\n[ADD] // validate assertion didn\\'t fail.\\n[ADD] int failedAssertions = result.getAggregatorValue(PAssert.FAILURE_COUNTER, Integer.class);\\n[ADD] assertThat(\\n[ADD] String.format(\"Found %d failed assertions.\", failedAssertions),\\n[ADD] failedAssertions,\\n[ADD] is(0));\\n[ADD] \\n[ADD] LOG.info(\\n[ADD] String.format(\\n[ADD] \"Successfully asserted pipeline %s with %d successful assertions.\",\\n[ADD] sparkOptions.getJobName(),\\n[ADD] successAssertions));\\n[ADD] } finally {\\n[ADD] try {\\n[ADD] // cleanup checkpoint dir.\\n[ADD] FileUtils.deleteDirectory(new File(sparkOptions.getCheckpointDir()));\\n[ADD] } catch (IOException e) {\\n[ADD] throw new RuntimeException(\"Failed to clear checkpoint tmp dir.\", e);\\n[ADD] }\\n[ADD] }\\n[ADD] } else {\\n[ADD] // for batch test pipelines, run and block until done.\\n[ADD] result = delegate.run(pipeline);\\n[ADD] result.waitUntilFinish();\\n[ADD] // assert via matchers.\\n[ADD] assertThat(result, testPipelineOptions.getOnCreateMatcher());\\n[ADD] assertThat(result, testPipelineOptions.getOnSuccessMatcher());\\n     }\\n     return result;\\n   }',\n",
              " '         }\\n \\n         if (FIELD_TIMESTAMP.equals(trimmedKey) && value != null && value instanceof Date) {\\n[DEL] fields.put(FIELD_TIMESTAMP, new DateTime(value));\\n[ADD] final DateTime timestamp = new DateTime(value);\\n[ADD] final Object previousValue = fields.put(FIELD_TIMESTAMP, timestamp);\\n[ADD] updateSize(trimmedKey, timestamp, previousValue);\\n         } else if (value instanceof String) {\\n             final String str = ((String) value).trim();\\n \\n             if (isRequiredField || !str.isEmpty()) {\\n[DEL] fields.put(trimmedKey, str);\\n[ADD] final Object previousValue = fields.put(trimmedKey, str);\\n[ADD] updateSize(trimmedKey, str, previousValue);\\n             }\\n         } else if (value != null) {\\n[DEL] fields.put(trimmedKey, value);\\n[ADD] final Object previousValue = fields.put(trimmedKey, value);\\n[ADD] updateSize(trimmedKey, value, previousValue);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] private void updateSize(String fieldName, Object newValue, Object previousValue) {\\n[ADD] // don\\'t count internal fields\\n[ADD] if (GRAYLOG_FIELDS.contains(fieldName)) {\\n[ADD] return;\\n[ADD] }\\n[ADD] long newValueSize = 0;\\n[ADD] long oldValueSize = 0;\\n[ADD] final long oldSize = sizeCounter.getCount();\\n[ADD] final int keyLength = fieldName.length();\\n[ADD] // if the field is being removed, also subtract the name\\'s length\\n[ADD] if (newValue == null) {\\n[ADD] sizeCounter.dec(keyLength);\\n[ADD] } else {\\n[ADD] newValueSize = sizeForValue(newValue);\\n[ADD] sizeCounter.inc(newValueSize);\\n[ADD] }\\n[ADD] // if the field is new, also count its name\\'s length\\n[ADD] if (previousValue == null) {\\n[ADD] sizeCounter.inc(keyLength);\\n[ADD] } else {\\n[ADD] oldValueSize = sizeForValue(previousValue);\\n[ADD] sizeCounter.dec(oldValueSize);\\n[ADD] }\\n[ADD] if (LOG.isTraceEnabled()) {\\n[ADD] final long newSize = sizeCounter.getCount();\\n[ADD] LOG.trace(\"[Message size update][{}] key {}/{}, new/old/change: {}/{}/{} total: {}\",\\n[ADD] getId(), fieldName, keyLength, newValueSize, oldValueSize, newSize - oldSize, newSize);\\n[ADD] }\\n[ADD] }\\n[ADD] \\n[ADD] static long sizeForValue(@Nonnull Object value) {\\n[ADD] long valueSize;\\n[ADD] if (value instanceof CharSequence) {\\n[ADD] valueSize = ((CharSequence) value).length();\\n[ADD] } else {\\n[ADD] final Integer classSize = classSizes.get(value.getClass());\\n[ADD] valueSize = classSize == null ? 0 : classSize;\\n         }\\n[ADD] return valueSize;\\n[ADD] }\\n[ADD] \\n[ADD] public long getSize() {\\n[ADD] return sizeCounter.getCount();\\n     }\\n \\n     public static boolean validKey(final String key) {',\n",
              " '         .replaceAll(\"\\\\\"\", \"\"))) {\\n       String message = (null != jsonObject.get(AzkabanClientParams.MESSAGE)) ? jsonObject.get(AzkabanClientParams.MESSAGE).toString()\\n           .replaceAll(\"\\\\\"\", \"\") : \"Unknown issue\";\\n[ADD] \\n[ADD] if (message.contains(\"Invalid Session\")) {\\n[ADD] throw new InvalidSessionException(message);\\n[ADD] }\\n[ADD] \\n       throw new IOException(message);\\n     }\\n ',\n",
              " \"     return CreateNodeT<nop>(ichMin, m_pscan->IchLimTok());\\n }\\n \\n[ADD] ParseNodePtr Parser::CreateProgNodeWithScanner(bool isModuleSource)\\n[ADD] {\\n[ADD] ParseNodePtr pnodeProg;\\n[ADD] \\n[ADD] if (isModuleSource)\\n[ADD] {\\n[ADD] pnodeProg = CreateNodeWithScanner<knopModule>();\\n[ADD] \\n[ADD] // knopModule is not actually handled anywhere since we would need to handle it everywhere we could\\n[ADD] // have knopProg and it would be treated exactly the same except for import/export statements.\\n[ADD] // We are only using it as a way to get the correct size for PnModule.\\n[ADD] // Consider: Should we add a flag to PnProg which is false but set to true in PnModule?\\n[ADD] //           If we do, it can't be a virtual method since the parse nodes are all in a union.\\n[ADD] pnodeProg->nop = knopProg;\\n[ADD] }\\n[ADD] else\\n[ADD] {\\n[ADD] pnodeProg = CreateNodeWithScanner<knopProg>();\\n[ADD] }\\n[ADD] \\n[ADD] return pnodeProg;\\n[ADD] }\\n[ADD] \\n ParseNodePtr Parser::CreateCallNode(OpCode nop, ParseNodePtr pnode1, ParseNodePtr pnode2)\\n {\\n     charcount_t ichMin;\",\n",
              " '   @Override\\n   public void verifyDeterministic() {}\\n \\n[DEL] /**\\n[DEL] * {@inheritDoc}\\n[DEL] *\\n[DEL] * @return {@code true}. This coder is injective.\\n[DEL] */\\n[DEL] @Override\\n[DEL] public boolean consistentWithEquals() {\\n[DEL] return true;\\n[DEL] }\\n[DEL] \\n   /**\\n    * {@inheritDoc}\\n    *',\n",
              " '           + \"record size estimate compute dynamically based on commit metadata. \"\\n           + \" This is critical in computing the insert parallelism and bin-packing inserts into small files.\");\\n \\n[ADD] public static final ConfigProperty<String> MAX_ARCHIVE_FILES_TO_KEEP_PROP = ConfigProperty\\n[ADD] .key(\"hoodie.max.archive.files\")\\n[ADD] .noDefaultValue()\\n[ADD] .withDocumentation(\"The numbers of kept archive files under archived.\");\\n[ADD] \\n[ADD] public static final ConfigProperty<String> AUTO_TRIM_ARCHIVE_FILES_DROP = ConfigProperty\\n[ADD] .key(\"hoodie.auto.trim.archive.files\")\\n[ADD] .defaultValue(\"false\")\\n[ADD] .withDocumentation(\"WARNING: do not use this config unless you know what you\\'re doing. \"\\n[ADD] + \"If enabled, Hoodie will keep the most recent \" + MAX_ARCHIVE_FILES_TO_KEEP_PROP.key() + \" archive files and details of older archived instants will be deleted, \"\\n[ADD] + \"resulting in information loss in the archived timeline, which may affect tools like CLI and repair. \"\\n[ADD] + \"Only enable this if you hit severe performance issues for retrieving archived timeline.\");\\n[ADD] \\n[ADD] \\n[ADD] \\n   /** @deprecated Use {@link #CLEANER_POLICY} and its methods instead */\\n   @Deprecated\\n   public static final String CLEANER_POLICY_PROP = CLEANER_POLICY.key();',\n",
              " '     session.setSecurityContext(context);\\n   }\\n \\n[ADD] private MuleEvent parent;\\n[ADD] private Correlation correlation;\\n[ADD] \\n[ADD] public void setParent(MuleEvent parent) {\\n[ADD] this.parent = parent;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public MuleEvent getParent() {\\n[ADD] return parent;\\n[ADD] }\\n[ADD] \\n[ADD] public void setCorrelation(Correlation correlation) {\\n[ADD] this.correlation = correlation;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public Correlation getCorrelation() {\\n[ADD] return correlation;\\n[ADD] }\\n[ADD] \\n[ADD] @Override\\n[ADD] public String getCorrelationId() {\\n[ADD] return getCorrelation().getId().orElse(getExecutionContext().getCorrelationId().orElse(getMessage().getUniqueId())\\n[ADD] + (getParent() != null && !getParent().getFlowCallStack().getElements().isEmpty()\\n[ADD] ? \":\" + getParent().getFlowCallStack().getElements().get(0).getProcessorPath() : \"\")\\n[ADD] + getCorrelation().getSequence().map(s -> \":\" + s.toString()).orElse(\"\"));\\n[ADD] }\\n[ADD] \\n   /**\\n    * Return the event associated with the currently executing thread.\\n    *',\n",
              " '         /// <summary>\\n         ///     The maximum amount of tesselation divisions used for geometry visualization.\\n         /// </summary>\\n[DEL] public int MaxTesselationDivisions { get; set; }\\n[ADD] public int MaxTessellationDivisions\\n[ADD] {\\n[ADD] get { return maxTessellationDivisions; }\\n[ADD] set\\n[ADD] {\\n[ADD] maxTessellationDivisions = value;\\n[ADD] if (RenderPackageFactory != null)\\n[ADD] {\\n[ADD] RenderPackageFactory.MaxTessellationDivisions = maxTessellationDivisions;\\n[ADD] }\\n[ADD] }\\n[ADD] }\\n \\n         /// <summary>\\n         ///     The Dynamo Node Library, complete with Search.',\n",
              " '       @Override\\n       protected void doConfigure(MuleContext muleContext) throws Exception {\\n         muleContext.getCustomizationService().overrideDefaultServiceImpl(FEATURE_FLAGGING_SERVICE_KEY,\\n[DEL] new DefaultFeatureFlaggingService());\\n[ADD] new DefaultFeatureFlaggingService(\"artifactId\",\\n[ADD] new HashMap<>()));\\n       }\\n     });\\n   }\\n \\n[ADD] \\n   @Before\\n   public void before() throws MuleException {\\n     CoreEvent response = testEvent();',\n",
              " '     }\\n   };\\n \\n[DEL] \\n   protected Scheduler cpuLight;\\n   protected Scheduler blocking;\\n   protected Scheduler cpuIntensive;\\n   protected Scheduler custom;\\n   protected Scheduler ringBuffer;\\n   protected Scheduler asyncExecutor;\\n[ADD] protected ExecutorService cachedThreadPool = newFixedThreadPool(4);\\n[ADD] \\n[ADD] \\n \\n   @Rule\\n   public ExpectedException expectedException = ExpectedException.none();\\n \\n   public AbstractProcessingStrategyTestCase(Mode mode) {\\n[DEL] super(mode);\\n[ADD] this.mode = mode;\\n[ADD] }\\n[ADD] \\n[ADD] @Parameterized.Parameters\\n[ADD] public static Collection<Mode> modeParameters() {\\n[ADD] return asList(new Mode[] {Mode.FLOW, SOURCE});\\n   }\\n \\n   @Before\\n   public void before() throws RegistrationException {\\n[DEL] cpuLight = new TestScheduler(2, CPU_LIGHT);\\n[DEL] blocking = new TestScheduler(4, IO);\\n[DEL] cpuIntensive = new TestScheduler(2, CPU_INTENSIVE);\\n[DEL] custom = new TestScheduler(1, CUSTOM);\\n[DEL] ringBuffer = new TestScheduler(1, RING_BUFFER);\\n[ADD] cpuLight = new TestScheduler(2, CPU_LIGHT, false);\\n[ADD] blocking = new TestScheduler(4, IO, true);\\n[ADD] cpuIntensive = new TestScheduler(2, CPU_INTENSIVE, true);\\n[ADD] custom = new TestScheduler(1, CUSTOM, true);\\n[ADD] ringBuffer = new TestScheduler(1, RING_BUFFER, true);\\n     asyncExecutor = muleContext.getRegistry().lookupObject(SchedulerService.class).ioScheduler();\\n \\n     flowBuilder = () -> builder(\"test\", muleContext)',\n",
              " '         # Mark everything in the spec as concrete, as well.\\n         self._mark_concrete()\\n \\n[ADD] # If any spec in the DAG is deprecated, throw an error\\n[ADD] deprecated = []\\n[ADD] for x in self.traverse():\\n[ADD] _, rec = spack.store.db.query_by_spec_hash(x.dag_hash())\\n[ADD] if rec and rec.deprecated_for:\\n[ADD] deprecated.append(rec)\\n[ADD] if deprecated:\\n[ADD] msg = \"\\\\n    The following specs have been deprecated\"\\n[ADD] msg += \" in favor of specs with the hashes shown:\\\\n\"\\n[ADD] for rec in deprecated:\\n[ADD] msg += \\'        %s  --> %s\\\\n\\' % (rec.spec, rec.deprecated_for)\\n[ADD] msg += \\'\\\\n\\'\\n[ADD] msg += \"    For each package listed, choose another spec\\\\n\"\\n[ADD] raise SpecDeprecatedError(msg)\\n[ADD] \\n         # Now that the spec is concrete we should check if\\n         # there are declared conflicts\\n         #',\n",
              " '     CUSTOM_ENTITY_RESOLVER\\n   }\\n \\n[DEL] private static class XxeSymbolicValue extends SymbolicValue {\\n[DEL] private final Tree init;\\n[ADD] protected static class XxeSymbolicValue extends SymbolicValue {\\n[ADD] protected final Tree init;\\n     private final Predicate<ConstraintsByDomain> conditionForSecured;\\n[DEL] private boolean isField;\\n[ADD] protected boolean isField;\\n \\n     private XxeSymbolicValue(Tree init, Predicate<ConstraintsByDomain> conditionForSecured) {\\n       this.init = init;',\n",
              " '                     return;\\n                 }\\n \\n[ADD] SystemVmTemplateRegistration.parseMetadataFile();\\n                 final CloudStackVersion currentVersion = CloudStackVersion.parse(currentVersionValue);\\n[ADD] SystemVmTemplateRegistration.CS_MAJOR_VERSION  = String.valueOf(currentVersion.getMajorRelease()) + \".\" + String.valueOf(currentVersion.getMinorRelease());\\n[ADD] SystemVmTemplateRegistration.CS_TINY_VERSION = String.valueOf(currentVersion.getPatchRelease());\\n                 s_logger.info(\"DB version = \" + dbVersion + \" Code Version = \" + currentVersion);\\n \\n                 if (dbVersion.compareTo(currentVersion) > 0) {',\n",
              " ' \\n \\tpublic TransactionSynchronizationFactoryBean afterCommit(String expression, String messageChannel) {\\n \\t\\tAssert.state(StringUtils.hasText(expression) || StringUtils.hasText(messageChannel),\\n[DEL] \"At least one attribute (\\'expression\\' and/or \\'messageChannel\\') must be defined\");\\n[ADD] EXPRESSION_OR_CHANNEL_NEEDED);\\n \\t\\tthis.afterCommitExpression = expression;\\n \\t\\tthis.afterCommitChannelName = messageChannel;\\n \\t\\tthis.afterCommitChannel = null;',\n",
              " ' import java.util.Map;\\n import java.util.Optional;\\n import java.util.Set;\\n[ADD] import java.util.TreeSet;\\n import java.util.regex.Pattern;\\n import java.util.stream.Collectors;\\n ',\n",
              " '     assertEquals(stateData, fakeClient.getData());\\n   }\\n \\n[ADD] /**\\n[ADD] * A simple Tuple class for creating a list of ExpectedMetrics using the stepName, metricName and\\n[ADD] * value of the MetricUpdate classes.\\n[ADD] */\\n[ADD] @AutoValue\\n[ADD] public abstract static class ExpectedMetric implements Serializable {\\n[ADD] static ExpectedMetric create(String stepName, MetricName metricName, long value) {\\n[ADD] return new AutoValue_FnApiDoFnRunnerTest_ExpectedMetric(stepName, metricName, value);\\n[ADD] }\\n[ADD] \\n[ADD] public abstract String stepName();\\n[ADD] \\n[ADD] public abstract MetricName metricName();\\n[ADD] \\n[ADD] public abstract long value();\\n[ADD] }\\n[ADD] \\n[ADD] @Test\\n[ADD] public void testUsingMetrics() throws Exception {\\n[ADD] MetricsContainerImpl metricsContainer = new MetricsContainerImpl(\"testUsingMetrics\");\\n[ADD] Closeable closeable = MetricsEnvironment.scopedMetricsContainer(metricsContainer);\\n[ADD] FixedWindows windowFn = FixedWindows.of(Duration.millis(1L));\\n[ADD] IntervalWindow windowA = windowFn.assignWindow(new Instant(1L));\\n[ADD] IntervalWindow windowB = windowFn.assignWindow(new Instant(2L));\\n[ADD] ByteString encodedWindowA =\\n[ADD] ByteString.copyFrom(CoderUtils.encodeToByteArray(windowFn.windowCoder(), windowA));\\n[ADD] ByteString encodedWindowB =\\n[ADD] ByteString.copyFrom(CoderUtils.encodeToByteArray(windowFn.windowCoder(), windowB));\\n[ADD] \\n[ADD] Pipeline p = Pipeline.create();\\n[ADD] PCollection<String> valuePCollection =\\n[ADD] p.apply(Create.of(\"unused\")).apply(Window.into(windowFn));\\n[ADD] PCollectionView<Iterable<String>> iterableSideInputView =\\n[ADD] valuePCollection.apply(View.asIterable());\\n[ADD] PCollection<Iterable<String>> outputPCollection =\\n[ADD] valuePCollection.apply(\\n[ADD] TEST_PTRANSFORM_ID,\\n[ADD] ParDo.of(new TestSideInputIsAccessibleForDownstreamCallersDoFn(iterableSideInputView))\\n[ADD] .withSideInputs(iterableSideInputView));\\n[ADD] \\n[ADD] SdkComponents sdkComponents = SdkComponents.create(p.getOptions());\\n[ADD] RunnerApi.Pipeline pProto = PipelineTranslation.toProto(p, sdkComponents, true);\\n[ADD] String inputPCollectionId = sdkComponents.registerPCollection(valuePCollection);\\n[ADD] \\n[ADD] RunnerApi.PTransform pTransform =\\n[ADD] pProto\\n[ADD] .getComponents()\\n[ADD] .getTransformsOrThrow(\\n[ADD] pProto\\n[ADD] .getComponents()\\n[ADD] .getTransformsOrThrow(TEST_PTRANSFORM_ID)\\n[ADD] .getSubtransforms(0));\\n[ADD] \\n[ADD] ImmutableMap<StateKey, ByteString> stateData =\\n[ADD] ImmutableMap.of(\\n[ADD] multimapSideInputKey(\\n[ADD] iterableSideInputView.getTagInternal().getId(), ByteString.EMPTY, encodedWindowA),\\n[ADD] encode(\"iterableValue1A\", \"iterableValue2A\", \"iterableValue3A\"),\\n[ADD] multimapSideInputKey(\\n[ADD] iterableSideInputView.getTagInternal().getId(), ByteString.EMPTY, encodedWindowB),\\n[ADD] encode(\"iterableValue1B\", \"iterableValue2B\", \"iterableValue3B\"));\\n[ADD] \\n[ADD] FakeBeamFnStateClient fakeClient = new FakeBeamFnStateClient(stateData);\\n[ADD] \\n[ADD] List<WindowedValue<Iterable<String>>> mainOutputValues = new ArrayList<>();\\n[ADD] ListMultimap<String, FnDataReceiver<WindowedValue<?>>> consumers = ArrayListMultimap.create();\\n[ADD] consumers.put(\\n[ADD] Iterables.getOnlyElement(pTransform.getOutputsMap().values()),\\n[ADD] (FnDataReceiver) (FnDataReceiver<WindowedValue<Iterable<String>>>) mainOutputValues::add);\\n[ADD] List<ThrowingRunnable> startFunctions = new ArrayList<>();\\n[ADD] List<ThrowingRunnable> finishFunctions = new ArrayList<>();\\n[ADD] \\n[ADD] new FnApiDoFnRunner.Factory<>()\\n[ADD] .createRunnerForPTransform(\\n[ADD] PipelineOptionsFactory.create(),\\n[ADD] null /* beamFnDataClient */,\\n[ADD] fakeClient,\\n[ADD] TEST_PTRANSFORM_ID,\\n[ADD] pTransform,\\n[ADD] Suppliers.ofInstance(\"57L\")::get,\\n[ADD] pProto.getComponents().getPcollectionsMap(),\\n[ADD] pProto.getComponents().getCodersMap(),\\n[ADD] pProto.getComponents().getWindowingStrategiesMap(),\\n[ADD] consumers,\\n[ADD] startFunctions::add,\\n[ADD] finishFunctions::add,\\n[ADD] null /* splitListener */);\\n[ADD] \\n[ADD] Iterables.getOnlyElement(startFunctions).run();\\n[ADD] mainOutputValues.clear();\\n[ADD] \\n[ADD] // Ensure that bag user state that is initially empty or populated works.\\n[ADD] // Ensure that the bagUserStateKey order does not matter when we traverse over KV pairs.\\n[ADD] FnDataReceiver<WindowedValue<?>> mainInput =\\n[ADD] Iterables.getOnlyElement(consumers.get(inputPCollectionId));\\n[ADD] mainInput.accept(valueInWindow(\"X\", windowA));\\n[ADD] mainInput.accept(valueInWindow(\"Y\", windowB));\\n[ADD] \\n[ADD] MetricsContainer mc = MetricsEnvironment.getCurrentContainer();\\n[ADD] MetricName metricName =\\n[ADD] MetricName.named(TestSideInputIsAccessibleForDownstreamCallersDoFn.class, \"countedElems\");\\n[ADD] List<ExpectedMetric> expectedMetrics = new ArrayList<ExpectedMetric>();\\n[ADD] expectedMetrics.add(ExpectedMetric.create(\"testUsingMetrics\", metricName, 2));\\n[ADD] \\n[ADD] closeable.close();\\n[ADD] MetricUpdates updates = metricsContainer.getUpdates();\\n[ADD] \\n[ADD] // Validate MetricUpdates\\n[ADD] int i = 0;\\n[ADD] for (MetricUpdate mu : updates.counterUpdates()) {\\n[ADD] assertEquals(expectedMetrics.get(i).metricName(), mu.getKey().metricName());\\n[ADD] assertEquals(expectedMetrics.get(i).stepName(), mu.getKey().stepName());\\n[ADD] assertEquals(expectedMetrics.get(i).value(), mu.getUpdate());\\n[ADD] i++;\\n[ADD] }\\n[ADD] assertEquals(1, i); // Validate the length.\\n[ADD] }\\n[ADD] \\n   private static class TestTimerfulDoFn extends DoFn<KV<String, String>, String> {\\n     @StateId(\"bag\")\\n     private final StateSpec<BagState<String>> bagStateSpec = StateSpecs.bag(StringUtf8Coder.of());',\n",
              " \"         }\\n \\n         /// <summary>\\n[DEL] ///  Cache window DpiContext awareness information that helps to create handle with right context at the later time.\\n[ADD] /// Cache window DpiContext awareness information that helps to create handle with right context at the later time.\\n         /// </summary>\\n[DEL] internal IntPtr DpiAwarenessContext { get; } = DpiHelper.IsScalingRequirementMet\\n[DEL] ? User32.GetThreadDpiAwarenessContext()\\n[DEL] : User32.UNSPECIFIED_DPI_AWARENESS_CONTEXT;\\n[ADD] internal IntPtr DpiAwarenessContext { get; } = User32.GetThreadDpiAwarenessContext();\\n \\n         /// <summary>\\n         ///  Override's the base object's finalize method.\",\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "train_ds_java['train']['extracted_diff']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ6oiJXtpG1f"
      },
      "source": [
        "Checking that the data set contains both y=0 and y=1 examples and deleting unneccery columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWSq4Jx7FTOQ"
      },
      "outputs": [],
      "source": [
        "y1_examples = train_ds_java['train'].filter(lambda ex: ex['y'] == 1)\n",
        "print(f\"Number of examples with y=1: {len(y1_examples)}\")\n",
        "print(train_ds_java['train'].column_names)\n",
        "\n",
        "# Remove unnecessary columns: 'oldf', 'word_count', and 'lang'\n",
        "# These columns are assumed to be irrelevant for the current task or already processed.\n",
        "train_ds_java['train'] = train_ds_java['train'].remove_columns(['oldf', 'word_count', 'lang'])\n",
        "print(train_ds_java['train'].column_names)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Translating the Java code samples**\n",
        "\n",
        "In order the code to work you need to insert an  API key and endpoint."
      ],
      "metadata": {
        "id": "GOfeYEm1V1vO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ft6sne5W5V3"
      },
      "outputs": [],
      "source": [
        "# Initialize an AzureOpenAI client for accessing the deployed language model\n",
        "client = AzureOpenAI(\n",
        "    api_key=\"YOUR API KEY\",\n",
        "    api_version=\"\",\n",
        "    azure_endpoint=\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFrmh_gazK96"
      },
      "outputs": [],
      "source": [
        "def translate_java_to_cpp(java_code: str, client, deployment_name=\"gpt-4.1\"):\n",
        "    \"\"\"\n",
        "    Translates a given Java code diff into idiomatic C++ using an OpenAI-compatible client (e.g., Azure or OpenAI SDK).\n",
        "\n",
        "    The function sends a structured prompt to a language model (e.g., GPT-4.1) instructing it to:\n",
        "    - Translate only the relevant changes in the Java code\n",
        "    - Avoid adding unnecessary boilerplate code such as includes or full class definitions\n",
        "    - Preserve the structure and intent of the original code\n",
        "    - Exclude unchanged lines or auxiliary helper functions unless strictly necessary\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    java_code : str\n",
        "        The Java code diff (textual format) that should be translated to C++.\n",
        "\n",
        "    client : object\n",
        "        A client instance for accessing the chat completion API (e.g., AzureOpenAI or OpenAI Python SDK).\n",
        "\n",
        "    deployment_name : str, optional (default=\"gpt-4.1\")\n",
        "        The name of the deployed model to use for translation (can refer to a custom deployment or model version).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        A string containing the translated C++ code, with unnecessary parts removed and structure preserved.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Translate only the relevant Java code changes to idiomatic C++.\n",
        "Do not add boilerplate code (e.g., includes, full class definitions, or mocks).\n",
        "Do not include unchanged lines or helper functions unless strictly necessary.\n",
        "Preserve structure and intent faithfully.\n",
        "\n",
        "Java diff:\n",
        "{java_code}\n",
        "\n",
        "C++ translation:\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=deployment_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates Java code to idiomatic C++.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=512\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwlKqQYLPPHn"
      },
      "outputs": [],
      "source": [
        "samples = train_ds_java['train']\n",
        "translated_rows = []\n",
        "\n",
        "# Iterate over each example with a progress bar\n",
        "for i, ex in enumerate(tqdm(samples, desc=\"Translating\")):\n",
        "    java_diff = ex['extracted_diff']\n",
        "    java_patch = ex['patch']\n",
        "\n",
        "    try:\n",
        "        cpp_diff = translate_java_to_cpp(java_diff, client, deployment_name=\"gpt-4.1\")\n",
        "    except Exception:\n",
        "        cpp_diff = \"ERROR\"\n",
        "\n",
        "    try:\n",
        "        cpp_patch = translate_java_to_cpp(java_patch, client, deployment_name=\"gpt-4.1\")\n",
        "    except Exception:\n",
        "        cpp_patch = \"ERROR\"  # In case of failure, store \"ERROR\"\n",
        "\n",
        "# Append the original and translated fields to the results list\n",
        "    translated_rows.append({\n",
        "        \"extracted_diff\": java_diff,\n",
        "        \"cppex\": cpp_diff,\n",
        "        \"patch\": java_patch,\n",
        "        \"patch_cpp\": cpp_patch,\n",
        "        \"y\": ex[\"y\"]\n",
        "    })\n",
        "\n",
        "\n",
        "df = pd.DataFrame(translated_rows)\n",
        "df.to_csv(\"cpp_translations_100.csv\", index=False)\n",
        "\n",
        "\n",
        "# Download the saved CSV file (if running in a notebook environment like Google Colab)\n",
        "files.download(\"cpp_translations.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing the translation with errors**\n",
        "\n",
        "**Note:**\n",
        "If you haven’t run the translation on the examples, the following code block won’t work.\n",
        "Skip to the next code block and upload the following file:\n",
        "cpp_translations_final\n",
        "\n",
        "The code block below simply filters out examples where the translation failed.\n",
        "The file cpp_translations_final already contains the cleaned and translated examples."
      ],
      "metadata": {
        "id": "1kx14zgu-h26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Steps:\n",
        "1. Load the original dataset from 'cpp_translations.csv'.\n",
        "2. Filter out rows where either 'cppex' or 'patch_cpp' is marked as \"ERROR\".\n",
        "3. Drop unused columns: 'extracted_diff' and 'patch'.\n",
        "4. Save the cleaned dataset to 'cpp_translations_final.csv'.\n",
        "5. Print summary statistics about removed rows and file creation.'''\n",
        "\n",
        "df = pd.read_csv(\"cpp_translations.csv\")\n",
        "\n",
        "\n",
        "original_len = len(df)\n",
        "df_filtered = df[(df[\"cppex\"] != \"ERROR\") & (df[\"patch_cpp\"] != \"ERROR\")]\n",
        "filtered_len = len(df_filtered)\n",
        "\n",
        "#Deleting the redundent columns\n",
        "df_filtered = df_filtered.drop(columns=[\"extracted_diff\", \"patch\"])\n",
        "\n",
        "\n",
        "df_filtered.to_csv(\"cpp_translations_final.csv\", index=False)\n",
        "\n",
        "print(f\"file saved cpp_translations_final.csv\")\n",
        "print(f\"deleted {original_len - filtered_len} lines with an Eror in the translation \")"
      ],
      "metadata": {
        "id": "-16SxNsJ8Yuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Upload the transleted file from your computer**\n",
        "\n",
        "import the file : \"cpp_translation_final\" in order to continue"
      ],
      "metadata": {
        "id": "gyjE422TXTdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "V52Z6_ChXRp-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "35311462-825a-403d-d855-32ff2388c3ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1fe754a1-49bd-4c2c-b2e7-5721c02bd722\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1fe754a1-49bd-4c2c-b2e7-5721c02bd722\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cpp_translations_final.csv to cpp_translations_final.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Class Distribution**"
      ],
      "metadata": {
        "id": "3hizWEwvaS64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(\"cpp_translations_final.csv\")\n",
        "\n",
        "value_counts = df[\"y\"].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(value_counts.index.astype(str), value_counts.values, color=[\"blue\", \"green\"])  # שינוי צבעים כאן\n",
        "plt.xlabel(\"y value\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of y values (0 and 1)\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "_fFe5Ful-fn4",
        "outputId": "d4764a07-d70d-4812-ba34-8c96bc9613be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "pQj0FDVeM79e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Splitting the data set**"
      ],
      "metadata": {
        "id": "nq0CgdrJOvgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"cpp_translations_final.csv\")\n",
        "df = df.rename(columns={\"y\": \"labels\"})\n"
      ],
      "metadata": {
        "id": "9mBRAhWoSLkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset.from_pandas(df)\n",
        "print(dataset.column_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKA2CY0aSTfC",
        "outputId": "772249a7-396b-4c42-a402-36bba875e237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cppex', 'patch_cpp', 'labels']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_test = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_valid = train_val_test[\"train\"].train_test_split(test_size=0.18, seed=42)\n",
        "\n",
        "train_ds = train_valid[\"train\"]\n",
        "val_ds = train_valid[\"test\"]\n",
        "test_ds = train_val_test[\"test\"]"
      ],
      "metadata": {
        "id": "Ouk65dI7gmUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Fine-Tuning**"
      ],
      "metadata": {
        "id": "ZouwN6Iqa7q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Initialize the tokenizer\n",
        "# Load and modify the config with custom dropout\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/codebert-base\", num_labels=2)\n",
        "\n",
        "\n",
        "special_tokens_dict = {'additional_special_tokens': ['[ADD]', '[DEL]']}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "xwpiVSffaR_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples, tokenizer, max_length=512):\n",
        "    combined_code = [f\"{before}\\n{after}\" for before, after in zip(examples[\"cppex\"], examples[\"patch_cpp\"])]\n",
        "    tokenized = tokenizer(\n",
        "        combined_code,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=None\n",
        "    )\n",
        "    tokenized[\"labels\"] = examples[\"labels\"]\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "8BsfOCmcdseW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples, tokenizer, max_length=512):\n",
        "    \"\"\"\n",
        "    Tokenizes C++ code diffs from 'cppex' and 'patch_cpp' fields that already include [DEL] and [ADD] markers.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    examples : dict\n",
        "        A dictionary containing:\n",
        "        - 'cppex' : list of str\n",
        "            Original C++ code diff (with [DEL] markers).\n",
        "        - 'patch_cpp' : list of str\n",
        "            Modified C++ code diff (with [ADD] markers).\n",
        "        - 'labels' : list of int\n",
        "            Class labels for each example.\n",
        "\n",
        "    tokenizer : PreTrainedTokenizer\n",
        "        A Hugging Face tokenizer including [ADD], [DEL] tokens.\n",
        "\n",
        "    max_length : int, optional (default=512)\n",
        "        Max token sequence length.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Tokenized input_ids, attention_mask, and labels.\n",
        "    \"\"\"\n",
        "\n",
        "    combined_code = [\n",
        "        f\"{before}\\n{after}\" for before, after in zip(examples[\"cppex\"], examples[\"patch_cpp\"])\n",
        "    ]\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        combined_code,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    tokenized[\"labels\"] = examples[\"labels\"]\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "AJTy66GEdu0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_ds = train_ds.map(tokenize_function, batched=True, batch_size=32, fn_kwargs={\"tokenizer\": tokenizer}, remove_columns=train_ds.column_names)\n",
        "val_ds   = val_ds.map(tokenize_function, batched=True, batch_size=32, fn_kwargs={\"tokenizer\": tokenizer}, remove_columns=val_ds.column_names)\n",
        "test_ds  = test_ds.map(tokenize_function, batched=True,  batch_size=32,fn_kwargs={\"tokenizer\": tokenizer}, remove_columns=test_ds.column_names)"
      ],
      "metadata": {
        "id": "fA0jx_hkeoUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Id00A34Wjq0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "Jix167WZmpas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n"
      ],
      "metadata": {
        "id": "IVfTWz8mfFhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",             # Directory to save checkpoints\n",
        "    overwrite_output_dir=True,          # Overwrite output directory if exists\n",
        "    eval_strategy=\"steps\",              # Evaluate every eval_steps\n",
        "    per_device_train_batch_size=32,     # Batch size for training\n",
        "    per_device_eval_batch_size=32,      # Batch size for evaluation\n",
        "    num_train_epochs=10,                # Train for 10 epochs\n",
        "    logging_steps=10,                   # Log every 100 steps\n",
        "    eval_steps=10,                      # Evaluate every 100 steps\n",
        "    save_steps=100,                     # Save checkpoint every 100 steps\n",
        "    save_total_limit=3,                 # Keep only the 3 latest checkpoints\n",
        "    load_best_model_at_end=True,        # Load best model at end\n",
        "    metric_for_best_model=\"f1\",         # Use F1 to choose best model\n",
        "    greater_is_better=True,             # Higher F1 is better\n",
        "    fp16=True,                          # Use mixed precision\n",
        "    learning_rate=1e-5,                 # learning rate\n",
        "    weight_decay=0.01,                  # Weight decay for regularization\n",
        "    warmup_ratio=0.1,                   # Warmup 10% steps\n",
        "    gradient_accumulation_steps=4,      # Gradient accumulation\n",
        "    dataloader_num_workers=4,           # More workers for data loading\n",
        "    logging_dir=\"./logs\",               # TensorBoard logs\n",
        "    seed=42,                            # Seed for reproducibility\n",
        "    label_smoothing_factor=0.1,         # Helps prevent overconfidence by slightly softening the target labels (e.g., instead of 1.0 use 0.9)\n",
        "    logging_strategy=\"steps\",           # Specifies that logging should happen every fixed number of steps during training\n",
        "    report_to=\"none\"                    # Disables reporting to external logging tools (like TensorBoard, Weights & Biases, etc.)\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "MlscC_LdteB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred) -> dict:\n",
        "\n",
        "    \"\"\"\n",
        "    Computes evaluation metrics based on model predictions and true labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    eval_pred : tuple\n",
        "        A tuple containing logits (np.ndarray) and true labels (np.ndarray).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        A dictionary with accuracy, F1 score, precision, and recall (all weighted).\n",
        "    \"\"\"\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, preds),\n",
        "        'f1': f1_score(labels, preds, average='weighted'),\n",
        "        'precision': precision_score(labels, preds, average='weighted'),\n",
        "        'recall': recall_score(labels, preds, average='weighted')\n",
        "    }"
      ],
      "metadata": {
        "id": "tYPwfxAeusxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the trainer to train the model with class weights, evaluation, and early stopping.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "dr-zGDWjvAi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "W4Fb7Albv1vS",
        "outputId": "822ab021-b6a2-4037-85e9-d3c873c7820b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [70/70 03:18, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.646900</td>\n",
              "      <td>0.676757</td>\n",
              "      <td>0.676301</td>\n",
              "      <td>0.637023</td>\n",
              "      <td>0.754517</td>\n",
              "      <td>0.676301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.616000</td>\n",
              "      <td>0.641689</td>\n",
              "      <td>0.705202</td>\n",
              "      <td>0.681041</td>\n",
              "      <td>0.756175</td>\n",
              "      <td>0.705202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.539800</td>\n",
              "      <td>0.609729</td>\n",
              "      <td>0.670520</td>\n",
              "      <td>0.665772</td>\n",
              "      <td>0.671410</td>\n",
              "      <td>0.670520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.573400</td>\n",
              "      <td>0.609663</td>\n",
              "      <td>0.682081</td>\n",
              "      <td>0.682401</td>\n",
              "      <td>0.683130</td>\n",
              "      <td>0.682081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.513000</td>\n",
              "      <td>0.607437</td>\n",
              "      <td>0.682081</td>\n",
              "      <td>0.682507</td>\n",
              "      <td>0.684071</td>\n",
              "      <td>0.682081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.552000</td>\n",
              "      <td>0.607400</td>\n",
              "      <td>0.687861</td>\n",
              "      <td>0.687986</td>\n",
              "      <td>0.694759</td>\n",
              "      <td>0.687861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.508300</td>\n",
              "      <td>0.607728</td>\n",
              "      <td>0.676301</td>\n",
              "      <td>0.676149</td>\n",
              "      <td>0.684921</td>\n",
              "      <td>0.676301</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=70, training_loss=0.5642034258161273, metrics={'train_runtime': 202.6147, 'train_samples_per_second': 38.645, 'train_steps_per_second': 0.345, 'total_flos': 2060159563468800.0, 'train_loss': 0.5642034258161273, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "Ysroev6WbaIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "4ZtczOf5v25_",
        "outputId": "6989145a-5930-402c-eef3-53838ca83ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.61404949426651,\n",
              " 'eval_accuracy': 0.6705202312138728,\n",
              " 'eval_f1': 0.6692637265592173,\n",
              " 'eval_precision': 0.6844771729164792,\n",
              " 'eval_recall': 0.6705202312138728,\n",
              " 'eval_runtime': 1.5295,\n",
              " 'eval_samples_per_second': 113.108,\n",
              " 'eval_steps_per_second': 3.923,\n",
              " 'epoch': 10.0}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib pandas\n",
        "logs = trainer.state.log_history\n",
        "\n",
        "train_steps = [log[\"step\"] for log in logs if \"loss\" in log]\n",
        "train_losses = [log[\"loss\"] for log in logs if \"loss\" in log]\n",
        "\n",
        "val_steps = [log[\"step\"] for log in logs if \"eval_loss\" in log]\n",
        "val_losses = [log[\"eval_loss\"] for log in logs if \"eval_loss\" in log]"
      ],
      "metadata": {
        "id": "F0wBbvKqEVt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logs = trainer.state.log_history\n",
        "print(logs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfDaUqUREmjA",
        "outputId": "19b45736-5645-4af0-e009-0b6bd08ecbdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'loss': 0.6405, 'grad_norm': 0.8406231999397278, 'learning_rate': 9.682539682539683e-06, 'epoch': 1.48, 'step': 10}, {'eval_loss': 0.6757466197013855, 'eval_accuracy': 0.5722543352601156, 'eval_f1': 0.5250481695568401, 'eval_precision': 0.6745094846535862, 'eval_recall': 0.5722543352601156, 'eval_runtime': 1.4097, 'eval_samples_per_second': 122.723, 'eval_steps_per_second': 4.256, 'epoch': 1.48, 'step': 10}, {'loss': 0.6094, 'grad_norm': 2.5360307693481445, 'learning_rate': 8.095238095238097e-06, 'epoch': 2.96, 'step': 20}, {'eval_loss': 0.613253116607666, 'eval_accuracy': 0.6589595375722543, 'eval_f1': 0.6517859481878547, 'eval_precision': 0.6611940071205209, 'eval_recall': 0.6589595375722543, 'eval_runtime': 1.4639, 'eval_samples_per_second': 118.181, 'eval_steps_per_second': 4.099, 'epoch': 2.96, 'step': 20}, {'loss': 0.5241, 'grad_norm': 1.7012901306152344, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.32, 'step': 30}, {'eval_loss': 0.6136412620544434, 'eval_accuracy': 0.6647398843930635, 'eval_f1': 0.6649873254338363, 'eval_precision': 0.665412269951957, 'eval_recall': 0.6647398843930635, 'eval_runtime': 1.5497, 'eval_samples_per_second': 111.635, 'eval_steps_per_second': 3.872, 'epoch': 4.32, 'step': 30}, {'loss': 0.5675, 'grad_norm': 0.9199573993682861, 'learning_rate': 5.07936507936508e-06, 'epoch': 5.8, 'step': 40}, {'eval_loss': 0.615108847618103, 'eval_accuracy': 0.6820809248554913, 'eval_f1': 0.6820809248554913, 'eval_precision': 0.6898276772950463, 'eval_recall': 0.6820809248554913, 'eval_runtime': 1.6961, 'eval_samples_per_second': 101.999, 'eval_steps_per_second': 3.538, 'epoch': 5.8, 'step': 40}, {'loss': 0.5122, 'grad_norm': 1.3944499492645264, 'learning_rate': 3.492063492063492e-06, 'epoch': 7.16, 'step': 50}, {'eval_loss': 0.605626106262207, 'eval_accuracy': 0.6647398843930635, 'eval_f1': 0.665076040926092, 'eval_precision': 0.6698624942931649, 'eval_recall': 0.6647398843930635, 'eval_runtime': 1.553, 'eval_samples_per_second': 111.399, 'eval_steps_per_second': 3.864, 'epoch': 7.16, 'step': 50}, {'loss': 0.5499, 'grad_norm': 4.392743110656738, 'learning_rate': 1.904761904761905e-06, 'epoch': 8.64, 'step': 60}, {'eval_loss': 0.6133889555931091, 'eval_accuracy': 0.6763005780346821, 'eval_f1': 0.675326362278366, 'eval_precision': 0.6892249616609649, 'eval_recall': 0.6763005780346821, 'eval_runtime': 1.522, 'eval_samples_per_second': 113.669, 'eval_steps_per_second': 3.942, 'epoch': 8.64, 'step': 60}, {'loss': 0.5084, 'grad_norm': 0.8506656885147095, 'learning_rate': 3.174603174603175e-07, 'epoch': 10.0, 'step': 70}, {'eval_loss': 0.61404949426651, 'eval_accuracy': 0.6705202312138728, 'eval_f1': 0.6692637265592173, 'eval_precision': 0.6844771729164792, 'eval_recall': 0.6705202312138728, 'eval_runtime': 1.5678, 'eval_samples_per_second': 110.349, 'eval_steps_per_second': 3.827, 'epoch': 10.0, 'step': 70}, {'train_runtime': 223.8041, 'train_samples_per_second': 34.986, 'train_steps_per_second': 0.313, 'total_flos': 2060159563468800.0, 'train_loss': 0.5588515554155622, 'epoch': 10.0, 'step': 70}, {'eval_loss': 0.61404949426651, 'eval_accuracy': 0.6705202312138728, 'eval_f1': 0.6692637265592173, 'eval_precision': 0.6844771729164792, 'eval_recall': 0.6705202312138728, 'eval_runtime': 1.5295, 'eval_samples_per_second': 113.108, 'eval_steps_per_second': 3.923, 'epoch': 10.0, 'step': 70}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the training loss curve\n",
        "plt.plot(train_steps, train_losses, label=\"Training Loss\", color=\"blue\", linewidth=2)\n",
        "\n",
        "# Plot the validation loss curve\n",
        "plt.plot(val_steps, val_losses, label=\"Validation Loss\", color=\"orange\", linewidth=2)\n",
        "\n",
        "\n",
        "plt.xlabel(\"Training Step\", fontsize=12, fontweight='bold')\n",
        "plt.ylabel(\"Loss\", fontsize=12, fontweight='bold')\n",
        "plt.title(\"Training and Validation Loss\", fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "MpC5UM4TyM5_",
        "outputId": "5689134f-2c59-4a51-818d-7a367b88b4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "acc_steps = [log[\"step\"] for log in logs if \"eval_accuracy\" in log]\n",
        "acc_scores = [log[\"eval_accuracy\"] for log in logs if \"eval_accuracy\" in log]\n",
        "\n",
        "# F1\n",
        "f1_steps = [log[\"step\"] for log in logs if \"eval_f1\" in log]\n",
        "f1_scores = [log[\"eval_f1\"] for log in logs if \"eval_f1\" in log]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(acc_steps, acc_scores, label=\"Accuracy\", color=\"green\")\n",
        "plt.plot(f1_steps, f1_scores, label=\"F1 Score\", color=\"purple\",)\n",
        "\n",
        "plt.xlabel(\"Training Step\",fontsize=12,fontweight='bold')\n",
        "plt.ylabel(\"Score\",fontsize=12,fontweight='bold')\n",
        "plt.title(\"Accuracy and F1 Score\",fontsize=14,fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "h8TfsU3YMYjP",
        "outputId": "4ca3d423-523a-4ddc-9df2-694e1b15f44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "JwUWm15AMraW",
        "outputId": "6891f461-3f77-4401-9db3-3b6197b7239d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = predictions.label_ids\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)"
      ],
      "metadata": {
        "id": "AFJ0LueNNqI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "OHsheCSPNuBP",
        "outputId": "25c1b1dd-7b5e-49db-fda8-9036dca97322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loads and prepares a real-world (non-synthetic) C++ evaluation dataset for model testing.**\n",
        "\n",
        "Steps:\n",
        "1. Load the 'test_quality' and 'validation_quality' splits from the 'fasterinnerlooper/codereviewer' dataset.\n",
        "2. Extract the 'train' portion of each split (these contain the actual labeled examples).\n",
        "3. Remove metadata columns that are not needed for model evaluation: 'msg', 'id', 'idx', 'proj'.\n",
        "4. Filter the datasets to include only examples where the programming language is 'cpp'.\n",
        "5. Concatenate both filtered datasets into a single C++ evaluation dataset.\n",
        "\n",
        "This combined dataset represents real, non-synthetic C++ code examples,\n",
        "and is used to evaluate the model's performance on realistic code changes."
      ],
      "metadata": {
        "id": "o1ZGgpX-Dzj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_ds = load_dataset(\"fasterinnerlooper/codereviewer\", \"test_quality\")[\"train\"]\n",
        "val_ds = load_dataset(\"fasterinnerlooper/codereviewer\", \"validation_quality\")[\"train\"]\n",
        "\n",
        "val_ds = val_ds.remove_columns(['msg', 'id', 'idx', 'proj'])\n",
        "test_ds = test_ds.remove_columns(['msg', 'id', 'idx', 'proj'])\n",
        "\n",
        "val_ds_cpp = val_ds.filter(lambda example: example['lang'] == 'cpp')\n",
        "test_ds_cpp = test_ds.filter(lambda example: example['lang'] == 'cpp')\n",
        "\n",
        "combined_cpp = concatenate_datasets([val_ds_cpp, test_ds_cpp])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dKObIlBmDxVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_cpp = combined_cpp.map(\n",
        "    apply_extract_diff,\n",
        "    batched=False,\n",
        "    remove_columns=None\n",
        ")"
      ],
      "metadata": {
        "id": "Ve7n20UTStUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess the dataset: renames fields to standard names\n",
        "\n",
        "def preprocess_cpp_dataset(dataset):\n",
        "    return dataset.map(lambda example: {\n",
        "        \"cppex\": example[\"oldf\"],\n",
        "        \"patch_cpp\": example[\"patch\"],\n",
        "        \"labels\": example[\"y\"]\n",
        "    })\n",
        "\n",
        "# Apply preprocessing to the combined C++ dataset\n",
        "prepared_ds = preprocess_cpp_dataset(combined_cpp)\n",
        "\n",
        "# Tokenize the preprocessed dataset using the provided tokenizer\n",
        "tokenized_ds = prepared_ds.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n"
      ],
      "metadata": {
        "id": "1G1QTFclRleG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model's performance on the tokenized evaluation dataset\n",
        "results = trainer.evaluate(eval_dataset=tokenized_ds)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "B7w63TW6uCcE",
        "outputId": "2cac6c77-0c6d-43c6-d86f-d17f8dab6e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [192/192 00:47]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.7039649486541748, 'eval_accuracy': 0.585115064468745, 'eval_f1': 0.5716861063992552, 'eval_precision': 0.567559025512151, 'eval_recall': 0.585115064468745, 'eval_runtime': 49.6379, 'eval_samples_per_second': 123.434, 'eval_steps_per_second': 3.868, 'epoch': 10.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOKLuGIBm4Ps"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}